<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="googlee4f5b3d387f2fae7">
  <meta name="msvalidate.01" content="B49368B5E1218EA9380A07C97E0E97B4">
  <meta name="yandex-verification" content="0da69d506cf33dfe">
  <meta name="baidu-site-verification" content="Elnplp8Jq5">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"congchan.github.io","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.">
<meta property="og:type" content="article">
<meta property="og:title" content="机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)">
<meta property="og:url" content="https://congchan.github.io/NLP-attention-02-lstm-reading-comprehension/index.html">
<meta property="og:site_name" content="Fly Me to the Moon">
<meta property="og:description" content="本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://allenai.github.io/bi-att-flow/BiDAF.png">
<meta property="article:published_time" content="2018-07-19T16:00:00.000Z">
<meta property="article:modified_time" content="2018-07-19T16:00:00.000Z">
<meta property="article:author" content="Cong">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="TensorFlow">
<meta property="article:tag" content="Attention">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://allenai.github.io/bi-att-flow/BiDAF.png">


<link rel="canonical" href="https://congchan.github.io/NLP-attention-02-lstm-reading-comprehension/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) | Fly Me to the Moon</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Fly Me to the Moon</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%BA%94%E7%94%A8%E4%BA%8E%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3"><span class="nav-number">1.</span> <span class="nav-text">注意力机制应用于阅读理解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Bi-Directional-Attention-Flow"><span class="nav-number">1.1.</span> <span class="nav-text">Bi-Directional Attention Flow</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedding"><span class="nav-number">2.1.</span> <span class="nav-text">Embedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder"><span class="nav-number">3.1.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder"><span class="nav-number">3.2.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E6%95%B4%E4%B8%AA%E7%B3%BB%E7%BB%9F"><span class="nav-number">3.3.</span> <span class="nav-text">搭建整个系统</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">105</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/congchan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;congchan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/congchan" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://congchan.github.io/NLP-attention-02-lstm-reading-comprehension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Fly Me to the Moon">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2018-07-20 00:00:00" itemprop="dateCreated datePublished" datetime="2018-07-20T00:00:00+08:00">2018-07-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/NLP-attention-02-lstm-reading-comprehension/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="NLP-attention-02-lstm-reading-comprehension/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.</p>
<a id="more"></a>

<h2 id="注意力机制应用于阅读理解"><a href="#注意力机制应用于阅读理解" class="headerlink" title="注意力机制应用于阅读理解"></a>注意力机制应用于阅读理解</h2><p>The Standford question and answer dataset <a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">(SQuAD)</a> 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。</p>
<p>由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。</p>
<p>但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。</p>
<p>虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用<a href="%5Cattention#%E5%85%A8%E5%B1%80%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6">全局注意力机制</a>中提到的<br>$$<br>\begin{aligned}<br>    score_{general}(t’ t) &amp;= s^\top_{t’} W_\alpha h_t, \<br>\end{aligned}<br>$$<br>就是用一个交互矩阵$W_\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 <strong>Bilinear</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forwards_bilinear</span>(<span class="params">self, hc, hq, hc_mask, hq_mask, max_context_length_placeholder,</span></span></span><br><span class="line"><span class="function"><span class="params">                                max_question_length_placeholder, is_train, keep_prob</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;combine context hidden state(hc) and question hidden state(hq) with global attention</span></span><br><span class="line"><span class="string">            bilinear score = hc.T *W *hq</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        d_en = hc.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># (BS, MPL, MQL)</span></span><br><span class="line">        interaction_weights = tf.get_variable(<span class="string">&quot;W_interaction&quot;</span>, shape=[d_en, d_en])</span><br><span class="line">        hc_W = tf.reshape(tf.reshape(hc, shape=[<span class="number">-1</span>, d_en]) @ interaction_weights,</span><br><span class="line">                          shape=[<span class="number">-1</span>, max_context_length_placeholder, d_en])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (BS, MPL, HS * 2) @ (BS, HS * 2, MCL) -&gt; (BS ,MCL, MQL)</span></span><br><span class="line">        score = hc_W @ tf.transpose(hq, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># Create mask (BS, MPL) -&gt; (BS, MPL, 1) -&gt; (BS, MPL, MQL)</span></span><br><span class="line">        hc_mask_aug = tf.tile(tf.expand_dims(hc_mask, <span class="number">-1</span>), [<span class="number">1</span>, <span class="number">1</span>, max_question_length_placeholder])</span><br><span class="line">        hq_mask_aug = tf.tile(tf.expand_dims(hq_mask, <span class="number">-2</span>), [<span class="number">1</span>, max_context_length_placeholder, <span class="number">1</span>])</span><br><span class="line">        hq_mask_aug = hc_mask_aug &amp; hq_mask_aug</span><br><span class="line">        score = softmax_mask_prepro(score, hq_mask_aug)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (BS, MPL, MQL)</span></span><br><span class="line">        alignment_weights = tf.nn.softmax(score)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (BS, MPL, MQL) @ (BS, MQL, HS * 2) -&gt; (BS, MPL, HS * 2)</span></span><br><span class="line">        context_aware = tf.matmul(alignment_weights, hq)</span><br><span class="line"></span><br><span class="line">        concat_hidden = tf.concat([context_aware, hc], axis=<span class="number">2</span>)</span><br><span class="line">        concat_hidden = tf.cond(is_train, <span class="keyword">lambda</span>: tf.nn.dropout(concat_hidden, keep_prob), <span class="keyword">lambda</span>: concat_hidden)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (HS * 4, HS * 2)</span></span><br><span class="line">        Ws = tf.get_variable(<span class="string">&quot;Ws&quot;</span>, shape=[d_en * <span class="number">2</span>, d_en])</span><br><span class="line">        attention = tf.nn.tanh(tf.reshape(tf.reshape(concat_hidden, [<span class="number">-1</span>, d_en * <span class="number">2</span>]) @ Ws,</span><br><span class="line">                                          [<span class="number">-1</span>, max_context_length_placeholder, d_en]))</span><br><span class="line">        <span class="keyword">return</span> (attention)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_similarity_matrix</span>(<span class="params">self, hq, hc, max_question_length, max_context_length, question_mask, context_mask, is_train,</span></span></span><br><span class="line"><span class="function"><span class="params">                           keep_prob</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_flatten</span>(<span class="params">tensor, keep</span>):</span></span><br><span class="line">            fixed_shape = tensor.get_shape().as_list()</span><br><span class="line">            start = len(fixed_shape) - keep</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Calculate (BS * MCL * MQL)</span></span><br><span class="line">            left = reduce(mul, [fixed_shape[i] <span class="keyword">or</span> tf.shape(tensor)[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(start)])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># out_shape is simply HS * 2</span></span><br><span class="line">            out_shape = [left] + [fixed_shape[i] <span class="keyword">or</span> tf.shape(tensor)[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(start, len(fixed_shape))]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (BS * MCL * MQL, HS * 2)</span></span><br><span class="line">            flat = tf.reshape(tensor, out_shape)</span><br><span class="line">            <span class="keyword">return</span> (flat)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_reconstruct</span>(<span class="params">tensor, ref, keep</span>):</span></span><br><span class="line">            ref_shape = ref.get_shape().as_list()</span><br><span class="line">            tensor_shape = tensor.get_shape().as_list()</span><br><span class="line">            ref_stop = len(ref_shape) - keep</span><br><span class="line">            tensor_start = len(tensor_shape) - keep</span><br><span class="line"></span><br><span class="line">            <span class="comment"># [BS, MCL, MQL]</span></span><br><span class="line">            pre_shape = [ref_shape[i] <span class="keyword">or</span> tf.shape(ref)[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(ref_stop)]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># [1]</span></span><br><span class="line">            keep_shape = [tensor_shape[i] <span class="keyword">or</span> tf.shape(tensor)[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(tensor_start, len(tensor_shape))]</span><br><span class="line">            <span class="comment"># pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))]</span></span><br><span class="line">            <span class="comment"># keep_shape = tensor.get_shape().as_list()[-keep:]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># [BS, MCL, MQL, 1]</span></span><br><span class="line">            target_shape = pre_shape + keep_shape</span><br><span class="line">            out = tf.reshape(tensor, target_shape)</span><br><span class="line">            out = tf.squeeze(out, [len(args[<span class="number">0</span>].get_shape().as_list()) - <span class="number">1</span>])</span><br><span class="line">            <span class="keyword">return</span> (out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (BS, MCL, MQL, HS * 2)</span></span><br><span class="line">        d = hq.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">        logging.debug(<span class="string">&quot;d is: &#123;&#125;&quot;</span>.format(d))</span><br><span class="line">        hc_aug = tf.tile(tf.reshape(hc, shape=[<span class="number">-1</span>, max_context_length, <span class="number">1</span>, d]),</span><br><span class="line">                         [<span class="number">1</span>, <span class="number">1</span>, max_question_length, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (BS, MCL, MQL, HS * 2)</span></span><br><span class="line">        hq_aug = tf.tile(tf.reshape(hq, shape=[<span class="number">-1</span>, <span class="number">1</span>, max_question_length, d]),</span><br><span class="line">                         [<span class="number">1</span>, max_context_length, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [(BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2)]</span></span><br><span class="line">        args = [hc_aug, hq_aug, hc_aug * hq_aug]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [(BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2)]</span></span><br><span class="line">        args_flat = [_flatten(arg, <span class="number">1</span>) <span class="keyword">for</span> arg <span class="keyword">in</span> args]</span><br><span class="line">        args_flat = [tf.cond(is_train, <span class="keyword">lambda</span>: tf.nn.dropout(arg, keep_prob), <span class="keyword">lambda</span>: arg) <span class="keyword">for</span> arg <span class="keyword">in</span> args_flat]</span><br><span class="line"></span><br><span class="line">        d_concat = d * <span class="number">3</span></span><br><span class="line">        W = tf.get_variable(<span class="string">&quot;W&quot;</span>, shape=[d_concat, <span class="number">1</span>])</span><br><span class="line">        b = tf.get_variable(<span class="string">&quot;b&quot;</span>, shape=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculating a(h, u) = w_s^(t)[h; u; h * u]</span></span><br><span class="line">        <span class="comment"># (BS * MCL * MQL, HS * 6) @ (HS * 6, 1) + (1) -&gt; (BS * MCL * MQL, 1)</span></span><br><span class="line">        res = tf.concat(args_flat, <span class="number">1</span>) @ W + b</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (BS * MCL * MQL, 1) -&gt; (BS, MCL, MQL)</span></span><br><span class="line">        similarity_matrix = _reconstruct(res, args[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        logging.debug(<span class="string">&quot;similiarity_matrix after reconstruct: &#123;&#125;&quot;</span>.format(similarity_matrix.get_shape()))</span><br><span class="line">        context_mask_aug = tf.tile(tf.expand_dims(context_mask, <span class="number">2</span>), [<span class="number">1</span>, <span class="number">1</span>, max_question_length])</span><br><span class="line">        question_mask_aug = tf.tile(tf.expand_dims(question_mask, <span class="number">1</span>), [<span class="number">1</span>, max_context_length, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        mask_aug = context_mask_aug &amp; question_mask_aug</span><br><span class="line">        similarity_matrix = softmax_mask_prepro(similarity_matrix, mask_aug)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (similarity_matrix)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Bi-Directional-Attention-Flow"><a href="#Bi-Directional-Attention-Flow" class="headerlink" title="Bi-Directional Attention Flow"></a>Bi-Directional Attention Flow</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01603">lSeo et al. (2016)</a>针对SQuAD提出了一个另一种更复杂的注意力机制, Bi-Directional Attention Flow (BiDAF)。<br><img src="https://allenai.github.io/bi-att-flow/BiDAF.png" title="image from: https://allenai.github.io/bi-att-flow/"><br>BiDAF顾名思义那个就是问题与段落的双向的注意力机制, 分别是 Context-to-query (C2Q) attention 和 Query-to-context (Q2C) attention. 两者都是基于传统的段落的背景向量 $H$ 与问题的背景向量 $U$ 间相似矩阵(similarity matrix) $S \in \mathbb{R^{T×J}}$衍生出来的.<br>$$<br>S_{tj} = \alpha(H_{:t}, U_{:j}) \in R \<br>\alpha(h, u) = w^{\top}_{(S)}[h; u; h \odot u]<br>$$</p>
<blockquote>
<p>Where $S_{tj}$ indicates the similarity between  t-th context word and j-th query word, $\alpha$ is a trainable scalar function that encodes the similarity between its two input vectors, $H_{:t}$ is t-th column vector of H, and $U_{:j}$ is j-th column vector of U, $w_{(S)} \in  R^{6d}$ is a trainable weight vector, $[;]$ is vector concatenation across row.</p>
</blockquote>
<p>相似矩阵S被用于计算两种方向的注意力向量.</p>
<blockquote>
<p>Context-to-query (C2Q) attention signifies which query words are most relevant to each context word</p>
</blockquote>
<p>$$<br>\tilde{U_{:t}} = \sum_j \alpha_{tj} U_{:j} \<br>\alpha_t = softmax(S_{t:})<br>$$<br>其中 $\alpha_t \in R^J 表示$t$段落词对各个问题词的注意力权重</p>
<blockquote>
<p>Query-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query.</p>
</blockquote>
<p>对段落的注意力权重为:<br>$$<br>b = softmax(max_{col}(S)) \in R^T<br>$$<br>其中$max_{col}$是在每行选出最大值.<br>然后对段落背景向量进行注意力加权:<br>$$<br>\tilde{h} = \sum_t b_t  H_{:t} \in R^{2d}<br>$$<br>这个$\tilde{h}$向量指的是在query眼里最重要的段落次的加权求和. 因为$\tilde{h}$是在每一个内去最大值, 所以还需要从新把$\tilde{h}$的值在每一个铺开$T$次得到一个$\tilde{H} \in R^{2dxT}$向量以方便后续的计算.</p>
<p>最后, 段落的embeddings向量和注意力向量结合为$G$, $G$的每一列向量可以理解为每个段落词的 query-aware representation:<br>$$<br>G_{:t} = \beta(H_{:t}, \tilde{U_{:t}}, \tilde{H_{:t}}) \in R^{d_G}<br>$$</p>
<blockquote>
<p>where $G_{:t}$ is the t-th column vector (corresponding to t-th context word), β is a trainable vector function that fuses its (three) input vectors, and $d_G$ is the output dimension of the β function.</p>
</blockquote>
<p>β 函数可以是任意的神经网络, 但是文章中指出使用简单的函数如 $\beta(h, \tilde{u}, \tilde{h}) = [h; \tilde{u}; h \odot \tilde{u}; h \odot \tilde{h}] \in R^{8dxT}$ (i.e., dG = 8d) 表现已经很好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forwards_complex</span>(<span class="params">self, hc, hq, hc_mask, hq_mask, max_context_length_placeholder,</span></span></span><br><span class="line"><span class="function"><span class="params">                  max_question_length_placeholder, is_train, keep_prob</span>):</span></span><br><span class="line">       <span class="string">&#x27;&#x27;&#x27;combine context hidden state(hc) and question hidden state(hq) with attention</span></span><br><span class="line"><span class="string">            measured similarity = hc : hq : hc.T * hq</span></span><br><span class="line"><span class="string">       &#x27;&#x27;&#x27;</span></span><br><span class="line">       s = self._similarity_matrix(hq, hc, max_question_length_placeholder,</span><br><span class="line">       max_context_length_placeholder, hq_mask, hc_mask, is_train, keep_prob)</span><br><span class="line">       <span class="comment"># C2Q</span></span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS, MCL, MQL)</span></span><br><span class="line">       weights_c2q = tf.nn.softmax(s)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS, MCL, MQL) @ (BS, MQL, HS * 2) -&gt; (BS, MCL, HS * 2)</span></span><br><span class="line">       query_aware = weights_c2q @ hq</span><br><span class="line"></span><br><span class="line">       <span class="comment"># Q2C</span></span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS, MCL, MQL) -&gt; (BS, MCL)</span></span><br><span class="line">       <span class="comment"># We are effectively looking through all the question words j&#x27;s to some context word i and finding the</span></span><br><span class="line">       <span class="comment"># maximum of those context words</span></span><br><span class="line">       score_q2c = tf.reduce_max(s, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS, MCL)</span></span><br><span class="line">       weights_q2c = tf.expand_dims(tf.nn.softmax(score_q2c), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS, HS)</span></span><br><span class="line">       context_aware = tf.reduce_sum(tf.multiply(weights_q2c, hc), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS, MCL, HS * 2)</span></span><br><span class="line">       context_aware = tf.tile(tf.expand_dims(context_aware, <span class="number">1</span>), [<span class="number">1</span>, max_context_length_placeholder, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">       <span class="comment"># [(BS, MCL, HS * 2), (BS, MCL, HS * 2), (BS, MCL, HS * 2), (BS, MCL, HS * 2)]</span></span><br><span class="line">       biattention = tf.nn.tanh(tf.concat([hc, query_aware, hc * query_aware, hc * context_aware], <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> (biattention)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_similarity_matrix</span>(<span class="params">self, hq, hc, max_question_length, max_context_length, question_mask, context_mask, is_train,</span></span></span><br><span class="line"><span class="function"><span class="params">                          keep_prob</span>):</span></span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">_flatten</span>(<span class="params">tensor, keep</span>):</span></span><br><span class="line">           fixed_shape = tensor.get_shape().as_list()</span><br><span class="line">           start = len(fixed_shape) - keep</span><br><span class="line"></span><br><span class="line">           <span class="comment"># Calculate (BS * MCL * MQL)</span></span><br><span class="line">           left = reduce(mul, [fixed_shape[i] <span class="keyword">or</span> tf.shape(tensor)[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(start)])</span><br><span class="line"></span><br><span class="line">           <span class="comment"># out_shape is simply HS * 2</span></span><br><span class="line">           out_shape = [left] + [fixed_shape[i] <span class="keyword">or</span> tf.shape(tensor)[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(start, len(fixed_shape))]</span><br><span class="line"></span><br><span class="line">           <span class="comment"># (BS * MCL * MQL, HS * 2)</span></span><br><span class="line">           flat = tf.reshape(tensor, out_shape)</span><br><span class="line">           <span class="keyword">return</span> (flat)</span><br><span class="line"></span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">_reconstruct</span>(<span class="params">tensor, ref, keep</span>):</span></span><br><span class="line">           ref_shape = ref.get_shape().as_list()</span><br><span class="line">           tensor_shape = tensor.get_shape().as_list()</span><br><span class="line">           ref_stop = len(ref_shape) - keep</span><br><span class="line">           tensor_start = len(tensor_shape) - keep</span><br><span class="line"></span><br><span class="line">           <span class="comment"># [BS, MCL, MQL]</span></span><br><span class="line">           pre_shape = [ref_shape[i] <span class="keyword">or</span> tf.shape(ref)[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(ref_stop)]</span><br><span class="line"></span><br><span class="line">           <span class="comment"># [1]</span></span><br><span class="line">           keep_shape = [tensor_shape[i] <span class="keyword">or</span> tf.shape(tensor)[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(tensor_start, len(tensor_shape))]</span><br><span class="line">           <span class="comment"># pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))]</span></span><br><span class="line">           <span class="comment"># keep_shape = tensor.get_shape().as_list()[-keep:]</span></span><br><span class="line"></span><br><span class="line">           <span class="comment"># [BS, MCL, MQL, 1]</span></span><br><span class="line">           target_shape = pre_shape + keep_shape</span><br><span class="line">           out = tf.reshape(tensor, target_shape)</span><br><span class="line">           out = tf.squeeze(out, [len(args[<span class="number">0</span>].get_shape().as_list()) - <span class="number">1</span>])</span><br><span class="line">           <span class="keyword">return</span> (out)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS, MCL, MQL, HS * 2)</span></span><br><span class="line">       d = hq.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">       logging.debug(<span class="string">&quot;d is: &#123;&#125;&quot;</span>.format(d))</span><br><span class="line">       hc_aug = tf.tile(tf.reshape(hc, shape=[<span class="number">-1</span>, max_context_length, <span class="number">1</span>, d]),</span><br><span class="line">                        [<span class="number">1</span>, <span class="number">1</span>, max_question_length, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS, MCL, MQL, HS * 2)</span></span><br><span class="line">       hq_aug = tf.tile(tf.reshape(hq, shape=[<span class="number">-1</span>, <span class="number">1</span>, max_question_length, d]),</span><br><span class="line">                        [<span class="number">1</span>, max_context_length, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">       <span class="comment"># [(BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2)]</span></span><br><span class="line">       args = [hc_aug, hq_aug, hc_aug * hq_aug]</span><br><span class="line"></span><br><span class="line">       <span class="comment"># [(BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2)]</span></span><br><span class="line">       args_flat = [_flatten(arg, <span class="number">1</span>) <span class="keyword">for</span> arg <span class="keyword">in</span> args]</span><br><span class="line">       args_flat = [tf.cond(is_train, <span class="keyword">lambda</span>: tf.nn.dropout(arg, keep_prob), <span class="keyword">lambda</span>: arg) <span class="keyword">for</span> arg <span class="keyword">in</span> args_flat]</span><br><span class="line"></span><br><span class="line">       d_concat = d * <span class="number">3</span></span><br><span class="line">       W = tf.get_variable(<span class="string">&quot;W&quot;</span>, shape=[d_concat, <span class="number">1</span>])</span><br><span class="line">       b = tf.get_variable(<span class="string">&quot;b&quot;</span>, shape=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">       <span class="comment"># Calculating a(h, u) = w_s^(t)[h; u; h * u]</span></span><br><span class="line">       <span class="comment"># (BS * MCL * MQL, HS * 6) @ (HS * 6, 1) + (1) -&gt; (BS * MCL * MQL, 1)</span></span><br><span class="line">       res = tf.concat(args_flat, <span class="number">1</span>) @ W + b</span><br><span class="line"></span><br><span class="line">       <span class="comment"># (BS * MCL * MQL, 1) -&gt; (BS, MCL, MQL)</span></span><br><span class="line">       similarity_matrix = _reconstruct(res, args[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">       logging.debug(<span class="string">&quot;similiarity_matrix after reconstruct: &#123;&#125;&quot;</span>.format(similarity_matrix.get_shape()))</span><br><span class="line">       context_mask_aug = tf.tile(tf.expand_dims(context_mask, <span class="number">2</span>), [<span class="number">1</span>, <span class="number">1</span>, max_question_length])</span><br><span class="line">       question_mask_aug = tf.tile(tf.expand_dims(question_mask, <span class="number">1</span>), [<span class="number">1</span>, max_context_length, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">       mask_aug = context_mask_aug &amp; question_mask_aug</span><br><span class="line">       similarity_matrix = softmax_mask_prepro(similarity_matrix, mask_aug)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> (similarity_matrix)</span><br></pre></td></tr></table></figure>

<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>内容段落摘自维基百科文章中的536篇文章，包含107,785对问题和答案，这使得SQuAD显着大于以前任何人类标注的数据集。在该数据集中，80％的数据用于训练，10％用于验证, 剩余10％用于测试。在训练集中，进一步划分出5％用于训练时的验证。</p>
<p>与其他问答数据集相比，SQUAD具有比较独特的特征，所有答案都是出自相应的上下文中。对于每一个段落, 众包人员生成几个问题，并选择原段落中的一小段作为答案. 答案由两个index组成, 对应答案在段落中的起始位置。因此，SQuAD数据集的答案可能比其他以单个单词和实体为答案为主的数据集长得多。实例:</p>
<blockquote>
<p>Question:<br>Why was Tesla returned to Gospic?</p>
</blockquote>
<blockquote>
<p>Context paragraph:<br>On 24 March 1879, Tesla was returned to Gospicunder police guard for <strong>not having a residence permit</strong>…</p>
</blockquote>
<blockquote>
<p>Answer:<br>{12, 16}</p>
</blockquote>
<h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>词向量使用预训练好的 Glove embedding.</p>
<blockquote>
<p>Glove is a log-bilinear regression model that combines the advantages of global matrix factorization and local context window methods.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_glove_embeddings</span>(<span class="params">embed_path</span>):</span></span><br><span class="line">    logger.info(<span class="string">&quot;Loading glove embedding...&quot;</span>)</span><br><span class="line">    glove = np.load(embed_path)[<span class="string">&#x27;glove&#x27;</span>]</span><br><span class="line">    logger.info(<span class="string">&quot;Dimension: &#123;&#125;&quot;</span>.format(glove.shape[<span class="number">1</span>]))</span><br><span class="line">    logger.info(<span class="string">&quot;Vocabulary: &#123;&#125;&quot;</span> .format(glove.shape[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">return</span> glove</span><br><span class="line"></span><br><span class="line">embeddings = load_glove_embeddings(embed_path)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">metaclass=ABCMeta</span>):</span></span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Loads distributed word representations based on placeholder tokens</span></span><br><span class="line"><span class="string">        :return: embeddings representaion of question and context.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embeddings&quot;</span>):</span><br><span class="line">            <span class="keyword">if</span> self.config.RE_TRAIN_EMBED:</span><br><span class="line">                embeddings = tf.get_variable(<span class="string">&quot;embeddings&quot;</span>, initializer=self.embeddings)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                embeddings = tf.cast(self.embeddings, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">            question_embeddings = tf.nn.embedding_lookup(embeddings, self.question_placeholder)</span><br><span class="line">            question_embeddings = tf.reshape(question_embeddings,</span><br><span class="line">                        shape = [<span class="number">-1</span>, self.max_question_length_placeholder, self.config.embedding_size])</span><br><span class="line"></span><br><span class="line">            context_embeddings = tf.nn.embedding_lookup(embeddings, self.context_placeholder)</span><br><span class="line">            context_embeddings = tf.reshape(context_embeddings,</span><br><span class="line">                        shape = [<span class="number">-1</span>, self.max_context_length_placeholder, self.config.embedding_size])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> question_embeddings, context_embeddings</span><br></pre></td></tr></table></figure>

<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>整体的模型由Embedding层，Encodr层，Attention层，Decoder层组成</p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>编码器就是一个双向GRU层:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    In a generalized encode function, you pass in your inputs,</span></span><br><span class="line"><span class="string">    masks, and an initial hidden state input into this function.</span></span><br><span class="line"><span class="string">    :param inputs: Symbolic representations of your input</span></span><br><span class="line"><span class="string">    :param masks: this is to make sure tf.nn.dynamic_rnn doesn&#x27;t iterate</span></span><br><span class="line"><span class="string">                  through masked steps</span></span><br><span class="line"><span class="string">    :param encoder_state_input: (Optional) pass this as initial hidden state</span></span><br><span class="line"><span class="string">                                to tf.nn.dynamic_rnn to build conditional representations</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">            outputs: The RNN output Tensor</span></span><br><span class="line"><span class="string">                      an encoded representation of your input.</span></span><br><span class="line"><span class="string">                      It can be context-level representation,</span></span><br><span class="line"><span class="string">                      word-level representation, or both.</span></span><br><span class="line"><span class="string">            state: The final state.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_size</span>):</span></span><br><span class="line">        self.state_size = state_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, inputs, masks, initial_state_fw=None, initial_state_bw=None, reuse=False, keep_prob = <span class="number">1.0</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> BiGRU_layer(inputs, masks, self.state_size, initial_state_fw, initial_state_bw, reuse, keep_prob)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BiGRU_layer</span>(<span class="params">inputs, masks, state_size, initial_state_fw=None, initial_state_bw=None, reuse = False, keep_prob=<span class="number">1.0</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; Wrapped BiGRU_layer for reuse&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># &#x27;outputs&#x27; is a tensor of shape [batch_size, max_time, cell_state_size]</span></span><br><span class="line">        cell_fw = tf.contrib.rnn.GRUCell(state_size, reuse = reuse)</span><br><span class="line">        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)</span><br><span class="line"></span><br><span class="line">        cell_bw = tf.contrib.rnn.GRUCell(state_size, reuse = reuse)</span><br><span class="line">        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)</span><br><span class="line"></span><br><span class="line">        sequence_length = tf.reduce_sum(tf.cast(masks, <span class="string">&#x27;int32&#x27;</span>), axis=<span class="number">1</span>)</span><br><span class="line">        sequence_length = tf.reshape(sequence_length, [<span class="number">-1</span>,])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Outputs Tensor shaped: [batch_size, max_time, cell.output_size]</span></span><br><span class="line">        (outputs_fw, outputs_bw), (final_state_fw, final_state_bw) = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">                                            cell_fw = cell_fw,\</span><br><span class="line">                                            cell_bw = cell_bw,\</span><br><span class="line">                                            inputs = inputs,\</span><br><span class="line">                                            sequence_length = sequence_length,</span><br><span class="line">                                            initial_state_fw = initial_state_fw,\</span><br><span class="line">                                            initial_state_bw = initial_state_bw,</span><br><span class="line">                                            dtype = tf.float32)</span><br><span class="line"></span><br><span class="line">        outputs = tf.concat([outputs_fw, outputs_bw], <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> outputs, final_state_fw, final_state_bw</span><br></pre></td></tr></table></figure>

<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>解码器也包含一个双向GRU层，输出的状态分别由两个softmax分类器计算出预测的答案的 start 和 end index 位置:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    takes in a knowledge representation</span></span><br><span class="line"><span class="string">    and output a probability estimation over</span></span><br><span class="line"><span class="string">    all paragraph tokens on which token should be</span></span><br><span class="line"><span class="string">    the start of the answer span, and which should be</span></span><br><span class="line"><span class="string">    the end of the answer span.</span></span><br><span class="line"><span class="string">    :param knowledge_rep: it is a representation of the paragraph and question,</span></span><br><span class="line"><span class="string">                          decided by how you choose to implement the encoder</span></span><br><span class="line"><span class="string">    :return: (start, end)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, output_size, state_size=None</span>):</span></span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.state_size = state_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, knowledge_rep, mask, max_input_length, keep_prob = <span class="number">1.0</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;Decode with BiGRU&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Modeling&#x27;</span>):</span><br><span class="line">            outputs, _, _ = BiGRU_layer(knowledge_rep, mask, self.state_size, keep_prob=keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;start&quot;</span>):</span><br><span class="line">            start = self.get_logit(outputs, max_input_length)</span><br><span class="line">            start = softmax_mask_prepro(start, mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;end&quot;</span>):</span><br><span class="line">            end = self.get_logit(outputs, max_input_length)</span><br><span class="line">            end = softmax_mask_prepro(end, mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (start, end)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_logit</span>(<span class="params">self, inputs, max_inputs_length</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; Get the logit (-inf, inf). &#x27;&#x27;&#x27;</span></span><br><span class="line">        d = inputs.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">assert</span> inputs.get_shape().ndims == <span class="number">3</span>, (<span class="string">&quot;Got &#123;&#125;&quot;</span>.format(inputs.get_shape().ndims))</span><br><span class="line">        inputs = tf.reshape(inputs, shape = [<span class="number">-1</span>, d])</span><br><span class="line">        W = tf.get_variable(<span class="string">&#x27;W&#x27;</span>, initializer=tf.contrib.layers.xavier_initializer(),</span><br><span class="line">                             shape=(d, <span class="number">1</span>), dtype=tf.float32)</span><br><span class="line">        pred = tf.matmul(inputs, W)</span><br><span class="line">        pred = tf.reshape(pred, shape = [<span class="number">-1</span>, max_inputs_length])</span><br><span class="line">        tf.summary.histogram(<span class="string">&#x27;logit&#x27;</span>, pred)</span><br><span class="line">        <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>

<h3 id="搭建整个系统"><a href="#搭建整个系统" class="headerlink" title="搭建整个系统"></a>搭建整个系统</h3><p>在整个QASystem类中初始化这些功能层:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QASystem</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embeddings, config</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Initializes System &quot;&quot;&quot;</span></span><br><span class="line">        self.embeddings = embeddings</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(config.encoder_state_size)</span><br><span class="line">        self.decoder = Decoder(output_size=config.output_size, state_size = config.decoder_state_size)</span><br><span class="line">        self.attention = Attention()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ==== set up placeholder tokens ========</span></span><br><span class="line">        self.context_placeholder = tf.placeholder(tf.int32, shape=(<span class="literal">None</span>, <span class="literal">None</span>))</span><br><span class="line">        self.context_mask_placeholder = tf.placeholder(tf.bool, shape=(<span class="literal">None</span>, <span class="literal">None</span>))</span><br><span class="line">        self.question_placeholder = tf.placeholder(tf.int32, shape=(<span class="literal">None</span>, <span class="literal">None</span>))</span><br><span class="line">        self.question_mask_placeholder = tf.placeholder(tf.bool, shape=(<span class="literal">None</span>, <span class="literal">None</span>))</span><br><span class="line"></span><br><span class="line">        self.answer_start_placeholder = tf.placeholder(tf.int32)</span><br><span class="line">        self.answer_end_placeholder = tf.placeholder(tf.int32)</span><br><span class="line"></span><br><span class="line">        self.max_context_length_placeholder = tf.placeholder(tf.int32)</span><br><span class="line">        self.max_question_length_placeholder = tf.placeholder(tf.int32)</span><br><span class="line">        self.dropout_placeholder = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ==== assemble pieces ====</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(self.config.which_model, initializer=tf.uniform_unit_scaling_initializer(<span class="number">1.0</span>)):</span><br><span class="line">            self.question_embeddings, self.context_embeddings = self.setup_embeddings()</span><br><span class="line">            self.preds = self.setup_system()</span><br><span class="line">            self.loss = self.setup_loss(self.preds)</span><br><span class="line">            self.f1_train = tf.Variable(<span class="number">0.</span>, tf.float64)</span><br><span class="line">            self.EM_train = tf.Variable(<span class="number">0.</span>, tf.float64)</span><br><span class="line">            self.f1_val = tf.Variable(<span class="number">0.</span>, tf.float64)</span><br><span class="line">            self.EM_val = tf.Variable(<span class="number">0.</span>, tf.float64)</span><br><span class="line">            tf.summary.scalar(<span class="string">&#x27;f1_train&#x27;</span>, self.f1_train)</span><br><span class="line">            tf.summary.scalar(<span class="string">&#x27;EM_train&#x27;</span>, self.EM_train)</span><br><span class="line">            tf.summary.scalar(<span class="string">&#x27;f1_val&#x27;</span>, self.f1_val)</span><br><span class="line">            tf.summary.scalar(<span class="string">&#x27;EM_val&#x27;</span>, self.EM_val)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ==== set up training/updating procedure ====</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; With gradient clipping&#x27;&#x27;&#x27;</span></span><br><span class="line">        opt_op = get_optimizer(self.config.optimizer, self.loss, config.max_gradient_norm, config.learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> config.exdma_weight_decay <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.train_op = self.build_exdma(opt_op)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.train_op = opt_op</span><br><span class="line">        self.merged = tf.summary.merge_all()</span><br></pre></td></tr></table></figure>

<p>把各个功能层搭建成一个完整的模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setup_system</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Connect all parts of your system here:</span></span><br><span class="line"><span class="string">    After your modularized implementation of encoder and decoder</span></span><br><span class="line"><span class="string">    you should call various functions inside encoder, decoder here</span></span><br><span class="line"><span class="string">    to assemble your reading comprehension system!</span></span><br><span class="line"><span class="string">    context: [None, max_context_length, d]</span></span><br><span class="line"><span class="string">    question: [None, max_question_length, d]</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    d = self.context_embeddings.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Step 1: encode context and question, respectively, with independent weights</span></span><br><span class="line"><span class="string">    e.g. hq = encode_question(question)  # get U (d*J) as representation of q</span></span><br><span class="line"><span class="string">    e.g. hc = encode_context(context, q_state)   # get H (d*T) as representation of x</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;question&#x27;</span>):</span><br><span class="line">        hq, question_state_fw, question_state_bw = \</span><br><span class="line">            self.encoder.BiGRU_encode(self.question_embeddings, self.question_mask_placeholder,</span><br><span class="line">                                keep_prob = self.dropout_placeholder)</span><br><span class="line">        <span class="keyword">if</span> self.config.QA_ENCODER_SHARE:</span><br><span class="line">            hc, context_state_fw, context_state_bw =\</span><br><span class="line">                 self.encoder.BiGRU_encode(self.context_embeddings, self.context_mask_placeholder,</span><br><span class="line">                         initial_state_fw = question_state_fw, initial_state_bw = question_state_bw,</span><br><span class="line">                         reuse = <span class="literal">True</span>, keep_prob = self.dropout_placeholder)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.config.QA_ENCODER_SHARE:</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;context&#x27;</span>):</span><br><span class="line">            hc, context_state_fw, context_state_bw =\</span><br><span class="line">                 self.encoder.BiGRU_encode(self.context_embeddings, self.context_mask_placeholder,</span><br><span class="line">                         initial_state_fw = question_state_fw, initial_state_bw = question_state_bw,</span><br><span class="line">                                     keep_prob=self.dropout_placeholder)</span><br><span class="line"></span><br><span class="line">    d_Bi = self.config.encoder_state_size*<span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> hc.get_shape().as_list() == [<span class="literal">None</span>, <span class="literal">None</span>, d_Bi], (</span><br><span class="line">            <span class="string">&quot;Expected &#123;&#125;, got &#123;&#125;&quot;</span>.format([<span class="literal">None</span>, self.max_context_length_placeholder,</span><br><span class="line">            self.config.encoder_state_size], hc.get_shape().as_list()))</span><br><span class="line">    <span class="keyword">assert</span> hq.get_shape().as_list() == [<span class="literal">None</span>, <span class="literal">None</span>, d_Bi], (</span><br><span class="line">            <span class="string">&quot;Expected &#123;&#125;, got &#123;&#125;&quot;</span>.format([<span class="literal">None</span>, self.max_question_length_placeholder,</span><br><span class="line">            self.config.encoder_state_size], hq.get_shape().as_list()))</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Step 2: combine context hidden state(hc) and question hidden state(hq) with attention</span></span><br><span class="line"><span class="string">         measured similarity = hc.T * hq</span></span><br><span class="line"><span class="string">         Context-to-query (C2Q) attention signifies which query words are most relevant to each P context word.</span></span><br><span class="line"><span class="string">            attention_c2q = softmax(similarity)</span></span><br><span class="line"><span class="string">            hq_hat = sum(attention_c2q*hq)</span></span><br><span class="line"><span class="string">         Query-to-context (Q2C) attention signifies which context words have the closest similarity</span></span><br><span class="line"><span class="string">            to one of the query words and are hence critical for answering the query.</span></span><br><span class="line"><span class="string">            attention_q2c = softmax(similarity.T)</span></span><br><span class="line"><span class="string">            hc_hat = sum(attention_q2c*hc)</span></span><br><span class="line"><span class="string">         combine with β activation: β function can be an arbitrary trainable neural network</span></span><br><span class="line"><span class="string">         g = β(hc, hq, hc_hat, hq_hat)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># concat[h, u_a, h*u_a, h*h_a]</span></span><br><span class="line">    attention = self.attention.forwards_bilinear(hc, hq, self.context_mask_placeholder, self.question_mask_placeholder,</span><br><span class="line">                                max_context_length_placeholder = self.max_context_length_placeholder,</span><br><span class="line">                                max_question_length_placeholder = self.max_question_length_placeholder,</span><br><span class="line">                                is_train=(self.dropout_placeholder &lt; <span class="number">1.0</span>), keep_prob=self.dropout_placeholder)</span><br><span class="line">    d_com = d_Bi*<span class="number">4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Step 3: decoding   &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;decoding&quot;</span>):</span><br><span class="line">        start, end = self.decoder.BiGRU_decode(attention, self.context_mask_placeholder,</span><br><span class="line">                                self.max_context_length_placeholder, self.dropout_placeholder)</span><br><span class="line">    <span class="keyword">return</span> start, end</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
              <a href="/tags/Attention/" rel="tag"># Attention</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/NLP-attention-01/" rel="prev" title="从头理解注意力机制">
                  <i class="fa fa-chevron-left"></i> 从头理解注意力机制
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/algorithms-applications-02-randomized-queue/" rel="next" title="Randomized Queue with Reservoir Sampling">
                  Randomized Queue with Reservoir Sampling <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cong Chan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b35f789bd238372" async="async"></script>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://shootingspace.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://congchan.github.io/NLP-attention-02-lstm-reading-comprehension/";
    this.page.identifier = "NLP-attention-02-lstm-reading-comprehension/";
    this.page.title = "机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)";
    };
  NexT.utils.loadComments('#disqus_thread', () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://shootingspace.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
