<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="googlee4f5b3d387f2fae7">
  <meta name="msvalidate.01" content="B49368B5E1218EA9380A07C97E0E97B4">
  <meta name="yandex-verification" content="0da69d506cf33dfe">
  <meta name="baidu-site-verification" content="Elnplp8Jq5">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"congchan.github.io","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="Links: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.07139 最新出炉的 Pre-Trained Models 综述速览。">
<meta property="og:type" content="article">
<meta property="og:title" content="Survey - Pre-Trained Models - Past, Present and Future">
<meta property="og:url" content="https://congchan.github.io/paper-Pre-Trained-Models-Past-Present-and-Future/index.html">
<meta property="og:site_name" content="Fly Me to the Moon">
<meta property="og:description" content="Links: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.07139 最新出炉的 Pre-Trained Models 综述速览。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://congchan.github.io/images/papers/paper11.png">
<meta property="og:image" content="https://congchan.github.io/images/papers/paper11-1.png">
<meta property="og:image" content="https://congchan.github.io/images/papers/paper11-2.png">
<meta property="og:image" content="https://congchan.github.io/images/papers/paper11-3.png">
<meta property="og:image" content="https://congchan.github.io/images/papers/paper11-4.png">
<meta property="article:published_time" content="2021-06-18T16:00:00.000Z">
<meta property="article:modified_time" content="2021-06-18T16:00:00.000Z">
<meta property="article:author" content="Cong">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="2021">
<meta property="article:tag" content="Survey">
<meta property="article:tag" content="Pre-Trained Models">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://congchan.github.io/images/papers/paper11.png">


<link rel="canonical" href="https://congchan.github.io/paper-Pre-Trained-Models-Past-Present-and-Future/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>Survey - Pre-Trained Models - Past, Present and Future | Fly Me to the Moon</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Fly Me to the Moon</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Background-%E8%84%89%E7%BB%9C%E5%9B%BE%E8%B0%B1"><span class="nav-number">1.</span> <span class="nav-text">2 Background 脉络图谱</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Transformer-and-Representative-PTMs"><span class="nav-number">2.</span> <span class="nav-text">3 Transformer and Representative PTMs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Designing-Effective-Architectures"><span class="nav-number">3.</span> <span class="nav-text">4 Designing Effective Architectures</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Unified-Sequence-Modeling"><span class="nav-number">3.1.</span> <span class="nav-text">4.1 Unified Sequence Modeling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Cognitive-Inspired-Architectures"><span class="nav-number">3.2.</span> <span class="nav-text">4.2 Cognitive-Inspired Architectures</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-%E5%85%B6%E4%BB%96-More-Variants-of-Existing-PTMs"><span class="nav-number">3.3.</span> <span class="nav-text">4.3 其他 More Variants of Existing PTMs</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Utilizing-Multi-Source-Data"><span class="nav-number">4.</span> <span class="nav-text">5 Utilizing Multi-Source Data</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E5%A4%9A%E8%AF%AD%E8%A8%80-Multilingual-Pre-Training-Language"><span class="nav-number">4.1.</span> <span class="nav-text">5.1 多语言 Multilingual Pre-Training Language</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E5%A4%9A%E6%A8%A1%E6%80%81-Multimodal-Pre-Training"><span class="nav-number">4.2.</span> <span class="nav-text">5.2 多模态 Multimodal Pre-Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-Knowledge-Enhanced-Pre-Training"><span class="nav-number">4.3.</span> <span class="nav-text">5.3 Knowledge-Enhanced Pre-Training</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-Improving-Computational-Efficiency"><span class="nav-number">5.</span> <span class="nav-text">6 Improving Computational Efficiency</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-Sstem-Level-Optimization"><span class="nav-number">5.1.</span> <span class="nav-text">6.1 Sstem-Level Optimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-Efficient-Pre-Training"><span class="nav-number">5.2.</span> <span class="nav-text">6.2 Efficient Pre-Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9"><span class="nav-number">5.3.</span> <span class="nav-text">6.3 模型压缩</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-Knowledge-of-PTMs"><span class="nav-number">5.4.</span> <span class="nav-text">7.1 Knowledge of PTMs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-Robustness-of-PTMs"><span class="nav-number">5.5.</span> <span class="nav-text">7.2 Robustness of PTMs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-Structural-Sparsity-of-PTMs"><span class="nav-number">5.6.</span> <span class="nav-text">7.3 Structural Sparsity of PTMs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-4-Theoretical-Analysis-of-PTMs"><span class="nav-number">5.7.</span> <span class="nav-text">7.4 Theoretical Analysis of PTMs</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-Future-Directions"><span class="nav-number">6.</span> <span class="nav-text">8 Future Directions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#8-1-Architectures-and-Pre-Training-Methods"><span class="nav-number">6.1.</span> <span class="nav-text">8.1 Architectures and Pre-Training Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-2-Multilingual-and-Multimodal-Pre-Training"><span class="nav-number">6.2.</span> <span class="nav-text">8.2 Multilingual and Multimodal Pre-Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-3-Computational-Efficiency"><span class="nav-number">6.3.</span> <span class="nav-text">8.3 Computational Efficiency</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">112</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/congchan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;congchan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/congchan" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://congchan.github.io/paper-Pre-Trained-Models-Past-Present-and-Future/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Fly Me to the Moon">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Survey - Pre-Trained Models - Past, Present and Future
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-19T00:00:00+08:00">2021-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/paper-Pre-Trained-Models-Past-Present-and-Future/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="paper-Pre-Trained-Models-Past-Present-and-Future/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Links: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.07139">https://arxiv.org/abs/2106.07139</a></p>
<p>最新出炉的 Pre-Trained Models 综述速览。</p>
<a id="more"></a>

<p>先确定综述中的一些名词的定义</p>
<ul>
<li>Transfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的</li>
<li>Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。<ul>
<li>一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题</li>
<li>而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题</li>
</ul>
</li>
<li>Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法</li>
</ul>
<h1 id="2-Background-脉络图谱"><a href="#2-Background-脉络图谱" class="headerlink" title="2 Background 脉络图谱"></a>2 Background 脉络图谱</h1><p>Pre-training 可分为两大类：</p>
<ul>
<li>2.1 <strong>Transfer Learning</strong> and <strong>Supervised</strong> Pre-Training<ul>
<li>此类可进一步细分为 feature transfer 和 parameter transfer.</li>
</ul>
</li>
<li>2.2 <strong>Self-Supervised Learning</strong> and Self-Supervised Pre-Training</li>
</ul>
<p><img src="/images/papers/paper11.png"></p>
<p>Transfer learning 可细分为四个子类</p>
<ul>
<li>inductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007),</li>
<li>transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006),</li>
<li>self-taught learning (Raina et al., 2007; Dai et al., 2008)</li>
<li>unsupervised transfer learning (Wang et al., 2008).</li>
</ul>
<p>inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域</p>
<p>self-taught learning 和 unsupervised transfer learning 则主要集中于NLP领域，由于NLP领域的数据标注难度更大，所以主要以无监督的语言模型训练为主，2013年到2017年主要是词向量这类Feature transfer应用为主，把训练好的词表示作为下游模型的输入，但feature是固定的（<strong>ELMO</strong>是作为往可修正的feature方向发展的跳板），2018年开始有了<strong>BERT和GPT</strong>这种基于上下文的表示，把预训练的模型<strong>参数</strong>迁移到下游任务。</p>
<h1 id="3-Transformer-and-Representative-PTMs"><a href="#3-Transformer-and-Representative-PTMs" class="headerlink" title="3 Transformer and Representative PTMs"></a>3 Transformer and Representative PTMs</h1><p>这部分主要介绍基于Transformer的各种表征学习PTMs， 如GPT和BERT，以及后续的家族图谱</p>
<p><img src="/images/papers/paper11-1.png" alt="/images/papers/paper11-1.png"></p>
<p>Transformer家族的四大优化方向：</p>
<ul>
<li>Some work <strong>improves the model architectures and explores novel pre- training tasks</strong>, such as XLNet (Yang et al., 2019), UniLM (Dong et al., 2019), MASS (Song et al., 2019), SpanBERT (Joshi et al., 2020) and ELEC- TRA (Clark et al., 2020).</li>
<li>Besides, <strong>incorporating rich data sources</strong> is also an important direction, such as utilizing multilingual corpora, knowledge graphs, and images.</li>
<li>Since the model scale is a crucial success factor of PTMs, researchers also explore to <strong>build larger models</strong> to reach over hundreds of billions of parameters, such as the series of GPT (Radford et al., 2019; Brown et al., 2020), Switch Transformer (Fedus et al., 2021),</li>
<li>mean- while conduct <strong>computational efficiency optimization</strong> for training PTMs (Shoeybi et al., 2019; Ra- jbhandari et al., 2020; Ren et al., 2021).</li>
</ul>
<h1 id="4-Designing-Effective-Architectures"><a href="#4-Designing-Effective-Architectures" class="headerlink" title="4 Designing Effective Architectures"></a>4 Designing Effective Architectures</h1><p>two motivations</p>
<ul>
<li>统一NLU和NLG任务</li>
<li>从人类cognitive science角度切入</li>
</ul>
<h2 id="4-1-Unified-Sequence-Modeling"><a href="#4-1-Unified-Sequence-Modeling" class="headerlink" title="4.1 Unified Sequence Modeling"></a>4.1 Unified Sequence Modeling</h2><p>NLP的三类versatile downstream tasks and applications：</p>
<ul>
<li>Natural language understanding:  grammatical analysis, syntactic analysis, word/sentence/paragraph classification, ques- tion answering, factual/commonsense knowl- edge inference and etc</li>
<li>Open-ended language generation: includes dialog generation, story generation, data-to- text generation and etc.</li>
<li>Non-open-ended language generation: includes machine translation, abstract summarizing, blank filling and etc.</li>
</ul>
<p>understanding tasks 可以转换为 generation tasks (Schick and Schütze, 2020)。同时生成式的GPT在一些理解类任务上也可以达到甚至超过BERT的效果，因此The boundary between understanding and generation is vague. 基于此观察有如下一些研究方向：</p>
<ul>
<li>Combining Autoregressive and Autoencoding Modeling: 就是把GPT的单向生成和BERT的双向理解结合起来, 先驱就是XLNet<ul>
<li>permutated language modeling: <strong>XLNet</strong> (Yang et al., 2019), MPNet (Song et al., 2020)</li>
<li>Multi-task training: <strong>UniLM</strong> (Dong et al., 2019)</li>
<li>Mask上面做文章: GLM (Du et al., 2021), fill in blanks with variable lengths</li>
</ul>
</li>
<li>Applying Generalized Encoder-Decoder: 为了生成可变长的目标序列, 采用encoder-decoder architectures<ul>
<li><strong>MASS</strong> (Song et al.,2019): introduces the masked-prediction strategy into the encoder-decoder structure.</li>
<li><strong>T5</strong> (Raffel et al., 2020), : masking a variable-length of span in text with only one mask token and asks the decoder to recover the whole masked sequence.</li>
<li><strong>BART</strong> (Lewis et al., 2020a): corrupting the source sequence with multiple operations such as truncation, deletion, re- placement, shuffling, and masking, instead of mere masking.</li>
</ul>
</li>
</ul>
<p>Encoder-Decoder架构导致参数更大, 虽然可以通过参数共享减轻, 但效率仍堪忧. Seq2seq的结构在NLU任务上表现不好，低于RoBERTa和GLM</p>
<p><img src="/images/papers/paper11-2.png" alt="/images/papers/paper11-2.png"></p>
<h2 id="4-2-Cognitive-Inspired-Architectures"><a href="#4-2-Cognitive-Inspired-Architectures" class="headerlink" title="4.2 Cognitive-Inspired Architectures"></a>4.2 Cognitive-Inspired Architectures</h2><p>Transformer的注意力机制利用了人的视觉感知, 但是对于人的decision making, logical reasoning, counterfactual reasoning and working memory (Baddeley, 1992) 没有很好的模拟. 因此就有基于cognitive science的改进方向</p>
<ul>
<li><strong>Maintainable Working Memory</strong>: 人的注意力机制和Transformer还是不一样的, 人的注意力机制没有Transformer那么long-range, 而是维护一个working memory(Baddeley, 1992; Brown, 1958; Barrouillet et al., 2004; Wharton et al., 1994), 负责记忆, 重组和选择性遗忘, 也就是LSTM所希望达到的目的.<ul>
<li><strong>Transformer-XL</strong> (Dai et al., 2019) : introduce segment-level recurrence and relative positional encoding</li>
<li>CogQA (Ding et al., 2019): maintain a cognitive graph in the multi-hop reading, the System 1 based on PTMs and the System 2 based on GNNs to model the cognitive graph for multi-hop understanding.</li>
<li>CogLTX (Ding et al., 2020): leverages a MemRecall language model to select sen- tences that should be maintained in the working memory and another model for answering or clas- sificatio</li>
</ul>
</li>
<li><strong>Sustainable Long-Term Memory</strong>: GPT-3 (Brown et al., 2020)表明Transformer有记忆能力, 那么就有动力去进一步挖掘Transformer的记忆能力. Lample et al. (2019)表示feed-forward networks in Transformers is equivalent to memory networks. 但记忆能力有限.<ul>
<li>REALM (Guu et al., 2020) : explore how to construct a sustainable external memory for Transformers. <strong>tensorize the whole Wikipedia sentence by sentence</strong>, and retrieve relevant sentences as context for masked pre-training.</li>
<li>RAG (Lewis et al., 2020b) extends the masked pre-training to autoregressive generation, which could be better than extractive question answering.</li>
<li>(Vergaet al., 2020; Févry et al., 2020) propose to <strong>tensorize entities and triples</strong> in existing knowledge bases, replace entity tokens’ embedding in an internal Transformer layer with the embedding from outer memory networks.</li>
<li>(Dhingra et al., 2020; Sun et al., 2021) maintain a virtual knowledge from scratch, and propose a differentiable reasoning training objective over it.</li>
</ul>
</li>
</ul>
<h2 id="4-3-其他-More-Variants-of-Existing-PTMs"><a href="#4-3-其他-More-Variants-of-Existing-PTMs" class="headerlink" title="4.3 其他 More Variants of Existing PTMs"></a>4.3 其他 More Variants of Existing PTMs</h2><p>focus on optimizing BERT’s architecture to boost language models’ performance on natural language understanding.</p>
<ul>
<li>improving the <strong>masking strategy</strong>: 可以视为一种数据增强<ul>
<li><strong>Span- BERT</strong> (Joshi et al., 2020): masking a continuous random-length span of tokens with a span boundary objective (SBO) could improve BERT’s performance</li>
<li><strong>ERNIE</strong> (Sun et al., 2019b,c): entity masking</li>
<li><strong>NEZHA</strong> (Wei et al., 2019)</li>
<li><strong>Whole Word Masking</strong> (Cui et al., 2019)</li>
</ul>
</li>
<li>change masked-prediction objective to <strong>GAN</strong>: <strong>ELECTRA</strong> (Clark et al., 2020) transform MLM to a replace token detection (RTD) objective, in which a generator will replace tokens in original sequences and a discriminator will predict whether a token is replaced.</li>
</ul>
<h1 id="5-Utilizing-Multi-Source-Data"><a href="#5-Utilizing-Multi-Source-Data" class="headerlink" title="5 Utilizing Multi-Source Data"></a>5 Utilizing Multi-Source Data</h1><h2 id="5-1-多语言-Multilingual-Pre-Training-Language"><a href="#5-1-多语言-Multilingual-Pre-Training-Language" class="headerlink" title="5.1 多语言 Multilingual Pre-Training Language"></a>5.1 多语言 Multilingual Pre-Training Language</h2><ul>
<li>M<strong>ultilingual masked language modeling (MMLM):</strong> multilingual BERT (mBERT) released by Devlin et al. (2019) is pre- trained with the <strong>MMLM</strong> task using non-parallel multilingual Wikipedia corpora in 104 languages.</li>
<li><strong>Translation language modeling (TLM)</strong> : MMLM task 无法利用 parallel corpora. 因此有<strong>XLM</strong> (Lample and Conneau, 2019) leverages bilingual sentence pairs to perform the <strong>translation language modeling (TLM)</strong> task.</li>
<li>Unicoder (Huang et al., 2019a): <strong>Cross-lingual word recovery (CLWR), Cross-lingual paraphrase classification (CLPC)</strong></li>
<li>Generative models for multilingual PTMs: <strong>MASS</strong> (Song et al., 2019) extends MLM to language genera- tion.</li>
<li><strong>mBART</strong> (Liu et al., 2020c) extends <strong>DAE</strong> to support multiple languages by adding special symbols.</li>
</ul>
<h2 id="5-2-多模态-Multimodal-Pre-Training"><a href="#5-2-多模态-Multimodal-Pre-Training" class="headerlink" title="5.2 多模态 Multimodal Pre-Training"></a>5.2 多模态 Multimodal Pre-Training</h2><p>Modalities can all be classified as vision and language (V&amp;L),</p>
<ul>
<li><strong>ViLBERT</strong> (Lu et al., 2019) is a model to learn task-agnostic joint representations of images and languages. two streams of input, by preprocessing textual and visual information separately.</li>
<li><strong>LXMERT</strong> (Tan and Bansal, 2019) has similar architecture compared to Vil- BERT but uses more pre-training tasks</li>
<li><strong>VisualBERT</strong> (Li et al., 2019), on the other side, extends the BERT architecture at the minimum. The Transformer layers of VisualBERT implicitly align elements in the input text and image regions.</li>
<li><strong>Unicoder-VL</strong> (Li et al., 2020a) moves the offsite visual detector in VisualBERT into an end-to-end version: It designs the image token for Transformers as the sum of the bounding box and object label features.</li>
<li><strong>VL-BERT</strong>(Su et al., 2020) also uses a similar architecture to VisualBERT. each input element is either a token from the input sentence or a region-of-interest (RoI) from the input image.</li>
<li><strong>UNITER</strong> (Chen et al., 2020e) learns unified representations between the two modali- ties.</li>
<li><strong>DALLE</strong> (Ramesh et al., 2021) : A bigger step towards <strong>conditional zero-shot image generation:</strong> transformer-based text-to-image zero- shot pre-trained model with around 10 billiion pa- rameters.</li>
<li><strong>CLIP</strong> (Radford et al., 2021) and Wen-Lan (Huo et al., 2021) explore enlarging web-scale data for V&amp;L pre-training with big success. Com-</li>
</ul>
<h2 id="5-3-Knowledge-Enhanced-Pre-Training"><a href="#5-3-Knowledge-Enhanced-Pre-Training" class="headerlink" title="5.3 Knowledge-Enhanced Pre-Training"></a>5.3 Knowledge-Enhanced Pre-Training</h2><p>PTMs 可以从大量语料中中提取统计信息. 同时外部知识(such as knowledge graphs, domain- specific data and extra annotations of pre-training data) 可以作为很好的统计先验.</p>
<h1 id="6-Improving-Computational-Efficiency"><a href="#6-Improving-Computational-Efficiency" class="headerlink" title="6 Improving Computational Efficiency"></a>6 Improving Computational Efficiency</h1><h2 id="6-1-Sstem-Level-Optimization"><a href="#6-1-Sstem-Level-Optimization" class="headerlink" title="6.1 Sstem-Level Optimization"></a>6.1 Sstem-Level Optimization</h2><p>系统层的优化是 model-agnostic and do not change underlying learning algorithms.</p>
<ul>
<li><p>单机优化:</p>
<ul>
<li>half-precision floating-point format (FP16), may fail because of the floating-point truncation and overflow</li>
<li>mixed- precision training methods: which preserve some critical weights in FP32 to avoid the floating-point overflow and use dynamic loss scaling operations to get rid of the floating-point truncation.</li>
<li>gradient checkpointing methods(Rasley et al., 2020) have been used to save memory by <strong>storing only a part of the activation states after forward pass</strong>.</li>
<li>如果模型参数太大无法塞入显存, store model parameters and activation states with the CPU memory, ZeRO-Offload (Ren et al., 2021) design delicate strategies to schedule the swap between the CPU memory and the GPU memory so that memory swap and device computation can be over- lapped as much as possible.</li>
</ul>
</li>
<li><p>多机优化</p>
<ul>
<li><p>数据并行 Data parallelism (Li et al., 2020d),</p>
<p>  <img src="/images/papers/paper11-3.png" alt="/images/papers/paper11-3.png"></p>
</li>
<li><p>模型并行, Model parallelism: Megatron- LM (Shoeybi et al., 2019) splits self-attention heads as well as feed-forward layers into differ- ent GPUs</p>
<p>  <img src="/images/papers/paper11-4.png" alt="/images/papers/paper11-4.png"></p>
</li>
<li><p>Model pipeline parallelism: partitions a deep neural network into multiple lay- ers and then puts different layers onto different nodes.</p>
<ul>
<li>GPipe (Huang et al., 2019b) which can send smaller parts of samples within a mini-batch to different nodes</li>
<li>TeraPipe (Li et al., 2021) which can apply token-level pipeline mechanisms for Transformer-based models to make each token in a sequence be processed by different nodes.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="6-2-Efficient-Pre-Training"><a href="#6-2-Efficient-Pre-Training" class="headerlink" title="6.2 Efficient Pre-Training"></a>6.2 Efficient Pre-Training</h2><ul>
<li>训练方法优化：改进BERT低效的mask机制<ul>
<li>selectively mask tokens based on their <strong>importance</strong> (Gu et al., 2020) or gra- dients (Chen et al., 2020b) in back-propagation to speed up model training.</li>
<li>ELECTRA需要识别所有token所以效率更高</li>
<li>warmup strategy</li>
<li>different layers can share similar self-attention patterns, 先训练浅层的神经网络, 再复制到更深的网络中</li>
<li>Some layers can also be dropped during training to reduce the complexity of back-propagation and weight update (Zhang and He, 2020)</li>
<li>对不同层使用不同学习率</li>
</ul>
</li>
<li>模型结构优化<ul>
<li>减小模型复杂度，设计low-rank kernels to theoretically approximate the original attention weights and result in linear complexity</li>
<li>在attention mechanisms中引入稀疏性，by limiting the view of each token to a fixed size and separating tokens into several chunks so that the computation of attention weights takes place in every single chunk rather than a complete sequence</li>
<li>Switch Transformers使用的Mix-of-experts to each layer of Transformers</li>
</ul>
</li>
</ul>
<h2 id="6-3-模型压缩"><a href="#6-3-模型压缩" class="headerlink" title="6.3 模型压缩"></a>6.3 模型压缩</h2><ul>
<li>参数共享 Parameter Sharing: ALBERT (Lan et al., 2019) uses factorized embedding parameterization and cross-layer parameter sharing</li>
<li>模型剪枝 Model Pruning</li>
<li>知识蒸馏 Knowledge Distillation: DistillBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019), BERT- PKD (Sun et al., 2019a) and MiniLM (Wang et al., 2020d).</li>
<li>Model Quantization: Q8BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2020a), Ternary- BERT (Zhang et al., 2020b) applies</li>
</ul>
<h2 id="7-1-Knowledge-of-PTMs"><a href="#7-1-Knowledge-of-PTMs" class="headerlink" title="7.1 Knowledge of PTMs"></a>7.1 Knowledge of PTMs</h2><p>知识分为linguistic knowledge and world knowledge.，</p>
<p>Linguistic Knowledge</p>
<ul>
<li>Representation Probing, 通过额外的线性层在下游任务探测Representation 中是否含有语言知识</li>
<li>Representation Analysis：Use the hidden representations of PTMs to compute some statistics such as distances or similarities, 如 <strong>BERT visualization</strong></li>
<li>Attention analysis：同上</li>
<li>Generation Analysis：预测单词或句子的分布</li>
<li>construct analysis tasks based on generation: <strong>Perturbed Masking</strong> (Wu et al., 2020) recovers syntactic trees from PTMs without any extra parameter and the structure given by PTMs are competitive with a human-designed dependency schema in some downstream tasks.</li>
</ul>
<p>在11个 linguistic tasks上的结果表明PTMs可以学习到tokens, chunks, and pairwise relations. 通过设计新的任务可以发现PTM编码了syntactic, semantic, local, and long- range information。</p>
<p>World Knowledge</p>
<ul>
<li>commonsense knowledge: Davison et al. (2019) propose to first transform relational triples into masked sen- tences and then rank these sentences according to the mutual information given by PTMs. In the ex- periments, the PTM-based extraction method with- out further training even generalizes better than current supervised approaches.</li>
<li>factual knowledge: Petroni et al. (2019) propose to formulate the relational knowledge generation as the completion of <strong>fill-in-the-blank</strong> statements.<ul>
<li>LPAQA (Jiang et al., 2020b) search better statements/prompts through mining- based and paraphrasing-based methods.</li>
<li>Auto - Prompt (Shin et al., 2020) proposes to train discrete prompts for knowledge probing.</li>
<li>In P-tuning (Liu et al., 2021b), the authors discover that the bet- ter prompts lie in continuous embedding space, rather than discrete space.</li>
</ul>
</li>
</ul>
<h2 id="7-2-Robustness-of-PTMs"><a href="#7-2-Robustness-of-PTMs" class="headerlink" title="7.2 Robustness of PTMs"></a>7.2 Robustness of PTMs</h2><p>用Adversarial attacks检验模型鲁棒性，</p>
<ul>
<li>PTMs can be easily fooled by <strong>synonym replacement</strong> (Jin et al., 2020; Zang et al., 2020).</li>
<li>I<strong>rrelevant artifacts</strong> such as form words can mislead the PTMs into making wrong predic- tions (Niven and Kao, 2019; Wallace et al., 2019a).</li>
</ul>
<p>如何生成对抗样本</p>
<ul>
<li>utilize the model prediction, prediction probabilities, and model gradients of the models, 但难以保证质量</li>
<li>human-in-the-loop methods (Wallace et al., 2019b; Nie et al., 2020) generate more natural, valid, and diverse adversarial examples。</li>
</ul>
<h2 id="7-3-Structural-Sparsity-of-PTMs"><a href="#7-3-Structural-Sparsity-of-PTMs" class="headerlink" title="7.3 Structural Sparsity of PTMs"></a>7.3 Structural Sparsity of PTMs</h2><ul>
<li><strong>The multi-head attention structures are redundant</strong> in the tasks of machine translation (Michel et al., 2019), abstractive summarization (Baan et al., 2019), and language understanding (Kovaleva et al., 2019). 部分研究移除head反而得到更好的表现，一些head的注意力pattern也是相似的。</li>
<li><strong>Sparsity of parameters</strong>:<ul>
<li>Gordon et al. (2020) show <strong>low levels of pruning</strong> (30-40%) do not affect pre-training loss or the performance on downstream tasks at all.</li>
<li>Prasanna et al. (2020) validate <strong>the lottery ticket hypothesis on PTMs</strong> and find that it is possible to find sub-networks achieving per- formance that is comparable with that of the full model.</li>
</ul>
</li>
</ul>
<h2 id="7-4-Theoretical-Analysis-of-PTMs"><a href="#7-4-Theoretical-Analysis-of-PTMs" class="headerlink" title="7.4 Theoretical Analysis of PTMs"></a>7.4 Theoretical Analysis of PTMs</h2><p>Erhan et al. (2010) propose two hypotheses</p>
<ul>
<li>better optimization：更接近全局最优</li>
<li>better regularization：更好的泛化能力</li>
</ul>
<p>Saunshi et al. (2019) conduct a theoretical analysis of c<strong>ontrastive unsupervised representation learning</strong>. they prove that <strong>the loss of contrastive learning is the upper bound of the downstream loss</strong>.</p>
<h1 id="8-Future-Directions"><a href="#8-Future-Directions" class="headerlink" title="8 Future Directions"></a>8 Future Directions</h1><h2 id="8-1-Architectures-and-Pre-Training-Methods"><a href="#8-1-Architectures-and-Pre-Training-Methods" class="headerlink" title="8.1 Architectures and Pre-Training Methods"></a>8.1 Architectures and Pre-Training Methods</h2><ul>
<li>New Architectures</li>
<li>New Pre-Training Tasks</li>
<li>Beyond Fine-Tuning：An improved solution is to fix the original parameters of PTMs and add small fine-tunable adaption modules for specific tasks.</li>
<li>Reliability</li>
</ul>
<h2 id="8-2-Multilingual-and-Multimodal-Pre-Training"><a href="#8-2-Multilingual-and-Multimodal-Pre-Training" class="headerlink" title="8.2 Multilingual and Multimodal Pre-Training"></a>8.2 Multilingual and Multimodal Pre-Training</h2><ul>
<li>More Modalities: image, text, video and audio</li>
<li>More Insightful Interpretation: why bridging vision and language works</li>
<li>More Downstream Applications: 现有的image-text retrieval, image-to-text generation, text-to-image generation 等并不是现实迫切需要的应用.</li>
<li>Transfer Learning.</li>
</ul>
<h2 id="8-3-Computational-Efficiency"><a href="#8-3-Computational-Efficiency" class="headerlink" title="8.3 Computational Efficiency"></a>8.3 Computational Efficiency</h2><ul>
<li>Data Movement: 设备通信瓶颈</li>
<li>Parallelism Strategies 设计自动化</li>
<li>Large-Scale Training</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/2021/" rel="tag"># 2021</a>
              <a href="/tags/Survey/" rel="tag"># Survey</a>
              <a href="/tags/Pre-Trained-Models/" rel="tag"># Pre-Trained Models</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/paper-CorefQA-Coreference-resolution-as-query-based-span-prediction/" rel="prev" title="CorefQA - Coreference resolution as query-based span prediction">
                  <i class="fa fa-chevron-left"></i> CorefQA - Coreference resolution as query-based span prediction
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cong Chan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b35f789bd238372" async="async"></script>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://shootingspace.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://congchan.github.io/paper-Pre-Trained-Models-Past-Present-and-Future/";
    this.page.identifier = "paper-Pre-Trained-Models-Past-Present-and-Future/";
    this.page.title = "Survey - Pre-Trained Models - Past, Present and Future";
    };
  NexT.utils.loadComments('#disqus_thread', () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://shootingspace.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
