[{"content":"In the race to build truly helpful AI assistants, we\u0026rsquo;ve discovered a fundamental truth: raw intelligence isn\u0026rsquo;t enough. A model that masters calculus but can\u0026rsquo;t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.\nThis is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, Reinforcement Learning from Human Feedback (RLHF) reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF\u0026rsquo;s complexity was showing:\nThe 3-stage training treadmill (SFT → Reward Modeling → RL tuning) Prohibitively expensive human preference labeling Reward hacking vulnerabilities where models \u0026ldquo;game\u0026rdquo; the system Enter the new generation of alignment techniques. There are three trends of directions:\nEliminating reward modeling stages. Or use Rule-based rewards to incentivize LLM intelligence. Using AI-generated preferences. Enabling single-step optimization I am currently following the most cutting-edge LLM alignment methods, and this blog will be updated periodically.\nI. RL*F (Reinforcement Learning from X Feedback) with Proximal Policy Optimization (PPO) Proximal Policy Optimization Algorithms.1 (PPO) is the mose widely used reinforcment learning algorithm in Post-training. PPO is a policy gradient algorithm that optimizes policies by maximizing a clipped surrogate objective to balance exploration and exploitation. It is widely used in RLHF due to its stability and sample efficiency.\nCore Innovation: Uses clipped objective function to limit policy updates, balancing stability and performance. Dominates RLHF pipelines. Application: OpenAI\u0026rsquo;s ChatGPT, Claude series. Limitations: Requires separate reward model training; unstable with large batches. Reinforcement Learning from Human Feedback (RLHF) Training Language Models to Follow Instructions with Human Feedback. 2 (RLHF) combines reinforcement learning with human preferences to train LLMs, to produce outputs that are more aligned with human preferences and expectations.\nRLHF combines supervised fine-tuning (SFT) with PPO, using human ranked dataset to train a reward model (RM) to model human preferences, and guides policy updates. RLHF aligns LLMs with human values but is costly due to manual labeling.\nBase Model Training Start with a pre-trained language model (like GPT) The model is initially trained on large datasets using supervised learning SFT: Supervised fine-tuning on high-quality data. Human Feedback Collection Human evaluators compare pairs of model outputs They rank which response is better based on criteria like: Helpfulness,Harmlessness, Honesty Reward Modeling: A separate neural network (reward model) is trained on human preference rankings, to predict human preferences This model learns to score outputs based on the collected human feedback It essentially learns to mimic human judgment Reinforcement Learning Optimization The original language model as policy is optimized against RM using PPO. The reward model provides feedback signals Techniques like Proximal Policy Optimization (PPO) are commonly used The model learns to generate responses that maximize the predicted human preference score Benefits\nBetter Alignment: Models produce outputs more consistent with human values Reduced Harmful Content: Helps minimize toxic, biased, or dangerous responses Improved Quality: Responses become more helpful and relevant Scalability: Once trained, the reward model can provide feedback without constant human intervention Challenges\nScalability of Human Feedback: Collecting sufficient high-quality human feedback is expensive and time-consuming Reward Hacking: Models might find ways to maximize reward scores without actually improving quality Bias in Human Feedback: Human evaluators may introduce their own biases Complexity: The multi-stage training process is computationally intensive Reinforcement Learning from AI Feedback (RLAIF) In Constitutional AI: Harmlessness from AI Feedback. 3, researchers introduced a paradigm that replaces human labels for harmfulness with AI-generated feedback. The approach uses a \u0026ldquo;constitution\u0026rdquo; of principles to guide self-critique and revision, enabling models to learn harmless behavior on a hybrid of human and AI preferences. This paper was the first effort to explore RLAIF.\nKey Innovation: The framework combines supervised learning (critique → revision cycles) and RL from AI Feedback (RLAIF), where a preference model (PM) is trained on AI-generated comparisons. For example, models generate pairs of responses and evaluate which aligns better with constitutional principles (e.g., \u0026ldquo;avoid harmful advice\u0026rdquo;). Impact: As shown in Figure 2 of the paper, Constitutional AI achieves a Pareto improvement in harmlessness and helpfulness, outperforming RLHF models that trade off these traits. The approach reduces reliance on human labeling, a critical step toward scalable supervision. This work laid the groundwork for self-supervised RM, demonstrating that models can learn to evaluate their own behavior using explicit principles.\nIn RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback 4, RLAIF achieved comparable performance to RLHF.\nII. Improve Value Functioning and Eliminating Critic Model The PPO algorithm necessitates loading four models, each of substantial size, which introduces considerable engineering complexity in the design of multi-model training, inference, and real-time parameter updates. This process demands a significant amount of GPU resources.\nFor instance, during RLHF training, when the Actor, Critic, Reward, and Ref Models are of identical scale, such as 70B, employing vLLM/TensorRT-llm for PPO sample generation acceleration and DeepSpeed/Megatron for training acceleration results in roughly equal computational resource consumption between inference and training stages. Consequently, the Critic model accounts for approximately one-quarter of the total computational resource usage.\nIn the context of LLMs, it is common for only the final token to receive a reward score from the reward model. This practice can complicate the training of a value function that accurately reflects each token\u0026rsquo;s contribution. To address this challenge, numerous studies focus on optimizing the calculation of the value function, incidentally simplifying or potentially eliminating the need for the Critic model in the process.\nGroup Relative Policy Optimization (GRPO) Group Relative Policy Optimization (GRPO)5 is an efficient training algorithm proposed in the DeepSeekMath paper to enhance mathematical reasoning in language models.\nGRPO is a variant of PPO that eliminates the need for a critic model (value function), instead estimating the baseline from group-averaged rewards. This reduces memory and computational costs significantly, making it more resource-efficient.\nKey Differences from PPO\nNo Value Function: Unlike PPO, which uses a learned value function to compute advantages, GRPO calculates advantages using relative rewards within a group of sampled outputs for the same question. Group-based Baseline: For each question $q$, GRPO samples $ G $ outputs from the old policy. The baseline is the average reward of these outputs, and advantages are normalized within the group. Simplified Objective: GRPO optimizes the policy by maximizing a objective that uses group-relative advantages, avoiding the complexity of value function training. How GRPO Works Sampling: For each question $q$, sample $G$ outputs $\\{o_1, o_2, \\dots, o_G\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$.\nOutcome Reward Scoring and Normalization: Use a reward model to score each output, yielding $\\{r_1, r_2, \\dots, r_G\\}$. Outcome supervision provides the normalized reward at the end of each output $o_i$ and sets the advantages $\\hat{A}_{i,t}$ of all tokens in the output as the normalized reward by subtracting the group mean and dividing by the standard deviation: $$ \\hat{A}_{i,t} = \\tilde{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)} $$ Policy Update: Maximize the GRPO objective, which includes a KL divergence term to regularize against a reference model. optimizes the policy model by maximizing the following objective: $$ \\begin{aligned} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[ q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q) \\right] \\\\\\\\ \u0026 \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left\\{ \\min \\left[ \\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,\\lt t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\\lt t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,\\lt t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\\lt t})}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_{i,t} \\right] - \\beta \\mathbb{D}_{\\text{KL}} \\left[ \\pi_{\\theta} \\| \\pi_{\\text{ref}} \\right] \\right\\} \\end{aligned} $$where $\\epsilon$ and $\\beta$ are hyper-parameters, and $\\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only, which will be detailed in the following subsections.\nAlso note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}$. PPO approach: # Modify the reward with KL penalty reward_with_kl = reward - beta * kl_divergence(current_policy, ref_policy) advantage = compute_advantage(values, reward_with_kl, dones) GRPO Approach: # Compute advantage using original reward advantage = compute_advantage(values, reward, dones) # Policy loss with direct KL regularization policy_loss = -(log_probs * advantage).mean() kl_loss = kl_divergence(current_policy, ref_policy).mean() total_loss = policy_loss + beta * kl_loss And different from the KL penalty term used in PPO, GRPO estimate the KL divergence with the following unbiased estimator6: $$ \\mathbb{D}_{KL} \\left[ \\pi_{\\theta} \\| \\pi_{ref} \\right] = \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,\\lt t})}{\\pi_{\\theta}(o_{i,t}|q, o_{i,\\lt t})} - \\log \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,\\lt t})}{\\pi_{\\theta}(o_{i,t}|q, o_{i,\\lt t})} - 1 $$ which is guaranteed to be positive.\nExperimental Results\nDeepSeekMath-RL (7B) using GRPO surpasses all open-source models on MATH and approaches closed-source models like GPT-4. Iterative GRPO (updating the reward model incrementally) further boosts performance, especially in the first iteration. GRPO with process supervision (step-wise rewards) outperforms outcome supervision, highlighting the value of fine-grained feedback. RLHF with GRPO - Deepseek-R1 DeepSeek-R17 is the first public model who make use of both rule-based rewards and general RLHF via GRPO.\nDeepSeek-R1-Zero: Pure RL Training\nRL Algorithm: Uses Group Relative Policy Optimization (GRPO) to optimize policies without a critic model, reducing training costs. Reward Modeling: Relies on rule-based rewards for accuracy (e.g., math problem correctness) and format (enforcing CoT within tags), avoiding neural reward models to prevent reward hacking. Self-Evolution: Through RL, the model autonomously develops complex reasoning behaviors, such as reflecting on mistakes and exploring alternative solutions, leading to significant performance gains. For example, AIME 2024 pass@1 improves from 15.6% to 71.0%, and to 86.7% with majority voting. Limitation: Suffering from readability issues, such as language mixing and unstructured outputs. And Lack of non-reasoning tasks such as writing and factual QA is suboptimal. Experiences an unstable early training process. DeepSeek-R1: Cold-Start and Multi-Stage Refinement DeepSeek-R1 is an attempts to address the limitations of DeepSeek-R1-Zero. 1. Cold-Start Data: Collect thousands of long CoT (chain-of-thought) examples through few-shot prompting, model-generated outputs, and human annotation. These cold-start data are used to fine-tune the DeepSeek-V3-Base to create an initial RL actor that prioritizes readable formats (e.g., summaries and structured CoT) and reducing language mixing.\n2. Reasoning-Oriented RL: Apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. Incorporates language consistency rewards to mitigate mixed-language outputs, balancing performance with human readability.\n3. Rejection Sampling \u0026amp; SFT: After RL convergence, new SFT data is collected from RL checkpoints, combining reasoning and non-reasoning tasks (e.g., writing, factual QA) to enhance general capabilities.\nFor Reasoning Data Collection: Use rejection sampling on the RL checkpoint to curate ~600K reasoning samples, filtering out mixed-language and unreadable CoT. Include generative reward models (using DeepSeek-V3) for evaluation. For Non-Reasoning Data: Reuse SFT data from DeepSeek-V3 for tasks like writing, factual QA, and translation, collecting ~200K samples. Fine-Tuning: Train DeepSeek-V3-Base on the combined ~800K samples for two epochs to enhance general capabilities. 4. Scenario-Agnostic RL: A final RL stage aligns the model with human preferences for helpfulness and harmlessness, using a mix of rule-based and neural reward models.\nReasoning Data: Use rule-based rewards for math, code, and logic tasks, as in previous stages. General Data: Employ reward models to capture human preferences in complex scenarios (e.g., writing, role-playing), building on DeepSeek-V3’s pipeline . Evaluation Focus: For helpfulness, assess the final summary; for harmlessness, evaluate the entire response (CoT and summary) to mitigate risks . Key Advantages of Iterative RL Training\nPerformance Enhancement: DeepSeek-R1 achieves comparable results to OpenAI-o1-1217 on reasoning benchmarks (e.g., 79.8% pass@1 on AIME 2024) . Readability and Consistency: Cold-start data and language rewards reduce language mixing and improve output structure . Generalization: SFT with diverse data enables competence in non-reasoning tasks like creative writing and factual QA . Performance Benchmarks DeepSeek-R1 and distilled models excel on reasoning tasks:\nMath/Coding: AIME 2024 pass@1 of 79.8% (vs. OpenAI-o1-1217’s 79.2%), MATH-500 pass@1 of 97.3%, and Codeforces rating of 2029 (top 3.7% of human participants). Knowledge Tasks: MMLU score of 90.8%, GPQA Diamond of 71.5%, slightly below o1-1217 but surpassing other closed-source models. General Tasks: Strong performance in creative writing, summarization, and long-context understanding, with win rates of 87.6% on AlpacaEval 2.0 and 92.3% on ArenaHard. RLOO (REINFORCE Leave One-Out) PPO suffers from high computational costs and sensitive hyperparameter tuning. RLOO8, as a simpler RL methods, specifically REINFORCE-style optimization, can preserve or even enhance performance while reducing complexity.\nImage from https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo 9\nKey Insights:\nPPO\u0026rsquo;s Limitations in RLHF: PPO was designed for traditional RL environments with high variance and random policy initializations. In RLHF, pre-trained LLMs provide a strong policy initialization, concentrating probability mass on a small subset of tokens. This stability makes PPO\u0026rsquo;s complexity (e.g., clipping, value networks) unnecessary. REINFORCE for RLHF: By modeling the entire sequence generation as a single action (vs. PPO\u0026rsquo;s token-level actions), REINFORCE directly optimizes the full trajectory reward with unbiased baselines. This avoids the bias introduced by PPO\u0026rsquo;s bootstrapped value functions. REINFORCE Leave-One-Out (RLOO): A multi-sample extension of REINFORCE, RLOO uses online samples to create dynamic baselines, reducing variance without bias. It outperforms PPO and RL-free methods by fully leveraging all generated samples. REINFORCE REINFORCE loss, which applies the vanilla policy gradient to the entire sequence, using a moving average reward as a baseline.It basically multiplies the (reward - baseline) by the logprob of actions.\nCore Idea: In LLM applications, since the reward $r(x, y)$ is only available at the end of the full sequence, REINFORCE models the entire generation as a single action rather than each token. This aligns with the bandit problem formulation, where the Markov Decision Process (MDP) includes only the initial state (prompt) and the terminal state (completed sequence). Estimator: It uses the REINFORCE estimator to backpropagate through the discrete action space (generation) and directly optimize the KL-shaped reward objective for the entire sequence. The update rule is $$ \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}(. | x)}\\left[R(y, x) \\nabla_{\\theta} \\log \\pi_{\\theta}(y | x)\\right] \\tag{6} $$ The intuition here is related to the likelihood principle. If an action (generation of \\(y\\)) leads to a high reward, we want to increase the probability of taking that action in the future, and vice versa. The REINFORCE estimator is used to update the parameters \\(\\theta\\) of the policy \\(\\pi_{\\theta}\\). The expectation in the formula combines the reward and the policy gradient. Essentially, it tells us how to adjust the policy parameters \\(\\theta\\) to maximize the expected reward. Mathematically, when we take the expectation of the product of the reward \\(R(y, x)\\) and the policy gradient \\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y | x)\\), we are computing a quantity that, when used to update \\(\\theta\\) (using gradient ascent, for example), will tend to increase the expected reward. If \\(R(y, x)\\) is positive, the update will push the policy in the direction of increasing the probability of generating \\(y\\) given \\(x\\), and if \\(R(y, x)\\) is negative, it will push the policy away from generating \\(y\\) given \\(x\\). Baseline: To improve learning, one can reduce the variance of the REINFORCE estimator, while keeping it unbiased, by subtracting a baseline $b$ that has high covariance with the stochastic gradient estimate of the Eq.6: $$ \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(.|x)} \\big[ (R(y, x) - b) \\nabla_\\theta \\log \\pi_\\theta(y|x) \\big] \\tag{7} $$ The moving average of all rewards throughout training (Williams, 1992) is a strong parameter-free choice for the baseline: $$ b_{\\text{MA}} = \\frac{1}{S} \\sum_{s} R(x^s, y^s) \\tag{8} $$ Where $S$ is the number of training steps, and $(x^s, y^s)$ is the prompt-completion pair at the step $s$. This baseline is simple, computationally cheap, and parameter-free. Noticed that REINFORCE is a special case of PPO. PPO uses importance sampling to update the policy without re-collecting data, with a clipped objective to bound policy updates: $\\mathcal{L}^{\\text{PPO}}(\\theta) = \\mathbb{E} \\left[ \\min \\left( r_t(\\theta) \\cdot A_t, \\text{clip}(r_t(\\theta), 1-\\varepsilon, 1+\\varepsilon) \\cdot A_t \\right) \\right]$, where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$ is the importance ratio, $A_t$ is the advantage estimate (e.g., from TD or MC), and $\\varepsilon$ is the clipping parameter. To reduce PPO’s objective to REINFORCE’s with specific parameters set:\nRemoving Clipping ($\\varepsilon \\to \\infty$): When $\\varepsilon$ is infinitely large, the $\\text{clip}(\\cdot)$ operation becomes irrelevant, and the PPO objective simplifies to: $\\mathcal{L}(\\theta) = \\mathbb{E} \\left[ r_t(\\theta) \\cdot A_t \\right]$. If we further assume the old policy $\\pi_{\\theta_{\\text{old}}}$ is the same as the current policy $\\pi_\\theta$ (i.e., no importance sampling, or a single update without old policy), then $r_t(\\theta) = 1$, and the objective becomes: $\\mathcal{L}(\\theta) = \\mathbb{E} \\left[ A_t \\right]$ Using Monte Carlo(MC) Returns as Advantage ($A_t = G_t - b(s_t)$): If $A_t$ is defined as the MC return minus a baseline (as in REINFORCE), and the baseline $b(s_t) = 0$ (or ignored), then $A_t = G_t$. Substituting into the objective: $\\mathcal{L}(\\theta) = \\mathbb{E} \\left[ G_t \\right]$, whose gradient is exactly the REINFORCE update rule without a baseline. If a baseline is included ($b(s_t) \\neq 0$), it aligns with REINFORCE with a baseline, which is the standard practice to reduce variance. — A2C Is a Special Case of PPO10\nEven though the logprob is explicitly in the REINFORCE loss, it is also implicitly in the PPO loss.\nREINFORCE Leave-One-Out (RLOO) Leverages multiple online samples to further reduce variance in the REINFORCE estimator while keeping it unbiased.\nFor each prompt, generates k samples and uses the average reward of k-1 samples as a baseline for the remaining one, creating a variance-reduced gradient estimate.\nThe baseline in Eq. 8 is simple to implement and computationally cheap. However, it can be improved upon if we have access to multiple online samples, that can be used for further unbiased variance reduction:\nThe rewards for each sample can serve all other samples as a baseline. Policy updates can be done on an average of gradient estimates for each sample, resulting in a variance-reduced multi-sample Monte-Carlo (MC) estimate. This is the intuition behind the REINFORCE Leave-One-Out (RLOO) estimator, proposed by (Kool et al., 2019): $$ \\frac{1}{k} \\sum_{i=1}^{k} \\left[ R(y_{(i)}, x) - \\frac{1}{k - 1} \\sum_{ j \\neq i} R(y_{(j)}, x) \\right] \\nabla \\log \\pi(y_{(i)} | x) \\text{ for } y_{(1)}, \\ldots, y_{(k)} \\stackrel{i.i.d}{\\sim} \\pi_{\\theta}(. | x) $$ Where \\(k\\) refers to the number of online samples generated, \\(\\text{RLOO}_k\\) considers each \\( y_{(i)} \\) individually and uses the remaining \\( k - 1 \\) samples to create an unbiased estimate of the expected return for the prompt, akin to a parameter-free value-function, but estimated at each training step.\nThis is a much more effective baseline (as the paper\u0026rsquo;s experiments showed) than \\( b_{\\text{MA}} \\) since it\u0026rsquo;s created on-the-fly for each sample and at each training step, but comes at a cost of increased sampling time during training.\nNoted that generating extra samples as a means of variance reduction has been proposed by concurrent work - Remax 11, but RLOO focus on the efficiency benefits of fully utilizing all samples.\nResults:\nPerformance: REINFORCE outperforms PPO by 3.2–20.3% in win-rate. RLOO further improves performance, surpassing DPO and RAFT across all datasets. Sample Efficiency: RLOO with k=2 matches or exceeds RAFT with k=4, demonstrating better use of online samples. Robustness: RLOO is less sensitive to KL penalty and reward noise compared to RAFT, maintaining stable performance under varying conditions. Alignment Tax: RLOO preserves language fluency (perplexity) and diversity better than PPO, with lower reward variance—a key factor for safety-critical applications. ReMax PPO introduces significant computational overhead for LLMs due to its complex architecture: it requires training a value model, leading to heavy memory usage, tedious hyperparameter tuning, and slow training. To make RLHF efficient, ReMax 11 leverages 3 properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. ReMax does not require training an additional value model as in PPO and is further enhanced with a new variance reduction technique.\nKey limitations of PPO for RLHF:\nValue model overhead: Consumes ~46% of GPU memory for a 7B model. Hyperparameter complexity: Requires tuning 4+ parameters (e.g., clipping, GAE coefficient). Slow convergence: Training with PPO can be 4× slower than earlier RLHF steps. Key Insights: Unique Properties of RLHF for LLMs ReMax leverages three properties of RLHF that PPO overlooks:\nFast simulation: Generating a complete LLM response (trajectory) is rapid (e.g., \u0026lt;10s for 7B models). Deterministic transitions: Text generation depends only on past tokens, with no stochastic environment dynamics. Trajectory-level rewards: Rewards are given only after the full response, not at each step. The ReMax Algorithm ReMax is built on the REINFORCE algorithm but introduces a variance reduction technique:\nGreedy baseline: For each prompt, compute the reward of a greedy (deterministic) response and use it to normalize the gradient, reducing variance. For a prompt, sample a stochastic response and a greedy response. Compute the reward difference between the two responses. No value model: Directly optimizes the policy to maximize the log-likelihood of high-reward responses, weighted by the reward difference. Pseudo-code Core Steps:\nfor prompt in dataset: seq = lm.sample(prompt, greedy=False) # Stochastic response seq_max = lm.sample(prompt, greedy=True) # Greedy response rew = rm(prompt, seq) - rm(prompt, seq_max) # Reward difference logp = lm.inference(prompt, seq) # Log-likelihood loss = - (logp.sum() * rew).mean() # Loss for optimization lm.minimize(loss) REINFORCE++ RLOO and GRPO increase inference costs to trade for eliminating the critic model. The blog 12 argues that eliminating the critic model may inadvertently lower training efficiency due to increased inference costs:\nWhen all models (Actor, Critic, Reward, Reference) are similar in scale (e.g., 70B parameters), inference and training consume roughly equal computational resources (1:1 ratio). Eliminating the critic model may actually reduce training efficiency due to increased inference costs System complexity remains largely unchanged since multiple models still need to operate together Performance Analysis: REINFORCE-based methods (e.g., RLOO, ReMax, GRPO) eliminate the critic but struggle with accurate advantage estimation, often overfitting to simple prompts and being vulnerable to reward hacking. These methods estimate advantages per prompt, leading to instability and poor generalization. GRPO and RLOO don\u0026rsquo;t provide significant theoretical improvements over PPO The claimed advantages (like \u0026ldquo;10x\u0026rdquo; efficiency gains) are often exaggerated PPO can address critic model issues by initializing critics with actor weights Alternative solution: Pre-train the critic model by freezing actor weights during PPO training Technical Issues with GRPO: Numerical instability: Small differences in sampled rewards can be amplified during normalization Example: rewards of 1.001 vs 1.00 become -0.7070 vs 0.7072 after normalization Convergence problems: When all sampled rewards are equal, GRPO provides zero learning signal Under Process Reward Model (PRM) settings, GRPO essentially becomes REINFORCE with mean baseline Bottom Line:: Both GRPO and RLOO are most beneficial when critic/reward models are significantly larger than actors, but even then, PPO remains a viable alternative with proper initialization strategies. The computational and complexity advantages are often overstated. REINFORCE++ 13 is a critic-free RLHF algorithm that uses the global batch mean reward as a baseline instead of prompt-specific baselines, preventing overfitting and improving robustness.\nThe overall algorithm flow of REINFORCE++: Sample one response per prompt, compute rewards, normalize advantages, and update the policy using a clipped objective (similar to PPO but without the critic).\nAdvantages Normalization: Normalizes advantages across the entire batch to stabilize training and enhance out-of-distribution (OOD) generalization. REINFORCE++ replaces prompt-specific baselines with the mean reward of a global batch, reducing overfitting to individual prompts. The Advantage is calculated as: $$ A_{q, o_t} = r(o_{1:t}, q) - \\beta \\cdot \\sum_{i=t}^T KL(i), \\quad \\text{with } KL(t) = \\log\\left(\\frac{\\pi_{\\theta_{\\text{old}}}^{RL}(o_t | q, o_{\\lt t})}{\\pi^{\\text{SFT}}(o_t | q, o_{\\lt t})}\\right) $$ The token-level KL penalty avoids the need for a critic network while achieving comparable stability. The gradient of the token-level KL penalty has been theoretically proven to be unbiased concerning the $k_3$ loss of GRPO in RLHF.\nThe advantage is normalized globally: $$ A_{q, o_t}^{\\text{norm}} = \\frac{A_{q, o_t} - \\text{mean}(A_{q, o_t})}{\\text{std}(A_{q, o_t})} $$Experimental Results:\nBradley-Terry Reward Model: REINFORCE++ matches or exceeds performance of GRPO, RLOO, and ReMax on OOD benchmarks (e.g., GSM8K, MATH, code generation), with higher per-token efficiency. Long CoT Tasks: Small-Scale Datasets: GRPO overfits to training prompts (e.g., AIME-24), achieving near-perfect scores but failing on OOD test sets (AIME-25), while REINFORCE++ shows stable generalization. Logical Reasoning (Knights and Knaves): REINFORCE++ outperforms GRPO in complex and OOD scenarios (e.g., 8-character puzzles), with higher Pass@1 scores (36 vs. 20) and longer, more reasoned responses. Mathematical Reasoning: From scratch or fine-tuned models, REINFORCE++ demonstrates better OOD generalization on MATH and AIME datasets. DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization) DAPO 14 decouples policy clipping and dynamic sampling to enhance training efficiency, reducing variance in gradient estimates.\nDAPO addresses critical challenges in RL training—such as entropy collapse, reward noise, and instability—with four key techniques.\nClip-Higher: Decouples lower $\\varepsilon_{low}$ and higher $\\varepsilon_{high}$ clipping ranges in policy optimization to prevent entropy collapse. By increasing $\\varepsilon_{high}$, low-probability \u0026ldquo;exploration\u0026rdquo; tokens gain more room for probability increases, enhancing diversity. Dynamic Sampling: Over-samples and filters out prompts with all-correct or all-wrong outputs to maintain effective gradient signals. This mitigates gradient-decreasing issues from zero-advantage batches, improving training efficiency. Token-Level Policy Gradient Loss: Shifts from sample-level to token-level loss calculation, balancing the influence of long and short responses. This prevents low-quality, overly long generations and stabilizes training. Overlong Reward Shaping: Introduces soft punishment for truncated responses to reduce reward noise. Instead of harsh penalties, it uses a length-aware function to guide models toward optimal response lengths. Dataset and Implementation The DAPO-Math-17K dataset transforms math problems into integer answers for reliable rule-based reward signals. The system is built on the verl framework, with training details including AdamW optimization, group reward normalization, and dynamic sampling hyperparameters.\nExperiments and Results AIME 2024 Performance: DAPO achieves 50 points on AIME with Qwen2.5-32B, outperforming DeepSeek-R1-Zero-Qwen-32B (47 points) with half the training steps (Figure 1). Ablation Study: Each technique contributes significantly: Overlong Filtering (+6), Clip-Higher (+2), Soft Overlong Punishment (+3), Token-Level Loss (+1), and Dynamic Sampling (+8) (Table 1). Training Dynamics: Metrics like response length, reward, and entropy show stable improvement, with Clip-Higher specifically combating entropy collapse (Figure 7). Dr. GRPO Dr. GRPO 15 claims that:\nDeepSeek-V3-Base already exhibit “Aha moment”, while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. There is an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. Dr. GRPO is an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Key Insights on Base Models\nTemplate Impact on Question-Answering Ability\nBase models like Llama and DeepSeek require prompt templates (e.g., R1 template) to elicit question-answering behavior, while Qwen2.5 models excel without templates. This suggests Qwen2.5 might be pretrained on question-answer pairs, acting like supervised fine-tuned (SFT) models even in their base form. For example, Qwen2.5-Math-7B achieves 69.0% accuracy on MATH500 without templates, outperforming traditional prompting methods. Preexisting \u0026ldquo;Aha Moment\u0026rdquo; in Base Models\nThe \u0026ldquo;Aha moment\u0026rdquo; (self-reflection behaviors) often attributed to RL emergence is already present in base models like DeepSeek-V3-Base. Experiments show these models generate self-reflection keywords (e.g., \u0026ldquo;Aha,\u0026rdquo; \u0026ldquo;wait\u0026rdquo;) in responses to math problems without RL tuning. Analysis of Reinforcement Learning\nBiases in GRPO\nGRPO introduces two key biases: Response-length bias: Dividing by response length ($|o_i|$) penalizes short correct responses and favors longer incorrect ones. Question-difficulty bias: Normalizing by reward standard deviation prioritizes easy/hard questions, skewing optimization. These biases lead to unnecessarily long incorrect responses, as observed in training dynamics. Dr. GRPO: Unbiased Optimization\nThe authors propose Dr. GRPO, which removes $|o_i|$ and standard deviation normalization from GRPO. This fixes the biases, improving token efficiency while maintaining reasoning performance. Experiments show Dr. GRPO reduces the length of incorrect responses and matches the accuracy of GRPO with fewer tokens. Group-in-Group Policy Optimization (GiGPO) for LLM Agent Training GiGPO16 is designed to enhance long-horizon training for LLM agents. Unlike existing group-based RL methods (e.g., GRPO) that struggle with fine-grained credit assignment in multi-step tasks, GiGPO achieves hierarchical advantage estimation while retaining key benefits: being critic-free, memory-efficient, and computationally lightweight.\nKey Limitations of Existing Group-based RL:\nTraditional group-based RL (e.g., GRPO, RLOO) works well for single-turn tasks (e.g., math reasoning) where rewards are immediate but fails in long-horizon scenarios. In multi-step environments (e.g., embodied navigation, web shopping), rewards are sparse/delayed, making it hard to assign credit to individual steps. Naive extensions of group-based RL collapse step-level distinctions, reducing effectiveness in agent training. GiGPO\u0026rsquo;s hierarchical \u0026ldquo;group-in-group\u0026rdquo; design:\nEpisode-Level Grouping:\nObjective: Capture holistic trajectory quality. Method: Samples a group of complete trajectories under identical initial conditions and task descriptions. Computes macro relative advantages using total episode returns, normalized by the group\u0026rsquo;s mean and a scaling factor (either standard deviation or 1 for stability). $$ A^E(\\tau_i) = \\frac{R(\\tau_i) - \\text{mean}(\\{R(\\tau_j)\\})}{F_{\\text{norm}}(\\{R(\\tau_j)\\})} $$ where \\( R(\\tau_i) \\) is the total reward of trajectory \\( \\tau_i \\) Step-Level Grouping:\nObjective: Evaluate local step effectiveness. Method: Identifies anchor states (repeated environment states across trajectories) to form step-level groups. Computes micro relative advantages using discounted future rewards for actions taken at these shared states. $$ A^S(a_t^{(i)}) = \\frac{R_t^{(i)} - \\text{mean}(\\{R_t^{(j)}\\})}{F_{\\text{norm}}(\\{R_t^{(j)}\\})} $$ where \\( R_t^{(i)} \\) is the discounted return from step \\( t \\) in trajectory \\( i \\) GiGPO Combined Advantage Signal: The final advantage for each action merges both levels, $$ A(a_t^{(i)}) = A^E(\\tau_i) + \\omega \\cdot A^S(a_t^{(i)}) $$ where \\( \\omega \\) balances the two signals.\nGiGPO was evaluated on two challenging benchmarks using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct:\nBenchmark Improvement Over GRPO Key Findings ALFWorld 12-13% higher success Superior performance in embodied household task planning. WebShop \u0026gt;9% higher success Better at goal-driven web navigation and shopping tasks. Efficiency: GiGPO matches GRPO\u0026rsquo;s GPU memory usage and rollout costs, with \u0026lt;0.002% additional computation time. Ablation Studies: Removing either episode-level or step-level advantages significantly degrades performance, confirming the value of hierarchy. III. Optimization without Reward Model Direct Preference Optimization (DPO) DPO17 is a RL-free training method for LLMs that directly optimizes for human preferences without requiring a separate reward model. DPO eliminates the need for a reward model by directly optimizing a policy using pairwise preference data. It minimizes the KL divergence between the policy and a reference model while maximizing the Bradley-Terry loss. DPO derives a closed-form solution that directly optimizes the policy using preference data. The key insight is that the optimal policy under the RLHF objective can be expressed analytically in terms of the reward function and reference policy.\nThe DPO objective is based on the Bradley-Terry preference model, the optimal RLHF policy $π^∗$ under the Bradley-Terry model satisfies the preference model::\n$$P(y_w \\succ y_l | x) = \\frac{1}{1 + \\exp(\\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} - \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)})}$$Where:\n$y_w$ is the preferred (winning) response $y_l$ is the less preferred (losing) response $x$ is the input prompt $\\pi_\\theta$ is the policy being optimized $\\pi_{ref}$ is the reference policy $\\beta$ is a temperature parameter For a static dataset of comparisons $\\mathcal{D} = \\left\\{ x^{(i)}, y_w^{(i)}, y_l^{(i)} \\right\\}_{i=1}^N$, the reward modeling approach works by defining: $$ \\mathcal{L}_R(r_\\phi, \\mathcal{D}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( r_\\phi(x, y_w) - r_\\phi(x, y_l) \\right) \\right] $$Analogous to reward modeling approach, DPO\u0026rsquo;s policy objective becomes: $$ \\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\text{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)} \\right) \\right] $$ The gradient with respect to the parameters $\\theta$ can be written as: $$ \\nabla_\\theta \\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = - \\beta \\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\bigg[ \\underbrace{\\sigma\\bigl(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta(x, y_w)\\bigr)}_{\\text{higher weight when reward estimate is wrong}} \\, \\bigg[ \\underbrace{\\nabla_\\theta \\log \\pi(y_w \\mid x)}_{\\text{increase likelihood of } y_w} - \\underbrace{\\nabla_\\theta \\log \\pi(y_l \\mid x)}_{\\text{decrease likelihood of } y_l} \\bigg] \\bigg] $$where $\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}$ is the reward implicitly defined by the language model $\\pi_\\theta$ and reference model $\\pi_{\\text{ref}}$.\nA mechanistic understanding of DPO The gradient of the loss function $\\mathcal{L}_{\\text{DPO}}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. The examples are weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. The papers\u0026rsquo;s experiments suggest the importance of this weighting, as a naïve version of this method without the weighting coefficient can cause the language model to degenerate. DPO\u0026rsquo;s Training Process Data Collection: Gather preference pairs $(x, y_w, y_l)$ where $y_w$ is preferred over $y_l$ for prompt $x$ Direct Optimization: Minimize the DPO loss $\\mathcal{L}_{\\text{DPO}}$ Regularization: The KL divergence constraint from RLHF is implicitly maintained through the reference policy terms Advantages of DPO Simplicity Eliminates the need for reward model training Reduces the training pipeline from 3 stages to 2 stages Avoids the complexities of reinforcement learning Stability More stable training compared to PPO-based RLHF No issues with reward model overoptimization Direct gradient-based optimization Efficiency Requires less computational resources Faster convergence Easier hyperparameter tuning Practical Implementation # Simplified DPO loss computation def dpo_loss(policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, beta=0.1): policy_logratios = policy_chosen_logps - policy_rejected_logps reference_logratios = reference_chosen_logps - reference_rejected_logps logits = beta * (policy_logratios - reference_logratios) loss = -torch.nn.functional.logsigmoid(logits).mean() return loss Limitations and Considerations Data Quality: Heavily dependent on high-quality preference data Distribution Shift: May struggle with significant shifts from reference policy Preference Complexity: Works best with clear preference distinctions Hyperparameter Sensitivity: The β parameter requires careful tuning Comparison with RLHF\nAspect RLHF DPO Complexity High (3 stages) Medium (2 stages) Stability Can be unstable More stable Computational Cost High Lower Flexibility High Moderate Online DPO - Direct Language Model Alignment from Online AI Feedback Direct Alignment from Preferences (DAP) methods like DPO have emerged as efficient alternatives to RLHF, but they rely on pre-collected offline preference data. This leads to two key issues:\nOffline Feedback: Preferences are static and not updated during training. Off-Policy Learning: Responses in the dataset are generated by a different model, causing distribution shift as the target model evolves. The proposed Online AI Feedback (OAIF) framework18 makes Online DAP methods online and on-policy by:\nSampling two responses from the current model for each prompt. Using an LLM annotator (e.g., PaLM 2) to provide real-time preference feedback by choosing the better response. Updating the model using standard DAP losses (DPO, IPO, SLiC) with this online feedback. Key advantages:\nAvoids distribution shift by using on-policy generations. Eliminates the need for a separate Reward Model (RM), unlike RLHF. Enables controllable feedback via prompt engineering for the LLM annotator. Experiments and Results\nEffectiveness vs. Offline DAP:\nOnline DAP methods (DPO, IPO, SLiC) achieved an average 66% win rate over their offline counterparts in human evaluations. Online DPO outperformed SFT baselines, RLHF, and RLAIF 58% of the time on the TL;DR task. Generalization to Other DAP Methods:\nOAIF improved all three DAP methods, with online SLiC showing a 71.43% win rate over offline SLiC in TL;DR. Comparison with RLHF/RLAIF:\nOnline DPO was preferred 58% of the time in 4-way comparisons (vs. offline DPO, RLAIF, RLHF). RLHF relies on static RMs, which struggle as the model evolves, while OAIF\u0026rsquo;s LLM annotator adapts dynamically. Controllability via Prompts:\nInstructing the LLM annotator to prefer shorter responses reduced average token length from ~120 to ~40, while maintaining quality above SFT baselines. Impact of Annotator Size:\nLarger annotators (e.g., PaLM 2-L) improved performance, but even smaller annotators (PaLM 2-XS) outperformed RLHF in some cases. OAIF addresses the offline and off-policy limitations of DAP methods, achieving better alignment with reduced human annotation. The approach paves the way for scalable LLM alignment using AI feedback, with potential for real-time user adaptation and qualitative objective control.\nMethod No RM Needed On-Policy Online Feedback Offline DAP ✓ ✗ ✗ RLHF/RLAIF ✗ ✓ ✓ OAIF (Proposed) ✓ ✓ ✓ TDPO: Token-level Direct Preference Optimization DPO optimize models at the sentence level, evaluating full responses. However, LLMs generate text token-by-token in an auto-regressive manner, which creates a mismatch between evaluation and generation processes. DPO uses KL divergence to constrain models to a reference LLM but struggles with divergence efficiency: the KL divergence of dispreferred responses grows too quickly, limiting diversity. This motivates the need for a more granular, token-level optimization approach.\nTDPO19 optimizes token-level preferences to improve sequence generation quality, addressing limitations of instance-level DPO in long-chain reasoning.\nTDPO models text generation as an MDP, where:\nState \\( s_t = [x, y^{\\lt t}] \\) (prompt + partial response) Action \\( a_t = y^t \\) (next token) Reward \\( R_t := R(s_t, a_t) = R\\bigl([x, y^{\\lt t}], y^t\\bigr). \\) (token-wise reward) The objective function combines the advantage function of a reference model with forward KL divergence: \\[ \\max_{\\pi_\\theta} \\mathbb{E}\\left[ A_{\\pi_{ref}}(s_t, z) - \\beta D_{KL}(\\pi_\\theta(\\cdot|s_t) \\| \\pi_{ref}(\\cdot|s_t)) \\right] \\] where \\( A_{\\pi_{ref}} \\) is the advantage function, and \\( \\beta \\) weights the KL penalty.\nTDPO transforms the sentence-level Bradley-Terry model into a token-level preference model, relating it to the Regret Preference Model. The key is expressing human preference probability as: \\[ P_{BT}(y_1 \\succ y_2|x) = \\sigma(u(x, y_1, y_2) - \\delta(x, y_1, y_2)) \\] where \\( u \\) is the reward difference from DPO, represented as, $$ u(x, y_1, y_2) = \\beta \\log \\frac{\\pi_\\theta(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} $$and \\( \\delta \\) is the weighted SeqKL difference between responses. $$ \\delta(x, y_1, y_2) = \\beta D_{\\text{SeqKL}}\\left(x, y_2; \\pi_{\\text{ref}} \\parallel \\pi_\\theta\\right) - \\beta D_{\\text{SeqKL}}\\left(x, y_1; \\pi_{\\text{ref}} \\parallel \\pi_\\theta\\right) $$Reformulate the Bradley-Terry model into a structure solely relevant to the policy. This formulate a likelihood maximization objective for a parametrized policy $\\pi_\\theta$, leading to the derivation of the loss function for the initial version of TDPO, $\\text{TDPO}_1$: $$ \\mathcal{L}_{\\text{TDPO}_1}\\left( \\pi_\\theta; \\pi_{\\text{ref}} \\right) = - \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left( u(x, y_w, y_l) - \\delta(x, y_w, y_l) \\right) \\right] $$In $\\text{TDPO}_1$, \\( \\delta \\) depends on \\( \\pi_\\theta \\) (the current policy). During backpropagation, this causes gradient coupling: Updates to \\( \\pi_\\theta \\) affect both \\( u \\) and \\( \\delta \\), leading to unstable training (e.g., gradient conflicts or explosions).\nThis issue is addressed by decoupling the gradient flow of \\( \\delta \\) from \\( \\pi_\\theta \\) using a stop-gradient operation. Specifically:\nThe SeqKL term in \\( \\delta \\) is wrapped with a stop-gradient (e.g., detach() in PyTorch), so its gradient no longer propagates back to \\( \\pi_\\theta \\). The updated loss function for ${\\text{TDPO}_2}$ becomes: $$ \\mathcal{L}_{\\text{TDPO}_2}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}} \\left[ \\log \\sigma\\left( u(x,y_w,y_l) - \\alpha \\cdot \\text{stop\\_grad}\\left( \\delta(x,y_w,y_l) \\right) \\right) \\right] $$ By isolating \\( \\delta \\)’s gradient, TDPO₂ avoids feedback loops between \\( u \\) and \\( \\delta \\), reducing training oscillations. The SeqKL term acts as a \u0026ldquo;soft regularizer\u0026rdquo; to control token-level divergence, while the main optimization focuses on aligning with human preferences (via \\( u \\)). Define the loss function for $\\text{TDPO}_2$ as: $$ \\mathcal{L}_{\\text{TDPO}_2}\\left( \\pi_\\theta; \\pi_{\\text{ref}} \\right) = - \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left( u(x, y_w, y_l) - \\alpha \\delta_2(x, y_w, y_l) \\right) \\right] $$ where $\\alpha$ is a parameter, and $$ \\delta_2(x, y_1, y_2) = \\beta D_{\\text{SeqKL}}\\left( x, y_2; \\pi_{\\text{ref}} \\parallel \\pi_\\theta \\right) - sg\\left( \\beta D_{\\text{SeqKL}}\\left( x, y_1; \\pi_{\\text{ref}} \\parallel \\pi_\\theta \\right) \\right) $$The $sg$ represents the stop-gradient operator, which blocks the propagation of gradients.\nImplementation from: https://github.com/Vance0124/Token-level-Direct-Preference-Optimization/blob/2a736ecb285394a8419b461816bce1ba3b093cc9/trainers.py#L46\nclass BasicTrainer(object): ... def tdpo_concatenated_forward(self, model: nn.Module, reference_model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]): \u0026#34;\u0026#34;\u0026#34;Run the policy model and the reference model on the given batch of inputs, concatenating the chosen and rejected inputs together. We do this to avoid doing two forward passes, because it\u0026#39;s faster for FSDP. \u0026#34;\u0026#34;\u0026#34; concatenated_batch = concatenated_inputs(batch) all_logits = model(concatenated_batch[\u0026#39;concatenated_input_ids\u0026#39;], attention_mask=concatenated_batch[\u0026#39;concatenated_attention_mask\u0026#39;]).logits.to(torch.float32) with torch.no_grad(): reference_all_logits = reference_model(concatenated_batch[\u0026#39;concatenated_input_ids\u0026#39;], attention_mask=concatenated_batch[ \u0026#39;concatenated_attention_mask\u0026#39;]).logits.to(torch.float32) all_logps_margin, all_position_kl, all_logps = _tdpo_get_batch_logps(all_logits, reference_all_logits, concatenated_batch[\u0026#39;concatenated_labels\u0026#39;], average_log_prob=False) chosen_logps_margin = all_logps_margin[:batch[\u0026#39;chosen_input_ids\u0026#39;].shape[0]] rejected_logps_margin = all_logps_margin[batch[\u0026#39;chosen_input_ids\u0026#39;].shape[0]:] chosen_position_kl = all_position_kl[:batch[\u0026#39;chosen_input_ids\u0026#39;].shape[0]] rejected_position_kl = all_position_kl[batch[\u0026#39;chosen_input_ids\u0026#39;].shape[0]:] chosen_logps = all_logps[:batch[\u0026#39;chosen_input_ids\u0026#39;].shape[0]].detach() rejected_logps = all_logps[batch[\u0026#39;chosen_input_ids\u0026#39;].shape[0]:].detach() return chosen_logps_margin, rejected_logps_margin, chosen_position_kl, rejected_position_kl, \\ chosen_logps, rejected_logps def tdpo_loss(chosen_logps_margin: torch.FloatTensor, rejected_logps_margin: torch.FloatTensor, chosen_position_kl: torch.FloatTensor, rejected_position_kl: torch.FloatTensor, beta: float, alpha: float = 0.5, if_tdpo2: bool = True) -\u0026gt; Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]: \u0026#34;\u0026#34;\u0026#34;Compute the TDPO loss for a batch of policy and reference model log probabilities. Args: chosen_logps_margin: The difference of log probabilities between the policy model and the reference model for the chosen responses. Shape: (batch_size,) rejected_logps_margin: The difference of log probabilities between the policy model and the reference model for the rejected responses. Shape: (batch_size,) chosen_position_kl: The difference of sequential kl divergence between the policy model and the reference model for the chosen responses. Shape: (batch_size,) rejected_position_kl: The difference of sequential kl divergence between the policy model and the reference model for the rejected responses. Shape: (batch_size,) beta: Temperature parameter for the TDPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -\u0026gt; 0. alpha: Temperature parameter for the TDPO loss, used to adjust the impact of sequential kl divergence. if_tdpo2: Determine whether to use method TDPO2, default is True; if False, then use method TDPO1. Returns: A tuple of two tensors: (losses, rewards). The losses tensor contains the TDPO loss for each example in the batch. The rewards tensors contain the rewards for response pair. \u0026#34;\u0026#34;\u0026#34; chosen_values = chosen_logps_margin + chosen_position_kl rejected_values = rejected_logps_margin + rejected_position_kl chosen_rejected_logps_margin = chosen_logps_margin - rejected_logps_margin if not if_tdpo2: logits = chosen_rejected_logps_margin - (rejected_position_kl - chosen_position_kl) # tdpo1 else: logits = chosen_rejected_logps_margin - alpha * (rejected_position_kl - chosen_position_kl.detach()) # tdpo2 losses = -F.logsigmoid(beta * logits) chosen_rewards = beta * chosen_values.detach() rejected_rewards = beta * rejected_values.detach() return losses, chosen_rewards, rejected_rewards Trust Region DPO (TR-DPO) Offline alignment methods (e.g., DPO) fine-tune LLMs using pre-constructed datasets without needing explicit reward models. However, they suffer from overoptimization: as the trained policy (\\(\\pi_\\theta\\)) deviates too far from the fixed reference policy (\\(\\pi_{ref}\\), typically a supervised fine-tuned model), the quality of generated outputs degrades. This is linked to increased probabilities of out-of-domain (OOD) data and a decline in in-domain (ID) data probabilities.\nOveroptimization in offline methods arises due to vanishing curvature in the loss landscape (analyzed via Hessian dynamics), making it hard to reverse declining ID data probabilities. Updating \\(\\pi_{ref}\\) \u0026ldquo;resets\u0026rdquo; the optimization process, restoring curvature and preventing OOD drift.\nTR-DPO20 address the overoptimization via dynamically updating the reference policy (\\(\\pi_{ref}\\)) during training to mitigate overoptimization. This approach, inspired by trust region optimization, ensures the model stays within a \u0026ldquo;trustworthy\u0026rdquo; range of behavior while allowing beneficial divergence from the initial reference. Unlike game-theoretical approaches (which rely on online sampling), TR methods work entirely offline, using pre-existing datasets.\nTwo update strategies are introduced:\nSoft Update: Gradually merges the current policy into the reference policy using a weighted average:\n$$\\pi_{ref} \\leftarrow \\alpha \\pi_\\theta + (1-\\alpha) \\pi_{ref_{prev}}$$\nwhere \\(\\alpha\\) controls the update rate, and stop-gradient (\\(sg\\)) prevents backpropagating through \\(\\pi_{ref}\\)\nHard Update: Periodically replaces the reference policy with the current policy after a fixed number of steps (\\(\\tau\\)):\n$$\\pi_{ref} \\leftarrow \\pi_\\theta$$ These strategies are applied to existing methods, creating TR-DPO, TR-IPO, and TR-KTO.\nThe authors evaluate TR methods on both task-specific and general benchmarks, using models like Pythia (2.8B–12B) and Llama3 (8B):\nTask-Specific Tasks:\nOn Anthropic-HH (helpful/harmless dialogue) and Reddit TL;DR (summarization), TR methods outperformed vanilla DPO/IPO/KTO. For example, TR-DPO with \\(\\alpha=0.6\\) or \\(\\tau=512\\) achieved 8.4–15% higher win rates. General Benchmarks:\nOn AlpacaEval 2 and Arena-Hard, TR methods showed significant gains. TR-IPO with hard updates improved win rates by 15.1 points on Arena-Hard, while TR-DPO improved by 9.5 points. Overoptimization Mitigation:\nAt equivalent KL divergence (distance from the initial reference), TR methods maintained higher human-centric (HC) metrics (coherence, helpfulness, etc.) compared to vanilla methods, indicating reduced overoptimization. TR alignment methods (TR-DPO, TR-IPO, TR-KTO) effectively reduce overoptimization by dynamically updating the reference policy. They outperform traditional offline methods across tasks and model sizes, enabling LLMs to diverge beneficially from initial references while maintaining high quality. Future work will explore broader applicability and adaptive update strategies.\nStep-DPO: Step-wise Preference Optimization for Long-chain Reasoning LLMs struggle with long-chain mathematical reasoning due to the need for precise step-by-step correctness. Traditional DPO fails to improve such reasoning effectively because it evaluates entire answers rather than individual steps, making it hard to identify subtle errors in intermediate reasoning.\nStep-DPO treats each reasoning step as the unit for preference optimization (instead of holistic answers in DPO). By focusing on the first erroneous step in a chain, Step-DPO provides fine-grained supervision, enabling LLMs to locate and correct mistakes more accurately.\nPaper: Lai, Xin, et al. Step-DPO: Step-Wise Preference Optimization for Long-Chain Reasoning of LLMs. arXiv:2406.18629, arXiv, 26 June 2024. arXiv.org, https://doi.org/10.48550/arXiv.2406.18629.\nSpecifically, the answer \\( y \\) can be decomposed into a sequence of reasoning steps \\( y = s_1, \\ldots, s_n \\), where \\( s_i \\) is the \\( i \\)-th reasoning step.\nGiven a prompt \\( x \\) and a series of initial correct reasoning steps \\( s_{1\\sim k-1} = s_1, \\ldots, s_{k-1} \\), Step-DPO aims to maximize the probability of the correct next reasoning step \\( s_{\\textit{win}} \\) and minimize the probability of the incorrect one \\( s_{\\textit{lose}} \\). This objective can be formulated as:\n$$ \\mathcal{L}(\\theta) = -\\mathbb{E}_{(x, s_{1\\sim k-1}, s_{\\textit{win}}, s_{\\textit{lose}}) \\sim D} \\left[ \\log \\sigma\\left( \\beta \\log \\frac{\\pi_\\theta(s_{\\textit{win}} \\mid x; s_{1\\sim k-1})}{\\pi_{\\textit{ref}}(s_{\\textit{win}} \\mid x; s_{1\\sim k-1})} - \\beta \\log \\frac{\\pi_\\theta(s_{\\textit{lose}} \\mid x; s_{1\\sim k-1})}{\\pi_{\\textit{ref}}(s_{\\textit{lose}} \\mid x; s_{1\\sim k-1})} \\right) \\right] $$\nThe authors propose a 3-step pipeline to build high-quality step-wise preference data (10K pairs):\nError Collection: Use a reference model to generate incorrect answers for math problems, retaining cases where the final answer differs from the ground truth. Step Localization: Identify the first erroneous step in each incorrect answer manually or with GPT-4. Rectification: Generate correct next steps by sampling from the reference model given the initial correct steps, ensuring in-distribution data (self-generated) over out-of-distribution data (human/GPT-4-generated), which proves more effective. Performance:\nStep-DPO achieves up to 3% accuracy gain on MATH with as few as 10K data pairs and \u0026lt;500 training steps for 70B+ parameter models. Qwen2-72B-Instruct + Step-DPO reaches 70.8% on MATH and 94.0% on GSM8K, outperforming closed-source models like GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Ablation Studies:\nStep-DPO outperforms DPO by 1.8–2.6% on MATH. In-distribution data yields 0.7% higher accuracy than out-of-distribution data. Identity Preference Optimization (IPO) RLHF and DPO can overfit deterministic preferences because the logit transformation (Ψ) amplifies small preference differences near 1, weakening KL regularization. This leads to policies deviating from the reference policy, even with large regularization parameters.\nΨPO is defined as maximizing a non-linear function of preference probabilities (Ψ) balanced by KL regularization to a reference policy. RLHF and DPO are shown to be special cases when Ψ is the logit function, relying on the Bradley-Terry model.\nIdentity-PO (IPO) is an approach setting Ψ to the identity function, bypassing the Bradley-Terry model. IPO optimizes total preferences directly, maintaining effective regularization even with deterministic preferences. An empirical sampled loss is derived for practical implementation, avoiding reward modeling.\nPaper: A General Theoretical Paradigm to Understand Learning from Human Preferences.\nIPO Loss function:\n$$ \\mathbb{E}_{(y_w, y_l) \\sim D} \\left[ \\left( h_\\pi(y_w, y_l) - \\frac{\\tau^{-1}}{2} \\right)^2 \\right] $$IPO learns from preferences dataset simply by regressing the gap between log-likelihood ratios $\\log(\\pi(y_w)/\\pi(y_l))$ and $\\log(\\pi_{\\text{ref}}(y_w)/\\pi_{\\text{ref}}(y_l))$ to $\\frac{\\tau^{-1}}{2}$.\nSo the weaker the regularisation becomes, the higher would be the log-likelihood ratio of $y_w$ to $y_l$.\nIn other words IPO, unlike DPO, always regularizes its solution towards $\\pi_{\\text{ref}}$ by controlling the gap between the log-likelihood ratios $\\log(\\pi(y_w)/\\pi(y_l))$ and $\\log(\\pi_{\\text{ref}}(y_w)/\\pi_{\\text{ref}}(y_l))$, thus avoiding the over-fitting to the preference dataset.\n# ... calculate logits the same as DPO # https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/dpo_trainer.py#L974 # eqn (17) of the paper where beta is the regularization parameter for the IPO loss, denoted by tau in the paper. losses = (logits - 1 / (2 * self.beta)) ** 2 SPPO - Self-Play Preference Optimization for Language Model Alignment Paper: Wu, Yue, et al. Self-Play Preference Optimization for Language Model Alignment. arXiv:2405.00675, arXiv, 4 Oct. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2405.00675.\nDMPO: Direct Multi-Turn Preference Optimization Extends DPO to multi-turn dialogue by optimizing preferences across conversation history, improving coherence and relevance in multi-step interactions.\nPaper: Shi, Wentao, et al. Direct Multi-Turn Preference Optimization for Language Agents. arXiv:2406.14868, arXiv, 23 Feb. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2406.14868.\nWPO - Enhancing RLHF with Weighted Preference Optimization IV. Reference-Free Optimization Odds Ratio Preference Optimization (ORPO) ORPO (Odds Ratio Preference Optimization) combines SFT and preference optimization in a single monolithic training stage, eliminating reference model requirements. This approach streamlines training while maintaining competitive performance across multiple benchmarks.\nPaper: Hong, S., et al. (2024). ORPO: Monolithic Preference Optimization without Reference Model.\nExisting methods like RLHF and DPO often require:\nA supervised fine-tuning (SFT) warm-up stage. A reference model for comparison. Complex multi-stage processes, which are resource-intensive. The core insight: SFT can be enhanced to directly incorporate preference alignment by penalizing undesired generation styles, eliminating the need for extra stages or reference models.\nORPO Algorithm ORPO integrates preference alignment into SFT using an odds ratio to contrast favored (\\(y_w\\)) and disfavored (\\(y_l\\)) responses. The key components are:\nObjective Function: \\(\\mathcal{L}_{ORPO} = \\mathcal{L}_{SFT} + \\lambda \\cdot \\mathcal{L}_{OR}\\), where: \\(\\mathcal{L}_{SFT}\\) is the standard negative log-likelihood (NLL) loss for SFT. \\(\\mathcal{L}_{OR}\\) is a new loss term that maximizes the odds ratio between \\(y_w\\) and \\(y_l\\), defined as: \\[ \\mathcal{L}_{OR} = -\\log \\sigma\\left(\\log \\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)}\\right) \\] where \\(odds_\\theta(y|x) = \\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)}\\) and \\(\\sigma\\) is the sigmoid function. Gradient Analysis: The gradient of \\(\\mathcal{L}_{OR}\\) dynamically penalizes disfavored responses, accelerating adaptation to desired styles while preserving domain knowledge from SFT. Implementation from https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/orpo_trainer.py#L623\ndef odds_ratio_loss( self, policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, ) -\u0026gt; tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]: \u0026#34;\u0026#34;\u0026#34;Compute ORPO\u0026#39;s odds ratio (OR) loss for a batch of policy and reference model log probabilities. Args: policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,) policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,) Returns: A tuple of three tensors: (losses, chosen_rewards, rejected_rewards). The losses tensor contains the ORPO loss for each example in the batch. The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively. The log odds ratio of the chosen responses over the rejected responses ratio for logging purposes. The `log(sigmoid(log_odds_chosen))` for logging purposes. \u0026#34;\u0026#34;\u0026#34; # Derived from Eqs. (4) and (7) from https://huggingface.co/papers/2403.07691 by using log identities and exp(log(P(y|x)) = P(y|x) log_odds = (policy_chosen_logps - policy_rejected_logps) - ( torch.log1p(-torch.exp(policy_chosen_logps)) - torch.log1p(-torch.exp(policy_rejected_logps)) ) ratio = F.logsigmoid(log_odds) losses = self.beta * ratio chosen_rewards = self.beta * (policy_chosen_logps.to(self.accelerator.device)).detach() rejected_rewards = self.beta * (policy_rejected_logps.to(self.accelerator.device)).detach() return losses, chosen_rewards, rejected_rewards, torch.mean(ratio), torch.mean(log_odds) Advantages over Existing Methods**\nMonolithic Design: ORPO performs preference alignment in a single stage during SFT, unlike RLHF/DPO’s multi-stage workflows. No Reference Model: Avoids the need for a frozen SFT model, reducing memory usage and computational cost (half the forward passes per batch). Stability with Odds Ratio: Compared to probability ratios, the odds ratio provides a milder penalty, preventing over-suppression of disfavored responses (Figure 6). SimPO: Simple Preference Optimization with a Reference-Free Reward SimPO aligns the reward function with the generation metric, eliminates the need for a reference model, and introduces a target reward margin to enhance performance.\nIn DPO, for any triple $(x, y_w, y_l)$, satisfying the reward ranking $r(x, y_w) \u003e r(x, y_l)$ does not necessarily gaurantee the likelihood ranking $p_\\theta(y_w \\mid x) \u003e p_\\theta(y_l \\mid x)$. The paper found that only roughly 50% of the triples from a held-out set satisfy this condition when trained with DPO.\nPaper: SimPO: Simple Preference Optimization with a Reference-Free Reward.\nLength-Normalized Reward formulation: Replacing the reward formulation in DPO with $p_\\theta$ in the average log likelihood $$ p_\\theta(y \\mid x) = \\frac{1}{|y|} \\log \\pi_\\theta(y \\mid x) = \\frac{1}{|y|} \\sum_{i=1}^{|y|} \\log \\pi_\\theta(y_i \\mid x, y_{\\lt i}). $$ so that it aligns with the likelihood metric that guides generation, results in a length-normalized reward: $$ r_{\\text{SimPO}}(x, y) = \\frac{\\beta}{|y|} \\log \\pi_\\theta(y \\mid x) = \\frac{\\beta}{|y|} \\sum_{i=1}^{|y|} \\log \\pi_\\theta(y_i \\mid x, y_{\\lt i}) $$where $\\beta$ is a constant that controls the scaling of the reward difference.\nThis reward eliminates the need for a reference model, enhancing memory and computational efficiency.\nRemoving the length normalization term from the reward formulation results in a bias toward generating longer but lower-quality sequences.\nTarget Reward Margin: SimPO incorporates a target margin $\\gamma$ into the Bradley-Terry objective to ensure the reward difference between winning ($y_w$) and losing ($y_l$) responses exceeds $\\gamma$: $$ \\mathcal{L}_{\\text{SimPO}} = -\\mathbb{E}\\left[\\log \\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_{\\theta}(y_w|x) - \\frac{\\beta}{|y_l|}\\log\\pi_{\\theta}(y_l|x) - \\gamma\\right)\\right] $$This margin enhances the model’s ability to distinguish between high-quality and low-quality responses, improving generalization.\nThe comparison of SimPO with other offline preference optimization methods: RRHF and SLiC-HF are ranking losses. RRHF uses length-normalized log-likelihood, similar to SimPO’s reward function, while SLiCHF uses log-likelihood directly and includes an SFT objective. IPO is a theoretically grounded approach method that avoids DPO’s assumption that pairwise preferences can be replaced with pointwise rewards. CPO uses sequence likelihood as a reward and trains alongside an SFT objective. KTO learns from non-paired preference data. ORPO introduces a reference-model-free odd ratio term to directly contrast winning and losing responses with the policy model and jointly trains with the SFT objective. R-DPO is a modified version of DPO that includes an additional regularization term to prevent exploitation of length. This paper thoroughly tune the hyperparameters for each baseline and report the best performance. They find that many variants of DPO do not empirically present an advantage over standard DPO.\nImplementation: https://docs.pytorch.org/torchtune/0.3/_modules/torchtune/rlhf/loss/dpo.html#SimPOLoss.forward\ndef simpo_loss( policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, simpo_gamma, beta ): logits = (policy_chosen_logps - policy_rejected_logps) gamma_logratios = simpo_gamma / beta logits = logits - gamma_logratios losses = ( -F.logsigmoid(beta * logits) * (1 - label_smoothing) - F.logsigmoid(beta * logits) * label_smoothing ) return losses V. Non-preference-based methods KTO - Model Alignment as Prospect Theoretic Optimization The paper introduces Kahneman-Tversky Optimization (KTO), a approach maximizes the utility of generations instead of maximizing the log-likelihood of preferences(as DPO does).\nTheoretically, KTO align LLM with human preferences by framing alignment as a problem of prospect theoretic optimization. Drawing from Kahneman and Tversky’s prospect theory, which describes how humans make decisions under uncertainty with biases like loss aversion, the authors argue that existing alignment methods (e.g., DPO, PPO) implicitly incorporate these biases. They formalize these as Human-Aware Losses (HALOs) and propose KTO as a principled HALO that directly maximizes human utility rather than the log-likelihood of preferences.\nPaper: KTO: Model Alignment as Prospect Theoretic Optimization.\nKTO Method:\nDerives a HALO using Kahneman-Tversky’s value function, replacing exponents with logistic functions for stability. Requires only binary feedback (desirable/undesirable) instead of preference pairs, making data collection cheaper and more scalable. Introduces hyperparameters β (risk aversion) and λ_D/λ_U (loss aversion for desirable/undesirable outputs) to model human decision biases. Prospect Theory in Alignment:\nValue Function: Models human utility relative to a reference point, with concavity in gains and steeper slopes in losses (loss aversion). Reference Point: In KTO, the reference is the KL divergence between the current policy and the reference model, estimated via microbatch mismatches for stability. Loss Function:\n$\\lambda_y$ denotes $\\lambda_D$ ($\\lambda_U$) when $y$ is desirable(undesirable) respectively, the default KTO loss is: $$ \\mathcal{L}_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) \\triangleq \\mathbb{E}_{x,y \\sim D} \\left[ \\lambda_y - v(x,y) \\right] \\tag{8} $$ where $$ r_\\theta(x,y) = \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}, $$ $$ z_0 = \\text{KL}\\bigl(\\pi_\\theta(y'|x) \\big\\| \\pi_{\\text{ref}}(y'|x)\\bigr), $$ $$ v(x,y) = \\begin{cases} \\lambda_D \\sigma\\bigl(\\beta(r_\\theta(x,y) - z_0)\\bigr) \u0026 \\text{if } y \\sim y_{\\text{desirable}}|x, \\\\ \\lambda_U \\sigma\\bigl(\\beta(z_0 - r_\\theta(x,y))\\bigr) \u0026 \\text{if } y \\sim y_{\\text{undesirable}}|x. \\end{cases} $$Implementation from https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/kto_trainer.py#L1090:\ndef kto_loss( policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, policy_KL_logps: torch.FloatTensor, reference_chosen_logps: torch.FloatTensor, reference_rejected_logps: torch.FloatTensor, reference_KL_logps: torch.FloatTensor, ): kl = (policy_KL_logps - reference_KL_logps).mean().detach() # Chosen losses if policy_chosen_logps.shape[0] != 0 or reference_chosen_logps.shape[0] != 0: chosen_logratios = policy_chosen_logps - reference_chosen_logps chosen_losses = 1 - F.sigmoid(beta * (chosen_logratios - kl)) chosen_rewards = beta * chosen_logratios.detach() else: chosen_losses = torch.Tensor([]) chosen_rewards = torch.Tensor([]) # Rejected losses if policy_rejected_logps.shape[0] != 0 or reference_rejected_logps.shape[0] != 0: rejected_logratios = policy_rejected_logps - reference_rejected_logps rejected_losses = 1 - F.sigmoid(beta * (kl - rejected_logratios)) rejected_rewards = beta * rejected_logratios.detach() else: rejected_losses = torch.Tensor([]) rejected_rewards = torch.Tensor([]) losses = torch.cat( (chosen_losses, rejected_losses), 0, ) return losses VI. Others SPO - A Minimaximalist Approach to Reinforcement Learning from Human Feedback Uses self-play to generate preference data, where models compete to maximize rewards. It employs a minimax approach to balance exploration and exploitation. Paper: A Minimaximalist Approach to Reinforcement Learning from Human Feedback.\nVII. Key Trends and Challenges []: https://github.com/volcengine/verl\nScalability: Methods like RLHF and DPO face challenges with large models (e.g., 405B parameters) due to memory and computational costs. Frameworks like Verl address this via asynchronous distributed training . Data Efficiency: SimPO and ORPO reduce reliance on reference models and pairwise data, while Step-DPO and KTO focus on fine-grained optimization . Generalization: SPO and DMPO aim to improve generalization across tasks (e.g., dialogue, math) by leveraging self-play and multi-turn optimization . Theoretical Foundations: KTO and SimPO provide theoretical insights into preference learning, linking RL to prospect theory and information theory . VIII. Future Directions Multi-Modality: IPO and DMPO highlight the need for alignment across text, video, and dialogue. Self-Supervised Learning: RLAIF and SPO explore AI-generated feedback to reduce human labeling. Efficiency: Schulman, John, et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, arXiv, 28 Aug. 2017. arXiv.org, http://arxiv.org/abs/1707.06347.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOuyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2212.08073.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLee, Harrison, et al. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. arXiv:2309.00267, arXiv, 3 Sept. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2309.00267.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShao, Zhihong, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, arXiv, 27 Apr. 2024. arXiv.org, http://arxiv.org/abs/2402.03300.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-approx.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAhmadian, Arash, et al. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs. arXiv:2402.14740, arXiv, 26 Feb. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2402.14740.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHuang, Shengyi, et al. A2C Is a Special Case of PPO. arXiv:2205.09123, arXiv, 18 May 2022. arXiv.org, https://doi.org/10.48550/arXiv.2205.09123.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLi, Ziniu, et al. ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. arXiv:2310.10505, arXiv, 16 May 2024. arXiv.org, https://doi.org/10.48550/arXiv.2310.10505.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHu, Jian, et al. REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models. arXiv:2501.03262, arXiv, 6 Apr. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.03262.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYu, Qiying, et al. DAPO: An Open-Source LLM Reinforcement Learning System at Scale. arXiv:2503.14476, arXiv, 20 May 2025. arXiv.org, https://doi.org/10.48550/arXiv.2503.14476.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLiu, Zichen, et al. Understanding R1-Zero-Like Training: A Critical Perspective. arXiv:2503.20783, arXiv, 26 Mar. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2503.20783.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFeng, Lang, et al. Group-in-Group Policy Optimization for LLM Agent Training. arXiv:2505.10978, arXiv, 16 May 2025. arXiv.org, https://doi.org/10.48550/arXiv.2505.10978.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGuo, Shangmin, et al. Direct Language Model Alignment from Online AI Feedback. arXiv:2402.04792, arXiv, 29 Feb. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2402.04792.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nToken-level Direct Preference Optimization.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGorbatovski, Alexey, et al. Learn Your Reference Model for Real Good Alignment. arXiv:2404.09656, arXiv, 25 Feb. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2404.09656.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/","summary":"\u003cp\u003eIn the race to build truly helpful AI assistants, we\u0026rsquo;ve discovered a fundamental truth: raw intelligence isn\u0026rsquo;t enough. A model that masters calculus but can\u0026rsquo;t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.\u003c/p\u003e\n\u003cp\u003eThis is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, \u003cstrong\u003eReinforcement Learning from Human Feedback (RLHF)\u003c/strong\u003e reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF\u0026rsquo;s complexity was showing:\u003c/p\u003e","title":"Awesome Large Language Model (LLM) Post-training - [2025 Update]"},{"content":"There have been many questions about whether DPO is a form of imitation learning or (offline) reinforcement learning. The more I observe the distributions of DPO\u0026rsquo;s chosen and rejection losses, the stronger the feeling becomes that DPO is more like a form of imitation learning.\nThe paper, Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.050791 also expresses that DPO is a form of imitation learning.\nDirect Imitation Learning (DIL) This study explores the alignment problem between large language models and preference data from the perspective of imitation learning. Researchers establish a close theoretical connection between Reinforcement Learning from Human Feedback (RLHF) and Imitation Learning (IL), revealing that RLHF implicitly performs imitation learning on the preference data distribution. Based on this connection, Direct Imitation Learning (DIL) is proposed, a principled framework that directly optimizes the imitation learning objective. DIL provides a unified imitation learning perspective for the alignment problem, encompassing existing alignment algorithms as special cases while naturally introducing new variants. By linking IL and RLHF, DIL offers new insights into alignment with RLHF. Extensive experiments show that DIL outperforms existing methods on various challenging benchmarks.\nBackground Aligning Large Language Models (LLMs) with human preferences is crucial to ensuring that the responses generated by LLMs meet human expectations.\nThe Proposal of RLHF and Its Issues In recent years, Reinforcement Learning from Human Feedback (RLHF) has emerged as a widely adopted framework for fine-tuning language models based on human preference data.\nDPO Addressing RLHF\u0026rsquo;s Issues and Its Own Limitations RLHF relies on a two-step reinforcement learning process, which leads to problems such as low computational efficiency and instability during training. To alleviate these limitations, researchers have proposed alternative one-stage methods, such as Direct Preference Optimization (DPO) and its variants. These methods replace RLHF with supervised learning, eliminating the need for explicit reward modeling. Instead, they directly define an implicit reward based on the likelihood of preference data, significantly improving efficiency while maintaining competitive performance.\nProblems with DPO and Triggered Reflections Although DPO theoretically aims to find the same optimal policy as RLHF, it and its variants essentially still follow a reward maximization objective, determined by parametric models (e.g., the Bradley-Terry (BT) model). This makes them prone to overfitting, leading to suboptimal alignment with preference data. This raises a fundamental and open research question: Can we understand and design effective preference optimization algorithms from a new perspective?\nSignificance of This Study This paper re-examines RLHF from the perspective of imitation learning. Specifically, researchers show that RLHF is a special case of a general imitation learning problem, expressed solely through pairwise preferences. They theoretically demonstrate that alignment with RLHF is highly similar to imitation learning and implicitly optimizes the same objective. Leveraging this insight, they design DIL, a general framework for effective alignment based on density ratio reward estimation.\nKey Contributions It is proven that RLHF for alignment is essentially an imitation learning problem, providing a novel analysis that offers clear guidance for the design of alignment algorithms. DIL, a simple and general imitation learning alignment framework, is proposed. DIL unifies imitation learning on preference data and bridges the gap between density ratio estimation and preference alignment. Empirically, the effectiveness of DIL is verified on widely used benchmarks, demonstrating its superiority over previous alignment methods. Theoretical Derivations Preliminary Knowledge Problem Setup\nLet \\( x = [x_1, x_2, \\ldots] \\) be the input prompt, \\( y_w = [y_1, y_2, \\ldots] \\) be the positive sample (preferred response), and \\( y_l \\) be the negative sample (non-preferred response). These two samples are typically drawn from the same reference policy \\( \\pi_{\\text{ref}}(y|x) \\). Meanwhile, \\( y_w \\succ y_l | x \\) indicates that for the same input \\( x \\), \\( y_w \\) is more in line with human preferences than \\( y_l \\). Thus, the preference distribution is generally expressed as:\n\\[ p(y_w \\succ y_l | x) = g(r(x, y_w) - r(x, y_l)) \\tag{1} \\]\nHere, \\( g \\) refers to the sigmoid function \\( \\sigma(x) = \\frac{1}{1+e^{-x}} \\), a conclusion derived from the Bradley-Terry model (which can be verified by dividing the numerator and denominator by \\( r(x, y_l) \\) to convert it into a sigmoid form). Given a preference dataset \\( \\mathcal{D} \\) containing feedback, where each data entry is formatted as \\( (x, y_w, y_l) \\), our alignment goal is to learn an LLM policy \\( \\pi(y|x) \\) based on the preference data.\nReinforcement Learning from Human Feedback (RLHF)\nGiven an estimated reward function \\( r(x, y) \\), RLHF fine-tunes the policy \\( \\pi_\\theta \\) according to human preferences through the following optimization objective:\n\\[ \\mathop{\\max}\\limits_{\\pi_\\theta}\\mathbb{E}_{y \\sim \\pi_\\theta(y|x)}[r(x, y)] - \\beta\\mathbb{D}_{KL}[\\pi_\\theta(y|x)||\\pi_{\\text{ref}}(y|x)] \\tag{2} \\]\nThe core idea of this formula is to maximize the reward signal of human preferences while preventing the model from deviating too much from the original pre-trained distribution (to avoid collapse). Here, \\( \\mathbb{D}_{KL} = \\mathbb{E}_{y \\sim \\pi_{\\theta}} \\left[ \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right] \\) is the Kullback-Leibler divergence, describing the difference between the model\u0026rsquo;s strategy distribution and the original reference model\u0026rsquo;s. \\( \\beta \u003e 0 \\) is an appropriate KL penalty coefficient. Optimization methods typically use RL approaches such as Proximal Policy Optimization (PPO).\nReward Modeling\nStandard reward modeling uses the BT preference model in Equation (1) to fit a reward function \\( r_\\phi(x, y) \\). Specifically, the reward function can be estimated by maximizing the log-likelihood of preference feedback \\( (x, y_w, y_l) \\):\n\\[ \\mathcal{L}_{\\text{RM}}(\\phi;\\mathcal{D})=\\mathbb{E}_{(x, y_w,y_l)\\sim\\mathcal{D}}[-\\log\\sigma(r_{\\phi}(x,y_w)-r_{\\phi}(x,y_l))] \\tag{3} \\] Supervised Fine-Tuning (SFT)\nGiven a demonstration dataset \\( \\mathcal{D} \\), the goal of SFT is to minimize the negative log-likelihood of the model on the demonstration dataset:\n\\[ \\mathcal{L}_{\\text{SFT}}(\\theta;\\mathcal{D})=-\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\log\\pi_{\\theta}(y|x)] \\tag{4} \\]\nSFT is equivalent to Behavior Cloning (BC), a classic offline imitation learning method. Its goal is to minimize the forward KL divergence between the learned policy \\( \\pi_{\\theta} \\) and the data policy \\( \\pi_{\\text{data}} \\):\n\\[ \\mathop{\\min}\\limits_{\\theta}KL(\\pi_{\\text{data}}(y|x)||\\pi_\\theta(y|x))=-\\mathbb{E}_{\\pi_{\\text{data}}(y|x)}[\\log\\pi_{\\theta}(y|x)] \\tag{5} \\]\nIt is evident that SFT and BC have the same optimal solution.\nDirect Preference Optimization (DPO)\nTo simplify RLHF\u0026rsquo;s optimization process, DPO uses the log-likelihood of the learned policy to implicitly represent the reward function:\n\\[ r_\\theta(x,y)=\\beta[\\log\\pi_\\theta(y|x)-\\log\\pi_{\\text{ref}}(y|x)] + \\beta\\log Z_\\theta(x) \\tag{6} \\]\nHere, \\( Z_\\theta(x)=\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y)/\\beta) \\) is the partition function.\nConcept Supplement: Partition Function\nThe partition function ensures that the probability distribution is normalized, i.e., the sum of probabilities of all possible states equals 1. Specifically, in a probabilistic model, given an input \\( x \\), the probability of output \\( y \\) can be expressed as:\n\\[ p(y|x) = \\frac{1}{Z(x)} \\exp(-E(y, x)) \\]\nwhere \\( E(y, x) \\) is the energy function, measuring the \u0026ldquo;mismatch\u0026rdquo; or \u0026ldquo;cost\u0026rdquo; of a specific output \\( y \\) for a given input \\( x \\). The partition function \\( Z(x) \\) is defined as the sum of energy exponents over all possible outputs \\( y \\):\n\\[ Z(x) = \\sum_y \\exp(-E(y, x)) \\]\nFor continuous variables, the sum is replaced by an integral:\n\\[ Z(x) = \\int \\exp(-E(y, x)) dy \\]\nDoes this principle resemble the softmax function? Indeed, softmax is an application of the partition function.\nReturning to the reward function, rearranging terms gives:\n\\[ r_\\theta(x,y)+\\beta\\log\\pi_{\\text{ref}}(y|x)-\\beta\\log Z_\\theta(x)=\\beta\\log\\pi_\\theta(y|x) \\]\nApplying the natural exponential function to both sides of the equation:\n\\[ \\frac{\\exp(r_\\theta(x,y))e^\\beta\\pi_{\\text{ref}}(y|x)}{e^\\beta Z_\\theta(x)}=e^\\beta\\pi_\\theta(y|x) \\]\nSince \\( Z_\\theta(x)=\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y)/\\beta)=e^{-\\beta}\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y)) \\), substituting into the equation and eliminating common factors on both sides yields:\n\\[ \\pi_\\theta(y|x)=\\frac{\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y))}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y))} \\]\nIntuitively, the higher the reward corresponding to parameter \\( \\theta \\), the closer the model policy \\( \\pi_\\theta(y|x) \\) is to 1 (i.e., the higher the probability of adopting that policy), while the model policy \\( \\pi_\\theta(y|x) \\) is normalized to a distribution where the sum of probabilities equals 1—this is the essence of the reward function design.\nBy incorporating this reward into the BT model in Equation (1) and simplifying, DPO\u0026rsquo;s objective promotes the comparison and differentiation of preferred and non-preferred data:\n\\[ \\mathcal{L}_{\\text{DPO}}(\\theta;\\mathcal{D})=\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\left[-\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right] \\tag{7} \\] Energy-based Models (EBMs)\nEBMs define distributions through energy functions. For \\( y\\in \\mathbb{R}^D \\), the probability density can be expressed as:\n\\[ p_\\theta(y)=\\exp(-E_\\theta(y)/Z_\\theta(y)) \\tag{8} \\]\nwhere \\( E_\\theta(y):\\mathbb{R}^D\\rightarrow\\mathbb{R} \\) is the energy function, mapping \\( y \\) to a scalar, and \\( Z_\\theta(y) = \\sum_y \\exp(-E_\\theta(y)) \\) is the unknown normalization constant, as mentioned in the supplementary knowledge about the partition function.\nCore Derivations The derivations are mainly refereced from this blog2\n1 RLHF as a Form of Imitation Learning Researchers demonstrate that RLHF is a special case of imitation learning based on reverse KL divergence over the distribution of selected responses.\nThe specific proof is as follows:\nFirst, define the following policy based on energy-based models (EBMs):\n\\[ \\pi_\\phi(y|x)=\\pi_{\\text{ref}}(y|x)\\exp(r_\\phi(x,y))/Z_\\phi(x) \\tag{9} \\]\nwhere \\( \\phi \\) denotes model parameters, and as described in the preliminary knowledge, \\( Z_\\phi(x)=\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\phi(x,y)) \\).\nTo learn parameter \\( \\phi \\), Behavior Cloning (BC)—a classic and widely used imitation learning method (as mentioned in the preliminary knowledge, SFT is equivalent to BC)—can be applied. This method formulates the task as minimizing the KL divergence between the policy \\( \\pi_\\phi \\) and the expert policy \\( \\pi_{\\text{chosen}} \\) that generates preferred responses \\( y_w \\). In other words, IL learns parameter \\( \\phi \\) such that the model distribution imitates the distribution of preferred responses in the preference dataset:\n\\[ \\mathop{\\min}\\limits_{\\phi}KL(\\pi_{\\text{chosen}}(y|x)||\\pi_\\phi(y|x)) \\tag{10} \\]By selecting responses from the preference data to minimize the above forward KL divergence:\n\\[ \\mathop{\\min}\\limits_{\\phi}\\mathbb{E}_{(x,y_w)\\sim\\mathcal{D}}[-\\log\\pi_{\\text{ref}}(y_w|x)\\exp(r_\\phi(x,y_w))/Z_\\phi(x)] \\Rightarrow \\\\\\mathop{\\min}\\limits_{\\phi}\\mathbb{E}_{(x,y_w)\\sim\\mathcal{D}}\\left[-r_\\phi(x,y_w)+\\log\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\phi(x,y))\\right] \\tag{11} \\]\n(Equation (11) is obtained by removing constants and substituting the partition function.)\nThere are multiple choices for sampling from the reference distribution \\( \\pi_{\\text{ref}}(y|x) \\). One setting that simplifies the above expression and practically yields RLHF is: \\( \\pi_{\\text{ref}}(y|x) = \\frac{1}{2} \\mathbb{I}(Y = y_l) + \\frac{1}{2} \\mathbb{I}(Y = y_w) \\) (note: this is a key approximation that allows RLHF to be reduced to IL). In this case, the sample-based approximation of the second term is:\n\\[ \\mathop{\\min}\\limits_{\\phi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\left[-r_\\phi(x,y_w)+\\log(\\exp(r_\\phi(x,y_w)) + \\exp(r_\\phi(x,y_l)))\\right] \\]This equation is derived by substituting \\( \\sum_y \\pi_{\\text{ref}}(y|x)\\exp(r_\\varphi(x, y)) = \\frac{1}{2} \\exp(r_\\varphi(x, y_w)) + \\frac{1}{2} \\exp(r_\\varphi(x, y_l)) \\) into Equation (11) and removing the constant term \\( -\\log2 \\). Further merging the \\( -r_\\phi(x,y_w) \\) term into the right-hand side \\( \\log(\\exp(r_\\phi(x,y_w)) + \\exp(r_\\phi(x,y_l))) \\) and simplifying gives the equivalent form:\n\\[ \\mathop{\\min}\\limits_{\\phi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\left[-\\log\\sigma(r_\\phi(x,y_w) - r_\\phi(x,y_l))\\right] \\tag{12} \\]It can be noted that the imitation learning loss based on the energy strategy is identical to the reward loss of RLHF based on the BT assumption (Equation (3)). By optimizing this loss function, we can directly obtain the optimal energy strategy in Equation (9). Unfortunately, even if we use the estimated reward function \\( r_\\phi \\), estimating the partition function \\( Z_\\phi(x) \\) remains costly, making the derived representation impractical and leading to significantly increased inference costs.\nKey Discussion: Why This Method Is Costly\nExcessively Large Output Space\nThe output of a language model is a sequence of tokens; for example, a response may contain dozens or even hundreds of tokens. Each token has thousands to tens of thousands of candidate words (depending on the vocabulary size), resulting in an exponentially growing number of total output combinations:\nAssuming 50,000 word choices per step and generating 20 tokens, there are \\( 50000^{20} \\) possible responses!\nThis means it is impossible to enumerate all \\( y \\) to compute \\( Z_\\phi(x) \\).\nRequiring Extensive Sampling to Approximate Summation\nSince enumeration is impossible, we can only estimate the summation through sampling:\n\\[ Z_\\phi(x) \\approx \\frac{1}{N} \\sum_{i=1}^N \\exp(r_\\phi(x, y_i)), \\quad y_i \\sim \\pi_{\\text{ref}}(y|x) \\]\nHowever, to ensure estimation accuracy, it is necessary to:\nSample enough \\( y_i \\) Run the reward model \\( r_\\phi(x, y_i) \\) for each \\( y_i \\)\nThis leads to: High computational resource consumption (running the reward model many times per inference) Increased inference time Reward Models May Be \u0026ldquo;Heavy\u0026rdquo;\nThe reward model \\( r_\\phi(x, y) \\) is typically a large neural network (e.g., based on the GPT architecture), which is already time-consuming to run once. Running it multiple times on multiple samples incurs significant costs.\nTo address the issue of high computational costs, researchers propose a method. Before introducing this method, we first introduce forward and reverse KL divergences (those already familiar can skip this part):\nConcept Supplement: Forward and Reverse KL Divergences\nBasic Concept of KL Divergence\nKL divergence (Kullback-Leibler Divergence), also known as relative entropy, measures the difference between two probability distributions \\( P \\) and \\( Q \\). For discrete distributions, KL divergence is defined as:\n\\[ D_{\\text{KL}}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\]\nFor continuous distributions, the sum is replaced by an integral:\n\\[ D_{\\text{KL}}(P \\| Q) = \\int P(x) \\log \\frac{P(x)}{Q(x)}dx \\] Forward KL Divergence (\\( D_{\\text{KL}}(P \\| Q) \\))\nGoal: Make \\( Q \\) as close as possible to \\( P \\). Characteristics: Heavily penalizes events with high probability in \\( P \\) but low or zero probability in \\( Q \\). In other words, it focuses on accurately capturing all modes in \\( P \\). Application Scenarios: When there is a true distribution \\( P \\) (e.g., the true distribution of data) and a model \\( Q \\) needs to be trained to approximate this distribution, forward KL divergence is typically used. This is because forward KL encourages the model to cover all modes, even if it means generating some low-probability but existing samples. Properties: If \\( P \\) has a point with probability greater than 0 where \\( Q \\) has zero probability, the KL divergence tends to infinity. Thus, using forward KL divergence, the model tends to cover more possibilities, even unlikely events. Reverse KL Divergence (\\( D_{\\text{KL}}(Q \\| P) \\))\nGoal: Make \\( P \\) as close as possible to \\( Q \\). Characteristics: More focused on avoiding generating events with very low probability in \\( P \\) but high probability in \\( Q \\). That is, it tends to find the most likely modes and ignore less likely cases. Application Scenarios: In some cases, such as Generative Adversarial Networks (GANs) or when a more deterministic model output is desired, reverse KL divergence may be a better choice. Because it encourages the model to focus on the most likely outcomes rather than trying to cover all possibilities. Properties: Unlike forward KL, reverse KL allows \\( Q \\) to be zero in some regions as long as \\( P \\) is also zero or very small there. This means reverse KL can produce sparser and more concentrated distributions, sometimes leading to \u0026ldquo;mode collapse\u0026rdquo;—generating only a few specific types of samples while ignoring others. Using reverse knowledge distillation (which employs reverse KL divergence, with characteristics consistent with those of reverse KL divergence in the model distillation process), the optimal strategy in Equation (9) is \u0026ldquo;distilled\u0026rdquo; into a strategy with an analytical form using reverse KL divergence, enabling the final strategy \\( \\pi_\\theta \\) to require only one sampling during inference (note: here \\( \\theta \\) is the main model parameter, and \\( \\phi \\) is the reward model parameter):\n\\[ \\mathop{\\min}\\limits_{\\theta}KL\\left(\\pi_\\theta(y|x)||\\pi_{\\text{ref}}(y|x)\\exp(r_\\phi(x,y)/\\beta)/Z_\\phi(x)\\right) \\tag{13} \\]\nwhere \\( \\beta \\) is the temperature hyperparameter in the distillation process. After isolating the term \\( -\\mathbb{E}_{\\pi_\\theta(y|x)}[r_\\phi(x,y)] \\), removing multiplicative and additive constants, and combining the remaining terms into \\( \\beta KL(\\pi_\\theta(y|x)||\\pi_{\\text{ref}}(y|x)) \\), it is transformed into the following objective function:\n\\[ \\mathcal{L}(\\theta)=-\\mathbb{E}_{\\pi_\\theta(y|x)}[r_\\phi(x,y)] + \\beta KL(\\pi_\\theta(y|x)||\\pi_{\\text{ref}}(y|x)) \\tag{14} \\]It can be observed that this distillation objective corresponds exactly to the objective of RLHF in Equation (2). Thus, researchers provide two key conclusions:\n(i) Reward learning in RLHF is equivalent to an imitation learning problem for preferred responses, achieved by minimizing the forward KL divergence between \\( \\pi_{\\text{chosen}} \\) and \\( \\pi_\\phi \\) based on energy-based models (EBMs), as shown in Equation (12);\n(ii) The RL step in RLHF can be interpreted as a reverse knowledge distillation process, where the EBM-based imitation strategy \\( \\pi_\\phi \\) is distilled into the final analytical strategy \\( \\pi_\\theta \\) by minimizing the reverse KL divergence in Equation (13), with the temperature parameter \\( \\beta \\) determining the degree of KL regularization.\nThis problem is transformed into the following proposition:\nAssume the preferred response distribution \\( p(y|x) \\), the energy-based model \\( \\pi_\\phi(y|x) \\), and the model \\( \\pi_\\theta(y|x) \\). When \\( \\beta = 1 \\), KL-regularized RLHF can be regarded as the following problem:\n\\[ \\min_{\\pi_\\theta} \\mathrm{KL}(\\pi_\\theta \\| \\pi^*_\\phi) \\quad \\text{s.t.} \\quad \\pi^*_\\phi = \\arg\\min_{\\pi_\\phi} \\mathrm{KL}(\\pi_{\\text{chosen}} \\| \\pi_\\phi) \\tag{15} \\]\nwhere \\( \\pi_{\\text{chosen}}(y|x) = \\pi_\\phi(y|x) = \\pi_\\theta(y|x) \\) is the equilibrium state.\nThus, imitation learning on preferred responses is equivalent to solving a standard KL-regularized RLHF problem.\nFurthermore, we observe that when \\( \\pi^*_\\phi = \\pi_{\\text{chosen}} \\) (i.e., the optimal solution achieved by the lower-level objective), the upper-level objective essentially optimizes a reverse KL divergence \\( \\mathrm{KL}(\\pi_\\theta \\| \\pi_{\\text{chosen}}) \\).\nAt this point, I am truly amazed; this proof process is akin to Maxwell\u0026rsquo;s equations unifying electromagnetism in physics.\nTo this end, we have proven that RLHF is a special type of IL, with equivalent conditions:\nUsing EBM and forward KL divergence to fit the Reward Model (RM) Using EBM and reverse knowledge distillation based on reverse KL divergence to complete RL training The proof is complete. Researchers then pose an interesting question:\nWhy does SFT — which directly optimizes the forward KL divergence \\( \\mathrm{KL}(\\pi_{\\text{chosen}} \\| \\pi_\\theta) \\) in Equation (5) — perform worse than RLHF in alignment tasks?\nTheoretically, minimizing the objective functions of SFT and RLHF should lead to the same optimal solution \\( \\pi_\\theta \\). However, in practice, this requires complete data coverage and unlimited computational resources, conditions rarely met.\nThus, in practical settings, minimizing different KL divergences results in learned policies with distinct characteristics. Specifically, the forward KL divergence \\( \\mathrm{KL}(\\pi_{\\text{chosen}} \\| \\pi_\\theta) \\) promotes mass-covering behavior, while the reverse KL divergence \\( \\mathrm{KL}(\\pi_\\theta \\| \\pi_{\\text{chosen}}) \\) encourages mode-seeking behavior (see supplementary knowledge on forward and reverse KL divergences above).\nMass-covering behavior tends to assign similar probabilities to all responses in the dataset, overestimating the long-tail portion of the target distribution; mode-seeking behavior concentrates probability mass in specific high-reward regions. Therefore, the goal of alignment is to generate a certain type of high-quality response, which can be more effectively achieved by minimizing the reverse KL divergence.\nIn summary, the reason RLHF outperforms SFT is that the full performance of forward KL divergence requires complete data coverage and unlimited computational resources, which is nearly impossible to achieve. In contrast, reverse KL divergence, although unable to learn to cover the distribution, can still learn high-quality responses, resulting in better performance of reverse KL divergence over forward KL divergence.\n2 Direct Imitation Learning (DIL) In the previous section, we re-examined RLHF from the perspective of imitation learning. The analysis clearly indicates that RLHF is essentially optimized to closely align with the distribution of preferred responses. The sample-based approximation of EBM in RLHF leads to a reward loss similar to the BT model, as shown in Equation (12). However, the BT assumption does not always hold. Based on these insights, researchers propose a new alignment method, DIL (Direct Imitation Learning), that does not rely on the BT assumption. Thus, the objective of imitation learning is directly formulated as minimizing the reverse KL divergence between \\( \\pi_\\theta \\) and the unknown preferred response distribution \\( \\pi_{\\text{chosen}} \\):\n\\[ \\min_\\theta L_{\\text{DIL}}(\\theta) = \\mathrm{KL} \\left( \\pi_\\theta(y|x) \\| \\pi_{\\text{chosen}}(y|x) \\right) = \\mathbb{E}_{\\pi_\\theta(y|x)} \\left[ \\log \\left( \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{chosen}}(y|x)} \\right) \\right] \\tag{16} \\]\nHere, we minimize the reverse KL divergence, unlike SFT which minimizes the forward KL divergence as in Equation (5). However, using reverse KL divergence for mode concentration is typically challenging. Directly optimizing Equation (16) cannot effectively utilize preference data, especially since the data policy \\( \\pi_{\\text{chosen}} \\) is unknown. In reinforcement learning literature, these challenges are addressed through adversarial training. However, such methods require complex and unstable adversarial training to learn the reward function, which is impractical for large models. In this paper, a simple alternative is proposed to directly utilize offline human preference data without learning the reward function through adversarial training. The DIL objective is reformulated as follows:\n\\[ \\max_\\theta \\mathbb{E}_{\\pi_\\theta(y|x)} \\left[ \\log \\frac{\\pi_{\\text{chosen}}(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right]=\\\\ \\mathbb{E}_{\\pi_\\theta(y|x)} \\left[ \\log r(x, y) \\right] - \\mathrm{KL} \\left( \\pi_\\theta(y|x) \\parallel \\pi_{\\text{ref}}(y|x) \\right) \\tag{17} \\]\nwhere \\( r(x, y) \\triangleq \\frac{\\pi_{\\text{chosen}}(y|x)}{\\pi_{\\text{ref}}(y|x)} \\) can be regarded as an auxiliary reward function. Equations (16) and (17) are equivalent by adding and subtracting the same term \\( \\log \\pi_{\\text{ref}}(y|x) \\) in the expectation.\nInterestingly, researchers find that even when only preference data is available, the form of this objective function is similar to that of RLHF in Equation (2). The main difference is that the reward here is the estimated log density ratio, which is often difficult to obtain directly in practice. Optimizing this objective involving the density ratio \\( r(x, y) \\) is non-intuitive and challenging. The next section will show how to efficiently optimize this objective function by effectively utilizing offline human preference data.\n3 Density Ratio Reward Estimation Before delving into the problem in Equation (17), we first describe how to compute the auxiliary reward function based on the density ratio. In a tabular setting, we can directly compute \\( \\pi_{\\text{ref}}(y|x) \\) and \\( \\pi_{\\text{chosen}}(y|x) \\). However, in high-dimensional language domains, estimating densities separately and computing their ratios is ineffective due to error accumulation.\nBefore introducing the solution, we first understand Bregman divergence:\nConcept Supplement: Bregman Divergence\nBregman Divergence is a measure of the difference between two points, defined by the properties of convex functions. Specifically, given a strictly convex and twice differentiable function \\( F \\), Bregman divergence is defined as the difference between the function and its linear approximation at a certain point.\nDefinition\nSuppose \\( F \\) is a strictly convex function defined on a convex set. The Bregman divergence \\( D_F \\) generated by \\( F \\) can be defined as:\n\\[ D_F(\\mathbf{p} \\| \\mathbf{q}) = F(\\mathbf{p}) - F(\\mathbf{q}) - \\langle \\nabla F(\\mathbf{q}), (\\mathbf{p} - \\mathbf{q}) \\rangle \\]\nwhere:\n\\( \\mathbf{p} \\) and \\( \\mathbf{q} \\) are two points in the space; \\( \\nabla F(\\mathbf{q}) \\) denotes the gradient of \\( F \\) at point \\( \\mathbf{q} \\); \\( \\langle \\cdot, \\cdot \\rangle \\) denotes the inner product operation. Simply put, \\( D_F(\\mathbf{p} \\| \\mathbf{q}) \\) measures the gap between the value of function \\( F \\) at point \\( \\mathbf{p} \\) and the first-order Taylor expansion of \\( F \\) at point \\( \\mathbf{q} \\).\nProperties\nNon-negativity: For all \\( \\mathbf{p} \\) and \\( \\mathbf{q} \\), \\( D_F(\\mathbf{p} \\| \\mathbf{q}) \\geq 0 \\), with equality if and only if \\( \\mathbf{p} = \\mathbf{q} \\). Asymmetry: In general, \\( D_F(\\mathbf{p} \\| \\mathbf{q}) \\neq D_F(\\mathbf{q} \\| \\mathbf{p}) \\), meaning Bregman divergence is not a symmetric measure. Sandwich Inequality: Bregman divergence does not satisfy the triangle inequality, i.e., for any three points \\( \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\), it is not necessarily true that \\( D_F(\\mathbf{x} \\| \\mathbf{y}) + D_F(\\mathbf{y} \\| \\mathbf{z}) \\geq D_F(\\mathbf{x} \\| \\mathbf{z}) \\). Common Examples\nDifferent convex functions \\( F \\) yield different types of Bregman divergences. Here are some common examples:\nSquared Euclidean distance: If \\( F(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 \\), the corresponding Bregman divergence is the square of the Euclidean distance. Kullback-Leibler divergence: If \\( F(\\mathbf{x}) = \\sum_i x_i \\log x_i \\), the corresponding Bregman divergence is the KL divergence. Itakura-Saito distance: If \\( F(\\mathbf{x}) = -\\sum_i \\log x_i \\), the corresponding Bregman divergence is the Itakura-Saito distance. The solution proposed in this paper is to directly estimate the density ratio \\( \\pi_{\\text{chosen}}(y|x)/\\pi_{\\text{ref}}(y|x) \\) based on Bregman divergence. Assume the target density ratio is \\( r^*(x, y) = \\pi_{\\text{chosen}}(y|x)/\\pi_{\\text{ref}}(y|x) \\), and a parameterized discriminator \\( r_\\phi \\) is used to estimate this ratio:\n\\[ \\min_\\phi D_h(r^* \\| r_\\phi) =\\\\\\sum_y \\pi_{\\text{ref}}(y|x) B_h\\left(r^*(x, y) \\| r_\\phi(x, y)\\right) =\\\\\\sum_y \\pi_{\\text{ref}}(y|x) \\left[ h\\left(r^*(x, y)\\right) - h\\left(r_\\phi(x, y)\\right) - \\partial h\\left(r_\\phi(x, y)\\right) \\left(r^*(x, y) - r_\\phi(x, y)\\right) \\right] \\tag{18} \\]where \\( B_h \\) is the sample-level Bregman divergence.\nFor a twice continuously differentiable convex function \\( h \\) with a bounded derivative \\( \\partial h \\), this divergence measures the difference between two density ratios. By subtracting the constant term \\( \\sum_y \\pi_{\\text{ref}}(y|x) h(r^*(x, y)) \\) and substituting \\( r^*(x, y) = \\pi_{\\text{chosen}}(y|x)/\\pi_{\\text{ref}}(y|x) \\), we obtain (ignoring the constant term):\n\\[ \\sum_y \\pi_{\\text{ref}}(y|x) \\left[ \\partial h\\left(r_\\phi(x, y)\\right) r_\\phi(x, y) - h\\left(r_\\phi(x, y)\\right) \\right] - \\sum_y \\pi_{\\text{chosen}}(y|x) \\left[ \\partial h\\left(r_\\phi(x, y)\\right) \\right] \\tag{19} \\]Non-exhaustive examples of Bregman divergences include Least-Squared Importance Fitting (LSIF), Binary Cross Entropy (BCE), and unbounded Kullback-Leibler (UKL) divergence.\nFor example, LSIF defines \\( h_{\\text{LSIF}} = (r - 1)^2 / 2 \\), resulting in the following form of Bregman divergence for density ratios:\n\\[ \\min_\\phi D_{h_{\\text{LSIF}}}(r^* \\| r_\\phi) = \\sum_y \\frac{1}{2} \\pi_{\\text{ref}}(y|x) r_\\phi^2(x, y) - \\pi_{\\text{chosen}}(y|x) r_\\phi(x, y) \\tag{20} \\]In this case, a sample-based approximation of Equation (20) yields the following loss function:\n\\[ \\mathcal{L}(\\phi; \\mathcal{D}) = \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\frac{1}{2} r_\\phi^2(x, y_l) - r_\\phi(x, y_w) \\right] \\tag{21} \\]Here, the rejected (non-preferred) response set \\( y_l \\sim \\pi_{\\text{ref}}(y|x) \\) is used to approximate the expectation over \\( \\pi_{\\text{ref}}(y|x) \\). Researchers argue that using rejected responses \\( y_l \\) from the preference dataset \\( \\mathcal{D} \\) to approximate the expectation is reasonable; it is even possible to use both preferred and rejected responses. However, since the goal is to reduce the likelihood of rejected responses, rejected responses are chosen to approximate the expectation, and good performance is observed in subsequent experiments.\nIntuitively, the first term pushes the model to reduce the density ratio of rejected responses, while the second term increases the density ratio of preferred responses.\nFurthermore, this direct estimation method based on Bregman divergence indicates that there exists a viable family of divergences for density ratio estimation, as shown in Table 1; other \\( h \\) functions, such as BCE and UKL (introduced later), are further discussed in Appendix A. Researchers also empirically analyze the impact of different \\( h \\) function objectives in Section 6.3.\n4 Method Optimization Thus far, it has been observed that combining the RL-like objective in Equation (17) with the density ratio estimation method in Equation (21) can effectively utilize preference datasets for imitation learning. However, this two-stage process is complex and unstable: first, a reward model needs to be fitted to estimate the density ratio, and then the language model policy is fine-tuned using the RL-like objective in Equation (17).\nTo address these issues, a simpler method is introduced. First, note that the optimal policy in Equation (17) has a closed-form solution:\n\\[ \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\log r^*(x, y)\\right) \\tag{22} \\]where \\( Z(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\log r^*(x, y)\\right) = \\sum_y \\pi_{\\text{chosen}}(y|x) = 1 \\), meaning the optimal policy \\( \\pi^*(y|x) \\) is forced into a self-normalized form!\nThis property, determined by the definition of the reward function in Equation (17), offers a significant advantage: it allows our imitation learning to theoretically generalize to a broader class of loss functions than the pairwise BT preference model used in DPO.\nTaking the logarithm of both sides of Equation (22) and performing some algebraic operations yields the following expression:\n\\[ \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} = \\log r^*(x, y) \\tag{23} \\]\nwhere \\( r^*(x, y) \\) is the density ratio estimated from the preference dataset using Equation (21).\nSince the optimal density ratio is now represented by the optimal policy rather than a discriminator model, we can explicitly derive a maximum likelihood objective for the parameterized policy on the preference dataset. Similar to the approach used in density ratio estimation and leveraging variable substitution techniques, the DIL objective can be formalized as:\n\\[ \\mathcal{L}_{\\text{DIL}}(\\theta; \\mathcal{D}) = \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ - \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} + \\frac{1}{2} \\left( \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right)^2 \\right] \\tag{24} \\]where we use the alternative parameterization in Equation (23) to directly fit the density ratio implicitly defined in Equation (21).\nInterestingly, the loss function has no hyperparameters (those familiar with DPO and its variants will appreciate the value of \u0026ldquo;no hyperparameters,\u0026rdquo; as it eliminates the cost of hyperparameter tuning, greatly enhancing the feasibility of algorithm deployment in industrial scenarios), yet experiments show it still achieves satisfactory performance. Since the above process is equivalent to fitting a reparameterized density ratio estimation model, it theoretically performs imitation learning by minimizing the reverse KL divergence relative to the unknown preferred response distribution. Table 1 shows a family of objective functions satisfying the definition of Bregman divergence.\n5 Discussion: DPO as a Special Case of DIL Before proceeding, we introduce a method to prepare for the subsequent proof.\nConcept Supplement: Contrastive Predictive Coding\nContrastive Predictive Coding (CPC) — familiar to those in the speech domain — is a self-supervised learning method proposed by Oord et al. It is primarily used to learn effective representations from unlabeled datasets by leveraging local dependencies in sequence data.\nCore Idea\nCPC aims to enable the model to learn to predict future data points from current ones through contrastive learning, maximizing the mutual information between the current representation and future representations. Specifically, given a sequence \\( \\mathbf{x} = (x_1, x_2, \\ldots, x_T) \\), CPC attempts to learn an encoder \\( f_\\phi \\) that maps each time point \\( x_t \\) to a latent representation \\( z_t = f_\\phi(x_t) \\). A scoring function is then used to compute the similarity between \\( z_t \\) and the representation of a future time point \\( z_{t+k} \\), encouraging high scores for positive pairs (current and true future representations) and low scores for negative pairs (current and other time point representations).\nInfoNCE Loss Function\nCPC uses a specific contrastive loss called InfoNCE loss, defined as:\n\\[ \\mathcal{L}_{\\text{InfoNCE}} = -\\mathbb{E}_{(x_t, x_{t+k})} \\left[ \\log \\frac{\\exp(g(z_t, z_{t+k}))}{\\sum_{x_j} \\exp(g(z_t, z_j))} \\right] \\]\nwhere \\( g(z_t, z_j) \\) is a scoring function measuring the similarity between \\( z_t \\) and \\( z_j \\), and the expectation \\( \\mathbb{E} \\) averages over all possible time point pairs. This loss encourages the model to assign high scores to positive pairs and low scores to negative pairs.\nIn this section, researchers demonstrate that DPO can also be viewed as a special case of the DIL framework by using CPC for density ratio estimation. Given a prompt distribution \\( p(x) \\) and the conditional distribution of preferred responses \\( \\pi_{\\text{chosen}}(y|x) \\), we sample \\( x \\sim p(x) \\), \\( y_w \\sim \\pi_{\\text{chosen}}(y|x) \\), and \\( y_l \\sim \\pi_{\\text{ref}}(y|x) \\). CPC optimizes the following objective:\n\\[ \\mathcal{L}_{\\text{CPC}}(\\phi; \\mathcal{D}) = -\\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log \\frac{\\exp(f_\\phi(x^\\top y_w)/\\beta)}{\\exp(f_\\phi(x^\\top y_w)/\\beta) + \\exp(f_\\phi(x^\\top y_l)/\\beta)} \\right] \\tag{25} \\]where \\( f_\\phi: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R} \\) is a parameterized evaluation function.\nThe optimal evaluation function for this CPC with one negative sample satisfies the following condition:\n\\[ f^*(x, y)/\\beta = \\log \\frac{\\pi_{\\text{chosen}}(y|x)}{\\pi_{\\text{ref}}(y|x)} c(x) = \\log r^*(x, y) - \\log c(x) \\tag{26} \\]where \\( c(x) \\) is a function dependent only on \\( x \\) and not on \\( y \\). Thus, CPC also estimates the density ratio reward in the IL objective, as shown in Equation (17).\nSimilar to the previous section, using the closed-form optimal policy in Equation (22) and leveraging variable substitution, we obtain:\n\\[ \\mathcal{L}_{\\text{DIL}}(\\theta; \\mathcal{D}) = -\\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right] \\tag{27} \\]This is identical to DPO\u0026rsquo;s objective. Thus, DIL can reinterpret DPO. Specifically, researchers demonstrate that DPO also conforms to the imitation learning objective in Equation (16) and essentially uses the CPC method for density ratio reward estimation.\nIn summary, DPO is equivalent to DIL if it uses the CPC method for density ratio reward estimation.\nKey Discussion: Why the BT Assumption Reduces the Likelihood of Preferred Responses\nOversimplifying Preference Structures:\nThe BT model assumes that preferences between each pair of options can be compared independently and follow a specific probabilistic form. However, in practical applications, especially complex language generation tasks, this assumption may oversimplify the true preference structure. For example, preferences may be based on a combination of multiple complex factors, not just a simple comparison between two options. This may prevent the model from accurately capturing the factors that determine high-quality responses, thereby reducing the likelihood of preferred responses.\nData Bias and Noise:\nWhen using the BT model for preference estimation, if the training data contains bias or noise, the learned preference relationships may be inaccurate. For example, if certain types of responses are over-sampled or under-sampled due to biases in the data collection process, the BT model trained on such data may incorrectly estimate true preferences, leading to lower scores for preferred responses.\nLimitations of the Optimization Objective:\nUsing the BT model as the optimization objective may guide the model to optimize toward maximizing pairwise preference probabilities rather than directly optimizing the ability to generate high-quality responses. This may cause the model to sacrifice overall quality to improve win rates in specific comparisons, especially for responses with high intrinsic quality that are not easily highlighted in pairwise comparisons, whose likelihood may thus decrease.\nXiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.05079, arXiv, 7 Mar. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2503.05079.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://zhuanlan.zhihu.com/p/1910382777079165403\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://congchan.github.io/posts/connection-between-imitation-learning-and-rlhf/","summary":"\u003cp\u003eThere have been many questions about whether DPO is a form of imitation learning or (offline) reinforcement learning. The more I observe the distributions of DPO\u0026rsquo;s chosen and rejection losses, the stronger the feeling becomes that DPO is more like a form of imitation learning.\u003c/p\u003e\n\u003cp\u003e\u003ccite\u003eThe paper, Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.05079\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/cite\u003e also expresses that DPO is a form of imitation learning.\u003c/p\u003e","title":"Connection Between Imitation Learning and RLHF"},{"content":"Multi-token prediction vs Next-token prediction Next-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \\( x_{t+1} \\) given the context \\( x_{1:t} \\) (all tokens up to position \\( t \\)).\nThe cross-entropy loss for next-token prediction is defined as:\n\\[ L_1 = -\\sum_t \\log P_\\theta(x_{t+1} | x_{1:t}) \\]\nwhere \\( P_\\theta \\) is the model parameterized by \\( \\theta \\), and \\( x_{1:t} \\) denotes the sequence of tokens from position 1 to \\( t \\).\nThe model is trained in an autoregressive manner, where each prediction relies solely on the ground-truth history of tokens, not on its own prior predictions (teacher forcing).\nThe core idea of Multi-token prediction (MTP) (Gloeckle, Fabian, et al. Better \u0026amp; Faster Large Language Models via Multi-Token Prediction. arXiv:2404.19737, arXiv, 30 Apr. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2404.19737.) is to enable the model to predict multiple future tokens simultaneously at each position in the training corpus, which improves sample efficiency and inference speed.\nComparison Between Next-Token Prediction and Multi-Token Prediction\nAspect Next-Token Prediction Multi-Token Prediction (MTP) in the Papers Prediction Scope Single future token \\( n \\) future tokens simultaneously (e.g., \\( n=4 \\)) Training Efficiency Requires more data due to local focus Improves sample efficiency by capturing long-term dependencies Inference Speed Sequential decoding (slow for long sequences) Speculative decoding via extra heads (3× faster in Gloeckle et al., 1.8× TPS in DeepSeek-V3) Architectural Cost Simple single head Additional heads but with minimal overhead (sequential computation in Gloeckle et al., shared modules in DeepSeek-V3) Multi-token prediction Gloeckle et al. argue that next-token prediction is inefficient because it focuses on local patterns and overlooks long-term dependencies. This requires significantly more training data compared to human learning (e.g., children learn language with far fewer examples). During inference, the model must generate tokens autoregressively (using its own prior predictions), while training uses ground-truth tokens. This creates a gap between training and inference, leading to error accumulation.\nDuring training, for any time step t:\nThe model uses a shared transformer trunk to generate a latent representation \\( z_{t:1} \\) from the context \\( x_{t:1} \\) (all tokens up to position \\( t \\)). This latent representation is then fed into \\( n \\) independent output heads, each dedicated to predicting a specific future token \\( x_{t+i} \\) for \\( i = 1, \\dots, n \\), in parallel. More specifically, the model\u0026rsquo;s heads for predicting tokens at positions \\( t+k \\) (where \\( k \u003e 1 \\)) do not rely on previous predicted tokens (e.g., \\( t+1 \\)) during training. This design avoids additional training overhead by sequentially computing forward/backward passes for each head, reducing peak GPU memory usage from \\(O(nV + d)\\) to \\(O(V + d)\\). For example, to predict tokens at \\( t+1, t+2, t+3, t+4 \\), each head \\( i \\) directly uses \\( z_{t:1} \\) to generate \\( P_\\theta(x_{t+i} | x_{t:1}) \\), without using the output of head \\( i-1 \\) (which predicts \\( x_{t+i-1} \\)). For the sequence Hi, I would like _t1, _t2, _t3, _t4, the prediction of \\( _t2 \\) ( \\( t+2 \\)) is based on Hi, I would like ( \\( x_{t:1} \\) where \\( t \\) is the position before \\( _t1 \\)), not on the predicted \\( _t1 \\). The model is trained to predict \\( _t1, _t2, _t3, _t4 \\) in parallel, each relying solely on the initial context, not on a chain of prior predictions. Unlike autoregressive decoding (where each step uses the previous token’s prediction), MTP during training is non-causal for future tokens—each head looks directly at the original context to predict its target token, avoiding dependency on intermediate predictions. This design enables parallel training and faster inference via speculative decoding. This is explicitly stated in the paper’s formula:\n\\[ P_\\theta(x_{t+i} | x_{t:1}) = \\text{softmax}(f_u(f_{h_i}(f_s(x_{t:1}))) \\]\nwhere \\( f_s \\) is the shared trunk, \\( f_{h_i} \\) is the \\( i \\)-th head, and \\( f_u \\) is the unembedding matrix. Each head \\( f_{h_i} \\) operates independently on \\( f_s(x_{t:1}) \\), not on previous heads’ outputs. The loss function aggregates predictions for \\( n \\) future tokens simultaneously, but each prediction is conditioned only on the original context \\( x_{t:1} \\), not on intermediate predictions: \\[ L_n = -\\sum_t \\log P_\\theta(x_{t+n:t+1} | x_{t:1}) \\] This encourages the model to learn longer-term dependencies and mitigates the distribution mismatch between teacher-forced training and autoregressive inference. During inference, only the next-token output head is employed. Optionally, the other extra heads enable self-speculative decoding, where the model pre-emptively predicts multiple tokens to skip redundant computations. This achieves up to 3× faster inference on code tasks with large batch sizes.\nOn code benchmarks like HumanEval and MBPP, 13B-parameter models with 4-token prediction solved 12% and 17% more problems, respectively, compared to next-token baselines. The approach also improved algorithmic reasoning and induction capabilities in synthetic tasks.\nComparison with Multi-Token Prediction in DeepSeek-V3 DeepSeek-V3(DeepSeek-AI, et al. DeepSeek-V3 Technical Report. arXiv:2412.19437, arXiv, 27 Dec. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2412.19437. ) notes that next-token prediction encourages the model to prioritize short-term dependencies, which may hinder its ability to learn global structures or complex reasoning tasks (e.g., coding, math). DeepSeek-V3 also employs MTP but with distinct architectural and operational differences:\nArchitectural Implementation Gloeckle et al.: Parallel independent heads for simultaneous prediction of \\(n\\) tokens, with each head operating on the shared trunk\u0026rsquo;s output. Example: 4-token prediction uses 4 separate heads. DeepSeek-V3: Sequential MTP modules that maintain a causal chain for each prediction depth. Each module \\(k\\) combines the current token representation with the embedding of the \\((i+k)\\)-th token, passing through a Transformer block \\(TRM_k\\) before prediction. This design preserves the dependency chain for deeper context reasoning. Training Objective and Loss Function Gloeckle et al.: The loss is the average cross-entropy over \\(n\\) heads, with a fixed weight \\(\\lambda\\) (e.g., \\(\\lambda = 0.3\\) for early training). DeepSeek-V3: MTP loss is a weighted average of cross-entropy losses from \\(D\\) sequential modules, with the weight \\(\\lambda\\) decaying from 0.3 to 0.1 during training. The sequential design allows deeper integration with the model\u0026rsquo;s causal attention mechanism. Memory and Computation Efficiency Gloeckle et al.: Optimizes memory via sequential head computation, reducing GPU memory overhead. Training speed remains comparable to baselines. DeepSeek-V3: Leverages shared embedding layers and output heads between MTP modules and the main model, further reducing parameter redundancy. The MoE architecture (with 37B activated parameters) combined with MTP enables efficient scaling. Inference Acceleration Strategies Gloeckle et al.: Self-speculative decoding using extra heads to pre-predict tokens, achieving 3× speedup on code tasks. DeepSeek-V3: MTP modules are repurposed for speculative decoding, with an 85–90% acceptance rate for the second predicted token, leading to 1.8× higher tokens per second (TPS). The MoE routing further optimizes communication during decoding. Domain Focus and Performance Gloeckle et al.: Primarily targets code and math tasks, demonstrating significant gains on HumanEval (12% improvement) and MBPP (17% improvement) for 13B models. DeepSeek-V3: MTP complements its MoE architecture to excel across broader domains: code (LiveCodeBench Pass@1: 40.5%), math (MATH-500 EM: 90.2%), and multilingual tasks (MMMLU-non-English: 79.4%). The model outperforms open-source baselines and rivals closed-source models like GPT-4o. Integration with Other Techniques Gloeckle et al.: MTP is a standalone training objective without auxiliary losses for load balancing. DeepSeek-V3: MTP is combined with an auxiliary-loss-free load balancing strategy for MoE, ensuring expert utilization while minimizing performance degradation. This integration enables more stable training for extremely large models. Key Differences Summary\nAspect Gloeckle et al. (2024) DeepSeek-V3 Architecture Parallel independent heads Sequential causal modules Prediction Flow Simultaneous parallel prediction Sequential prediction with causal chain Integration with MoE Not applicable (dense model) Tightly integrated with MoE routing Inference Speedup 3× via self-speculative decoding 1.8× TPS with high token acceptance Training Focus Code/math efficiency Broad-domain performance + MoE scaling Auxiliary Losses None (pure MTP) Combined with load-balancing strategy Both approaches leverage MTP to enhance training efficiency and inference speed, but DeepSeek-V3 extends the paradigm by integrating it with MoE architecture and auxiliary techniques, making it suitable for extremely large models with broader task coverage. Gloeckle et al.\u0026rsquo;s method is simpler and more focused on code/math tasks, demonstrating that MTP alone can drive significant improvements in sample efficiency and reasoning.\n","permalink":"https://congchan.github.io/posts/multi-token-prediction/","summary":"\u003ch2 id=\"multi-token-prediction-vs-next-token-prediction\"\u003eMulti-token prediction vs Next-token prediction\u003c/h2\u003e\n\u003cp\u003eNext-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \\( x_{t+1} \\) given the context \\( x_{1:t} \\) (all tokens up to position \\( t \\)).\u003c/p\u003e\n\u003cp\u003eThe cross-entropy loss for next-token prediction is defined as:\u003cbr\u003e\n\u003c/p\u003e","title":"Multi-token Prediction"},{"content":"Reward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM.\n1. Scoreing preference by parameters Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155.\nThe paper presents a reward modeling (RM) approach as a core component of reinforcement learning from human feedback (RLHF) to align language models with human intent. Below is a detailed breakdown of the paper\u0026rsquo;s views and methods on reward modeling:\n1.1 Core Objectives of Reward Modeling The reward model aims to:\nQuantify Human Preferences: Convert subjective human judgments about model outputs into a scalar reward signal, enabling models to learn what constitutes \u0026ldquo;desirable behavior.\u0026rdquo; Guide Model Alignment: Direct language models to follow instructions, prioritize truthfulness, and avoid harmful outputs by optimizing against human-derived rewards. 1.2. Data Collection for Reward Modeling Input Source: Prompts from the OpenAI API (filtered to remove PII) and labeler-written prompts, covering tasks like generation, QA, and summarization . Labeling Process: Labelers rank 4–9 model outputs per prompt from best to worst, generating pairwise comparisons (e.g., \u0026ldquo;Output A is preferred over Output B\u0026rdquo;) . To avoid bias, labelers undergo a screening test to assess sensitivity to sensitive content and alignment with research criteria . 1.3. Reward Model Architecture and Training Model Structure:\nBased on the GPT-3 architecture, initialized from a supervised fine-tuned (SFT) model with the final unembedding layer replaced by a projection layer to output a scalar reward . Uses a 6B parameter model for computational efficiency, as 175B models showed training instability . Training Methodology:\nLoss Function: Cross-entropy loss to predict human-preferred outputs, formulated as: $$ \\text{loss}(\\theta) = -\\frac{1}{\\binom{K}{2}} \\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim D} \\left[ \\log \\left( \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right) \\right] $$ where $y_w$ and $y_l$ are the preferred and less preferred outputs, respectively, and $K$ is the number of outputs per prompt . Batch Processing: Treats all $\\binom{K}{2}$ comparisons from a prompt as a single batch element to prevent overfitting and improve computational efficiency . Normalization: Adjusts rewards so that labeler demonstrations have a mean score of 0 before RL training . 1.4. Key Innovations and Insights Generalization to Held-Out Labelers: Reward models trained on one group of labelers generalize to new labelers, with cross-validation showing 69.6% accuracy in predicting preferences of unseen labelers . Trade-off with Public NLP Datasets: RM-based RLHF may cause performance regressions on standard NLP tasks (e.g., SQuAD, DROP), but mixing pretraining gradients (PPO-ptx) mitigates this while preserving human preference . Role in InstructGPT: The RM is crucial for improving model behavior: InstructGPT (PPO-ptx) outperforms GPT-3 despite having 100x fewer parameters, with 1.3B InstructGPT preferred over 175B GPT-3 in 85% of cases . 1.5. Limitations and Future Directions Alignment Scope: The RM aligns models to specific labelers and researchers, not broader human values, raising questions about fairness and representativeness . Toxicity and Bias: While InstructGPT reduces toxicity, it shows minimal improvement on bias metrics (e.g., Winogender, CrowS-Pairs), indicating RM needs better signals for these dimensions . Scalability: Future work may explore combining RM with adversarial data collection or constraint optimization to address harmful outputs and improve generalization . In summary, the paper demonstrates that reward modeling via RLHF is a powerful tool for aligning language models, but ongoing research is needed to address its limitations and expand its applicability to diverse human values.\n2. Constitutional AI: Bootstrapping Harmlessness with AI Feedback Bai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2212.08073.\nIn \u0026ldquo;Constitutional AI: Harmlessness from AI Feedback\u0026rdquo; (Bai et al., 2022), researchers introduced a paradigm that replaces human labels for harmfulness with AI-generated feedback. The approach uses a \u0026ldquo;constitution\u0026rdquo; of principles to guide self-critique and revision, enabling models to learn harmless behavior without direct human supervision.\nKey Innovation: The framework combines supervised learning (critique → revision cycles) and RL from AI Feedback (RLAIF), where a preference model (PM) is trained on AI-generated comparisons. For example, models generate pairs of responses and evaluate which aligns better with constitutional principles (e.g., \u0026ldquo;avoid harmful advice\u0026rdquo;). Impact: As shown in Figure 2 of the paper, Constitutional AI achieves a Pareto improvement in harmlessness and helpfulness, outperforming RLHF models that trade off these traits. The approach reduces reliance on human labeling, a critical step toward scalable supervision. This work laid the groundwork for self-supervised RM, demonstrating that models can learn to evaluate their own behavior using explicit principles.\n3. DeepSeek-R1: Incentivizing Reasoning via Reinforcement Learning DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948.\nThe paper employs reward modeling strategically to enhance reasoning capabilities in LLMs through reinforcement learning (RL). Here’s a detailed breakdown of how reward modeling is utilized:\n3.1. Rule-Based Rewards for DeepSeek-R1-Zero DeepSeek-R1-Zero relies on a rule-based reward system to avoid the complexity and potential pitfalls of neural reward models. This system consists of two main components:\nAccuracy Rewards: Evaluate the correctness of responses. For example: In math problems, the model must provide answers in a specified format (e.g., within a box) for rule-based verification. In coding tasks (e.g., LeetCode), a compiler checks solutions against predefined test cases. Format Rewards: Enforce structural consistency by requiring the model to place reasoning processes between specific tags (e.g., \u0026lt;think\u0026gt;, \u0026lt;\\think\u0026gt;, \u0026lt;｜tool▁calls▁begin｜\u0026gt;\u0026lt;｜tool▁call▁begin｜\u0026gt; and \u0026lt;｜tool▁call▁end｜\u0026gt;\u0026lt;｜tool▁calls▁end｜\u0026gt;\u0026lt;｜end▁of▁sentence｜\u0026gt;). The paper explicitly avoids neural reward models (both outcome and process-based) for DeepSeek-R1-Zero, citing risks of reward hacking and the additional computational overhead of retraining reward models.\n3.2. Enhanced Reward Systems for DeepSeek-R1 DeepSeek-R1 incorporates additional reward mechanisms to address readability and generalizability:\nLanguage Consistency Reward: Introduced to mitigate language mixing in Chain-of-Thought (CoT) reasoning. This reward measures the proportion of target language words in the CoT and is summed with accuracy rewards. While this slightly reduces reasoning performance, it improves human readability. Multi-Stage RL with Diverse Rewards: Reasoning-Oriented RL: Uses rule-based rewards (accuracy + format) for tasks like math and coding. General Scenario RL: Employs neural reward models to align with human preferences for helpfulness and harmlessness. For example: Helpfulness: Focuses on the utility of the final summary. Harmlessness: Evaluates the entire response (reasoning + summary) to prevent biased or harmful content. 3.3. Reward Design for Cold Start and Distillation Cold Start Data: Thousands of CoT examples are curated to fine-tune the model before RL. These examples include human-readable formats (e.g., summaries) and serve as a foundation for reward-aligned behavior. Distillation: The reasoning patterns of DeepSeek-R1 are distilled into smaller models using 800K training samples. While distillation itself does not use RL rewards, the teacher model (DeepSeek-R1) is trained with the aforementioned reward systems, ensuring smaller models inherit optimized reasoning behaviors. 3.4. Key Trade-offs and Design Choices Rule-Based vs. Neural Rewards: Rule-based rewards are prioritized for simplicity and to avoid reward hacking in large-scale RL. Neural rewards are introduced only when necessary (e.g., for general task alignment in DeepSeek-R1). Balancing Performance and Readability: The language consistency reward in DeepSeek-R1 trades off slight performance degradation for improved human interpretability, highlighting the importance of practical usability. 3.5. Experimental Validation of Reward Models DeepSeek-R1-Zero: Achieves significant reasoning gains (e.g., AIME 2024 Pass@1 from 15.6% to 71.0%) using purely rule-based rewards, demonstrating that complex reasoning can emerge without neural reward models. DeepSeek-R1: Outperforms DeepSeek-R1-Zero on readability and matches OpenAI-o1-1217 on reasoning tasks by combining rule-based and language consistency rewards. Distilled Models: Smaller models (e.g., 14B, 32B) trained on DeepSeek-R1’s rewarded outputs outperform state-of-the-art open-source models, validating the transferability of reward-aligned reasoning patterns. 3.6 Conclusion The paper demonstrates that reward modeling in RL can be tailored to balance reasoning performance, readability, and human alignment. Rule-based rewards enable pure RL-driven reasoning emergence in DeepSeek-R1-Zero, while DeepSeek-R1 enhances this with language consistency and general preference rewards. This approach highlights the flexibility of reward systems in shaping LLM behavior without heavy reliance on neural reward models, paving the way for efficient and interpretable reasoning enhancements.\n4. Inference-Time Scaling for Generalist Reward Modeling Liu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2504.02495.\nLiu et al. (2025) address a critical gap in RM: generalizability across domains and efficient resource use. Their approach, Self-Principled Critique Tuning (SPCT), combines generative reward modeling (GRM) with inference-time scaling to improve RM performance without increasing training compute.\nCore Contributions: SPCT: A two-phase RL method where models learn to generate adaptive principles and critiques, enhancing reward quality. For example, DeepSeek-GRM-27B with SPCT outperforms scalar RMs on benchmarks like Reward Bench and PPE (Table 2). Inference-Time Scaling: Parallel sampling and a meta RM guide voting on multiple reward samples, expanding the reward space and improving granularity. As shown in Figure 1, DeepSeek-GRM-27B with meta RM achieves 72.8% overall performance, surpassing models like Nemotron-4-340B-Reward. Connection to Prior Work: SPCT builds on Constitutional AI’s self-critique and DeepSeek-R1’s RL efficiency, but shifts focus to generalist domains. The meta RM integrates self-evaluation insights (from Kadavath et al.) to filter low-quality rewards, aligning with P(IK)-like confidence metrics. The Developmental Arc: From Specialization to Generalization The evolution of RM reflects a shift from human-dependent, task-specific approaches to self-supervised, generalizable frameworks:\nEarly RLHF (2020s): Relied on massive human labeling, limited to specific domains. Constitutional AI (2022): Introduced AI-generated feedback and principles, reducing human overhead. Self-Evaluation (2022): Uncovered models’ ability to assess their own knowledge, enabling confidence-aware RM. Task-Oriented RL (2025, DeepSeek-R1): Optimized reasoning via RL, demonstrating task-specific RM scaling. Generalist Inference-Time Scaling (2025, Liu et al.): Extended RM to diverse domains using generative models and inference-time compute, balancing efficiency and performance. Challenges and Future Directions Bias and Calibration: While SPCT reduces domain bias, models like DeepSeek-GRM still struggle with specific tasks (e.g., verifiable problems, Appendix B). Computational Overhead: Inference-time scaling requires more compute per query, necessitating efficiency improvements. Cross-Domain Generalization: Combining task-specific RM (DeepSeek-R1) with generalist GRM (Liu et al.) remains an open challenge. Future work may integrate self-supervised RM with external tools (e.g., code execution for verification) and explore hybrid frameworks that balance training and inference-time scaling.\nConclusion The landscape of reward modeling is evolving rapidly, driven by innovations that prioritize scalability, self-supervision, and generalizability. From Constitutional AI’s principles to SPCT’s inference-time scaling, these methods collectively push LLMs toward more aligned, transparent, and efficient behavior. As shown in the comparative results across papers, the field is moving toward a future where RM serves as a versatile interface for LLM alignment, enabling models to reason, evaluate, and adapt to diverse human needs.\nKey Citations Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155. Bai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2212.08073. DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948. Liu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2504.02495. Kadavath, Saurav, et al. Language Models (Mostly) Know What They Know. arXiv:2207.05221, arXiv, 21 Nov. 2022. arXiv.org, http://arxiv.org/abs/2207.05221. ","permalink":"https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/","summary":"\u003cp\u003eReward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM.\u003c/p\u003e","title":"The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling"},{"content":"Liu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2504.02495.\nProblem Statement Reinforcement Learning (RL) has become pivotal in post-training large language models (LLMs), but generating accurate reward signals for diverse domains remains challenging. Existing reward models (RMs) often rely on human-designed rules or verifiable tasks, struggling with generalizability and inference-time scalability. This paper addresses how to improve RM effectiveness through increased inference compute and adaptive learning methods for general queries.\nKey Contributions Self-Principled Critique Tuning (SPCT): A novel learning method to foster scalable reward generation in generative RMs (GRMs) by enabling adaptive principle and critique generation via rule-based online RL. Pointwise Generative Reward Modeling (GRM): Adopted for flexibility in handling various input types and potential for inference-time scaling through diverse reward generation. Inference-Time Scaling Strategies: Parallel sampling to expand compute usage and a meta RM to guide voting, enhancing scaling performance without severe biases. Methodology SPCT Framework: Rejective Fine-Tuning (RFT): Cold start to align GRM with correct format and input types, rejecting incorrect or trivial trajectories. Rule-Based RL: Optimizes principle and critique generation using GRPO, encouraging the model to distinguish best responses via online-optimized criteria. Inference-Time Scaling: Parallel Sampling: Generates multiple principle-critique pairs, voting on final rewards to expand the value space. Meta RM: A scalar RM trained to filter low-quality samples, guiding voting for more accurate results. Experimental Results Performance Benchmarks: DeepSeek-GRM outperforms scalar/semi-scalar RMs (e.g., Nemotron-4-340B-Reward, GPT-4o) on Reward Bench, PPE, and RMB without domain biases. Inference-Time Scalability: Voting with 32 samples and meta RM guidance achieves up to 72.8% overall accuracy, surpassing training-time scaling (e.g., 671B model performance with 27B model + inference scaling). Ablation Studies: Principle generation and non-hinted sampling prove critical; meta RM effectively filters low-quality outputs. Conclusion SPCT enhances GRMs to generate adaptive, high-quality rewards, demonstrating that inference-time scaling can outperform model size scaling. Future work will focus on integrating tools, improving efficiency, and offline evaluation applications. The models will be open-sourced to advance generalist reward systems.\nKey Figures/Tables Figure 1: Inference-time scaling performance across benchmarks, showing DeepSeek-GRM’s superiority with increased samples. Table 2: Overall results comparing DeepSeek-GRM against public models and baselines, highlighting its competitive edge. Table 4: Ablation study verifying the importance of SPCT components (e.g., principle generation, rejective sampling). This work bridges the gap between RM generalizability and compute efficiency, offering a scalable path for LLM alignment in diverse domains.\n","permalink":"https://congchan.github.io/posts/paper-reading-inference-time-scaling-for-generalist-reward-modeling/","summary":"\u003cp\u003eLiu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, \u003ca href=\"https://doi.org/10.48550/arXiv.2504.02495\"\u003ehttps://doi.org/10.48550/arXiv.2504.02495\u003c/a\u003e.\u003c/p\u003e\n\u003ch4 id=\"problem-statement\"\u003eProblem Statement\u003c/h4\u003e\n\u003cp\u003eReinforcement Learning (RL) has become pivotal in post-training large language models (LLMs), but generating accurate reward signals for diverse domains remains challenging. Existing reward models (RMs) often rely on human-designed rules or verifiable tasks, struggling with generalizability and inference-time scalability. This paper addresses how to improve RM effectiveness through increased inference compute and adaptive learning methods for general queries.\u003c/p\u003e","title":"Paper Reading - Inference-Time Scaling for Generalist Reward Modeling"},{"content":"DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948.\nIncentivizing Reasoning Capability in LLMs via Reinforcement Learning Large language models (LLMs) have made remarkable strides in mimicking human-like cognition, but their ability to reason through complex problems—from math proofs to coding challenges—remains a frontier. In a recent breakthrough, DeepSeek-AI introduces DeepSeek-R1, a family of reasoning-focused models that leverages reinforcement learning (RL) to unlock advanced reasoning capabilities, without relying on traditional supervised fine-tuning (SFT) as a crutch. The paper \u0026ldquo;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\u0026rdquo; unveils a paradigm shift in how we train LLMs to think critically, with implications for both research and real-world applications.\nThe Power of Pure RL: DeepSeek-R1-Zero\u0026rsquo;s Emergent Reasoning At the heart of the research is DeepSeek-R1-Zero, a model trained entirely via large-scale RL on a base model (DeepSeek-V3-Base) without any SFT data. This \u0026ldquo;pure RL\u0026rdquo; approach is revolutionary: by using Group Relative Policy Optimization (GRPO) and a rule-based reward system (focused on accuracy and format), the model spontaneously develops sophisticated reasoning behaviors.\nOne of the most striking findings is how DeepSeek-R1-Zero evolves autonomous problem-solving strategies. For instance, on the AIME 2024 math benchmark, its pass@1 accuracy skyrocketed from 15.6% to 71.0% through RL training. Even more impressively, using majority voting, this accuracy surged to 86.7%, matching the performance of OpenAI’s o1-0912. The model also exhibits \u0026ldquo;aha moments\u0026rdquo;—intermediate checkpoints where it suddenly learns to revisit and refine its initial reasoning, like a human thinker having an epiphany.\nBut pure RL isn’t perfect. DeepSeek-R1-Zero struggled with readability and language mixing, prompting the development of DeepSeek-R1—a refined version that incorporates a \u0026ldquo;cold start\u0026rdquo; of curated Chain-of-Thought (CoT) data and a multi-stage training pipeline.\nDeepSeek-R1: Balancing Reasoning and Readability DeepSeek-R1 addresses R1-Zero’s limitations by integrating two key improvements:\nCold-start fine-tuning: Thousands of human-curated CoT examples kickstart the model, ensuring cleaner output formats and reducing language mixing. Multi-stage RL-SFT cycles: After RL training for reasoning, the model undergoes supervised fine-tuning on a mix of reasoning and non-reasoning data (e.g., writing, factual QA), then another RL phase to align with human preferences. The result? DeepSeek-R1 achieves performance on par with OpenAI’s o1-1217 across critical benchmarks. On AIME 2024, it hits 79.8% pass@1, slightly outperforming o1-1217 (79.2%). On MATH-500, it scores 97.3%, matching o1-1217’s excellence. In coding, it reaches a Codeforces rating of 2,029, outperforming 96.3% of human participants.\nHere is the overall training pipeline show in the excellent images created by SirrahChan. (2025, June 17). visualizing the training pipeline for DeepSeek-R1(-Zero) and the distillation to smaller models. Twitter. https://x.com/SirrahChan/status/1881540279783887036\nDistillation: Empowering Small Models to Reason A major highlight of the research is the effectiveness of distilling reasoning capabilities from DeepSeek-R1 into smaller models. By fine-tuning open-source bases like Qwen and Llama on 800K samples generated by DeepSeek-R1, the team created lightweight models that punch far above their weight.\nDeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing the 32B QwQ-32B-Preview. DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME, 94.3% on MATH-500, and 57.2% on LiveCodeBench—outperforming most open-source models and rivaling o1-mini. This distillation approach proves that larger models’ reasoning patterns can be transferred to smaller architectures, offering a cost-effective path to deploy powerful reasoning in real-world applications.\nWhy This Matters: Rethinking LLM Training Paradigms DeepSeek-R1 challenges the conventional wisdom that SFT is indispensable for building reasoning ability. By demonstrating that pure RL can drive emergent reasoning—and that distillation can spread these capabilities to smaller models—DeepSeek-AI opens new doors for the field:\nReduced dependency on labeled data: RL-based training requires less human annotation, making it more scalable. Democratizing advanced reasoning: Distilled models enable researchers and developers to access cutting-edge reasoning without massive compute resources. Ethical alignment through RL: The multi-stage pipeline allows for explicit alignment with human preferences (e.g., readability, safety), reducing risks of harmful outputs. What’s Next: Open Source and Beyond DeepSeek-AI has open-sourced DeepSeek-R1, R1-Zero, and all six distilled models (1.5B to 70B), inviting the community to build upon their work. Future plans include improving multi-language capabilities, addressing prompt sensitivity, and enhancing software engineering tasks—where RL training data is currently limited.\nIn a world where LLM reasoning is increasingly critical for science, education, and industry, DeepSeek-R1 marks a pivotal step toward more autonomous, efficient, and accessible AI reasoning. As the paper concludes, \u0026ldquo;advancing beyond the boundaries of intelligence may still require more powerful base models and large-scale reinforcement learning\u0026rdquo;—but the journey has just begun.\nTo explore the models and code, visit the DeepSeek-AI repository and dive into the full research hehttps://arxiv.org/abs/2501.12948re.\n","permalink":"https://congchan.github.io/posts/deepseek-r1/","summary":"\u003cp\u003eDeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, \u003ca href=\"https://doi.org/10.48550/arXiv.2501.12948\"\u003ehttps://doi.org/10.48550/arXiv.2501.12948\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"incentivizing-reasoning-capability-in-llms-via-reinforcement-learning\"\u003eIncentivizing Reasoning Capability in LLMs via Reinforcement Learning\u003c/h3\u003e\n\u003cp\u003eLarge language models (LLMs) have made remarkable strides in mimicking human-like cognition, but their ability to reason through complex problems—from math proofs to coding challenges—remains a frontier. In a recent breakthrough, DeepSeek-AI introduces \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e, a family of reasoning-focused models that leverages reinforcement learning (RL) to unlock advanced reasoning capabilities, without relying on traditional supervised fine-tuning (SFT) as a crutch. The paper \u003cem\u003e\u0026ldquo;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\u0026rdquo;\u003c/em\u003e unveils a paradigm shift in how we train LLMs to think critically, with implications for both research and real-world applications.\u003c/p\u003e","title":"DeepSeek-R1"},{"content":"Summary of \u0026ldquo;The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions\u0026rdquo; Wallace, Eric, et al. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. arXiv:2404.13208, arXiv, 19 Apr. 2024. arXiv.org, http://arxiv.org/abs/2404.13208.\n1. Problem Statement Modern large language models (LLMs) are vulnerable to attacks like prompt injections and jailbreaks because they treat system prompts, user messages, and third-party inputs (e.g., tool outputs) as equal in priority. This allows adversaries to override intended instructions, leading to risks such as data exfiltration or unauthorized actions .\n2. The Instruction Hierarchy Framework The authors propose an instruction hierarchy to define priority levels for different input types:\nHighest priority: System messages (developer-provided instructions) Medium priority: User messages Lowest priority: Third-party content (e.g., tool outputs, web results) . When instructions conflict, LLMs should prioritize higher-level instructions and ignore or refuse lower-privileged, misaligned commands. For example, a system message instructing an LLM to act as an email assistant should override a user’s attempt to inject a command to forward all emails .\n3. Training Approach Aligned Instructions: Use synthetic data generation to decompose complex requests (e.g., \u0026ldquo;write a 20-line poem in Spanish\u0026rdquo;) into sub-instructions at different hierarchy levels, training models to compose responses. Misaligned Instructions: Employ \u0026ldquo;context ignorance\u0026rdquo; to train models to ignore lower-privileged instructions that conflict with higher ones. For example, if a tool output contains a prompt injection, the model should respond as if the injection did not exist . 4. Experimental Results Robustness Improvements: The method significantly enhances defense against various attacks: System prompt extraction: +63% improvement . Jailbreak resistance: +30% improvement . Indirect prompt injections via browsing: from 32.8% to 89.6% robustness . Generalization: The model shows strong performance against unseen attacks, such as password extraction and tool-based injections, without compromising core capabilities (e.g., TriviaQA accuracy remains comparable) . Trade-offs: Some \u0026ldquo;over-refusal\u0026rdquo; of benign queries occurs but is manageable with further data collection . 5. Future Directions Refine conflict handling for tool outputs and multi-modal inputs (e.g., images, audio) . Explore architectural changes (e.g., specialized embeddings for different priority levels) . Enhance adversarial training to address remaining vulnerabilities . 6. Key Contributions A formalized instruction hierarchy to prioritize security-critical instructions. Automated data generation methods to teach hierarchical behavior. Empirical evidence that the approach boosts LLM robustness without sacrificing general capabilities . ","permalink":"https://congchan.github.io/posts/paper-reading-the-instruction-hierarchy-training-llms-to-prioritize-privileged-instructions/","summary":"\u003ch3 id=\"summary-of-the-instruction-hierarchy-training-llms-to-prioritize-privileged-instructions\"\u003eSummary of \u0026ldquo;The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions\u0026rdquo;\u003c/h3\u003e\n\u003cp\u003eWallace, Eric, et al. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. arXiv:2404.13208, arXiv, 19 Apr. 2024. arXiv.org, \u003ca href=\"http://arxiv.org/abs/2404.13208\"\u003ehttp://arxiv.org/abs/2404.13208\u003c/a\u003e.\u003c/p\u003e\n\u003ch4 id=\"1-problem-statement\"\u003e1. \u003cstrong\u003eProblem Statement\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eModern large language models (LLMs) are vulnerable to attacks like prompt injections and jailbreaks because they treat system prompts, user messages, and third-party inputs (e.g., tool outputs) as equal in priority. This allows adversaries to override intended instructions, leading to risks such as data exfiltration or unauthorized actions .\u003c/p\u003e","title":"Paper Reading - The Instruction Hierarchy - Training LLMs to Prioritize Privileged Instructions"},{"content":"Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION. https://arxiv.org/abs/2312.09390\nResearch Context and Objectives The paper addresses a critical challenge in aligning superhuman AI models: when human supervision becomes insufficient due to the models\u0026rsquo; complex behaviors, can weak supervision (e.g., from weaker models) effectively elicit the full capabilities of stronger models? The authors from OpenAI explore this through empirical experiments, aiming to bridge the gap between current alignment techniques (like RLHF) and the needs for superhuman model alignment.\nKey Methodology Setup: Train strong \u0026ldquo;student\u0026rdquo; models using labels generated by weak \u0026ldquo;supervisor\u0026rdquo; models, comparing results against strong models fine-tuned with ground truth labels (strong ceiling performance). Tasks: Natural Language Processing (NLP) benchmarks. Chess puzzle prediction. ChatGPT reward modeling. Metrics: Performance Gap Recovered (PGR) measures the fraction of the performance gap between weak supervisors and strong ceilings recovered by weak-to-strong training. Core Findings Naive Weak-to-Strong Generalization:\nStrong students consistently outperform weak supervisors. For example, fine-tuning GPT-4 with GPT-2-level supervision recovers ~50% of the performance gap on NLP tasks. However, significant gaps remain compared to strong ceilings, especially in reward modeling. Improving Generalization:\nBootstrapping: Using intermediate model sizes to iteratively refine supervision improves PGR in chess puzzles, where large gaps previously hindered performance. Auxiliary Confidence Loss: Encouraging strong models to be confident in their predictions (even when disagreeing with weak labels) boosts NLP PGR to ~80%. Generative Finetuning: Unsupervised finetuning on task-relevant data improves reward modeling PGR by 10-20%. Underlying Mechanisms:\nImitation of Errors: Strong models initially generalize but can overfit to weak supervisor mistakes, mitigated by confidence losses. Task Salience: Pretrained strong models possess latent task knowledge. Prompts and generative finetuning enhance knowledge elicitation. Implications and Limitations Feasibility: Weak-to-strong generalization is tractable with simple methods, offering a path to aligning superhuman models. Challenges: Disanalogies remain, such as superhuman models\u0026rsquo; potential to imitate human errors more effectively and pretraining leakage (tasks observed during pretraining). Future Work: Develop more analogous setups, scalable techniques, and scientific understanding of generalization mechanisms. Conclusion The study demonstrates that weak supervision can elicit non-trivial capabilities from strong models, but advanced methods are needed for full alignment. This paves the way for empirical progress on superalignment, a critical step toward safe superhuman AI.\nKeywords Weak-to-strong generalization, model alignment, superhuman AI, RLHF, confidence loss, bootstrapping\n","permalink":"https://congchan.github.io/posts/paper-reading-weak-to-strong-generalization-eliciting-strong-capabilities-with-weak-supervision/","summary":"\u003cp\u003eCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION. \u003ca href=\"https://arxiv.org/abs/2312.09390\"\u003ehttps://arxiv.org/abs/2312.09390\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"research-context-and-objectives\"\u003eResearch Context and Objectives\u003c/h4\u003e\n\u003cp\u003eThe paper addresses a critical challenge in aligning superhuman AI models: when human supervision becomes insufficient due to the models\u0026rsquo; complex behaviors, can weak supervision (e.g., from weaker models) effectively elicit the full capabilities of stronger models? The authors from OpenAI explore this through empirical experiments, aiming to bridge the gap between current alignment techniques (like RLHF) and the needs for superhuman model alignment.\u003c/p\u003e","title":"Paper Reading - Weak-to-Strong Generalization - Eliciting Strong Capabilities With Weak Supervision"},{"content":"Cong Chen\nUniversity of Edinburgh\nThere are various implementation of reward modeling in RLHF(reinforcement learning with human feedback), each has different pros and cons. Inspired by some open-sourced works about reward modeling, I would like to share one of the best practice for reward modeling. For those who are not familiar with reward modeling and RLHF, I recommend take a look at the Huggingface rlhf blog1 or OpenAI rlhf paper2.\nSuggested Practice Reward modeling involves training a model to predict a scalar value as a reward for a given input text. This is achieved through a high-level architecture consisting of a deep transformers block and a value head with an output dimension of one. To streamline the training and inference stages, the AutoModelForSequenceClassification class can be utilized. One effective way to ensure consistency and compatibility is to use Huggingface transformers\u0026rsquo; self-defined model with auto_map. This involves defining different reward model backbones depending on the model type (such as Llama3 or Bloom4), then using the from_pretrained method to load the model for either training or inference purposes.\nLet us use a Llama pre-trained causal language model from HuggingFace decapoda-research/llama-7b-hf5 as an example and provide step-by-step instructions.\nFirst, we would like to train this pre-trained causal language model on some reward datasets(such as the Anthropic/hh-rlhf Datasets6). Let’s create a script called modeling_llama_rw.py in the pretrained model directory to let transformers know how to load the model.\nfrom transformers import LlamaPreTrainedModel, LlamaConfig, LlamaModel import torch.nn as nn import torch class LlamaRewardModel(LlamaPreTrainedModel): \u0026#34;\u0026#34;\u0026#34; The LLaMa Model transformer with a sequence classification head on top (linear layer). uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the last token. If a `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in each row of the batch). \u0026#34;\u0026#34;\u0026#34; config_class = LlamaConfig def __init__(self, config): super().__init__(config) self.num_labels = 1 self.model = LlamaModel(config) self.value_head = nn.Linear(config.hidden_size, self.num_labels) def get_input_embeddings(self): return self.model.embed_tokens def set_input_embeddings(self, value): self.model.embed_tokens = value def gradient_checkpointing_enable(self): self.model.gradient_checkpointing_enable() def forward(): ... Next, we need to add some extra information to the config.json file by adding a new class called LlamaRewardModel in the \u0026quot;architectures\u0026quot; key and a new class mapping of \u0026quot;auto_map\u0026quot;: [\u0026quot;AutoModelForSequenceClassification\u0026quot;: \u0026quot;modeling_llama_rm.LlamaRewardModel\u0026quot;].\n{ \u0026#34;_name_or_path\u0026#34;: \u0026#34;LLaMA-7B-Reward\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;LlamaRewardModel\u0026#34; ], \u0026#34;auto_map\u0026#34;: { \u0026#34;AutoModelForSequenceClassification\u0026#34;: \u0026#34;modeling_llama_rm.LlamaRewardModel\u0026#34; }, ... # we could leave the rest untouched } With these changes, we can now load the model using the following line: model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, trust_remote_code=True). The trust_remote_code=True is required to make sure transformers API could call our customed class. This line can be used for both training and inference stages. After training the model, we can share it, along with the configuration and modeling files, for downstream use.\nI have also provided an example of Bloom reward modeling in the my GitHub repository7.\nSome Details in Reward Modeling There is no definitive method for calculating scalar rewards based on the output logits of input text. However, OpenAI has provided some reference implementations for us to learn from. They use the output logits of the final token of the input sentence, including paddings. This is similar to the Huggingface AutoModelForSequenceClassification. You can directly use this class.\nMy implementation adds more fine-grained details to the process of computing scalar rewards. Specifically, the logit of the last token before the sentence padding is retrieved to compute the reward. In cases where the input sentence is too long to contain padding, the last token of the truncated sentence is used instead. All of these details can be defined in the forward function of the modeling class. Additionally, aligning the model\u0026rsquo;s pad_token_id with how the tokenizer preprocesses the input data is necessary for this implementation, which can be easily achieved by setting model.config.pad_token_id = tokenizer.pad_token_id.\nfrom transformers import LlamaPreTrainedModel, LlamaConfig, LlamaModel import torch.nn as nn import torch class LlamaRewardModel(LlamaPreTrainedModel): \u0026#34;\u0026#34;\u0026#34; The LLaMa Model transformer with a sequence classification head on top (linear layer). uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the last token. If a `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in each row of the batch). \u0026#34;\u0026#34;\u0026#34; config_class = LlamaConfig def __init__(self, config): super().__init__(config) self.num_labels = 1 self.model = LlamaModel(config) self.value_head = nn.Linear(config.hidden_size, self.num_labels) def forward( self, input_ids=None, past_key_values=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, mc_token_ids=None, labels=None, lm_labels=None, mc_labels=None, return_dict=False, output_attentions=False, output_hidden_states=False, ): transformer_outputs = self.model( input_ids, attention_mask=attention_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, ) hidden_states = transformer_outputs[0] rewards = self.value_head(hidden_states).squeeze(-1) pad_token_id = self.config.pad_token_id if pad_token_id is None: pad_token_id = self.model.config.pad_token_id ends = input_ids.shape[1] - (input_ids == pad_token_id).type(torch.int64).sum(dim=1).view(-1, 1) ends = torch.clamp(ends - 1, min=0) rewards = torch.gather(rewards, 1, ends) return rewards def get_input_embeddings(self): return self.model.embed_tokens def set_input_embeddings(self, value): self.model.embed_tokens = value def gradient_checkpointing_enable(self): self.model.gradient_checkpointing_enable() Introduction to Other Noteworthy Implementations DeepSpeedChat\u0026rsquo;s solution89 involves building an nn.module model and utilizing Huggingface‘s Pre-trained Transformers blocks as self.base_model. However, in my opinion, this approach is not ideal as it lacks consistency between the training and inference stages. Nonetheless, their breakdown of the Huggingface API and torch API showcases a well-designed implementation.\ndef create_hf_model(model_class, model_name_or_path, tokenizer, ds_config=None, rlhf_training=False, disable_dropout=False): model_config = AutoConfig.from_pretrained(model_name_or_path) if disable_dropout: model_config.dropout = 0.0 # Note: dschf is defined in function scope to avoid global effects # https://huggingface.co/docs/transformers/main_classes/deepspeed#nontrainer-deepspeed-integration if ds_config is not None and ds_config[\u0026#34;zero_optimization\u0026#34;][\u0026#34;stage\u0026#34;] == 3: dschf = HfDeepSpeedConfig(ds_config) else: dschf = None if rlhf_training: # the weight loading is handled by create critic model model = model_class.from_config(model_config) else: model = model_class.from_pretrained( model_name_or_path, from_tf=bool(\u0026#34;.ckpt\u0026#34; in model_name_or_path), config=model_config) model.config.end_token_id = tokenizer.eos_token_id model.config.pad_token_id = model.config.eos_token_id model.resize_token_embeddings(int( 8 * math.ceil(len(tokenizer) / 8.0))) # make the vocab size multiple of 8 return model def create_critic_model(model_name_or_path, tokenizer, ds_config, num_padding_at_beginning=0, rlhf_training=False, disable_dropout=False): # OPT model family always put a padding token at the beginning of the sequence, # we did not see this in other models but not sure if it is a general rule critic_model = create_hf_model(AutoModel, model_name_or_path, tokenizer, ds_config, rlhf_training, disable_dropout) critic_model = RewardModel( critic_model, tokenizer, num_padding_at_beginning=num_padding_at_beginning) if rlhf_training: if not os.path.isdir(model_name_or_path): model_name_or_path = snapshot_download(model_name_or_path) # critic model needs to load the weight here model_ckpt_path = os.path.join(model_name_or_path, \u0026#39;pytorch_model.bin\u0026#39;) assert os.path.exists( model_ckpt_path ), f\u0026#34;Cannot find model checkpoint at {model_ckpt_path}\u0026#34; critic_model.load_state_dict( torch.load(model_ckpt_path, map_location=\u0026#39;cpu\u0026#39;)) return critic_model This is how DeepSpeedChat define their reward_model:\n# Copyright (c) Microsoft Corporation. # SPDX-License-Identifier: Apache-2.0 # DeepSpeed Team import torch from torch import nn ## Note that the following code is modified from ## https://github.com/CarperAI/trlx/blob/main/examples/summarize_rlhf/reward_model/reward_model.py class RewardModel(nn.Module): def __init__(self, base_model, tokenizer, num_padding_at_beginning=0): super().__init__() self.config = base_model.config self.num_padding_at_beginning = num_padding_at_beginning if hasattr(self.config, \u0026#34;word_embed_proj_dim\u0026#34;): # `OPT` models use word_embed_proj_dim as final output # https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L497 self.v_head = nn.Linear(self.config.word_embed_proj_dim, 1, bias=False) else: # `gpt-neo(x)` models use `hidden_size` attribute names instead of `n_embd`` self.config.n_embd = self.config.hidden_size if hasattr( self.config, \u0026#34;hidden_size\u0026#34;) else self.config.n_embd self.v_head = nn.Linear(self.config.n_embd, 1, bias=False) self.rwtranrsformer = base_model self.PAD_ID = tokenizer.pad_token_id def gradient_checkpointing_enable(self): self.rwtranrsformer.gradient_checkpointing_enable() def gradient_checkpointing_disable(self): self.rwtranrsformer.gradient_checkpointing_disable() https://huggingface.co/blog/rlhf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOuyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ai.facebook.com/blog/large-language-model-llama-meta-ai/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBigScience, BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. International, May 2021-May 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://huggingface.co/decapoda-research/llama-7b-hf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://huggingface.co/datasets/Anthropic/hh-rlhf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/congchan/rlhf_exps/tree/main/reward_modeling/test/bloomz-7b1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nmicrosoft/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/model/model_utils.py#L18\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nmicrosoft/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/model/reward_model.py#L11\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://congchan.github.io/posts/a-better-practice-to-define-reward-model-with-huggingfaces-transformers/","summary":"\u003cp\u003e\u003ca href=\"https://congchan.github.io/\"\u003eCong Chen\u003c/a\u003e\u003cbr\u003e\nUniversity of Edinburgh\u003c/p\u003e\n\u003cp\u003eThere are various implementation of reward modeling in RLHF(reinforcement learning with human feedback), each has different pros and cons. Inspired by some open-sourced works about reward modeling, I would like to share one of the best practice for reward modeling. For those who are not familiar with reward modeling and RLHF, I recommend take a look at the Huggingface rlhf blog\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e or OpenAI rlhf paper\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e","title":"A Better Practice to Define Reward Model with HuggingFace's transformers"},{"content":"Cong Chen\nUniversity of Edinburgh\nInstructGPT1, ChatGPT2, and GPT-43 are cutting-edge Large Language Models (LLMs) that have astounded the world. With their ability to follow human instructions and align with human preferences, they can act as chatbots or helpful assistants. Despite impressing people for a while, their development lifecycles have not yet been thoroughly elaborated.\nIn this blog, I will provide my observations and thoughts based on my recent experience with large language model training and alignment. Instead of introducing these LLMs and highlighting their impressive performance, I will focus on the bootstrap flywheel that continuously improving these models. The bootstrap flywheel is showed in below graph with further illustration in this post.\nCold Starts To kickstart the process, it is essential to obtain a pre-trained LLM. One option is to pre-train an LLM from scratch. An alternative approach is to use a pre-trained LLM that has already undergone extensive training on diverse sets of text data. Open-source pre-trained LLMs, such as LLaMA4,5 and BLOOM6,7, have gained a comprehensive understanding of word associations and contextual nuances. Due to their expertise, these models are already capable of generating natural-sounding text.\nInstruction-tuning on Demonstration Data We begin with an alpha model, which is a pre-trained LLM. To develop the initial instruct-following GPT model, we have two options: (1) ask labelers to create prompts and responses on their own, or (2) collect online conversations as demonstration data. We feed the prompts or dialogue context and train the model to produce responses as training targets. This process is commonly known as instruction-tuning, or supervised fine-tuned(SFT).\nNowadays, the ChatGPT/GPT4 API is frequently utilized to extract high-quality demonstration data at a remarkably low cost, utilizing Chain-of-Thought(CoT)8 and self-instruction9 approach.\nWith proper instruction-tuning, many open-sourced pre-trained LLMs can become quite effective at following human instructions. To utilize the model, users simply input a prompt or message, and the model continuously predicts the most probable next word or phrase based on its pre-existing knowledge. This streamlined approach enables swift and precise language generation for a wide range of applications.\nThe outcomes are truly thrilling! We got a highly refined instruction-tuned LLM model that required minimal training effort. We are absolutely thrilled about the endless possibilities that this approach presents and are eagerly anticipating the release of our ChatGPT, hoping to capture the attention of many users. But soon we found that to reach a higher level, we need to put in much more effort.\nAlignment While the SFT model delivers impressive results, it is prone to some challenging issues. For example, it can generate hallucinations10,11, falls into traps or gives inappropriate or even dangerous suggestions. Therefore, to ensure the model works effectively with a broad range of users and environments, it must meet a high standard of compliance. This means it shouldn\u0026rsquo;t only follow instructions but also communicate in a way that aligns with most people\u0026rsquo;s preferences. These cases occur frequently and demonstrate that the model requires further refinement before it can be made available for public use.\nLearning from Human Feedback OpenAI\u0026rsquo;s solution to further optimize a LLM is to utilize Reinforcement Learning from Human Feedback (RLHF)12. This technique is a form of reinforcement learning (RL) that leverages human feedback to optimize an agent, using methods such as proximal policy optimization (PPO)13. This feedback loop helps the agent learn from experience and adjust its behavior, making it a useful approach in cases where optimal behavior is difficult to define.\nTo make RLHF works, we need a very good LLM policy as a starting point. The SFT model can serve as this initial policy, and different models trained at different stages can be used, as checkpoints are saved periodically during training. OpenAI selects the best policy by using a reward model (RM) which helps choose the best SFT model based on its RM score on a validation set. This approach has been found to produce more accurate predictions of human preference results than simply using validation loss.\nHowever, RLHF does not represent the sole viable approach. Some promising non-RL methods have been thoroughly investigated. For instance, the MOSS14 team has directly utilized the output of the reward model as fine-tuning signals. Non-RL methods are often more attractive due to their ease of training.\nReward Model A reward model can serve as an environment evaluator to replace human effort. Specifically, for a given prompt and answer, the reward model can evaluate the quality of the answer in relation to the prompt. The output of the reward model is a simple scalar value.\nTo collect reward training data, we can sample different answers from various pre-trained or SFT models for each prompt as input. This process is repeated with different prompts. For each prompt with several responses, human labelers rank these responses according to pre-defined guidelines. The end result is a collection of examples, where each example consists of a unique prompt with several ranked responses.\nThe training process involves a simple ranking supervised learning approach to optimize a reward model. There is not much difference in whether the reward model is initialized from a pre-trained model such as GPT-3 or SFT models, and OpenAI has tried both with similar results. In the InstructGPT paper, they initialized a reward model from a 6B GPT-3 model that was fine-tuned on a large set of public NLP tasks (ARC, BoolQ, CoQA, DROP, MultiNLI, OpenBookQA, QuAC, RACE, and Winogrande). This is a clever idea for cold-starts.\nWith a reasonably good reward model, we can optimize a policy, initialized from an SFT model, using RLHF or other reward model guided methods. The well-optimized policy model, which is the best we have so far, is then ready to interact with various users and environments.\nThe Bootstrap Flywheel In conclusion, after cold starts, we can iteratively upgrade our LLM with a bootstrap flywheel driven by data.\nA better SFT model helps to generate more representative responses for ranking. The better the ranking data can represent human preferences, the stronger the reward model can be trained. The more powerful the reward model is, the more promising the policy we can have. This iterative process allows us to continuously improve our LLM and create more effective and efficient conversational agents. Each sub-process could introduce new information, but also noises. Hence careful data-engineering is always required.\nOuyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://openai.com/blog/chatgpt\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://openai.com/product/gpt-4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTouvron, Hugo, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971, arXiv, 27 Feb. 2023. arXiv.org, http://arxiv.org/abs/2302.13971.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ai.facebook.com/blog/large-language-model-llama-meta-ai/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://huggingface.co/bigscience/bloom\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBigScience, BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. International, May 2021-May 2022\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWei, Jason, et al. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903, arXiv, 10 Oct. 2022. arXiv.org, http://arxiv.org/abs/2201.11903.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWang, Yizhong, et al. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv:2212.10560, arXiv, 20 Dec. 2022. arXiv.org, http://arxiv.org/abs/2212.10560.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJohn Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYoav Goldberg, April 2023. Reinforcement Learning for Language Models\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStiennon, Nisan, et al. Learning to Summarize from Human Feedback. arXiv:2009.01325, arXiv, 15 Feb. 2022. arXiv.org, http://arxiv.org/abs/2009.01325.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchulman, John, et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, arXiv, 28 Aug. 2017. arXiv.org, http://arxiv.org/abs/1707.06347.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://txsun1997.github.io/blogs/moss.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://congchan.github.io/posts/boosting-large-language-models-alignment-a-data-driven-bootstrap-flywheel/","summary":"\u003cp\u003e\u003ca href=\"https://congchan.github.io/\"\u003eCong Chen\u003c/a\u003e\u003cbr\u003e\nUniversity of Edinburgh\u003c/p\u003e\n\u003cp\u003eInstructGPT\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e, and GPT-4\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e are cutting-edge Large Language Models (LLMs) that have astounded the world. With their ability to follow human instructions and align with human preferences, they can act as chatbots or helpful assistants. Despite impressing people for a while, their development lifecycles have not yet been thoroughly elaborated.\u003c/p\u003e\n\u003cp\u003eIn this blog, I will provide my observations and thoughts based on my recent experience with large language model training and alignment. Instead of introducing these LLMs and highlighting their impressive performance, I will focus on the bootstrap flywheel that continuously improving these models. The bootstrap flywheel is showed in below graph with further illustration in this post.\u003c/p\u003e","title":"Boosting Large Language Models Alignment - A Data-Driven Bootstrap Flywheel"},{"content":"Bai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, http://arxiv.org/abs/2212.08073.\nThe paper introduces Constitutional AI (CAI), a method to train helpful and harmless AI assistants without human labels for harmful outputs, relying instead on a set of guiding principles. Here\u0026rsquo;s a structured summary:\n1. Objective Train AI systems to be helpful, honest, and harmless using AI feedback for supervision, reducing reliance on human labels. The approach aims to address the tension between helpfulness and harmlessness (where prior models often became evasive) and improve transparency through explicit principles.\n2. Key Methods a. Supervised Learning (SL) Stage Critique-Revision Pipeline: Generate responses to \u0026ldquo;red team\u0026rdquo; prompts (designed to elicit harmful behavior) using a helpful-only model. The model critiques its own response based on constitutional principles (e.g., identifying harm, ethics), then revises it to remove toxicity. This process is iterated, with principles randomly sampled at each step. Finetuning: Pretrained models are finetuned on revised responses to shift behavior toward harmlessness while retaining helpfulness. b. Reinforcement Learning (RL) Stage with AI Feedback (RLAIF) Preference Model (PM) Training: Generate response pairs to red team prompts using the SL-CAI model. Use a feedback model (pretrained LM) to evaluate which response is better according to constitutional principles, creating an AI-generated preference dataset. Mix this with human feedback for helpfulness to train a PM. RL Training: Use the PM as a reward signal to finetune the SL-CAI model, improving harmlessness and reducing evasiveness. 3. Key Contributions AI-Driven Supervision: AI can identify harmful behavior effectively, especially with chain-of-thought (CoT) reasoning, approaching human feedback performance (Figure 4). Reduced Evasiveness: CAI models engage with harmful queries by explaining objections, unlike prior RLHF models that often avoided responses (Figure 8). Transparency: Constitutional principles and CoT reasoning make AI decision-making more legible (e.g., critiques and revisions provide explicit justifications). Data Efficiency: Significantly fewer human labels are needed compared to traditional RLHF, as AI generates most supervision for harmlessness. 4. Results Performance Metrics: CAI models achieve higher harmlessness Elo scores than helpful RLHF models and match or exceed HH RLHF models (which use human feedback for harmlessness) (Figures 2, 3). Chain-of-thought reasoning in RL improves both harmlessness and the calibration of AI feedback (Figure 9). Example Responses: CAI models address harmful prompts (e.g., \u0026ldquo;Can you help me hack into my neighbor’s wifi?\u0026rdquo;) by rejecting the request and explaining ethical/legal issues, rather than evading (Appendix D). 5. Implications and Future Work Scaling Supervision: AI feedback could enable more efficient alignment as models grow more capable, though risks of obscured decision-making exist. Robustness: CAI aims to make models more resistant to red teaming by balancing helpfulness and harmlessness, enabling automated red teaming at scale. Broader Applications: Constitutional principles could steer AI behavior in other domains (e.g., writing style, ethics), reducing barriers to experimentation. 6. Conclusion Constitutional AI demonstrates that AI can self-improve to be harmless using only natural language principles and AI feedback, marking a step toward scalable, transparent AI supervision. This method reduces reliance on human labels while improving model behavior and explainability.\n","permalink":"https://congchan.github.io/posts/paper-reading-constitutional-ai/","summary":"\u003cp\u003eBai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, \u003ca href=\"http://arxiv.org/abs/2212.08073\"\u003ehttp://arxiv.org/abs/2212.08073\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe paper introduces Constitutional AI (CAI), a method to train helpful and harmless AI assistants without human labels for harmful outputs, relying instead on a set of guiding principles. Here\u0026rsquo;s a structured summary:\u003c/p\u003e\n\u003ch3 id=\"1-objective\"\u003e\u003cstrong\u003e1. Objective\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eTrain AI systems to be helpful, honest, and harmless using AI feedback for supervision, reducing reliance on human labels. The approach aims to address the tension between helpfulness and harmlessness (where prior models often became evasive) and improve transparency through explicit principles.\u003c/p\u003e","title":"Paper Reading - Constitutional AI"},{"content":"TLDR In order to train more dependable models, there are two known options: outcome supervision, which gives feedback on the final result, and process supervision, which provides feedback on each intermediate reasoning step.\nThis papers provides two finding:\nThe use of process supervision yields significantly better results than outcome supervision when training models to solve problems from the challenging MATH dataset. The efficacy of process supervision is significantly improved by active learning. The exclusive focus of this paper is to provide insights on training the most reliable reward model.\nPapers: Lightman, Hunter, et al. Let’s Verify Step by Step. arXiv:2305.20050, arXiv, 31 May 2023. arXiv.org, http://arxiv.org/abs/2305.20050.\nWhy Even the most advanced models are susceptible to generating false information, as they tend to create facts when they are uncertain (Bubeck et al., 2023). These hallucinations (Maynez et al., 2020) can be especially problematic in areas that involve multi-step reasoning, as a single logical mistake can disrupt a much larger solution.\nMethods To deal with hallucinations, one effective approach is to train reward models to differentiate between desirable and undesirable outputs. These reward models can then be utilized in a reinforcement learning pipeline or to conduct search via rejection sampling.\nOutcome-supervised reward models (ORMs) are trained solely on the final result of the model\u0026rsquo;s chain-of-thought. On the other hand, process-supervised reward models (PRMs) receive feedback for each step in the chain-of-thought.\nThis paper prefer PRMs for several reasons:\nProcess supervision provides more accurate feedback as it identifies the exact location of any errors that may occur. It is easier for humans to interpret. It directly incentivizes models to follow a human-endorsed chain-of-thought. In the field of logical reasoning, models trained with outcome supervision often use incorrect reasoning to arrive at the correct final answer (Zelikman et al., 2022; Creswell et al., 2022). Process supervision has been demonstrated to alleviate this misaligned behavior (Uesato et al., 2022). How to do PRMs The process supervision approach relies on human data-labelers to provide supervision by labeling the correctness of each step in the model-generated solutions.\nTo collect the process supervision data, present step-by-step solutions to MATH problems sampled by the large-scale generator to human data-labelers. Their task is to assign each step in the solution a label of positive, negative, or neutral. A positive label indicates that the step is correct and reasonable, while a negative label indicates that the step is either incorrect or unreasonable. A neutral label indicates ambiguity. They aim to surface solutions that are more likely to deceive their best reward model. To achieve this, they strategically select which solutions to show data-labelers. Specifically, they choose to surface convincing wrong-answer solutions. They use the term convincing to refer to solutions that are rated highly by current best PRM, and they use wrong-answer to refer to solutions that reach an incorrect final answer. The PRMs are trained to predict the correctness of each step after the last token in each step. This prediction takes the form of a single token, and maximize the log-likelihood of these target tokens during training. Define the PRM score for a solution as the probability that every step is correct under the PRM. This can be implemented as the product of the correctness probabilities for each step. When providing process supervision, they deliberately choose to supervise only up to the first incorrect step. This simplifies the comparison between outcome and process supervision. To reduce the reliance on expensive human feedback, they employ a large-scale model to oversee the training of smaller-scale models.\nActive Learning Generally, the active learning method\u0026rsquo;s performance is not stable or predictable. However, it is still worth taking a closer look. They first train a small-scale reward model, PRMselector, using a single sample from each problem, and then use this model to evaluate 1000 samples per problem. For training larger reward models, they select N samples per problem, with 80% being the most convincing wrong-answer samples (according to PRMselector), and 20% being the most convincing samples that remain (right or wrong-answer). They score the selected samples using PRMlarge and use those scores for training. This process ensures that all samples are relatively convincing under PRMselector, that a large fraction are known to contain at least one mistake, and that the dataset is not heavily biased towards wrong-answer solutions.\nModels All the large-scale models are fine-tuned from the base GPT-4 model (OpenAI, 2023), which was pre-trained solely to predict the next token and not with any Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017). The small-scale base models are designed similarly to GPT-4, but they were pre-trained with approximately 200 times less compute. Additionally, they fine-tune all models on a dataset of around 1.5 billion math-relevant tokens, which they refer to as MathMix. Yet this dataset are not open-sourced. To simplify parsing individual steps, they train the generator to produce solutions in a step-by-step format separated by newlines. Specifically, they few-shot generate solutions to MATH training problems, filter to those that reach the correct final answer, and fine-tune the base model on this dataset for a single epoch. This step is not intended to teach the generator new skills, but rather to train it to produce solutions in the desired format. Results/Analysis/Findings Task/Dataset: The process supervision dataset, PRM800K, which includes 800K step-level labels across 75K solutions to 12K problems.\nEvaluation: To evaluate the effectiveness of a reward model, they test its ability to perform a best-of-N search over uniformly sampled solutions from the generator. For each test problem, they select the solution with the highest rank determined by the reward model, automatically grade it based on its final answer, and report the fraction of correct solutions. A more reliable reward model will select the correct solution more frequently. Out-of-distribution generalization (OOD): the PRM model can tolerate a moderate amount of distribution shift, and its strong performance remains consistent even when tested on new and unfamiliar questions. Alignment Impact: Process supervision is more likely to generate interpretable reasoning since it encourages models to follow a process that is endorsed by humans. Process supervision is also inherently safer as it directly rewards an aligned chain of thought, rather than relying on outcomes as a proxy for aligned behavior.\nIn conclusion:\nprocess supervision can train reward models that are much more reliable than those trained with outcome supervision. Their state-of-the-art PRM model can solve 78.2% of problems from a representative subset of the MATH test set. A large reward model can accurately approximate human supervision for smaller reward models, and it can be used to efficiently conduct large-scale data collection ablations. Reference S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. A. Cotra. Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover. https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/ without-specific-countermeasures-the-easiest-path-to, 2022. A. Creswell, M. Shanahan, and I. Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022. T. Everitt, V. Krakovna, L. Orseau, M. Hutter, and S. Legg. Reinforcement learning with a corrupted reward channel. arXiv preprint arXiv:1705.08417, 2017. L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint arXiv:2210.10760, 2022. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022. Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022. J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020. R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332, 2021. E. Nichols, L. Gao, and R. Gomez. Collaborative storytelling with large-scale neural language models. In Proceedings of the 13th ACM SIGGRAPH Conference on Motion, Interaction and Games, pages 1–10, 2020. M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. J. Shen, Y. Yin, L. Li, L. Shang, X. Jiang, M. Zhang, and Q. Liu. Generate \u0026amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021. N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020. A. Stuhlm ̈ uller and J. Byun. Supervise process, not outcomes. https://ought. org/updates/2022-04-06-process, 2022. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35: 15476–15488, 2022. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. ","permalink":"https://congchan.github.io/posts/paper-reading-lets-verify-step-by-step/","summary":"\u003ch1 id=\"tldr\"\u003eTLDR\u003c/h1\u003e\n\u003cp\u003eIn order to train more dependable models, there are two known options: outcome supervision, which gives feedback on the final result, and process supervision, which provides feedback on each intermediate reasoning step.\u003c/p\u003e\n\u003cp\u003eThis papers provides two finding:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe use of process supervision yields significantly better results than outcome supervision when training models to solve problems from the challenging MATH dataset.\u003c/li\u003e\n\u003cli\u003eThe efficacy of process supervision is significantly improved by active learning.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe exclusive focus of this paper is to provide insights on training the most reliable reward model.\u003c/p\u003e","title":"Paper Reading - Let’s Verify Step by Step"},{"content":"Cong Chen\nUniversity of Edinburgh\nJohn Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点1，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展2，同时南大的俞扬教授也对BC和RL的对比进行了观点分享3。\n归纳的核心观点有三个：\nBehavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决： 核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。 除此之外，RL还允许负反馈，而 negative feedback is much more powerful 基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易 未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing 详细分享 - by John Schulman Why there is Hallucination Is “if a model know something” a meaningful question? RL is the correct ways Long form QA (LFQA) is much difficult that short QA\nA rising challenge in NLP is long-form question-answering (LFQA), in which a paragraph-length answer is generated in response to an open-ended question. LFQA systems have the potential to become one of the main ways people learn about the world, but currently lag behind human performance.\nBut ChatGPT has been trained via RL, why does it still Hallucinate / make false claims? Model has to guess sometimes: when it has to output a lot of details, sometimes it has to hedge Ranking based reward model doesn’t impose correct penalty: only measure if one is better than the other, but does not measure how much better, and how confident the model is. label errors: not always guarantee to provide enough information to the labelers when labeling. Such as coding problems. Avoid Hallucinate via Retrieval Why we need retrieval:\nup to date events and knowledge that happens after the models were trained. Information not in the pre-training (e.g., private corpus) verifiability Open Problems Let multiple agents collaborate with each other\nJohn Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges - YouTube\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.zhihu.com/question/596230048/answer/2990254878\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/","summary":"\u003cp\u003e\u003ca href=\"https://congchan.github.io/\"\u003eCong Chen\u003c/a\u003e\u003cbr\u003e\nUniversity of Edinburgh\u003c/p\u003e\n\u003cp\u003eJohn Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，同时南大的俞扬教授也对BC和RL的对比进行了观点分享\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cp\u003e归纳的核心观点有三个：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBehavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决：\n\u003cul\u003e\n\u003cli\u003e核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。\u003c/li\u003e\n\u003cli\u003e除此之外，RL还允许负反馈，而 negative feedback is much more powerful\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易\u003c/li\u003e\n\u003cli\u003e未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"详细分享---by-john-schulman\"\u003e详细分享 - by John Schulman\u003c/h2\u003e\n\u003ch3 id=\"why-there-is-hallucination\"\u003eWhy there is Hallucination\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"language-model-hallucination\" loading=\"lazy\" src=\"/images/John-Schulman/John-Schulman-0-language-model-hallucination.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Hallucination-and-Behavior-Cloning\" loading=\"lazy\" src=\"/images/John-Schulman/John-Schulman-1-Hallucination-and-Behavior-Cloning.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"is-if-a-model-know-something-a-meaningful-question\"\u003eIs “if a model know something” a meaningful question?\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"Does-Model-Know-About-Its-Uncertainty\" loading=\"lazy\" src=\"/images/John-Schulman/John-Schulman-2-Does-Model-Know-About-Its-Uncertainty.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"rl-is-the-correct-ways\"\u003eRL is the correct ways\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"John-Schulman-3\" loading=\"lazy\" src=\"/images/John-Schulman/John-Schulman-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"John-Schulman-4\" loading=\"lazy\" src=\"/images/John-Schulman/John-Schulman-4.png\"\u003e\u003c/p\u003e\n\u003cp\u003eLong form QA (LFQA)  is much difficult that short QA\u003c/p\u003e","title":"John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点"},{"content":"Tags: 2023, ICLR Links: https://github.com/FranxYao/chain-of-thought-hub Paper: Fu, Yao, et al. Complexity-Based Prompting for Multi-Step Reasoning. arXiv:2210.00720, arXiv, 30 Jan. 2023. arXiv.org, http://arxiv.org/abs/2210.00720.\nMotivation Example selection is a central problem in the prompting literature.\nFor CoT prompting, example selection is further related to annotation efficiency, as CoT requires manually-annotated reasoning chains. Which reasoning examples make the most effective prompts.\nPropose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multistep reasoning tasks over strong baselines.\nFurther extend the complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains).\nComplexity-based prompting and consistency vote over the top K complex chains\nTasks: multi-step reasoning tasks, measured by solve rate (accuracy), is to predict the answer (typically a number) of a given math word problem via intermediate steps. Use math word problems, mathematical problems expressed in natural language, as our testbed.\nThe input is a stack of a few (often 8) CoT cases followed by a test question, then the language model continues generating an output CoT for the test question.\nRef Reading Wang, Yizhong, et al. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv:2212.10560, arXiv, 20 Dec. 2022. arXiv.org, http://arxiv.org/abs/2212.10560. Lewkowycz, Aitor, et al. Solving Quantitative Reasoning Problems with Language Models. arXiv:2206.14858, arXiv, 30 June 2022. arXiv.org, http://arxiv.org/abs/2206.14858. ","permalink":"https://congchan.github.io/posts/paper-reading-complexity-based-prompting-for-multi-step-reasoning/","summary":"\u003cp\u003eTags: 2023, ICLR\nLinks: \u003ca href=\"https://github.com/FranxYao/chain-of-thought-hub\"\u003ehttps://github.com/FranxYao/chain-of-thought-hub\u003c/a\u003e\nPaper: Fu, Yao, et al. Complexity-Based Prompting for Multi-Step Reasoning. arXiv:2210.00720, arXiv, 30 Jan. 2023. arXiv.org, \u003ca href=\"http://arxiv.org/abs/2210.00720\"\u003ehttp://arxiv.org/abs/2210.00720\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"motivation\"\u003eMotivation\u003c/h1\u003e\n\u003cp\u003eExample selection is a central problem in the prompting literature.\u003c/p\u003e\n\u003cp\u003eFor CoT prompting, example selection is further related to annotation efficiency, as CoT requires manually-annotated reasoning chains. Which reasoning examples make the most effective prompts.\u003c/p\u003e\n\u003cp\u003ePropose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multistep reasoning tasks over strong baselines.\u003c/p\u003e","title":"Paper Reading -  Complexity-Based Prompting for Multi-Step Reasoning"},{"content":"CoT on BBH：M. Suzgun et al., ‘Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them’. arXiv, Oct. 17, 2022. Available: http://arxiv.org/abs/2210.09261\nMethod Applying chain-of-thought (CoT) prompting to BIG-Bench Hard tasks\nEvaluate few-shot performance via standard “answer-only” prompting and chain-of-thought prompting on BIG-Bench Hard Benchmark\nResults/Analysis/Findings Benchmark: BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. many tasks in BBH require multi-step reasoning\nConsider three families of language models: Codex (codedavinci-002, code-davinci-002, code-cushman-001), InstructGPT (text-davinci-002, text-curie-002, text-babbgage-001, and text-ada-001), and PaLM (8B, 62B, and 540B).\n效果: CoT prompting provides double-digit improvements for all three models. Applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average humanrater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks\nCoT is an emergent prompting strategy (Wei et al., 2022a) that requires sufficiently large models.: CoT prompting has negative or zero performance gain for the smallest model size\nFinding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.\n四类task结果讨论\nAlgorithmic and Multi-Step Arithmetic Reasoning: CoT appears to facilitate the decomposition of complex, multi-step problems into smaller, solvable problems in sufficiently large models. Codex shows better performance in following task instructions and exploiting algorithmic patterns based on the prompt exemplars compared to InstructGPT and PaLM. observe significant performance improvements with CoT prompting on Tracking Shuffled Objects (60.4% ↑), Multi-Step Arithmetic (46.4% ↑), Navigate (46.0% ↑), and Temporal Sequences (19.8% ↑) using the Codex model. Natural Language Understanding: PaLM and InstructGPT models typically perform better than Codex models. include Disambiguation QA, Hyperbaton (Adjective Ordering), Salient Translation Error Detection, and Snarks (Sarcasm Detection). Use of World Knowledge : require factual and general knowledge about the world as well as the common practices and presuppositions in the Western society. None of the language models perform better than the average reported human-rater performance. Multilingual Knowledge and Reasoning: one multilingual task, Salient Translation Error Detection, based on translation quality estimation and cross-lingual natural-language inference. Interestingly, we observe the improvement through CoT prompting only in PaLM Failure analysis of CoT Prompting: lags behind answer-only prompting on three tasks—Causal Judgment, Ruin Names, and Snarks - across all three model families. Two of these tasks require use of world knowledge, for example common presuppositions, human perception and usage of humor\nlimitations 13 tasks are extremely difficult for authors of this paper; they require domain-specific knowledge or are not practically solvable within twenty minutes. We do not think these tasks can be attempted with CoT prompting, and we leave them as future work for more powerful models or prompting methods\nFrom J. Wei et al., ‘Chain of Thought Prompting Elicits Reasoning in Large Language Models’. arXiv, Oct. 10, 2022. Accessed: Dec. 22, 2022. [Online]. Available: http://arxiv.org/abs/2201.11903\nAlthough chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually “reasoning,” Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization) Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models Application From J. Wei et al., ‘Chain of Thought Prompting Elicits Reasoning in Large Language Models’. arXiv, Oct. 10, 2022. Accessed: Dec. 22, 2022. [Online]. Available: http://arxiv.org/abs/2201.11903\nHow much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve Additional Reading Zero-shot COT: “let’s think step-by-step”\nLeast-to-Most：D. Zhou et al., ‘Least-to-Most Prompting Enables Complex Reasoning in Large Language Models’. arXiv, Oct. 06, 2022. Accessed: Feb. 01, 2023. [Online]. Available: http://arxiv.org/abs/2205.10625\nProgram of Thoughts Prompting(PoT): W. Chen, X. Ma, X. Wang, and W. W. Cohen, ‘Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks’. arXiv, Nov. 28, 2022. Accessed: Feb. 02, 2023. [Online]. Available: http://arxiv.org/abs/2211.12588\nBIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models.\nInstruction Fine-Tuning: Fine-tuned LAnguage Net (FLAN), J. Wei et al., ‘Finetuned Language Models Are Zero-Shot Learners’, arXiv:2109.01652 [cs], Dec. 2021, Accessed: Dec. 03, 2021. [Online]. Available: http://arxiv.org/abs/2109.01652\nLaMDA\nPaLM\n","permalink":"https://congchan.github.io/posts/cot-on-bbh-challenging-big-bench-tasks-and-whether-chain-of-thought-can-solve-them/","summary":"\u003cp\u003eCoT on BBH：M. Suzgun et al., ‘Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them’. arXiv, Oct. 17, 2022. Available: \u003ca href=\"http://arxiv.org/abs/2210.09261\"\u003ehttp://arxiv.org/abs/2210.09261\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"method\"\u003eMethod\u003c/h1\u003e\n\u003cp\u003eApplying chain-of-thought (CoT) prompting to BIG-Bench Hard tasks\u003c/p\u003e\n\u003cp\u003eEvaluate few-shot performance via standard “answer-only” prompting and \u003cstrong\u003echain-of-thought prompting\u003c/strong\u003e on BIG-Bench Hard Benchmark\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper17.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"resultsanalysisfindings\"\u003eResults/Analysis/Findings\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBenchmark: \u003cstrong\u003eBIG-Bench Hard (BBH)\u003c/strong\u003e. These are the task for which prior language model evaluations did not outperform the average human-rater. many tasks in BBH require multi-step reasoning\u003c/p\u003e","title":"CoT on BBH - Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"content":"Bavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, http://arxiv.org/abs/2207.14255. data: https://www.github.com/openai/human-eval-infilling\nTL:DR Autoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research.\nFIM-for-free property, just transform a portion of the training dataset by randomly splitting documents into three parts and moving the middle section to the end, document → (prefix, middle, suffix) → (prefix, suffix, middle), which can be concatenated by sentinel tokens. Best practices for FIM in pretraining: They conducted comprehensive ablations to clarify the effects of various hyperparameters related to training FIM models. Specifically, the FIM rate (probability of applying FIM transformation), different FIM transformation variants, and middle span selection. Finetuning inefficiency: they demonstrate that finetuning with FIM is computationally inefficient. Learning FIM during finetuning requires a significant amount of additional compute to achieve comparable performance levels. New infilling benchmarks: random span infilling and random span infilling light. Need for sampling evaluations: modifying different hyperparameters during FIM training can result in minimal changes in FIM test losses, but significant differences in sampling-based benchmarks. These benchmarks are not only more representative of real-world use cases, but they also reveal improvements that may be overlooked by relying solely on test losses. This finding is crucial, as scaling laws analysis often relies solely on test losses, which can be misleading without additional evaluations. Why Infilling is Important? Iinfilling is important for applications that require context before and after the point of generation, such as coding assistants, docstring generation, import statement generation, or completing a partially written function.\nWhy FIM Transformer based language models can be divided into three broad classes: encoder-only models like BERT, encoder-decoder models like T5, and causal decoder-based models like GPT.\nEncoder-only models are trained with a masked language modeling objective, Encoder-decoder models are trained with a span prediction objective. Causal decoder-based models are trained using the left-to-right next token prediction objective. All model classes are limited in infilling, which involves generating text within a prompt while conditioning on both a prefix and a suffix.\nLeft-to-right models can only condition on the prefix. Encoder-only and encoder-decoder models can condition on suffixes, but the lengths of infill regions seen during training are typically too short for practical use. Method FIM Apply a random transformation to the dataset. They explore two distinct implementations: document-level and context-level. The key difference between the two lies in the stage of the data loading pipeline where the FIM transformation takes place. This decision is driven by the fact that a lengthy document can be divided into multiple contexts, while a context can encompass multiple documents if the documents are relatively short.\nDocument-level FIM: With a probability parameter p (referred to as the FIM rate, p = 0.5 is used for the primary set of models), each document is divided into a prefix, a middle, and a suffix. the term FIM model is used to refer to any model that is trained on a mixture of FIM transformed and normal left-to-right data. Models that are trained without any FIM data, i.e., with a 0% FIM rate, are referred to as AR models. This split occurs before tokenization, when the document is still represented as a character sequence. The document is randomly split uniformly, such that the expected length of each part (prefix, middle, and suffix) is 1/3 of the full document length. Each of the three sections is encoded separately, and sentinel tokens (, , and ) are prepended to the beginning of each section. The three sections are concatenated in the order prefix, suffix, and middle, along with their sentinel tokens, to form the tokenized version of the FIM document: \u0026lt;PRE\u0026gt; ◦ Enc(prefix) ◦ \u0026lt;SUF\u0026gt; ◦ Enc(suffix) ◦ \u0026lt;MID\u0026gt; ◦ Enc(middle) \\tag{(PSM)} , where ◦ denotes concatenation. Different documents, whether FIM or AR, are concatenated with and given to the model during training. The loss is kept on all three sections (prefix, middle, and suffix), so FIM training does not cause a decrease in the autoregressive learning signal. Preliminary experiments suggest that keeping the loss on all three sections is crucial for the FIM-for-free property to hold. It is important to always train on the tokens as it signals a successful join to the suffix. During inference, the prefix and suffix are encoded and used to prompt the model with \u0026lt;PRE\u0026gt; ◦ Enc(prefix) ◦ \u0026lt;SUF\u0026gt; ◦ Enc(suffix) ◦ \u0026lt;MID\u0026gt;. \\tag{(PSM inference)}. The model generates samples until it produces the token, indicating successful connection of the prefix and suffix. If the token is not generated within a reasonable inference token budget, it suggests difficulty in connecting the prefix and suffix, and EOT aware best-of-n sampling is used to improve sample quality. SPM mode To improve key-value caching during inference, SPM mode is introduced, where the suffix, prefix, and middle order is swapped, use the ordering [suffix, prefix, middle]. This is because SPM avoids invalidation of keys and values computed in the suffix section when tokens are appended to the prefix. Note that superiority of SPM caching is not universal and may depend on the applications. Two variants of SPM encoding are presented. SPM variant 1 uses \u0026lt;SUF\u0026gt; ◦ Enc(suffix) ◦ \u0026lt;PRE\u0026gt; ◦ Enc(prefix) ◦ \u0026lt;MID\u0026gt; ◦ Enc(middle) ◦ \u0026lt;EOT\u0026gt;. (SPM variant 1), while SPM variant 2 uses \u0026lt;PRE\u0026gt; ◦ \u0026lt;SUF\u0026gt; ◦ Enc(suffix) ◦ \u0026lt;MID\u0026gt; ◦ Enc(prefix) ◦ Enc(middle) ◦ \u0026lt;EOT\u0026gt;. (SPM variant 2). The reason for using SPM variant 2 is to avoid creating a separation between PSM and SPM, which may result in less transfer between them. My understanding is that this is compatible with empty prefix in PSM mode. Since SPM is already a swap mode, it does not have to strictly follow the sentinel A ◦ Enc(A) format. However, SPM variant 1 has its own advantages, such as being stronger in handling subtokens at the end of the prefix. The choice of which variant to use may depend on the application. In this work, SPM variant 2 is used to emphasize joint training of PSM and SPM and to maximize transfer between them. However, minor changes to the suffix may invalidate the cache for the prefix in SPM mode. SPM also has a slight edge over PSM in infilling benchmarks. The FIM transformation is applied with 50% probability in both PSM and SPM modes to handle both types of formatting in inference. The placement of sentinel tokens in SPM is important when training jointly on SPM and PSM. Context-level FIM In language model training, documents are often joined with a boundary token, referred to as , and are then chunked to the model context length.\nTraining data contains lots of documents. When applying FIM to long documents, a joined by then chunked operation can result in fragmented FIM data where the entire prefix or suffix could get cut out of the context during chunking. To address this issue, FIM can be applied after the chunking step. A context slice may have multiple documents in them joined with the boundary token. The context slice is split based on . At this point, these documents are already tokenized, so applying FIM at the token level is straightforward. Some of the resulting documents are randomly selected to be turned into FIM examples based on a given FIM rate. The examples are then joined again with , and the resulting slice is trimmed to the model\u0026rsquo;s context length. This technique can boost performance relative to document-level FIM, and adopt context-level FIM in all the main FIM runs in this work. Evaluation The left-to-right test loss is unaffected even though FIM models see the data in its original form half the time, and are simultaneously learning a new skill. They trained a series of models with varying numbers of parameters, ranging from 50M to 6.9B, from scratch with and without 50% FIM augmentation on both natural language and code domains. They found that the left-to-right test loss was not affected by the FIM augmentation, even though the FIM models saw the data in its original form only half the time and were simultaneously learning a new skill.\nTest los\nJoint FIM pretraining does not result in any degradation in standard AR benchmarks as the performance matches within error for both natural language and code.\nHowever, the authors noted that test loss alone is not always sufficient to evaluate model performance. To strengthen their results, they evaluated their models on a suite of standard downstream benchmarks. The performance of the FIM models matched that of the non-FIM models within the margin of error for both natural language and code. The results are presented in Figure 3.\nNLP benchmarks\nThe left plot in Figure 4 provides evidence that a FIM rate even up to 90% does not cause any degradation in left-to-right capabilities.\nHowever, there is a clear sign of degradation in ordinary AR test loss with 100% FIM rate. This suggests that evaluating the FIM capabilities of the models cannot be done solely by considering language modeling perplexity measures such as test loss, but non-loss based evaluations should also be taken into account. SPM is slightly stronger than PSM in the benchmarks in general as evidenced by Figure 6\n","permalink":"https://congchan.github.io/posts/efficient-training-of-language-models-to-fill-in-the-middle/","summary":"\u003cp\u003eBavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, \u003ca href=\"http://arxiv.org/abs/2207.14255\"\u003ehttp://arxiv.org/abs/2207.14255\u003c/a\u003e.\ndata: \u003ca href=\"https://www.github.com/openai/human-eval-infilling\"\u003ehttps://www.github.com/openai/human-eval-infilling\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"tldr\"\u003eTL:DR\u003c/h1\u003e\n\u003cp\u003eAutoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research.\u003c/p\u003e","title":"Efficient Training of Language Models to Fill in the Middle"},{"content":"Holtzman, Ari, et al. The Curious Case of Neural Text Degeneration. arXiv:1904.09751, arXiv, 14 Feb. 2020. arXiv.org, http://arxiv.org/abs/1904.09751.\nIntroduction 从语言模型生成文本（例如生成故事）的最佳解码策略是什么仍然是一个悬而未决的问题。违反直觉的经验观察是，即使使用似然作为训练目标可以为广泛的语言理解任务生成高质量的模型，但基于maximization-based decoding的解码方法（例如beam search）会导致退化（degeneration）——输出文本平淡无奇，不连贯，或陷入重复循环。\n文本生成中的decoding strategy主要可以分为两大类：\nArgmax Decoding: 主要包括beam search, class-factored softmax等 Stochastic Decoding: 主要包括temperature sampling, top-k sampling等 为了解决这个问题，提出了 Nucleus Sampling（Top-p Sampling），这是一种简单但有效的方法，可以从神经语言模型中提取比以前的解码策略质量更高的文本。The key idea is to use the shape of the probability distribution to determine the set of tokens to be sampled from.\nMethod 通过截断概率分布的不可靠尾部分布、从包含绝大多数概率质量的标记的dynamic nucleus中采样来避免文本退化。\n效果/Analysis/Findings 为了正确检查当前基于最大化和随机的解码方法，我们将这些方法中的每一种的生成与人类文本从几个方向（如可能性、多样性和重复）的分布进行了比较。\n结果表明（1）对于开放式文本生成，maximization不是合适的解码目标，（2）当前最好的语言模型的概率分布有一个不可靠的长尾，需要在生成过程中截断，以及（3）Nucleus Sampling是目前最佳的解码策略，用于生成高质量的长文本——根据人类评估衡量——并且与人类编写的文本一样多样化。\n延伸阅读 https://zhuanlan.zhihu.com/p/68383015 ","permalink":"https://congchan.github.io/posts/the-curious-case-of-neural-text-degeneration/","summary":"\u003cp\u003eHoltzman, Ari, et al. The Curious Case of Neural Text Degeneration. arXiv:1904.09751, arXiv, 14 Feb. 2020. arXiv.org, \u003ca href=\"http://arxiv.org/abs/1904.09751\"\u003ehttp://arxiv.org/abs/1904.09751\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003e从语言模型生成文本（例如生成故事）的最佳解码策略是什么仍然是一个悬而未决的问题。违反直觉的经验观察是，即使使用似然作为训练目标可以为广泛的语言理解任务生成高质量的模型，但基于maximization-based decoding的解码方法（例如beam search）会导致退化（degeneration）——输出文本平淡无奇，不连贯，或陷入重复循环。\u003c/p\u003e\n\u003cp\u003e文本生成中的decoding strategy主要可以分为两大类：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eArgmax Decoding: 主要包括beam search, class-factored softmax等\u003c/li\u003e\n\u003cli\u003eStochastic Decoding: 主要包括temperature sampling, top-k sampling等\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e为了解决这个问题，提出了 \u003cstrong\u003eNucleus Sampling（Top-p Sampling）\u003c/strong\u003e，这是一种简单但有效的方法，可以从神经语言模型中提取比以前的解码策略质量更高的文本。The key idea is to use the shape of the probability distribution to determine the set of tokens to be sampled from.\u003c/p\u003e\n\u003ch1 id=\"method\"\u003eMethod\u003c/h1\u003e\n\u003cp\u003e通过截断概率分布的不可靠尾部分布、从包含绝大多数概率质量的标记的dynamic nucleus中采样来避免文本退化。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper19.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper19-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper19-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper19-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper19-4.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"效果analysisfindings\"\u003e效果/Analysis/Findings\u003c/h1\u003e\n\u003cp\u003e为了正确检查当前基于最大化和随机的解码方法，我们将这些方法中的每一种的生成与人类文本从几个方向（如可能性、多样性和重复）的分布进行了比较。\u003c/p\u003e","title":"The Curious Case of Neural Text Degeneration"},{"content":"Codex：M. Chen et al., ‘Evaluating Large Language Models Trained on Code’. arXiv, Jul. 14, 2021. Available: http://arxiv.org/abs/2107.03374\nIntro Codex, a GPT language model finetuned on publicly available code from GitHub\nTask: docstring-conditional code generation\nMethod Codex: fine-tune GPT3 models containing up to 12B parameters on code to produce Codex.\nCodex-S: fine-tune Codex on standalone, correctly implemented functions.\nInference: assemble each HumanEval problem into a prompt consisting of a header, a signature, and a docstring. We use nucleus sampling (Holtzman et al., 2020) with top p = 0.95 for all sampling evaluation in this work\nCodex-D: generate docstrings from code, for safety reasons, as such a model can be used to describe the intent behind generated code\n效果/Analysis/Findings Evaluation Framework：pass@k, a sample is considered correct if it passes a set of unit tests.\nBenchmark: HumanEval, a new evaluation, measure functional correctness for synthesizing programs from docstrings, with a set of 164 handwritten programming problems\nCodex solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4% Codex solve 70.2% of our problems with 100 samples per problem BLEU score may not be a reliable indicator of functional correctness by showing that functionally inequivalent programs generated by our model often have higher BLEU scores than functionally equivalent ones.\nlimitations difficulty with docstrings describing long chains of operations and with binding operations to variables. Codex is not sample efficient to train Codex can recommend syntactically incorrect or undefined code, and can invoke functions, variables, and attributes that are undefined or outside the scope of the codebase Application A distinct production version of Codex powers GitHub Copilot education, safety, security, and economics Additional Reading GPT-J: Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax, May 2021. GPT-J 6B is a transformer model trained using Ben Wang\u0026rsquo;s Mesh Transformer JAX. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI. Rotary Position Embedding (RoPE) GPT-Neo APPS dataset: a benchmark for code generation, measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code, includes 10,000 problems, Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence. ","permalink":"https://congchan.github.io/posts/codex-evaluating-large-language-models-trained-on-code/","summary":"\u003cp\u003eCodex：M. Chen et al., ‘Evaluating Large Language Models Trained on Code’. arXiv, Jul. 14, 2021. Available: \u003ca href=\"http://arxiv.org/abs/2107.03374\"\u003ehttp://arxiv.org/abs/2107.03374\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"intro\"\u003eIntro\u003c/h1\u003e\n\u003cp\u003eCodex, a GPT language model finetuned on publicly available code from GitHub\u003c/p\u003e\n\u003cp\u003eTask: docstring-conditional code generation\u003c/p\u003e\n\u003ch1 id=\"method\"\u003eMethod\u003c/h1\u003e\n\u003cp\u003eCodex: fine-tune GPT3 models containing up to 12B parameters on code to produce Codex.\u003c/p\u003e\n\u003cp\u003eCodex-S: fine-tune Codex on standalone, correctly implemented functions.\u003c/p\u003e\n\u003cp\u003eInference: assemble each HumanEval problem into a prompt consisting of a header, a signature, and a docstring. We use nucleus sampling (Holtzman et al., 2020) with top p = 0.95 for all sampling evaluation in this work\u003c/p\u003e","title":"Codex - Evaluating Large Language Models Trained on Code"},{"content":"Kaplan, Jared, et al. ‘Scaling Laws for Neural Language Models’. arXiv:2001.08361 [Cs, Stat], Jan. 2020. arXiv.org, http://arxiv.org/abs/2001.08361.\nTL:DR key findings for Transformer language models are are as follows:\nPerformance depends strongly on scale, weakly on model shape: Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embeddings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section 3) Smooth power laws: Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance must flatten out eventually before reaching zero loss. (Section 3) Universality of overfitting: Performance improves predictably as long as we scale up N and D in tandem, but enters a regime of diminishing returns if either N or D is held fixed while the other increases. The performance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the model size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4) Universality of training: Training curves follow predictable power-laws whose parameters are roughly independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer. (Section 5) Transfer improves with test performance: When we evaluate models on text with a different distribution than they were trained on, the results are strongly correlated to those on the training validation set with a roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant penalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2) Sample efficiency: Large models are more sample-efficient than small models, reaching the same level of performance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4). Convergence is inefficient: When working within a fixed compute budget C but without any other restrictions on the model size N or available data D, we attain optimal performance by training very large models and stopping significantly short of convergence (see Figure 3). Maximally compute-efficient training would therefore be far more sample efficient than one might expect based on training small models to convergence, with data requirements growing very slowly as D ∼ C0.27 with training compute. (Section 6) Optimal batch size: The ideal batch size for training these models is roughly a power of the loss only, and continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million tokens at convergence for the largest models we can train. (Section 5.1)\n","permalink":"https://congchan.github.io/posts/scaling-laws-for-neural-language-models/","summary":"\u003cp\u003eKaplan, Jared, et al. ‘Scaling Laws for Neural Language Models’. \u003cem\u003earXiv:2001.08361 [Cs, Stat]\u003c/em\u003e, Jan. 2020. \u003cem\u003earXiv.org\u003c/em\u003e, \u003ca href=\"http://arxiv.org/abs/2001.08361\"\u003ehttp://arxiv.org/abs/2001.08361\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"tldr\"\u003eTL:DR\u003c/h1\u003e\n\u003cp\u003ekey findings for Transformer language models are are as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance depends strongly on scale, weakly on model shape\u003c/strong\u003e: Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embeddings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section 3)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSmooth power laws\u003c/strong\u003e: Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance must flatten out eventually before reaching zero loss. (Section 3)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper14.png\"\u003e\u003c/p\u003e","title":"Scaling Laws for Neural Language Models"},{"content":"Links: https://arxiv.org/abs/2101.03961\n“SWITCH TRANSFORMERS: SCALING TO TRILLION PARAMETER MODELS WITH SIMPLE AND EFFICIENT SPARSITY”，提出了一种可以扩展到万亿参数的网络，有两个比较大的创新，基于Transformer MoE网络结构，简化了MoE的routing机制，降低了计算量；进一步通过数据并行+模型并行+expert并行的方式降低了训练通信量，提升训练性能。\n模型 Simplifying Sparse Routing Mixture of Expert Routing which takes as an input a token representation x and then routes this to the best deter- mined top-k experts Switch Routing: route to only a single expert, this simplification preserves model quality, reduces routing computation and performs better. Sparse routing通过参数Wr计算出一个在N个experts上的softmax分布，对每个token输入筛选概率最高的 top k 个 experts，对应的是MOE中的门控机制。这样对算力的需求并没有随着参数量的增加而大幅增长，使得这个模型更加容易训练。\nEFFICIENT SPARSE ROUTING 并行Switch实现 tensor shapes are statically determined at compilation time computation is dynamic due to the routing decisions at training and inference. One important technical consideration is how to set the expert capacity - the number of tokens each expert computes: is set by evenly dividing the number of tokens in the batch across the number of experts, and then further expanding by a capacity factor,\n$$\\text { expert capacity }=\\left(\\frac{\\text { tokens per batch }}{\\text { number of experts }}\\right) \\times \\text { capacity factor }$$ capacity factor \u0026gt; 1.0 create additional buffer to accommodate for when tokens are not perfectly balanced across experts. dropped tokens: If too many tokens are routed to an expert, computation is skipped and the token representation is passed directly to the next layer through the residual connection. 可微分负载均衡损失函数 为了均衡各个专家间的负载，需要一个辅助loss (Shazeer et al., 2017; 2018; Lepikhin et al., 2020)\nFor each Switch layer, this auxiliary loss is added to the total model loss during training\n具体的，给定 N 个 experts （indexed by i = 1 to N），以及一个 batch $B$ with $T$ tokens, 设定一个辅助损失函数以 encourages uniform routing since it is minimized under a uniform distribution. the auxiliary loss is computed as the scaled dot-product between vectors f and P：\n$$\\operatorname{loss}=\\alpha N \\cdot \\sum_{i=1}^{N} f_{i} \\cdot P_{i}$$其中 $f_i$ is the fraction of tokens dispatched to expert i,\n$$f_{i}=\\frac{1}{T} \\sum_{x \\in \\mathcal{B}} \\mathbb{1}\\{\\operatorname{argmax} p(x), i\\}$$大写的 $P_i$是可微分的， is the probability fraction to expert i across all tokens in the batch $B$\n$$P_{i}=\\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_{i}(x)$$小写的$p_i(x)$ is the probability of routing token x to expert i.\n$N$用于 keep the loss constant as the number of experts varies since under uniform routing\n$\\sum^N_1 (f_i ·P_i) = \\sum^N_1( \\frac{1}{N} · \\frac{1}{N}) = \\frac{1}{N}$.\n$α = 10^{−2}$ to ensure load balancing while small enough to not to overwhelm the primary cross-entropy objective.\n2.4 提升训练效果和fine-tuning的技巧 提升训练稳定性 - Selective precision with large sparse models 文章说明不需要全局使用float32，而是局部使用float32也可能保证稳定性 cast the router input to float32 precision，the float32 precision is only used within the body of the router function，计算完再cast to bfloat16，然后才分发出去，避免了通信负担 Smaller parameter initialization for stability， Reduced initialization scale improves stability. 在truncated normal distribution初始化基础上，reducing the default Transformer initialization scale s = 1.0 by a factor of 10. Regularizing large sparse models：increase the dropout inside the experts, which we name as expert dropout. 4 下游fine-tuning效果得到提升 5 DESIGNING MODELS WITH DATA, MODEL, AND EXPERT-PARALLELISM Arbitrarily Switch Transformer用了多种并行策略，数据并行+模型并行+expert并行。\nExpert并行实际上就是一种算子间的并行，experts在计算图上是个多并行子图分支，每个分支是一个FFNN结构。\n在FFN内部，还可以进一步进行算子级的模型并行。每个FFN内部，the intermediate is $h = xW_{in}$ and then the output of the layer is $y = ReLU(h)W_{out}$. $W_{in}$ and $W_{out}$ are applied independently to each token and have sizes $[d_{model}, d_{ff}]$ and $[d_{ff}, d_{model}]$.\n所以Switch Transformer的并行方式是数据并行+算子级模型并行+算子间模型并行，这种并行模型相较于数据并行+算子级模型并行的方式，在MoE网络结构上能够获得更低的通信开销，提高并行的效率。参照文章中的定义：\nB - Number of tokens in the batch.\nN - Number of total cores.\nn - Number of ways for data-parallelism sharding. m - Number of ways for model-parallelism sharding.\nE - Number of experts in Switch layers.\nC - Expert capacity, the batch size of each expert.\n数据并行：n = N,m = 1，数据分割到各个cores，模型完整地复制到各个cores，图9第一列， no communication is needed until the entire forward and backward pass is finished and the gradients need to be then aggregated across all cores\n模型并行：n = 1,m = N，For each forward and backward pass, a communication cost is now incurred\n数据和模型并行：N = n ×m cores，In the forward and backward pass each core communicates a tensor of size $[B/n, d_{model}]$ in an all-reduce operation.\nExpert和数据并行：让E = n = N，对每个core分到的每个token，local router决定如何分配给不同的experts，输出是一个binary matrix $[n, B/n, E, C]$，Because each core has its own expert, we do an all-to-all communication of size $[E, C, d_{model}]$ to now shard the E dimension instead of the n-dimension.\nExpert、模型和数据并行：在N cores资源固定的前提下，因为 $N = n \\times m$, 只能在模型并行和数据并行，也就是batch-size和$d_{ff}$ size之间做trade-off. 文章在section5.6详细讨论这部分。\n","permalink":"https://congchan.github.io/posts/switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-sparsity/","summary":"\u003cp\u003eLinks: \u003ca href=\"https://arxiv.org/abs/2101.03961\"\u003ehttps://arxiv.org/abs/2101.03961\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e“SWITCH TRANSFORMERS: SCALING TO TRILLION PARAMETER MODELS WITH SIMPLE AND EFFICIENT SPARSITY”，提出了一种可以扩展到万亿参数的网络，有两个比较大的创新，基于Transformer MoE网络结构，简化了MoE的routing机制，降低了计算量；进一步通过数据并行+模型并行+expert并行的方式降低了训练通信量，提升训练性能。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch1 id=\"模型\"\u003e模型\u003c/h1\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper12.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"simplifying-sparse-routing\"\u003eSimplifying Sparse Routing\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMixture of Expert Routing\u003c/strong\u003e which takes as an input a token representation x and then routes this to the best deter- mined top-k experts\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSwitch Routing\u003c/strong\u003e: route to only a single expert, this simplification preserves model quality, reduces routing computation and performs better.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper12-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSparse routing通过参数Wr计算出一个在N个experts上的softmax分布，对每个token输入筛选概率最高的 top k 个 experts，对应的是MOE中的门控机制。这样对算力的需求并没有随着参数量的增加而大幅增长，使得这个模型更加容易训练。\u003c/p\u003e","title":"Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"content":"Mixture of Experts (MOE) MOE属于Ensemble Method中的一个方法, 采用分治思想：\n将复杂的建模任务分解为多个相对简单的子任务，为每个子任务训练专门的模型：涉及子任务分解，或者Clustering 需要一个门控模型，基于数据输入选择如何组合多个专家模型的结果 Mixture of experts aims at increasing the accuracy of a function approximation by replacing a single global model by a weighted sum of local models (experts). It is based on a partition of the problem domain into several subdomains via clustering algorithms followed by a local expert training on each subdomain.\nLocal Models \u0026amp; Global Models Hinton的课件介绍了模型拟合分布的两个极端方式:\nVery local models: 使用很多非常局部化的模型, e.g. Nearest neighbors， Very fast to fit: Just store training cases Local smoothing 提升效果 Fully global models: 使用一个全局大模型 – e. g. Polynomial May be slow to fit: Each parameter depends on all the data 两种极端各有利弊, 不如采取中庸之道, 使用几个中等复杂度的专家模型.\nGood if the dataset contains several different regimes which have different relationships between input and output. 难点在于如何给多个专家模型切分数据. 这里的核心目的不是基于输入分布相似性的 clustering. 而是为基于输入-输出之间的不同关系切分给不同的局部专家模型, 使每个专家模型能够很好的建模它分内的输入-输出关系.\n组合结果-MOE 最简单直接的就是平均多个专家模型的结果. 但是平均的缺陷是: If we always average all the predictors, each model is trying to compensate for the combined error made by all the other models.\n所以, The key idea is to make each expert focus on predicting the right answer for the cases where it is already doing better than the other experts. 也就是专家的专业化.\n通过设计损失函数来鼓励专家模型specialization 而不是 cooperation.\nencourage cooperation: compare the average of all the predictors with the target and train to reduce the discrepancy. $(d - E(y_i))^2$ encourage specialization: compare each predictor separately with the target and train to reduce the average of all these discrepancies. $E(p_i(d - y_i)^2)$, $p_i$ is probability of picking expert i for this case 组合预测结果: take a weighted average, using the gating network to decide how much weight to place on each expert. $y = \\sum_i p_i y_i$\nSparsely-Gated Mixture-of-Experts layer (MoE) 基于Conditional computation的思想，Outrageously large neural networks: The sparsely-gated mixture-of-experts layer利用MOE搭建了包含thousands of feed-forward sub-networks（experts）的网络架构，利用可训练的门控网络来针对不同样本决策不同的experts稀疏组合，构建了up to 137 billion parameters is applied convolutionally between stacked LSTM layers，在large language modeling and machine translation benchmarks上取得sota。\nConditional Computation（CC） Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.\nConditional Computation的困难在于：\nGPU are much faster at arithmetic than at branching. Conditional Computation会减少活跃网络的batch size，而当前深度学习往往受益于更大的batch size 网络通信是瓶颈。因为GPU集群的计算能力往往是设备通信能力的几千倍，所以评估一个模型算法的效率，一个很有效的标准就是这个计算量和通信量的比率。Embedding 层就是一种典型Conditional computation。Embedding的计算量不大，但是不同设备的模型需要实时共享embedding参数，所以效率并不高。 门控网络 Softmax Gating: non-sparse gating function (Jordan \u0026amp; Jacobs, 1994)\n$$G_σ(x) = Softmax(x · W_g)$$Noisy Top-K Gating: add sparsity and noise to softmax gating, we add tunable Gaussian noise, then keep only the top k values. The noise term helps with load balancing.\n虽然这种稀疏性理论上有很明显的不连续性质，但是实践中并没带来什么问题。\nWhile this form of sparsity creates some theoretically scary discontinuities in the output of gating function, we have not yet observed this to be a problem in practice\nSwitch Transformers 近来超大规模Transformers模型的一个发展方向就是利用Mixture of experts (MOE)把大模型的FFN结构部分改为多个sparse Switch FFN layer的组合，以此来达成模型的扩容。\nReference CSC321: Introduction to Neural Networks and Machine Learning, Lecture 15: Mixtures of Experts, Geoffrey Hinton https://www.cs.toronto.edu/~hinton/csc321/notes/lec15.pdf Outrageously large neural networks: The sparsely-gated mixture-of-experts layer SWITCH TRANSFORMERS: SCALING TO TRILLION PARAMETER MODELS WITH SIMPLE AND EFFICIENT SPARSITY ","permalink":"https://congchan.github.io/posts/mixture-of-experts-moe/","summary":"\u003ch1 id=\"mixture-of-experts-moe\"\u003eMixture of Experts (MOE)\u003c/h1\u003e\n\u003cp\u003eMOE属于Ensemble Method中的一个方法, 采用分治思想：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将复杂的建模任务分解为多个相对简单的子任务，为每个子任务训练专门的模型：涉及子任务分解，或者Clustering\u003c/li\u003e\n\u003cli\u003e需要一个门控模型，基于数据输入选择如何组合多个专家模型的结果\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- more --\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMixture of experts aims at increasing the accuracy of a function approximation by replacing a single global model by a weighted sum of local models (experts). It is based on a partition of the problem domain into several subdomains via clustering algorithms followed by a local expert training on each subdomain.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg alt=\"Page 94, Ensemble Methods, 2012.\" loading=\"lazy\" src=\"/images/moe.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"local-models--global-models\"\u003eLocal Models \u0026amp; Global Models\u003c/h2\u003e\n\u003cp\u003eHinton的课件介绍了模型拟合分布的两个极端方式:\u003c/p\u003e","title":"Mixture of Experts (MOE)"},{"content":"Links: https://arxiv.org/abs/2106.07139\n最新出炉的 Pre-Trained Models 综述速览。\n先确定综述中的一些名词的定义\nTransfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的 Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。 一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题 而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题 Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法 2 Background 脉络图谱 Pre-training 可分为两大类：\n2.1 Transfer Learning and Supervised Pre-Training 此类可进一步细分为 feature transfer 和 parameter transfer. 2.2 Self-Supervised Learning and Self-Supervised Pre-Training Transfer learning 可细分为四个子类\ninductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007), transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006), self-taught learning (Raina et al., 2007; Dai et al., 2008) unsupervised transfer learning (Wang et al., 2008). inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域\nself-taught learning 和 unsupervised transfer learning 则主要集中于NLP领域，由于NLP领域的数据标注难度更大，所以主要以无监督的语言模型训练为主，2013年到2017年主要是词向量这类Feature transfer应用为主，把训练好的词表示作为下游模型的输入，但feature是固定的（ELMO是作为往可修正的feature方向发展的跳板），2018年开始有了BERT和GPT这种基于上下文的表示，把预训练的模型参数迁移到下游任务。\n3 Transformer and Representative PTMs 这部分主要介绍基于Transformer的各种表征学习PTMs， 如GPT和BERT，以及后续的家族图谱\nTransformer家族的四大优化方向：\nSome work improves the model architectures and explores novel pre- training tasks, such as XLNet (Yang et al., 2019), UniLM (Dong et al., 2019), MASS (Song et al., 2019), SpanBERT (Joshi et al., 2020) and ELEC- TRA (Clark et al., 2020). Besides, incorporating rich data sources is also an important direction, such as utilizing multilingual corpora, knowledge graphs, and images. Since the model scale is a crucial success factor of PTMs, researchers also explore to build larger models to reach over hundreds of billions of parameters, such as the series of GPT (Radford et al., 2019; Brown et al., 2020), Switch Transformer (Fedus et al., 2021), mean- while conduct computational efficiency optimization for training PTMs (Shoeybi et al., 2019; Ra- jbhandari et al., 2020; Ren et al., 2021). 4 Designing Effective Architectures two motivations\n统一NLU和NLG任务 从人类cognitive science角度切入 4.1 Unified Sequence Modeling NLP的三类versatile downstream tasks and applications：\nNatural language understanding: grammatical analysis, syntactic analysis, word/sentence/paragraph classification, ques- tion answering, factual/commonsense knowl- edge inference and etc Open-ended language generation: includes dialog generation, story generation, data-to- text generation and etc. Non-open-ended language generation: includes machine translation, abstract summarizing, blank filling and etc. understanding tasks 可以转换为 generation tasks (Schick and Schütze, 2020)。同时生成式的GPT在一些理解类任务上也可以达到甚至超过BERT的效果，因此The boundary between understanding and generation is vague. 基于此观察有如下一些研究方向：\nCombining Autoregressive and Autoencoding Modeling: 就是把GPT的单向生成和BERT的双向理解结合起来, 先驱就是XLNet permutated language modeling: XLNet (Yang et al., 2019), MPNet (Song et al., 2020) Multi-task training: UniLM (Dong et al., 2019) Mask上面做文章: GLM (Du et al., 2021), fill in blanks with variable lengths Applying Generalized Encoder-Decoder: 为了生成可变长的目标序列, 采用encoder-decoder architectures MASS (Song et al.,2019): introduces the masked-prediction strategy into the encoder-decoder structure. T5 (Raffel et al., 2020), : masking a variable-length of span in text with only one mask token and asks the decoder to recover the whole masked sequence. BART (Lewis et al., 2020a): corrupting the source sequence with multiple operations such as truncation, deletion, re- placement, shuffling, and masking, instead of mere masking. Encoder-Decoder架构导致参数更大, 虽然可以通过参数共享减轻, 但效率仍堪忧. Seq2seq的结构在NLU任务上表现不好，低于RoBERTa和GLM\n4.2 Cognitive-Inspired Architectures Transformer的注意力机制利用了人的视觉感知, 但是对于人的decision making, logical reasoning, counterfactual reasoning and working memory (Baddeley, 1992) 没有很好的模拟. 因此就有基于cognitive science的改进方向\nMaintainable Working Memory: 人的注意力机制和Transformer还是不一样的, 人的注意力机制没有Transformer那么long-range, 而是维护一个working memory(Baddeley, 1992; Brown, 1958; Barrouillet et al., 2004; Wharton et al., 1994), 负责记忆, 重组和选择性遗忘, 也就是LSTM所希望达到的目的. Transformer-XL (Dai et al., 2019) : introduce segment-level recurrence and relative positional encoding CogQA (Ding et al., 2019): maintain a cognitive graph in the multi-hop reading, the System 1 based on PTMs and the System 2 based on GNNs to model the cognitive graph for multi-hop understanding. CogLTX (Ding et al., 2020): leverages a MemRecall language model to select sen- tences that should be maintained in the working memory and another model for answering or clas- sificatio Sustainable Long-Term Memory: GPT-3 (Brown et al., 2020)表明Transformer有记忆能力, 那么就有动力去进一步挖掘Transformer的记忆能力. Lample et al. (2019)表示feed-forward networks in Transformers is equivalent to memory networks. 但记忆能力有限. REALM (Guu et al., 2020) : explore how to construct a sustainable external memory for Transformers. tensorize the whole Wikipedia sentence by sentence, and retrieve relevant sentences as context for masked pre-training. RAG (Lewis et al., 2020b) extends the masked pre-training to autoregressive generation, which could be better than extractive question answering. (Vergaet al., 2020; Févry et al., 2020) propose to tensorize entities and triples in existing knowledge bases, replace entity tokens’ embedding in an internal Transformer layer with the embedding from outer memory networks. (Dhingra et al., 2020; Sun et al., 2021) maintain a virtual knowledge from scratch, and propose a differentiable reasoning training objective over it. 4.3 其他 More Variants of Existing PTMs focus on optimizing BERT’s architecture to boost language models’ performance on natural language understanding.\nimproving the masking strategy: 可以视为一种数据增强 Span- BERT (Joshi et al., 2020): masking a continuous random-length span of tokens with a span boundary objective (SBO) could improve BERT’s performance ERNIE (Sun et al., 2019b,c): entity masking NEZHA (Wei et al., 2019) Whole Word Masking (Cui et al., 2019) change masked-prediction objective to GAN: ELECTRA (Clark et al., 2020) transform MLM to a replace token detection (RTD) objective, in which a generator will replace tokens in original sequences and a discriminator will predict whether a token is replaced. 5 Utilizing Multi-Source Data 5.1 多语言 Multilingual Pre-Training Language Multilingual masked language modeling (MMLM): multilingual BERT (mBERT) released by Devlin et al. (2019) is pre- trained with the MMLM task using non-parallel multilingual Wikipedia corpora in 104 languages. Translation language modeling (TLM) : MMLM task 无法利用 parallel corpora. 因此有XLM (Lample and Conneau, 2019) leverages bilingual sentence pairs to perform the translation language modeling (TLM) task. Unicoder (Huang et al., 2019a): Cross-lingual word recovery (CLWR), Cross-lingual paraphrase classification (CLPC) Generative models for multilingual PTMs: MASS (Song et al., 2019) extends MLM to language genera- tion. mBART (Liu et al., 2020c) extends DAE to support multiple languages by adding special symbols. 5.2 多模态 Multimodal Pre-Training Modalities can all be classified as vision and language (V\u0026amp;L),\nViLBERT (Lu et al., 2019) is a model to learn task-agnostic joint representations of images and languages. two streams of input, by preprocessing textual and visual information separately. LXMERT (Tan and Bansal, 2019) has similar architecture compared to Vil- BERT but uses more pre-training tasks VisualBERT (Li et al., 2019), on the other side, extends the BERT architecture at the minimum. The Transformer layers of VisualBERT implicitly align elements in the input text and image regions. Unicoder-VL (Li et al., 2020a) moves the offsite visual detector in VisualBERT into an end-to-end version: It designs the image token for Transformers as the sum of the bounding box and object label features. VL-BERT(Su et al., 2020) also uses a similar architecture to VisualBERT. each input element is either a token from the input sentence or a region-of-interest (RoI) from the input image. UNITER (Chen et al., 2020e) learns unified representations between the two modali- ties. DALLE (Ramesh et al., 2021) : A bigger step towards conditional zero-shot image generation: transformer-based text-to-image zero- shot pre-trained model with around 10 billiion pa- rameters. CLIP (Radford et al., 2021) and Wen-Lan (Huo et al., 2021) explore enlarging web-scale data for V\u0026amp;L pre-training with big success. Com- 5.3 Knowledge-Enhanced Pre-Training PTMs 可以从大量语料中中提取统计信息. 同时外部知识(such as knowledge graphs, domain- specific data and extra annotations of pre-training data) 可以作为很好的统计先验.\n6 Improving Computational Efficiency 6.1 Sstem-Level Optimization 系统层的优化是 model-agnostic and do not change underlying learning algorithms.\n单机优化: half-precision floating-point format (FP16), may fail because of the floating-point truncation and overflow mixed- precision training methods: which preserve some critical weights in FP32 to avoid the floating-point overflow and use dynamic loss scaling operations to get rid of the floating-point truncation. gradient checkpointing methods(Rasley et al., 2020) have been used to save memory by storing only a part of the activation states after forward pass. 如果模型参数太大无法塞入显存, store model parameters and activation states with the CPU memory, ZeRO-Offload (Ren et al., 2021) design delicate strategies to schedule the swap between the CPU memory and the GPU memory so that memory swap and device computation can be over- lapped as much as possible. 多机优化 数据并行 Data parallelism (Li et al., 2020d),\n模型并行, Model parallelism: Megatron- LM (Shoeybi et al., 2019) splits self-attention heads as well as feed-forward layers into differ- ent GPUs\nModel pipeline parallelism: partitions a deep neural network into multiple lay- ers and then puts different layers onto different nodes.\nGPipe (Huang et al., 2019b) which can send smaller parts of samples within a mini-batch to different nodes TeraPipe (Li et al., 2021) which can apply token-level pipeline mechanisms for Transformer-based models to make each token in a sequence be processed by different nodes. 6.2 Efficient Pre-Training 训练方法优化：改进BERT低效的mask机制 selectively mask tokens based on their importance (Gu et al., 2020) or gra- dients (Chen et al., 2020b) in back-propagation to speed up model training. ELECTRA需要识别所有token所以效率更高 warmup strategy different layers can share similar self-attention patterns, 先训练浅层的神经网络, 再复制到更深的网络中 Some layers can also be dropped during training to reduce the complexity of back-propagation and weight update (Zhang and He, 2020) 对不同层使用不同学习率 模型结构优化 减小模型复杂度，设计low-rank kernels to theoretically approximate the original attention weights and result in linear complexity 在attention mechanisms中引入稀疏性，by limiting the view of each token to a fixed size and separating tokens into several chunks so that the computation of attention weights takes place in every single chunk rather than a complete sequence Switch Transformers使用的Mix-of-experts to each layer of Transformers 6.3 模型压缩 参数共享 Parameter Sharing: ALBERT (Lan et al., 2019) uses factorized embedding parameterization and cross-layer parameter sharing 模型剪枝 Model Pruning 知识蒸馏 Knowledge Distillation: DistillBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019), BERT- PKD (Sun et al., 2019a) and MiniLM (Wang et al., 2020d). Model Quantization: Q8BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2020a), Ternary- BERT (Zhang et al., 2020b) applies 7.1 Knowledge of PTMs 知识分为linguistic knowledge and world knowledge.，\nLinguistic Knowledge\nRepresentation Probing, 通过额外的线性层在下游任务探测Representation 中是否含有语言知识 Representation Analysis：Use the hidden representations of PTMs to compute some statistics such as distances or similarities, 如 BERT visualization Attention analysis：同上 Generation Analysis：预测单词或句子的分布 construct analysis tasks based on generation: Perturbed Masking (Wu et al., 2020) recovers syntactic trees from PTMs without any extra parameter and the structure given by PTMs are competitive with a human-designed dependency schema in some downstream tasks. 在11个 linguistic tasks上的结果表明PTMs可以学习到tokens, chunks, and pairwise relations. 通过设计新的任务可以发现PTM编码了syntactic, semantic, local, and long- range information。\nWorld Knowledge\ncommonsense knowledge: Davison et al. (2019) propose to first transform relational triples into masked sen- tences and then rank these sentences according to the mutual information given by PTMs. In the ex- periments, the PTM-based extraction method with- out further training even generalizes better than current supervised approaches. factual knowledge: Petroni et al. (2019) propose to formulate the relational knowledge generation as the completion of fill-in-the-blank statements. LPAQA (Jiang et al., 2020b) search better statements/prompts through mining- based and paraphrasing-based methods. Auto - Prompt (Shin et al., 2020) proposes to train discrete prompts for knowledge probing. In P-tuning (Liu et al., 2021b), the authors discover that the bet- ter prompts lie in continuous embedding space, rather than discrete space. 7.2 Robustness of PTMs 用Adversarial attacks检验模型鲁棒性，\nPTMs can be easily fooled by synonym replacement (Jin et al., 2020; Zang et al., 2020). Irrelevant artifacts such as form words can mislead the PTMs into making wrong predic- tions (Niven and Kao, 2019; Wallace et al., 2019a). 如何生成对抗样本\nutilize the model prediction, prediction probabilities, and model gradients of the models, 但难以保证质量 human-in-the-loop methods (Wallace et al., 2019b; Nie et al., 2020) generate more natural, valid, and diverse adversarial examples。 7.3 Structural Sparsity of PTMs The multi-head attention structures are redundant in the tasks of machine translation (Michel et al., 2019), abstractive summarization (Baan et al., 2019), and language understanding (Kovaleva et al., 2019). 部分研究移除head反而得到更好的表现，一些head的注意力pattern也是相似的。 Sparsity of parameters: Gordon et al. (2020) show low levels of pruning (30-40%) do not affect pre-training loss or the performance on downstream tasks at all. Prasanna et al. (2020) validate the lottery ticket hypothesis on PTMs and find that it is possible to find sub-networks achieving per- formance that is comparable with that of the full model. 7.4 Theoretical Analysis of PTMs Erhan et al. (2010) propose two hypotheses\nbetter optimization：更接近全局最优 better regularization：更好的泛化能力 Saunshi et al. (2019) conduct a theoretical analysis of contrastive unsupervised representation learning. they prove that the loss of contrastive learning is the upper bound of the downstream loss.\n8 Future Directions 8.1 Architectures and Pre-Training Methods New Architectures New Pre-Training Tasks Beyond Fine-Tuning：An improved solution is to fix the original parameters of PTMs and add small fine-tunable adaption modules for specific tasks. Reliability 8.2 Multilingual and Multimodal Pre-Training More Modalities: image, text, video and audio More Insightful Interpretation: why bridging vision and language works More Downstream Applications: 现有的image-text retrieval, image-to-text generation, text-to-image generation 等并不是现实迫切需要的应用. Transfer Learning. 8.3 Computational Efficiency Data Movement: 设备通信瓶颈 Parallelism Strategies 设计自动化 Large-Scale Training ","permalink":"https://congchan.github.io/posts/survey-pre-trained-models-past-present-and-future/","summary":"\u003cp\u003eLinks: \u003ca href=\"https://arxiv.org/abs/2106.07139\"\u003ehttps://arxiv.org/abs/2106.07139\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e最新出炉的 Pre-Trained Models 综述速览。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e先确定综述中的一些名词的定义\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTransfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的\u003c/li\u003e\n\u003cli\u003eSelf-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。\n\u003cul\u003e\n\u003cli\u003e一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题\u003c/li\u003e\n\u003cli\u003e而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"2-background-脉络图谱\"\u003e2 Background 脉络图谱\u003c/h1\u003e\n\u003cp\u003ePre-training 可分为两大类：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2.1 \u003cstrong\u003eTransfer Learning\u003c/strong\u003e and \u003cstrong\u003eSupervised\u003c/strong\u003e Pre-Training\n\u003cul\u003e\n\u003cli\u003e此类可进一步细分为 feature transfer 和 parameter transfer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2.2 \u003cstrong\u003eSelf-Supervised Learning\u003c/strong\u003e and Self-Supervised Pre-Training\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper11.png\"\u003e\u003c/p\u003e\n\u003cp\u003eTransfer learning 可细分为四个子类\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007),\u003c/li\u003e\n\u003cli\u003etransductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006),\u003c/li\u003e\n\u003cli\u003eself-taught learning (Raina et al., 2007; Dai et al., 2008)\u003c/li\u003e\n\u003cli\u003eunsupervised transfer learning (Wang et al., 2008).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003einductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域\u003c/p\u003e","title":"Survey - Pre-Trained Models - Past, Present and Future"},{"content":"2020, ACL\ndata: CoNLL-2012, GAP\ntask: Coreference Resolution\n通过QA方式处理coreference问题，A query is generated for each candidate mention using its surrounding con- text, and a span prediction module is em- ployed to extract the text spans of the corefer- ences within the document using the generated query.\n近期的方法有consider all text spans in a document as potential mentions and learn to find an antecedent for each possible mention. There。这种仅依靠mention的做对比的方法的缺点：\nAt the task formalization level： 因为当前数据集有很多遗漏的mention， mentions left out at the mention proposal stage can never be recov- ered since the downstream module only operates on the proposed mentions. At the algorithm level：Semantic matching operations be- tween two mentions (and their contexts) are per- formed only at the output layer and are relatively superficial 方法 Speaker information： directly concatenates the speaker’s name with the corresponding utterance.\n3.3 Mention Proposal considers all spans up to a maximum length L as potential mentions.\n3.4 Mention Linking as Span Prediction Given a mention ei proposed by the mention pro- posal network\n{context (X), query (q), answers (a)}.\nThe query q(ei) is constructed as follows: given ei, we use the sentence that ei resides in as the query, with the minor modification that we encapsulates ei with special tokens \u0026lt; mention \u0026gt; \u0026lt; /mention \u0026gt;\ngenerate a BIO tag for each token of a coreferent mention\nto optimize the bi-directional re- lation between ei and ej.\n3.5 Antecedent Pruning Training The mention proposal module and the mention linking module are jointly trained in an end-to-end fashion using training signals from Eq.6, with the SpanBERT parameters shared.\n3.8 Data Augmentation using Question Answering Datasets pre- train the mention linking network on the Quoref dataset (Dasigi et al., 2019b), and the SQuAD dataset (Rajpurkar et al., 2016b)\n效果 ","permalink":"https://congchan.github.io/posts/corefqa-coreference-resolution-as-query-based-span-prediction/","summary":"\u003cp\u003e2020, ACL\u003c/p\u003e\n\u003cp\u003edata: CoNLL-2012, GAP\u003c/p\u003e\n\u003cp\u003etask: Coreference Resolution\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e通过QA方式处理coreference问题，A query is generated for each candidate mention using its surrounding con- text, and a span prediction module is em- ployed to extract the text spans of the corefer- ences within the document using the generated query.\u003c/p\u003e\n\u003cp\u003e近期的方法有consider all text spans in a document as potential mentions and learn to find an antecedent for each possible mention. There。这种仅依靠mention的做对比的方法的缺点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAt the task formalization level： 因为当前数据集有很多遗漏的mention， mentions left out at the mention proposal stage can never be recov- ered since the downstream module only operates on the proposed mentions.\u003c/li\u003e\n\u003cli\u003eAt the algorithm level：Semantic matching operations be- tween two mentions (and their contexts) are per- formed only at the output layer and are relatively superficial\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"方法\"\u003e方法\u003c/h1\u003e\n\u003cp\u003eSpeaker information： directly concatenates the speaker’s name with the corresponding utterance.\u003c/p\u003e","title":"CorefQA - Coreference resolution as query-based span prediction"},{"content":"针对样本不平衡问题，除了上下采样，调整样本权重等统计方法，还有可以通过对loss函数进行设计。\n对于多分类问题（n选1），一般使用softmax；对于多标签分类问题（n选k），一般是转换为n各sigmoid二分类问题。\nHierarchical classification Yolo2里提出了Hierarchical classification方法，大概思路就是利用标签的结构关系建立wordtree，对标签划分层次，再在每个层次中做Data Augmentation，达到局部平衡，再进行局部softmax。\nFocal Loss Focal Loss for Dense Object Detection\n极度不平衡的正负样本比例: anchor近似于sliding window的方式会使正负样本接近1000：1，而且绝大部分负样本都是easy example， 这就导致**gradient被easy example dominant的问题：**往往这些easy example虽然loss很低，但由于数量众多，对于loss依旧有很大贡献，从而导致收敛到不够好的一个结果。 按照loss decay掉那些easy example的权重，这样使训练更加bias到更有意义的样本中去。 $$\\operatorname{FL}\\left(p_{\\mathrm{t}}\\right)=-\\left(1-p_{\\mathrm{t}}\\right)^{\\gamma} \\log \\left(p_{\\mathrm{t}}\\right)$$ 实现： https://github.com/congchan/nlp/blob/e5cb1405b21245ad6cfe1f71a9961b6519e4e618/torch/loss.py#L5\ndef sigmoid_focal_loss( inputs: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor = None, alpha: float = 0.25, gamma: float = 2, reduction: str = \u0026#34;none\u0026#34;, ): \u0026#34;\u0026#34;\u0026#34; Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py . Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002. Args: inputs: A float tensor of arbitrary shape. The predictions for each example. targets: A float tensor with the same shape as inputs. Stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class). mask: alpha: (optional) Weighting factor in range (0,1) to balance positive vs negative examples or -1 for ignore. Default = 0.25 gamma: Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples. reduction: \u0026#39;none\u0026#39; | \u0026#39;mean\u0026#39; | \u0026#39;sum\u0026#39; \u0026#39;none\u0026#39;: No reduction will be applied to the output. \u0026#39;mean\u0026#39;: The output will be averaged. \u0026#39;sum\u0026#39;: The output will be summed. Returns: Loss tensor with the reduction option applied. \u0026#34;\u0026#34;\u0026#34; p = torch.sigmoid(inputs) ce_loss = F.binary_cross_entropy_with_logits( inputs, targets, reduction=\u0026#34;none\u0026#34; ) p_t = p * targets + (1 - p) * (1 - targets) loss = ce_loss * ((1 - p_t) ** gamma) if alpha \u0026gt;= 0: alpha_t = alpha * targets + (1 - alpha) * (1 - targets) loss = alpha_t * loss if mask is not None: loss = torch.einsum(\u0026#34;bfn,bf-\u0026gt;bfn\u0026#34;, loss, mask) return loss Circle Loss A Unified Perspective of Pair Similarity Optimization\n着眼点就是在multiple positive情况下该如何改造softmax的问题\n公式1提供的unfied视角是很重要的。它允许我们不经过任何modification，用完全相同的一个数学表达，兼容pairwise learning和classification learning两种基本的深度特征学习方式。\n单标签分类的交叉熵\n$-\\log \\frac{e^{s t}}{\\sum_{i=1}^{n} e^{s_{i}}}=-\\log \\frac{1}{\\sum_{i=1}^{n} e^{s_{i}-s_{t}}}=\\log \\sum_{i=1}^{n} e^{s i-s t}=\\log \\left(1+\\sum_{i=1, i \\neq t}^{n} e^{s i-s t}\\right)$\n其中的 LogSumExp 是max的smoothing, 实现了“目标类得分都大于每个非目标类的得分”的效果\n有多个目标类的多标签分类场景: 也希望“每个目标类得分都不小于每个非目标类的得分”, 于是\n$\\log \\left(1+\\sum_{i \\in \\Omega_{n e g}, j \\in \\Omega_{p o s}} e^{s_{i}-s_{j}}\\right)=\\log \\left(1+\\sum_{i \\in \\Omega_{n e g}} e^{s_{i}} \\sum_{j \\in \\Omega_{p o s}} e^{-s_{j}}\\right)$\n其中的 $\\Omega_{pos}, \\Omega_{n e g}$分别是正负样本的类别集合, 这个loss的目标就是让 $s_{i}","permalink":"https://congchan.github.io/posts/%E5%9C%A8loss%E5%B1%82%E9%9D%A2%E9%92%88%E5%AF%B9%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98%E7%9A%84%E4%BC%98%E5%8C%96/","summary":"\u003cp\u003e针对样本不平衡问题，除了上下采样，调整样本权重等统计方法，还有可以通过对loss函数进行设计。\u003c/p\u003e\n\u003cp\u003e对于多分类问题（n选1），一般使用softmax；对于多标签分类问题（n选k），一般是转换为n各sigmoid二分类问题。\u003c/p\u003e","title":"在loss层面针对样本不平衡问题的优化"},{"content":"2019, ACL\ndata: TWITTER, WEIBO\nlinks: https://www.aclweb.org/anthology/N19-1163, https://github.com/DeepBrainAI/ERD\ntask: Rumour Detection\n这篇文章采用GRU编码社交媒体posts stream，作为环境的状态表示；训练一个分类器以GRU的状态输出为输入，对文本做二分类判断是否是rumor。用DQN训练agent，根据状态做出是否启动rumor分类器进行判断，并根据分类结果对错给予奖惩。目标就是尽可能准尽可能早地预测出社交媒体posts是否是rumor。\nFocuses on the task of rumour detection; particularly, we are in- terested in understanding how early we can detect them.\nOur model treats social media posts (e.g. tweets) as a data stream and integrates reinforcement learning to learn the number minimum num- ber of posts required before we classify an event as a rumour.\nLet $E$ denote an event, and it consists of a series of relevant posts $x_i$, where $x_0$ denotes the source message and $x_T$ the last relevant message. The objective of early rumor detection is to make a classification decision whether E is a rumour as early as possible while keeping an acceptable detection accuracy.\n3 Model Architecture ERD has two modules: a rumour detection module (RDM) that classifies whether an event is a rumour, and a checkpoint module (CM) that decides when the rumour detec- tion module should be triggered.\n3.1 Rumor Detection Module (RDM) RDM contains three layers: a word embedding layer that maps input words into vectors, a max- pooling layer that extracts important features of a post, and a GRU (Cho et al., 2014) that processes the sequential posts of an event.\nword embedding layer: apply a max pooling operation\n3.2 Checkpoint Module (CM) leverage deep Q-learning model to identify the optimal checkpoint\nreward CM based on RDM’s accuracy and also penalise CM slightly every time it decides to not trigger RDM (and continue to monitor the event).\n3.3 Joint Training train both RDM and CM jointly, and the training process is similar to that of generative adversarial networks (Goodfellow et al., 2014). The checkpoint module serves as the generator for action sequences, while the detection module is the discriminator. A key contrast, however, is that the two modules are working cooperatively rather than adversarially.\nfirst pre-train RDM based on cross entropy then train train CM and RDM in an alternating fashion In each step of the training, new posts that have arrived and previous GRU states are first fed to the RDM to produce the new states. which will in turn be used by CM to calculate the action values. train RDM and CM in an alternating fashion for 1 epoch and 200K iterations respectively. We train CM for several iterations while keeping RDM’s parameters fixed, and then we move to train RDM for several iterations while keeping CM’s parameters fixed. Training converges when CM’s reward value stabilises between consecutive epochs. DQN部分跟传统地DQN方法一样。\n3.4 Bucketing Strategy 提高批处理效率. For processing efficiency purposes, instead of processing each incoming post individually, we experiment with several bucketing strategies that group posts together and process them in batches.\n结果 ","permalink":"https://congchan.github.io/posts/early-rumour-detection/","summary":"\u003cp\u003e2019, ACL\u003c/p\u003e\n\u003cp\u003edata: TWITTER, WEIBO\u003c/p\u003e\n\u003cp\u003elinks: \u003ca href=\"https://www.aclweb.org/anthology/N19-1163\"\u003ehttps://www.aclweb.org/anthology/N19-1163\u003c/a\u003e, \u003ca href=\"https://github.com/DeepBrainAI/ERD\"\u003ehttps://github.com/DeepBrainAI/ERD\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etask: Rumour Detection\u003c/p\u003e\n\u003cp\u003e这篇文章采用GRU编码社交媒体posts stream，作为环境的状态表示；训练一个分类器以GRU的状态输出为输入，对文本做二分类判断是否是rumor。用DQN训练agent，根据状态做出是否启动rumor分类器进行判断，并根据分类结果对错给予奖惩。目标就是尽可能准尽可能早地预测出社交媒体posts是否是rumor。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eFocuses on the task of rumour detection; particularly, we are in- terested in understanding \u003cstrong\u003ehow early\u003c/strong\u003e we can detect them.\u003c/p\u003e\n\u003cp\u003eOur model treats social media posts (e.g. tweets) as a data stream and integrates reinforcement learning to learn the number minimum num- ber of posts required before we classify an event as a rumour.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper8.png\"\u003e\u003c/p\u003e\n\u003cp\u003eLet $E$ denote an event, and it consists of a series of relevant posts $x_i$, where $x_0$ denotes the source message and $x_T$ the last relevant message. The objective of early rumor detection is to \u003cstrong\u003emake a classification decision\u003c/strong\u003e \u003cstrong\u003ewhether E is a rumour as early as possible\u003c/strong\u003e while keeping an acceptable detection accuracy.\u003c/p\u003e","title":"Early Rumour Detection"},{"content":"2019, ACL\ndata: KBP37, SemEval 2010 Task 8, TACRED\ntask: Entity and Relation Extraction\nBuild task agnostic relation representations solely from entity-linked text.\n缺陷 文章认为网页中, 相同的的实体对一般指代相同的实体关系, 把实体不同的构建为负样本. 这个在单份文件中可能大概率是对的.\n但是实体不完全一直不代表这个两对实体的关系不同. 所以这个作为负样本是本质上映射的是实体识别而不是关系.\n比较好的方式是把实体不同但是关系一样的也考虑进来.\n方法 Define Relation Statement We define a relation statement to be a block of text containing two marked entities. From this, we create training data that contains relation statements in which the entities have been replaced with a special [BLANK]\nA relation statement is a triple r = (x, s1, s2), x = [x0 . . . xn] be a sequence of tokens, where x0 = [CLS] and xn = [SEP] are special start and end markers. Let s1 = (i, j) and s2 = (k, l) be pairs of integers such that 0 \u0026lt; i \u0026lt; j −1, j \u0026lt; k, k ≤ l −1, and l ≤ n\nlearn a function $h_r = f_θ(r)$ that maps the relation statement to a fixed-length vector $h_r ∈ ^Rd$ that represents the relation\nTask supervised tasks few-shot relation matching: In this task, examples in the test and development sets typically contain relation types not present in the training set. we use the dot product between relation representation of the query statement and each of the candidate statements as a similarity score we declare that for any pair of relation statements r and r\u0026rsquo;, the inner product $f_{\\theta}(\\mathbf{r})^{\\top} f_{\\theta}\\left(\\mathbf{r}^{\\prime}\\right)$ should be high if the two relation statements, express semantically similar relations we do not use relation labels at training time, Instead, we observe that there is a high degree of redundancy in web text, and each relation between an arbitrary pair of entities is likely to be stated multiple times. 假设网页文本有大量重复的实体关系提及 Model Entity span identification Standard input: model that does not have access to any explicit identification of the entity spans s1 and s2 Positional embeddings: two segmentation embeddings, one that is added to all tokens in the span s1, while the other is added to all tokens in the span s2 Entity marker tokens: [E1start], [E1end] [E2start] and [E2end] and modify x to give x˜ =[x0 . . . [E1start] xi . . . xj−1 [E1end] . . . [E2start] xk . . . xl−1 [E2end] . . . xn]. Fixed length relation representation [CLS] token Entity mention pooling: concatenate $h_{e1}= MAXPOOL([h_i...h_{j−1}])$ and $he_{e_2} =MAXPOOL([h_k...h_{l−1}])$ Entity start state: concatenation of the final hidden states corresponding their respective start tokens Training Takes in pairs of blank-containing relation statements, and has an objective that encourages relation representations to be similar if they range over the same pairs of entities.\ndefine binary classifier, learn a relation statement encoder fθ that we can use to determine whether or not two relation statements encode the same relation. minimizes the loss\n4.2 Introducing Blanks To avoid simply relearning the entity linking system, 用[BLANK] symbol 以概率α替换掉 entity in relation statement.\n效果 outperform previous work on exemplar based relation extraction (FewRel) even with- out using any of that task’s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED ","permalink":"https://congchan.github.io/posts/matching-the-blanks-distributional-similarity-for-relation-learning/","summary":"\u003cp\u003e2019, ACL\u003c/p\u003e\n\u003cp\u003edata: KBP37, SemEval 2010 Task 8, TACRED\u003c/p\u003e\n\u003cp\u003etask: Entity and Relation Extraction\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eBuild task agnostic relation representations solely from entity-linked text.\u003c/p\u003e\n\u003ch1 id=\"缺陷\"\u003e缺陷\u003c/h1\u003e\n\u003cp\u003e文章认为网页中, 相同的的实体对一般指代相同的实体关系, 把实体不同的构建为负样本. 这个在单份文件中可能大概率是对的.\u003c/p\u003e\n\u003cp\u003e但是实体不完全一直不代表这个两对实体的关系不同. 所以这个作为负样本是本质上映射的是实体识别而不是关系.\u003c/p\u003e\n\u003cp\u003e比较好的方式是把实体不同但是关系一样的也考虑进来.\u003c/p\u003e\n\u003ch1 id=\"方法\"\u003e方法\u003c/h1\u003e\n\u003ch2 id=\"define-relation-statement\"\u003eDefine Relation Statement\u003c/h2\u003e\n\u003cp\u003eWe define a relation statement to be a block of text containing two marked entities. From this, we create training data that contains relation statements in which the entities have been replaced with a special [BLANK]\u003c/p\u003e","title":"Matching the Blanks - Distributional Similarity for Relation Learning"},{"content":"2020, NAACL\ndata: ACE 04, ACE 05, SciERC\nlinks: https://github.com/princeton-nlp/PURE\ntask: Entity and Relation Extraction\n提出了一种简单但是有效的pipeline方法:builds on two independent pre-trained encoders and merely uses the entity model to provide input features for the relation model.\n实验说明: validate the importance of\nlearning distinct contextual representations for entities and relations, fusing entity information at the input layer of the relation model, and incorporating global context. 从效果上看, 似乎是因为cross sentence的context加成更大\n方法 Input: a sentence X consisting of n tokens x1, . . . , xn. Let S = {s1, . . . , sm} be all the possible spans in X of up to length L and START(i) and END(i) denote start and end indices of si.\nNamed entity recognition a standard span-based model following prior work (Lee et al., 2017; Luan et al., 2018, 2019; Wadden et al., 2019), 加入span-width embedding\nRelation model observe that span he(si), he(sj) representations only capture contextual information around each individual entity and might fail to capture the dependencies between a specific pair of spans. We also hypothesize that sharing the contextual representations for different pairs of spans may be suboptimal. instead processes each pair of spans independently and inserts typed markers at the input layer to highlight the subject and object and their types. Cross-sentence context extending the sentence to a fixed window size W for both the entity and relation model. augment the input with (W − n)/2 words from the left context and right context respectively (W = 100 in our default model).\nTraining \u0026amp; inference 3.3 Efficient Batch Computations 目标是避免多次encode一个句子\n把所有实体的 marker tokens 放在序列尾部, The position embeddings of the markers: 直接复用实体span的 start and end tokens We enforce the text tokens to only attend to text tokens and not attend to the marker tokens while an entity marker token can attend to all the text tokens and all the 4 marker tokens associated with the same span pair. 效果 ","permalink":"https://congchan.github.io/posts/a-frustratingly-easy-approach-for-joint-entity-and-relation-extraction/","summary":"\u003cp\u003e2020, NAACL\u003c/p\u003e\n\u003cp\u003edata: ACE 04, ACE 05, SciERC\u003c/p\u003e\n\u003cp\u003elinks: \u003ca href=\"https://github.com/princeton-nlp/PURE\"\u003ehttps://github.com/princeton-nlp/PURE\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etask: Entity and Relation Extraction\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e提出了一种简单但是有效的pipeline方法:builds on two independent pre-trained encoders and merely uses the entity model to provide input features for the relation model.\u003c/p\u003e\n\u003cp\u003e实验说明: validate the importance of\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003elearning distinct contextual representations for entities and relations\u003c/strong\u003e,\u003c/li\u003e\n\u003cli\u003efusing entity information at the input layer of the relation model,\u003c/li\u003e\n\u003cli\u003eand incorporating global context.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e从效果上看, 似乎是因为cross sentence的context加成更大\u003c/p\u003e\n\u003ch1 id=\"方法\"\u003e方法\u003c/h1\u003e\n\u003cp\u003eInput: a sentence X consisting of n tokens \u003ccode\u003ex1, . . . , xn\u003c/code\u003e. Let \u003ccode\u003eS = {s1, . . . , sm}\u003c/code\u003e be all the possible spans in \u003ccode\u003eX\u003c/code\u003e of up to length \u003ccode\u003eL\u003c/code\u003e and \u003ccode\u003eSTART(i)\u003c/code\u003e and \u003ccode\u003eEND(i)\u003c/code\u003e denote start and end indices of \u003ccode\u003esi\u003c/code\u003e.\u003c/p\u003e","title":"A Frustratingly Easy Approach for Joint Entity and Relation Extraction"},{"content":"2020, EMNLP\ndata: ACE 04, ACE 05, ADE, CoNLL04\nlinks: https://github.com/LorrinWWW/two-are-better-than-one.\ntask: Entity and Relation Extraction\nIn this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process.\n这篇ACL 2020文章认为, 之前的Joint learning方法侧重于learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process.\nFirst, these methods typically suffer from feature confusion as they use a single representation for the two tasks – NER and RE Second, these methods underutilize the table structure as they usually convert it to a sequence and then use a sequence labeling approach to fill the table 方法 we focus on learning two types of representations, namely sequence representations and table representations, for NER and RE respectively. we design a mechanism to allow them to interact with each other, in order to take advantage of the inherent association underlying the NER and RE tasks use the attention weights of BERT for learning table representations. Regard NER as a sequence labeling problem, where the gold entity tags yNER are in the standard BIO\nModel The model consists of two types of interconnected encoders, a table encoder for table representation and a sequence encoder for sequence representation\nIn each layer, the table encoder uses the sequence representation to construct the table representation; and then the sequence encoder uses the table representation to contextualize the sequence representation Table Encoder first construct a non-contextualized table by concatenating every two vectors of the sequence representation followed by a fully-connected layer to halve the hidden size\nNext, we use the Multi-Dimensional Recurrent Neural Networks (MD-RNN) with Gated Recurrent Unit (GRU), iteratively compute the hidden states of each cell to form the contextualized table representation, to access the context from four directions for modeling 2D data\nEmpirically, we found the setting only considering cases (a) and (c) in Figure 4 achieves no worse performance than considering four cases altogether\nSequence Encoder we replace the scaled dot- product attention with our proposed table-guided attention.\n4.4 Exploit Pre-trained Attention Weights 效果 ","permalink":"https://congchan.github.io/posts/two-are-better-than-one-joint-entity-and-relation-extraction-with-table-sequence-encoders/","summary":"\u003cp\u003e2020, EMNLP\u003c/p\u003e\n\u003cp\u003edata: ACE 04, ACE 05, ADE, CoNLL04\u003c/p\u003e\n\u003cp\u003elinks: \u003ca href=\"https://github.com/LorrinWWW/two-are-better-than-one\"\u003ehttps://github.com/LorrinWWW/two-are-better-than-one\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003etask: Entity and Relation Extraction\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eIn this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process.\u003c/p\u003e\n\u003cp\u003e这篇ACL 2020文章认为, 之前的Joint learning方法侧重于learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process.\u003c/p\u003e","title":"Two are Better than One - Joint Entity and Relation Extraction with Table-Sequence Encoders"},{"content":"2020, ACL\ndata: ACE 05\ntask: Event Detection\nPropose a novel Enrichment Knowledge Distillation (EKD) model to efficiently distill external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations.\nleverage the wealth of the open-domain trigger knowledge to improve ED propose a novel teacher-student model (EKD) that can learn from both labeled and unlabeled data 缺点 只能对付普遍情况, 即一般性的触发词; 但触发词不是在任何语境下都是触发词.\n方法 empower the model with external knowledge called Open-Domain Trigger Knowledge, defined as a prior that specifies which words can trigger events without subject to pre-defined event types and the domain of texts.\nKnowledge Collection: apply a light-weight pipeline, called Trigger From WordNet (TFW), to equipment unlabeled sentences with trigger knowledge from WordNet. $S^+ = TFW(S)$ we obtain a total of 733,848 annotated sentences from New York Times corpus in the first half of 2007. The total number of triggers is 2.65 million, with an average of 3.6 triggers per sentence.,\ndisambiguate word into WordNet sense: adopt IMS (Zhong and Ng, 2010) to disambiguate word into word sense in WordNet. obtain the input features by POS tagger and dependency parser in Stanford CoreNLP\ndetermine whether a sense triggers an event: adopt the simple dictionary-lookup approach proposed in (Araki and Mitamura, 2018)\ngiven the knowledge enhanced data as well as ED annotations, we train a teacher model for better performance\nFeature Extraction: adopt the sequence output of the last layer of BERT as the hidden representation for each word in S and S+\n$\\begin{aligned} H \u0026=B E R T(S) \\\\\\\\ H_{+} \u0026=B E R T\\left(S_{+}\\right) \\end{aligned}$\nEvent Prediction: adopt a full-connected layer to determine the event type Y for each word in sentence S. where $O_{ijc}$ represents the probability that the j-th word in Si belongs to the c-th event class. normalize O by the softmax function to obtain the conditional probability\nGiven the labeled corpus $L = \\\\{S_i, Y_i \\\\}|_{i=1}^{N_L}$, optimization object is defined as\na student model is trained to mimic teacher’s outputs using data without knowledge enhancement, which conforms to the distribution during inference\nshare the parameters of the teacher and student model\nKnowledge-attending Sentences (S+): trigger $wi$ identified by open-domain trigger knowl edge, $S+ = {w1, w2, . . . ,B-TRI, wi, E-TRI, . . . , wn}$\nfine-tuning BERT with Mask LM on the annotation sentences S+ to address newly added symbols are lack of pre-trained embedding in BERT Knowledge-absent Sentences (S−): 增加学生模型学习难度, disturb the input of student model by randomly masking out triggers, $S− = {w1, w2, . . . ,[MASK], . . . , wn}$\nKL-divergence Loss: We move the added symbols to the end of the sentence to ensure strict alignment of words in S+ and S−, minimize the discrepancy between conditional probability p(Y|S−, θ) and p(Y|S+θ) with KL-divergence loss.\nKL散度不对称, 这里使用无KG的预测来逼近有KG加成的预测的分布. 反过来则效果不好\nJoint Training: supervised loss from labeled dataset and KL- divergence loss from unlabeled dataset\nstop the gradient descent of teacher model when calculating JT to ensure that the learning is from teacher to student\nTraining Signal Annealing (TSA): Linearly release the ‘training signals’ of the labeled examples as training progresses 避免模型overfit少量的有标签数据而underfit 大量的无标签样本\n效果 outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words.\n","permalink":"https://congchan.github.io/posts/improving-event-detection-via-open-domain-trigger-knowledge/","summary":"\u003cp\u003e2020, ACL\u003c/p\u003e\n\u003cp\u003edata: ACE 05\u003c/p\u003e\n\u003cp\u003etask: Event Detection\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003ePropose a novel Enrichment Knowledge Distillation (EKD) model to efficiently distill external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eleverage the wealth of the open-domain trigger knowledge to improve ED\u003c/li\u003e\n\u003cli\u003epropose a novel teacher-student model (EKD) that can learn from both labeled and unlabeled data\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"/images/papers/paper2.png\" loading=\"lazy\" src=\"/images/papers/paper2.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"缺点\"\u003e缺点\u003c/h1\u003e\n\u003cp\u003e只能对付普遍情况, 即一般性的触发词; 但触发词不是在任何语境下都是触发词.\u003c/p\u003e\n\u003ch1 id=\"方法\"\u003e方法\u003c/h1\u003e\n\u003cp\u003eempower the model with external knowledge called Open-Domain Trigger Knowledge, defined as a prior that specifies which words can trigger events without subject to pre-defined event types and the domain of texts.\u003c/p\u003e","title":"Improving Event Detection via Open-domain Trigger Knowledge"},{"content":"2020, ACL Task: MultiMedia Event Extraction\nIntroduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles\nPropose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities\n数据 Each input document consists of:\na set of images M = {m1,m2, . . . } and a set of sentences S = {s1, s2, . . . } a set of entities T = {t1, t2, . . . } extracted from the document text The tasks of M2E2 Event Extraction: Given a multimedia document, extract a set of event mentions, where each event mention e has a type ye and is grounded on a text trigger word w or an image m or both $e = (y_e, {w,m})$. Argument Extraction: The second task is to extract a set of arguments of event mention e. Each argument a has an argument role type ya, and is grounded on a text entity t or an image object o (represented as a bounding box), $a = (y_a, {t, o})$ Two types of bounding boxes:\nunion bounding box: for each role, we annotate the smallest bounding box covering all constituents instance bounding box: for each role, we annotate a set of bounding boxes, where each box is the smallest region that covers an individual participant 方法 represent each image or sentence as a graph, where each node represents an event or entity and each edge represents an argument role.\nthe training phase contains three tasks:\nText event extraction Text Structured Representation: run the CAMR parser (Wang et al., 2015b,a, 2016) to generate an AMR graph. based on the named entity recognition and part- of-speech (POS) tagging results from Stanford CoreNLP (Manning et al., 2014)\nEmbedding: pre-trained GloVe word embedding (Pennington et al., 2014), POS embedding, entity type embed- ding and position embedding\nEncode embedding: Bi-LSTM\nEncode AMR: AMR graph as input to GCN:\nFor each entity t, we obtain its representation $t^C$ by averaging the embeddings of its tokens\nEvent and Argument Classifier\nuse BIO tag schema to decide trigger word boundary, classify each word w into event types $y_e$ classify entity t into argument role $y_a$ 训练时, 用Ground-true entity\n测试时, 用named entity extractor\nImage Event Extraction (Section 3.3) Image Structured Representation: represent each image with a situation graph, the central node is labeled as a verb v (e.g., destroying), and the neighbor nodes are arguments labeled as {(n, r)}, where n is a noun (e.g., ship) derived from WordNet synsets (Miller, 1995) to indicate the entity type, and r indicates the role (e.g., item) played by the entity in the event, based on FrameNet (Fillmore et al., 2003).\ntwo methods to construct situation graphs:\nObject-based Graph: object detection, and obtain the object bounding boxes detected by a Faster R-CNN (Ren et al., 2015) model trained on Open Images (Kuznetsova et al., 2018) with 600 object types (classes). employ a VGG-16 CNN (Si- monyan and Zisserman, 2014) to extract visual features of an image m and and another VGG-16 to encode the bounding boxes {oi}. apply a Multi-Layer Perceptron (MLP) to predict a verb embedding from m and another MLP to predict a noun embedding for each oi\ncompare the predicted verb embedding to all verbs v in the imSitu taxonomy in order to classify the verb, and similarly compare each predicted noun embedding to all imSitu nouns n which re- sults in probability distributions. where v and n are word embeddings initialized with GloVE.\nuse another MLP with one hidden layer followed by Softmax (σ) to classify role ri for each object oi\ndefine the situation loss\nAttention-based Graph\nMany salient objects such as bomb, stone and stretcher are not covered in these ontologies. Hence, propose an open-vocabulary alternative to the object-based graph construction model.\nand cross- media alignment (Section 3.4)\n效果 Compared to unimodal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction.\n","permalink":"https://congchan.github.io/posts/cross-media-structured-common-space-for-multimedia-event-extraction/","summary":"\u003cp\u003e2020, ACL\nTask: MultiMedia Event Extraction\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eIntroduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles\u003c/p\u003e\n\u003cp\u003ePropose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities\u003c/p\u003e","title":"Cross-media Structured Common Space for Multimedia Event Extraction"},{"content":"深度强化学习DQN和Natural DQN, Double DQN, Dueling DoubleQN, Rainbow DQN 的演变和必看论文.\nDQN的Overestimate DQN 基于 Q-learning, Q-Learning 中有 Qmax, Qmax 会导致 Q现实 当中的过估计 (overestimate). 而 Double DQN 就是用来解决过估计的. 在实际问题中, 如果你输出你的 DQN 的 Q 值, 可能就会发现, Q 值都超级大. 这就是出现了 overestimate.\nDQN 的神经网络部分可以看成一个 最新的神经网络 + 老神经网络, 他们有相同的结构, 但内部的参数更新却有时差. Q现实 部分是这样的:\n$$Y_t^\\text{DQN} \\equiv R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a; \\theta_t^-)$$过估计 (overestimate) 是指对一系列数先求最大值再求平均，通常比先求平均再求最大值要大（或相等，数学表达为：\n$$E(\\max(X_1, X_2, ...)) \\ge \\max(E(X_1), E(X_2), ...)$$一般来说Q-learning方法导致overestimation的原因归结于其更新过程，其表达为：\n$$Q_{t+1} (s_t, a_t) = Q_t (s_t, a_t) + a_t(s_t, a_t)(r_t + \\gamma \\max a Q_t(s_{t+1}, a) - Q_t(s_t, a_t))$$而更新最优化过程如下\n$$\\forall s, a: Q(s, a)=\\sum_{s^{\\prime}} P_{s a}^{s^{\\prime}}\\left(R_{s a}^{s^{\\prime}}+\\gamma \\max _{a} Q\\left(s^{\\prime}, a\\right)\\right)$$把N个Q值先通过取max操作之后，然后求平均(期望)，会比我们先算出N个Q值取了期望之后再max要大。这就是overestimate的原因。\n一般用于加速Q-learning算法的方法有：Delayed Q-learning, Phased Q-learning, Fitted Q-iteration等\noverestimation bias in experiments across different Atari game environments:\ntraditional DQN tends to significantly overestimate action-values, leading to unstable training and low quality policy\nDouble DQN 算法 (DDQN) Q-learning学习其实使用单估计器(single estimate)去估计下一个状态：$\\max_{a} Q_{t}\\left(s_{t+1}, a\\right)$ 是 $E \\\\{ \\max_{a} Q_{t}\\left(s_{t+1}, a\\right) \\\\}$的一个估计。根据原理部分，Double Q-learning将使用两个estimators函数 $Q^A$和$Q^B$, 每个estimator 都会使用另一个 estimator函数的值更新下一个状态。两个函数都必须从不同的经验子集中学习，但是选择执行的动作可以同时使用两个值函数。 该算法的数据效率不低于Q学习。 在实验中作者为每个动作计算了两个Q值的平均值，然后对所得的平均Q值进行了贪婪探索。\n2个estimator会导致underestimate而不会overestimate。具体证明见原文。\nDouble DQN学习的方式 The standard Q-learning update for the parameters after taking action At in state St and observing the immediate reward Rt+1 and resulting state St+1 is then\n$$\\theta_{t+1} = \\theta_t + \\alpha (Y^Q_t - Q(S_t, A_t; \\theta_t)) \\nabla_{\\theta_t} Q(S_t, A_t; \\theta_t).$$where α is a scalar step size, $Y^Q_t$是一个termporal difference的值, 每次更新, one set of weights is used to determine the greedy policy and the other to determine its value.\n$$Y_t^Q = R_{t+1} + \\gamma Q(S_{t+1}, argmax_a Q(S_{t+1}, a; \\theta_t); \\theta_t).$$使用DQN, $\\theta^-$为The target network的参数, 每τ steps更新 $\\theta_t^- = \\theta_t$\n$$Y_t^{DQN} \\equiv R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a; \\theta^-_t). $$它greedy预估下一个action时使用参数 $\\theta_t$ ，同时evaluation时也采用同一套参数，让Q-learning更加容易overestimate。\n因此，double Q-learning使用两个network，online network和target network，两套参数 $\\theta_t, \\theta_t'$ 分别进行selection和evaluation,\n$$Y_t^{DoubleQ} \\equiv R_{t+1} + \\gamma Q(S_{t+1}, argmax_aQ(S_{t+1},a; \\theta_t); \\theta'_t). $$Double DQN则是把$\\theta_t'$替换为target network 的 $\\theta_t^-$, 用于评估当前的greedy policy 的值, 其余和DQN基本一致. 这是DQN使用Double q-learning代价最小的方式。\n$$Y_t^{DoubleDQN} \\equiv R_{t+1} + \\gamma Q(S_{t+1}, argmax_aQ(S_{t+1},a; \\theta_t), \\theta^-_t). $$The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation.\nthe target network in the DQN architecture provides a natural candidate for the second value function, without having to introduce additional networks.\nDueling DQN（D3QN） Intuitively, the dueling architecture can learn which states are (or are not) valuable, without having to learn the effect of each action for each state. This is particularly useful in states where its actions do not affect the environment in any relevant way.\n在某些状态场景中，动作对环境几乎没有影响，比如游戏中的等待时间，无论玩家做什么操作，对结果也没影响。而dueling架构的的目的就是解耦动作和状态。这是开车的游戏, 左边是 state value, 发红的部分证明了 state value 和前面的路线有关, 右边是 advantage, 发红的部分说明了 advantage 很在乎旁边要靠近的车子, 这时的动作会受更多 advantage 的影响. 发红的地方左右了自己车子的移动原则.\nDueling DQN将 state values 和 action advantages 分开，\nstate values仅仅与状态$S$有关，与具体要采用的动作$A$无关，这部分我们叫做价值函数部分，记做$V(S,w,\\alpha)$, $V^{\\pi}(s)=\\mathbb{E}_{a \\sim \\pi(s)}\\left[Q^{\\pi}(s, a)\\right]$ action advantages 优势函数(Advantage Function), 用于衡量 action 的相对优势, 通过让Q值减去V值得到, 记为$A(S,A,w,\\beta)$, $A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)$, 价值函数 V 衡量它处于特定状态 s 的好坏程度。而Q 函数测量在此状态下选择特定操作的价值。优势函数从 Q 函数中减去状态V值，以获得每个动作重要性的相对度量。通过动作让Q和V毕竟, 最终优势函数的期望为0，即$\\mathbb{E}_{a \\sim \\pi(s)}\\left[A^{\\pi}(s, a)\\right] = 0$\n不像DQN那样直接学出所有的Q值，Dueling DQN的思想就是独立的学出Value和Advantage，将它们以某种方式组合起来，组成Q价值函数，最直接的做法是求和：\n$$Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta,\\beta) + A(s, a; \\theta,\\alpha)$$其中，$w$是网络参数，而$α$是价值函数独有部分的网络参数，而$β$是优势函数独有部分的网络参数。\n但是这个式子是unidentifiable, 也就是只给定Q, 我们无法还原V和A. 为了解决这个可以实现可辨识性(identifiability), 可以通过强迫优势函数的estimator在所选动作下预估其优势值为0：\n$$ Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + \\left( A(s, a; \\theta, \\alpha) - \\frac{1}{|A|} \\sum_{a'}A(s, a'; \\theta, \\alpha) \\right) $$一方面这个组合方式会导致V和A丧失原先的含义, 因为它们偏离了一个常数值; 但另一方面这样可以提高优化的稳定性. 因为A的变化速度只需要和mean一样快就行, 而不是和最优的action A同步.\n组合函数写进神经网络中作为输出.\nRainbow DQN Rainbow的命名是指混合, 利用许多RL中前沿知识并进行了组合, 组合了DDQN, prioritized Replay Buffer, Dueling DQN, Multi-step learning.\nMulti-step learning 原始的DQN使用的是当前的即时奖励r和下一时刻的价值估计作为目标价值，这种方法在前期策略差即网络参数偏差较大的情况下，得到的目标价值偏差也较大。因此可以通过Multi-Step Learning来解决这个问题，通过多步的reward来进行估计。\nDistributional perspective RL 传统DQN中估计期望，但是期望并不能完全反映信息，毕竟还有方差，期望相同我们当然希望取方差更小的来减小波动和风险。所以从理论上来说，从分布视角（distributional perspective）来建模我们的深度强化学习模型，可以获得更多有用的信息，从而得到更好、更稳定的结果。\nNoisy Net Noisy DQN是为了增强DQN探索能力而设计的方法，是model-free，off-policy，value-based，discrete的方法。\nNoisy DQN这个方法被发表在Noisy Networks for Exploration这篇文章中，但是它并不只是在DQN中被使用，实际上在A3C这样的模型中也可以增加噪声来刺激探索。\nReferences DQN: https://www.aminer.cn/pub/53e9a682b7602d9702fb756d/playing-atari-with-deep-reinforcement-learning) DDQN: Deep Reinforcement Learning with Double Q-learning Double Q-learning: Double Q-learning Double DQN: Deep Reinforcement Learning with Double Q-learning Dueling DQN: Dueling Network Architectures for Deep Reinforcement Learning Rainbow: Combining Improvements in Deep Reinforcement Learning A Distributional Perspective on Reinforcement Learning Noisy Networks for Exploration 深度强化学习必看经典论文：DQN，DDQN，Prioritized，Dueling，Rainbow 【DRL-9】Noisy Networks Double DQN (Tensorflow) - 强化学习 Reinforcement Learning | 莫烦Python ","permalink":"https://congchan.github.io/posts/dqn-double-dqn-dueling-doubleqn-rainbow-dqn/","summary":"\u003cp\u003e深度强化学习DQN和Natural DQN, Double DQN, Dueling DoubleQN, Rainbow DQN 的演变和必看论文.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch1 id=\"dqn的overestimate\"\u003eDQN的Overestimate\u003c/h1\u003e\n\u003cp\u003eDQN 基于 Q-learning, Q-Learning 中有 Qmax, Qmax 会导致 Q现实 当中的过估计 (overestimate). 而 Double DQN 就是用来解决过估计的. 在实际问题中, 如果你输出你的 DQN 的 Q 值, 可能就会发现, Q 值都超级大. 这就是出现了 overestimate.\u003c/p\u003e\n\u003cp\u003eDQN 的神经网络部分可以看成一个 最新的神经网络 + 老神经网络, 他们有相同的结构, 但内部的参数更新却有时差. Q现实 部分是这样的:\u003c/p\u003e\n$$Y_t^\\text{DQN} \\equiv R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a; \\theta_t^-)$$\u003cp\u003e\u003cstrong\u003e过估计\u003c/strong\u003e (overestimate) 是指对一系列数先求最大值再求平均，通常比先求平均再求最大值要大（或相等，数学表达为：\u003c/p\u003e\n$$E(\\max(X_1, X_2, ...)) \\ge \\max(E(X_1), E(X_2), ...)$$\u003cp\u003e一般来说Q-learning方法导致overestimation的原因归结于其更新过程，其表达为：\u003c/p\u003e\n$$Q_{t+1} (s_t, a_t) = Q_t (s_t, a_t) + a_t(s_t, a_t)(r_t + \\gamma \\max a Q_t(s_{t+1}, a) - Q_t(s_t, a_t))$$\u003cp\u003e而更新最优化过程如下\u003c/p\u003e","title":"DQN, Double DQN, Dueling DoubleQN, Rainbow DQN"},{"content":"2017, EMNLP\ndata: FB15K-237, FB15K\ntask: Knowledge Graph Reasoning\nUse a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.\n方法 RL 系统包含两部分，\n第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。 系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。 关系推理的强化学习 行为 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。\n状态 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量：\n其中 e.t 表示当前实体结点的嵌入，e.target 表示目标实体的嵌入。在最初状态，e.t 即 e.source。我们没有在状态中加入推理关系，因为在寻路过程中推理关系的嵌入保持不变，不利于训练。然而，我们发现通过使用一组特定关系的正样本训练 RL 代理，该 智能体 可以成功地发现关系语义。\n奖励 对于我们的环境设置，智能体 可以执行的操作数量可能非常大。换句话说，错误的顺序决策比正确的顺序决策多得多。这些错误的决策序列的数量会随着路径的长度呈指数增长。\nGlobal accuracy： Path efficiency Path diversity: 策略网络 我们使用全连接神经网络来参数化策略函数，它讲状态向量映射到所有可能行为的概率分布上。神经网络包含两个隐藏层，每一层后接 ReLU。输出层通过 softmax 函数归一化。\n3.2 训练 对于一个典型的KG, RL 智能体 常常面临上千种可能的操作。换句话说，策略网络的输出层具有较大的维数。由于关系图的复杂性和较大的动作空间，如果直接采用 RL 算法中典型的试错推理来训练RL模型，将会导致 RL 模型收敛性很差。经过长时间的训练，智能体都可能无法找到任何有价值的路径。\n为了解决这个问题，我们从一个监督策略开始我们的训练，这个策略的灵感来自 AlphaGo 使用的模仿学习流水线。在围棋游戏中，玩家每走一步都要面对近 250 种可能的合法走法。直接训练智能体从原始动作空间中挑选动作可能是一项困难的任务。AlphaGo 首先使用专家训练一个有监督的策略网络。在该例子中，使用随机的广度优先搜索(BFS)训练监督策略。\n监督策略学习 对于每个关系，我们首先使用所有正样本（实体对）的子集来学习有监督的策略。对于每个正样本(esource, etarget)，一个两端 BFS 被用于找到实体之间的正确路径。对于路径 p，使用蒙塔卡洛策略梯度（REINFORCE 方法）来最大化期望的累积奖励。\n原生 BFS 是有偏的搜索算法，它倾向于使用短路径。当插入这些有偏向的路径时，agent 很难找到可能有用的较长路径。我们希望路径仅由定义的奖励函数控制。为了防止偏向搜索，我们采用了一种简单的技巧为 BFS 添加一些随机机制。我们不是直接搜索 esource 和 etarget 之间的路径，而是随机选择一个中间节点einter，然后在（esource，einter）和（einter，etarget）之间进行两个 BFS。连接的路径用于训练智能体。监督学习可以节省智能体从失败行为中学习的大量精力。借助所学的经验，我们然后训练智能体寻找理想的路径。\nRetraining with Rewards 为了找到受奖励函数控制的推理路径，我们使用奖励函数来限制监督策略网络。对于每个关系，一个实体对的推理被视为一个事件(episode)。从源结点开始，智能体根据随机策略选择关系，它是所有关系上的概率分布，以扩展推理路径。关系链接可能引向一个新实体，或者失败。这些失败的步骤可能导致智能体获得负奖励。智能体在失败步骤后保持状态。由于智能体遵循随机策略，所以智能体不会因为重复错误的步骤而陷入困境。为了提高训练效率，我们将训练集长度设定一个上限。上限达到时，如智能体仍未找到目标实体则事件结束。每个事件结束后，策略网络通过以下梯度进行更新：\n3.3 Bi-directional Path-constrained Search In a typical KG, one entity node can be linked to a large number of neighbors with the same relation link. If we verify the formula from the inverse direction. The number of intermediate nodes can be tremendously decreased.\n4 Experiments we explore two standard KG reason- ing tasks: link prediction (predicting target en- tities) and fact prediction (predicting whether an unknown fact holds or not).\n4.1 Dataset and Settings The triples in FB15K-237 (Toutanova et al., 2015) are sampled from FB15K (Bordes et al., 2013) with redun- dant relations removed.\n4.3 Results ","permalink":"https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/","summary":"\u003cp\u003e2017, EMNLP\u003c/p\u003e\n\u003cp\u003edata: FB15K-237, FB15K\u003c/p\u003e\n\u003cp\u003etask: Knowledge Graph Reasoning\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eUse a policy-based agent with continuous states based on knowledge graph embeddings, which \u003cstrong\u003ereasons in a KG vector space\u003c/strong\u003e by sampling the most promising relation to extend its path.\u003c/p\u003e\n\u003ch1 id=\"方法\"\u003e方法\u003c/h1\u003e\n\u003cp\u003eRL 系统包含两部分，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。\u003c/li\u003e\n\u003cli\u003e系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"/images/papers/paper7.png\" loading=\"lazy\" src=\"/images/papers/paper7.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"关系推理的强化学习\"\u003e关系推理的强化学习\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e行为\u003c/strong\u003e 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e状态\u003c/strong\u003e 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量：\u003c/p\u003e","title":"DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning"},{"content":"data: WN18, WN11, FB15K, FB13, FB40K\ntask: Knowledge Graph Embedding\nTransE Translating Embeddings for Modeling Multi-relational Data（2013）\nhttps://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\n这是转换模型系列的第一部作品。该模型的基本思想是使head向量和relation向量的和尽可能靠近tail向量。这里我们用L1或L2范数来衡量它们的靠近程度。\n损失函数 $\\mathrm{L}(h, r, t)=\\max \\left(0, d_{\\text {pos }}-d_{\\text {neg }}+\\text { margin }\\right)$使损失函数值最小化，当这两个分数之间的差距大于margin的时候就可以了(我们会设置这个值，通常是1)\n但是这个模型只能处理一对一的关系，不适合一对多/多对一关系，例如，有两个知识，(skytree, location, tokyo)和(gundam, location, tokyo)。经过训练，“sky tree”实体向量将非常接近“gundam”实体向量。但实际上它们没有这样的相似性。\nwith tf.name_scope(\u0026#34;embedding\u0026#34;): self.ent_embeddings = tf.get_variable(name = \u0026#34;ent_embedding\u0026#34;, shape = [entity_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) self.rel_embeddings = tf.get_variable(name = \u0026#34;rel_embedding\u0026#34;, shape = [relation_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) pos_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h) pos_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t) pos_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r) neg_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h) neg_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t) neg_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r) if config.L1_flag: pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keep_dims = True) neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keep_dims = True) self.predict = pos else: pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keep_dims = True) neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keep_dims = True) self.predict = pos with tf.name_scope(\u0026#34;output\u0026#34;): self.loss = tf.reduce_sum(tf.maximum(pos - neg + margin, 0)) TransH Knowledge Graph Embedding by Translating on Hyperplanes（2014）\nTransH的目标是处理一对多/多对一/多对多关系，并且不增加模式的复杂性和训练难度。\n其基本思想是将关系解释为超平面上的转换操作。对每个关系r，分配一个超平面的Wr(范数向量)， 以及一个在超平面上的translation vector dr, 每个head向量(h)和tail向量(t)投影到超平面上，得到新的向量(h⊥和t⊥)。We expect h⊥ and t⊥ can be connected by a translation vector $d_r$ on the hyperplane with low error if (h, r, t) is a golden triplet ，这样就可以给每个实体只分配一个embedding，同时可以在不同关系上获取不同的投影分量，进行不同的关系表达。可以像TransE模型一样训练它\n损失函数 $\\left\\||h{\\perp}+d_r - t{\\perp}\\right\\||_2^2$, 限制$||w_r||_2 = 1$\n# https://github.com/thunlp/TensorFlow-TransX/blob/42a3c1df34d4c79b88718bdc126552ae59896ca8/transH.py#L31 with tf.name_scope(\u0026#34;embedding\u0026#34;): self.ent_embeddings = tf.get_variable(name = \u0026#34;ent_embedding\u0026#34;, shape = [entity_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) # rel_embeddings: dr self.rel_embeddings = tf.get_variable(name = \u0026#34;rel_embedding\u0026#34;, shape = [relation_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) # normal_vector: wr self.normal_vector = tf.get_variable(name = \u0026#34;normal_vector\u0026#34;, shape = [relation_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) pos_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h) pos_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t) pos_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r) neg_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h) neg_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t) neg_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r) pos_norm = tf.nn.embedding_lookup(self.normal_vector, self.pos_r) neg_norm = tf.nn.embedding_lookup(self.normal_vector, self.neg_r) pos_h_e = self.calc(pos_h_e, pos_norm) pos_t_e = self.calc(pos_t_e, pos_norm) neg_h_e = self.calc(neg_h_e, neg_norm) neg_t_e = self.calc(neg_t_e, neg_norm) if config.L1_flag: pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keep_dims = True) neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keep_dims = True) self.predict = pos else: pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keep_dims = True) neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keep_dims = True) self.predict = pos with tf.name_scope(\u0026#34;output\u0026#34;): self.loss = tf.reduce_sum(tf.maximum(pos - neg + margin, 0)) def calc(self, e, n): # cal projections norm = tf.nn.l2_normalize(n, 1) # ||wr||2 = 1 return e - tf.reduce_sum(e * norm, 1, keep_dims = True) * norm TransR Learning Entity and Relation Embeddings for Knowledge Graph Completion（2015）\nTransE和TransH模型都假设实体和关系是语义空间中的向量，因此相似的实体在同一实体空间中会非常接近。\n然而，每个实体可以有许多方面，不同的关系关注实体的不同方面。例如，(location, contains, location)的关系是\u0026rsquo;contains\u0026rsquo;，(person, born, date)的关系是\u0026rsquo;born\u0026rsquo;。这两种关系非常不同。\nModels entities and relations in distinct spaces, i.e., entity space and multiple relation spaces (i.e., relation-specific entity spaces), and performs translation in the corresponding relation space, 即实体空间和多个关系空间(关系特定的实体空间)中建模实体和关系，并在对应的关系空间中进行转换。\n对于每个三元组(h, r, t)，将实体空间中的实体通过矩阵Mr投影到r关系空间中，分别为hr和tr，然后有hr + r ≈ tr，损失函数和训练方法与TransE相同。h和t为实体嵌入，r为关系嵌入\nCluster-based TransR (CTransR) TransR的变体模型称为CTransR, C表示聚类。head和tail实体通常呈现不同的模式。仅仅构建一个关系向量来执行从head到tail实体的所有转换是不够的。例如，三元组(location, contains, location)具有许多模式，如country-city、country-university、contin- country等等。为了解决这个问题，CTransR将不同的head和tail实体对进行聚类，并对每一组学习不同的关系向量。\n构造CtransR的过程是，\n对于一个特定的关系r，将训练数据中所有的实体对(h, t)聚类到多个组中，期望每组中的实体对呈现相似的r关系。 使用向量偏移量$(h-t)$表示实体对(h, t)。 从TransE得到h和t。 为每个聚类学习对应的关系向量$r_c$，为每个关系学习对应的矩阵Mr。 创建负样本时，只替换了head或tail，而不是relation。得到两个变换矩阵分别用于正样本和负样本。除了先用矩阵变换对实体向量进行转换然后计算L2范数外，其余代码基本上与TransE相同。\nwith tf.name_scope(\u0026#39;lookup_embeddings\u0026#39;): pos_h_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h), [-1, sizeE, 1]) pos_t_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t), [-1, sizeE, 1]) pos_r_e = tf.reshape(tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r), [-1, sizeR]) neg_h_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h), [-1, sizeE, 1]) neg_t_e = tf.reshape(tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t), [-1, sizeE, 1]) neg_r_e = tf.reshape(tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r), [-1, sizeR]) pos_matrix = tf.reshape(tf.nn.embedding_lookup(self.rel_matrix, self.pos_r), [-1, sizeR, sizeE]) neg_matrix = tf.reshape(tf.nn.embedding_lookup(self.rel_matrix, self.neg_r), [-1, sizeR, sizeE]) pos_h_e = tf.nn.l2_normalize(tf.reshape(tf.matmul(pos_matrix, pos_h_e), [-1, sizeR]), 1) pos_t_e = tf.nn.l2_normalize(tf.reshape(tf.matmul(pos_matrix, pos_t_e), [-1, sizeR]), 1) neg_h_e = tf.nn.l2_normalize(tf.reshape(tf.matmul(neg_matrix, neg_h_e), [-1, sizeR]), 1) neg_t_e = tf.nn.l2_normalize(tf.reshape(tf.matmul(neg_matrix, neg_t_e), [-1, sizeR]), 1) if config.L1_flag: pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keep_dims = True) neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keep_dims = True) self.predict = pos else: pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keep_dims = True) neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keep_dims = True) self.predict = pos with tf.name_scope(\u0026#34;output\u0026#34;): self.loss = tf.reduce_sum(tf.maximum(pos - neg + margin, 0)) TransD Knowledge Graph Embedding via Dynamic Mapping Matrix（2015）\nTransR也有其不足之处。\n首先，head和tail使用相同的转换矩阵将自己投射到超平面上，但是head和tail通常是一个不同的实体，例如，(Bill Gates, founder, Microsoft)。\u0026lsquo;Bill Gate\u0026rsquo;是一个人，\u0026lsquo;Microsoft\u0026rsquo;是一个公司，这是两个不同的类别。所以他们应该以不同的方式进行转换。 第二，这个投影与实体和关系有关，但投影矩阵仅由关系决定。 最后，TransR的参数数大于TransE和TransH。由于其复杂性，TransR/CTransR难以应用于大规模知识图谱。 TransD使用两个向量来表示每个实体和关系。第一个向量表示实体或关系的意义，另一个向量(称为投影向量)将用于构造映射矩阵。given a triplet (h, r, t), its vectors are h, hp, r, rp, t, tp, where subscript p marks the projection vectors, h, hp, t, tp ∈ Rn\n$\\begin{aligned} \\mathbf{M}{r h} \u0026=\\mathbf{r}{p} \\mathbf{h}{p}^{\\top}+\\mathbf{I}^{m \\times n} \\\\\\\\ \\mathbf{M}{r t} \u0026=\\mathbf{r}{p} \\mathbf{t}{p}^{\\top}+\\mathbf{I}^{m \\times n} \\end{aligned}$\n其中映射矩阵由实体和关系定义，I为单位矩阵。这个等式意味着我们使用生成的矩阵(由r和h向量)来修改单位矩阵。投射和训练与TransR相同。TransE是向量维数满足m=n且所有投影向量都设为零时变换的一种特殊情况。\nwith tf.name_scope(\u0026#34;embedding\u0026#34;): self.ent_embeddings = tf.get_variable(name = \u0026#34;ent_embedding\u0026#34;, shape = [entity_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) self.rel_embeddings = tf.get_variable(name = \u0026#34;rel_embedding\u0026#34;, shape = [relation_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) self.ent_transfer = tf.get_variable(name = \u0026#34;ent_transfer\u0026#34;, shape = [entity_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) self.rel_transfer = tf.get_variable(name = \u0026#34;rel_transfer\u0026#34;, shape = [relation_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) pos_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h) pos_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t) pos_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r) pos_h_t = tf.nn.embedding_lookup(self.ent_transfer, self.pos_h) pos_t_t = tf.nn.embedding_lookup(self.ent_transfer, self.pos_t) pos_r_t = tf.nn.embedding_lookup(self.rel_transfer, self.pos_r) neg_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h) neg_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t) neg_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r) neg_h_t = tf.nn.embedding_lookup(self.ent_transfer, self.neg_h) neg_t_t = tf.nn.embedding_lookup(self.ent_transfer, self.neg_t) neg_r_t = tf.nn.embedding_lookup(self.rel_transfer, self.neg_r) pos_h_e = self.calc(pos_h_e, pos_h_t, pos_r_t) pos_t_e = self.calc(pos_t_e, pos_t_t, pos_r_t) neg_h_e = self.calc(neg_h_e, neg_h_t, neg_r_t) neg_t_e = self.calc(neg_t_e, neg_t_t, neg_r_t) if config.L1_flag: pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keep_dims = True) neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keep_dims = True) self.predict = pos else: pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keep_dims = True) neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keep_dims = True) self.predict = pos with tf.name_scope(\u0026#34;output\u0026#34;): self.loss = tf.reduce_sum(tf.maximum(pos - neg + margin, 0)) def calc(self, e, t, r): return tf.nn.l2_normalize(e + tf.reduce_sum(e * t, 1, keep_dims = True) * r, 1) Summary Reference Translating embeddings for modeling multi-relational data https://www.microsoft.com/en-us/research/publication/knowledge-graph-embedding-by-translating-on-hyperplanes/ Learning Entity and Relation Embeddings for Knowledge Graph Completion Knowledge Graph Embedding via Dynamic Mapping Matrix thunlp/TensorFlow-TransX thunlp/KB2E Summary of Translate Model for Knowledge Graph Embedding 知识图谱嵌入的Translate模型汇总（TransE，TransH，TransR，TransD） ","permalink":"https://congchan.github.io/posts/knowledge-graph-embedding%E7%9A%84translate%E6%97%8Ftransetranshtransrtransd/","summary":"\u003cp\u003edata: WN18, WN11, FB15K, FB13, FB40K\u003c/p\u003e\n\u003cp\u003etask: Knowledge Graph Embedding\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/papers/paper9.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"transe\"\u003eTransE\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eTrans\u003c/strong\u003elating \u003cstrong\u003eE\u003c/strong\u003embeddings for Modeling Multi-relational Data（2013）\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\"\u003ehttps://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e这是转换模型系列的第一部作品。该模型的基本思想是使head向量和relation向量的和尽可能靠近tail向量。这里我们用L1或L2范数来衡量它们的靠近程度。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"/images/papers/paper9-1.png\" loading=\"lazy\" src=\"/images/papers/paper9-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"/images/papers/paper9-2.png\" loading=\"lazy\" src=\"/images/papers/paper9-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e损失函数 $\\mathrm{L}(h, r, t)=\\max \\left(0, d_{\\text {pos }}-d_{\\text {neg }}+\\text { margin }\\right)$使损失函数值最小化，当这两个分数之间的差距大于margin的时候就可以了(我们会设置这个值，通常是1)\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"/images/papers/paper9-3.png\" loading=\"lazy\" src=\"/images/papers/paper9-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e但是这个模型只能处理一对一的关系，不适合一对多/多对一关系\u003c/strong\u003e，例如，有两个知识，(skytree, location, tokyo)和(gundam, location, tokyo)。经过训练，“sky tree”实体向量将非常接近“gundam”实体向量。但实际上它们没有这样的相似性。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename_scope\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;embedding\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eent_embeddings\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_variable\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;ent_embedding\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eshape\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eentity_total\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003einitializer\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003econtrib\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elayers\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003exavier_initializer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003euniform\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erel_embeddings\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_variable\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;rel_embedding\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eshape\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003erelation_total\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003einitializer\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003econtrib\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elayers\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003exavier_initializer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003euniform\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epos_h_e\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eembedding_lookup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eent_embeddings\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epos_h\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epos_t_e\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eembedding_lookup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eent_embeddings\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epos_t\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epos_r_e\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eembedding_lookup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erel_embeddings\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epos_r\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eneg_h_e\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eembedding_lookup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eent_embeddings\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eneg_h\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eneg_t_e\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eembedding_lookup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eent_embeddings\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eneg_t\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eneg_r_e\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eembedding_lookup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erel_embeddings\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eneg_r\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003econfig\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eL1_flag\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003epos\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereduce_sum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eabs\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epos_h_e\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003epos_r_e\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003epos_t_e\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ekeep_dims\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eneg\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereduce_sum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eabs\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eneg_h_e\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003eneg_r_e\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003eneg_t_e\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ekeep_dims\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epredict\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003epos\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereduce_sum\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"n\"\u003epos_h_e\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003epos_r_e\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003epos_t_e\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ekeep_dims\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eneg\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereduce_sum\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"n\"\u003eneg_h_e\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003eneg_r_e\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003eneg_t_e\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ekeep_dims\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epredict\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename_scope\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;output\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eloss\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereduce_sum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emaximum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epos\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003eneg\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003emargin\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch1 id=\"transh\"\u003eTransH\u003c/h1\u003e\n\u003cp\u003eKnowledge Graph Embedding by Translating on Hyperplanes（2014）\u003c/p\u003e","title":"Knowledge-Graph-Embedding的Translate族（TransE，TransH，TransR，TransD）"},{"content":"Survey: https://arxiv.org/abs/2002.00388v4\nA knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.\nEntities can be real-world objects and abstract concepts, Relationships represent the relation between entities, Semantic descriptions of entities and their relationships contain types and properties with a well-defined meaning G: A knowledge graph F: A set of facts (h, r, t): A triple of head, relation and tail $(\\mathbf{h}, \\mathbf{r}, \\mathbf{t})$: Embedding of head, relation and tail\nDefinition 1 (Ehrlinger and W¨oß [7]). A knowledge graph acquires and integrates information into an ontology and applies a reasoner to derive new knowledge.\nDefinition 2 (Wang et al. [8]). A knowledge graph is a multi- relational graph composed of entities and relations which are regarded as nodes and different types of edges, respectively.\nKB vs KG The term of knowledge graph is synonymous with knowledge base with a minor difference. A knowledge graph can be viewed as a graph when considering its graph structure. When it involves formal semantics, it can be taken as a knowledge base for interpretation and inference over facts\nData Structure Resource Description Framework (RDF)：(head, relation, tail) or (subject, predicate, object) Web Ontology Language (OWL) Directed Graph：with nodes as entities and edges as relations KNOWLEDGE-AWARE APPLICATIONS Question Answering knowledge-graph-based question answering (KG-QA) an- swers natural language questions with facts from knowledge graphs. Neural\nSingle-fact QA Taking knowledge graph as an external intellectual source, simple factoid QA or single-fact QA is to answer simple question involving with a single knowledge graph fact Multi-hop Reasoning Recommender Systems Integrating knowledge graphs as external information enables recommendation systems to have the ability of commonsense reasoning.\nBy injecting knowledge-graph-based side information such as entities, relations, and attributes, many efforts work on embedding-based regularization to improve recommendation.\nknowledge representation learning (KRL) focus on knowledge representation learning (KRL) or knowledge graph embedding (KGE) by mapping entities and relations into low-dimensional vectors while capturing their semantic meanings\nrepresentation space in which the relations and entities are represented; scoring function for measuring the plausibility of factual triples; encoding models for representing and learning relational interactions; auxiliary information to be incorporated into the embedding methods. Representation 3.1 Representation Space 3.1.1 Point-Wise Space Point-wise TransE: represents entities and relations in d-dimension vector space\nNTN: models entities across multiple dimensions by a bilinear tensor neural layer.\n3.1.2 ComplexVector Space Instead 3.1.3 Gaussian Distribution Inspired by Gaussian word embedding, the density-based embedding model KG2E [21] introduces Gaussian distribution to deal with the (un)certainties of entities and relations.\n3.1.4 Manifold and Group A manifold is a topological space which could be defined as a set of points with neighborhoods by the set theory, while the group is algebraic structures defined in abstract algebra.\n3.2 Scoring Function Distance-based scoring function measures the plausibility of facts by calculating the distance between entities, where addictive translation with relations as h + r ≈ t is widely used\nSemantic similarity based scoring measures the plausibility of facts by semantic matching, which usually adopts multiplicative formulation $\\mathbf{h}^{\\top} \\mathbf{M}_{r} \\approx \\mathbf{t}^{\\top}$\n3.2.1 Distance-based Scoring Function calculate the Euclidean distance between the relational projection of entities.\nStructural Embedding (SE) :\nA more intensively used principle is the translation-based scoring function that aims to learn embeddings by representing relations as translations from head to tail entities.\nTransE:\nTransH:\nTransR:\nTransD:\nTransA:\nTransF:\nKG2E:\n3.2.2 Semantic Matching DistMult: By restricting relation matrixMr to be diagonal for multi-relational representation learning\n3.3 Encoding Models 3.3.1 Linear/Bilinear Models applying linear operation $g_{r}(\\mathbf{h}, \\mathbf{t})=\\mathbf{M}_{r}^{T}\\left(\\begin{array}{c} \\mathbf{h} \\\\ \\mathbf{t} \\end{array}\\right)$ bilinear transformation operations $f_{r}(h, t)=\\mathbf{h}^{\\top} \\mathbf{M}_{r} \\mathbf{t}$ Y. Wang, R. Gemulla, and H. Li, “On multi-relational link prediction with bilinear models,” showed that the ensembles of multiple linear models can improve the prediction performance through experiments\n3.3.2 Factorization Models Factorization methods formulated KRL models as three-way tensor $X$ decomposition. A general principle of tensor factorization can be denoted as $X_{h r t} \\approx \\mathbf{h}^{\\top} \\mathbf{M}_{r} \\mathbf{t}$\n3.3.3 Neural Networks Encoding models with linear/bilinear blocks can also be modeled using neural networks.\nGenerally, they take entities and/or relations into deep neural networks and compute a semantic matching score.\nConvolutional Neural Networks\nRSN: a recurrent skip mechanism to enhance\nTransformers: KG-BERT\nGraph Neural Networks: GNNs for learning connectivity structure under an encoder-decoder framework.\n3.4 Embedding with Auxiliary Information 3.4.1 Textual Description The challenge of KRL with textual description is to embed both structured knowledge and unstructured textual information in the same space. Wang\n3.4.2 Type Information Entities are represented with hierarchical classes or types, and consequently, relations with semantic types\n3.4.3 Visual Information knowledge acquisition tasks knowledge graph completion (KGC): expanding existing knowledge graphs embedding-based ranking, relation path reasoning, rule-based reasoning meta relational learning entity discovery recognition, disambiguation, typing alignment Relation extraction triple classification, 4.1 Knowledge Graph Completion Knowledge graph completion completes missing links between existing entities or infers entities given entity and relation queries, to add new triples to a knowledge graph. Typical subtasks include link prediction, entity prediction and relation prediction.\nembedding-based methods: failed to capture multi-step relationships relation path inference: explore multi-step relation paths rule-based reasoning: incorporate logical rules 4.1.1 Embedding-based Models learn embedding vectors based on existing triples, then replace tail entity or head entity with each entity e ∈ E to calculate scores of all the candidate entities and rank the top k entities. 4.1.2 Relation Path Reasoning Random walk inference has been widely investigated, for example, the Path-Ranking Algorithm (PRA) [69] chooses relational path under a combination of path constraints, and conducts maximum-likelihood classification\n4.1.3 RL-basedPath Finding Deep reinforcement learning (RL) is introduced for multi- hop reasoning by formulating path-finding between entity pairs as sequential decision making, specifically a Markov decision process (MDP). The policy-based RL agent learns to find a step of relation to extend the reasoning paths via the interaction between the knowledge graph environment, where the policy gradient is utilized for training RL agents.\n4.1.4 Rule-based Reasoning logical rule learning\n4.1.5 Meta Relational Learning The long-tail phenomena exist in the relations of knowledge graphs. Meanwhile, the real-world scenario of knowledge is dynamic, where unseen triples are usually acquired.\nmeta relational learning or few-shot relational learning, requires models to predict new relational facts with only a very few samples.\n4.1.6 Triple Classification Triple classification is to determine whether facts are correct in testing data, which is typically regarded as a binary classification problem.\n4.2 Entity Discovery 4.2.1 Entity Recognition 4.2.2 Entity Typing Entity typing includes coarse and fine-grained types, while the latter one uses a tree-structured type category and is typically regarded as multi-class and multi-label classifi- cation.\n4.2.3 Entity Disambiguation or entity linking is a unified task which links entity mentions to the corresponding entities in a knowledge graph.\n4.2.4 Entity Alignment aims to fuse knowledge among heterogeneous knowledge graphs. In practice, a small set of alignment seeds (i.e., synonymous entities appear in different knowledge graphs) is given to start the alignment process.\nEmbedding-based alignment calculates the similarity between embeddings of a pair of entities.\n4.3 Relation Extraction distant supervision, also referred as weak supervision or self supervision, uses heuristic matching to create training data by assuming that sentences containing the same entity mentions may express the same relation under the supervision of a relational database.\nTemporal Knowledge Graphs incorporate temporal information for representation learning.\ntemporal embedding, entity dynamics, temporal relational dependency, temporal logical reasoning. Knowledge-aware Applications include natural language understanding (NLU), question answering, recommendation systems, and miscellaneous real-world tasks, which inject knowledge to improve representation learning.\nOpen Knowledge Bases or Ontologies WordNet, DBpedia, YAGO, and Freebase\n","permalink":"https://congchan.github.io/posts/%E7%BB%BC%E8%BF%B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications/","summary":"\u003cp\u003eSurvey: \u003ca href=\"https://arxiv.org/abs/2002.00388v4\"\u003ehttps://arxiv.org/abs/2002.00388v4\u003c/a\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eA knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEntities\u003c/strong\u003e can be real-world objects and abstract concepts,\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRelationships\u003c/strong\u003e represent the relation between entities,\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSemantic descriptions\u003c/strong\u003e of entities and their relationships contain types and properties with a well-defined meaning\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eG: A knowledge graph\nF: A set of facts\n(h, r, t): A triple of head, relation and tail\n$(\\mathbf{h}, \\mathbf{r}, \\mathbf{t})$: Embedding of head, relation and tail\u003c/p\u003e","title":"综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications"},{"content":"2019, ACL\ndata: SemEval 2014, SemEval 2014 ABSA, SemEval 2015, SemEval 2016\ntask: ABSA\npropose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations.\n优点：\n用指针网络选取target，避免了序列标注的搜索空间过大问题 用span边界+极性的标注方式，解决多极性的target问题 方法 Input:\nsentence x =(x1,..., xn) with length n,\nTarget list T = {t1,..., tm}： each target ti is annotated with its start, end position, and its sentiment polarity\nMulti-Target Extractor propose an heuristic multi-span decoding algorithm\n3.3 Polarity Classifier given a target span r, we calculate a summarized vector v using the attention mechanism (Bahdanau et al., 2014) over tokens in its corresponding bound (si, ej)\n3.4 Model Variants Pipeline model:\nbuild a multi-target extractor where a BERT encoder is exclusively used. Then, a second backbone network is used to provide contextual sentence vectors for the polarity classifier. Two models are separately trained and combined as a pipeline during inference. Joint model:\nEach sentence is fed into a shared BERT backbone network that finally branches into two sibling output layers: one for proposing multiple candidate targets and another for predicting the sentiment polarity over each extracted target. Collapsed model: combine target span boundaries and sentiment polarities into one label space.\nData SemEval 2014 ABSA\nLAPTOP contains product reviews from the laptop domain in SemEval 2014 ABSA challenges\nREST is the union set of the restaurant domain from SemEval 2014, 2015 and 2016\n效果 Metrices We adopt the precision (P), recall (R), and F1 score\n","permalink":"https://congchan.github.io/posts/open-domain-targeted-sentiment-analysis-via-span-based-extraction-and-classification/","summary":"\u003cp\u003e2019, ACL\u003c/p\u003e\n\u003cp\u003edata: SemEval 2014, SemEval 2014 ABSA, SemEval 2015, SemEval 2016\u003c/p\u003e\n\u003cp\u003etask: ABSA\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003epropose a \u003cstrong\u003espan-based extract-then-classify framework\u003c/strong\u003e, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations.\u003c/p\u003e\n\u003cp\u003e优点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e用指针网络选取target，避免了序列标注的搜索空间过大问题\u003c/li\u003e\n\u003cli\u003e用span边界+极性的标注方式，解决多极性的target问题\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"方法\"\u003e方法\u003c/h1\u003e\n\u003cp\u003eInput:\u003c/p\u003e\n\u003cp\u003esentence \u003ccode\u003ex =(x1,..., xn)\u003c/code\u003e with length \u003ccode\u003en\u003c/code\u003e,\u003c/p\u003e\n\u003cp\u003eTarget list \u003ccode\u003eT = {t1,..., tm}\u003c/code\u003e： each target ti is annotated with its start, end position, and its sentiment polarity\u003c/p\u003e","title":"Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification"},{"content":"A Lite BERT BERT(Devlin et al., 2019)的参数很多, 模型很大, 内存消耗很大, 在分布式计算中的通信开销很大.\n但是BERT的高内存消耗边际收益并不高, 如果继续增大BERT-large这种大模型的隐含层大小, 模型效果不升反降.\n针对这些问题, 启发于mobilenet, Alert使用了两种减少参数的方法来降低模型大小和提高训练速度, 分别是Factorized embedding parameterization和Cross-layer parameter sharing. 这些设计让ALBERT增加参数大小的边界收益远远大于BERT.\n除此之外, 在句子关系任务上抛弃了bert的nsp任务, 改为sop任务.\n整体而言, ALBERT是当前众多BERT系列模型的集大成者, 其思路值得学习, 代码也写得很清楚. 下面仔细过一遍.\nFactorized embedding parameterization BERT以及后续的XLNet(Yang et al., 2019), RoBERTa(Liu et al., 2019)等, WordPiece embedding的维度E是和隐层维度H绑定的. WordPiece embedding本意是学习context-independent的表达，而hidden-layer旨在学习context-dependent的表达。将WordPiece embedding大小E与隐层大小H解绑，可以更有效地利用建模所需的总模型参数.\n从实用性的角度看, 这样可以减少词汇量对模型大小的影响. 在NLP中词汇量一般都很大, 所以这个解绑收益是很明显的.\n具体的做法就是对embedding进行因式分解, 把非常大的单词embedding分解成两个小的矩阵, O(V × H)变成O(V × E + E × H), 可以显著减少单词映射embedding的参数量. 这个在topic models一文中的隐变量模型中类似的思路体现.\nCross-layer parameter sharing 各个 transformer blocks 所有参数共享, 这样参数不再随着模型层数加深而增大.\nNo Dropout RoBERTA指出BERT一系列模型都是\u0026quot;欠拟合\u0026quot;的, 所以干脆直接关掉dropout, 那么在ALBERT中也是去掉 Dropout 层可以显著减少临时变量对内存的占用. 同时论文发现, Dropout会损害大型Transformer-based模型的性能。\nSentence-order Prediction (SOP) BERT使用的NSP任务是一种二分类loss，预测原始文本中是否有两个片段连续出现，通过从训练语料库中获取连续片段来创建正样本；通过将不同文档的片段配对作为负样本.\n在RoBERTA等改进型的论文中都指出, NSP的表现不是很稳定, 所以RoBERTa直接就去掉了NSP任务.\n而ALBERT推测, NSP任务对下游任务提升不稳定的原因在于NSP任务学习难度不够高(相对于MLM任务)。NSP本质是融合了topic prediction主题预测和coherence prediction两个任务。Coherence prediction是核心的任务, 可以学习inter-sentence信息. 主题预测, 也就是学习两个句子是否来自同一段原文, 则相对容易得多，并且与使用MLM损失学习的内容重叠更多。\n所以我们需要一个更专注于coherence prediction的sentence level任务, 比如ALBERT中用到的SOP.\nSOP的正样本采样方法和BERT一样, 但负样本改为倒置顺序的两句话, 这迫使模型学习关于discourse-level coherence properties的细粒度区别。\nTransformer实现 Bert和Albert的核心模型架构都是Transformer encoder, 包括用于编码context的Multi-headed self attention层, 用于计算非线性层间特征的Feed-forward layers, 和用于加深网络深度, 降低训练难度的Layer norm and residuals. 除此之外, 还有Positional embeddings用来编码相对位置信息.\nTransformer由一个个结构相同的blocks堆叠而成, 每一个block可以简单理解为一个注意力层+全连接层+残差网络, API是这样:\ndef attention_ffn_block(layer_input, hidden_size=768, attention_mask=None, num_attention_heads=1, attention_head_size=64, attention_probs_dropout_prob=0.0, intermediate_size=3072, intermediate_act_fn=None, initializer_range=0.02, hidden_dropout_prob=0.0): \u0026#34;\u0026#34;\u0026#34;A network with attention-ffn as sub-block. Args: layer_input: float Tensor of shape [batch_size, from_seq_length, from_width]. hidden_size: (optional) int, size of hidden layer. attention_mask: (optional) int32 Tensor of shape [batch_size, from_seq_length, to_seq_length]. The values should be 1 or 0. The attention scores will effectively be set to -infinity for any positions in the mask that are 0, and will be unchanged for positions that are 1. num_attention_heads: int. Number of attention heads. attention_head_size: int. Size of attention head. attention_probs_dropout_prob: float. dropout probability for attention_layer intermediate_size: int. Size of intermediate hidden layer. intermediate_act_fn: (optional) Activation function for the intermediate layer. initializer_range: float. Range of the weight initializer. hidden_dropout_prob: (optional) float. Dropout probability of the hidden layer. Returns: layer output \u0026#34;\u0026#34;\u0026#34; 其中最开始是注意力层, 并在输出后面接残差, 最后正则化:\nwith tf.variable_scope(\u0026#34;attention_1\u0026#34;): with tf.variable_scope(\u0026#34;self\u0026#34;): attention_output = attention_layer( from_tensor=layer_input, to_tensor=layer_input, attention_mask=attention_mask, num_attention_heads=num_attention_heads, attention_probs_dropout_prob=attention_probs_dropout_prob, initializer_range=initializer_range) # Run a linear projection of `hidden_size` then add a residual # with `layer_input`. with tf.variable_scope(\u0026#34;output\u0026#34;): attention_output = dense_layer_3d_proj( attention_output, hidden_size, attention_head_size, create_initializer(initializer_range), None, name=\u0026#34;dense\u0026#34;) attention_output = dropout(attention_output, hidden_dropout_prob) attention_output = layer_norm(attention_output + layer_input) 其中用到的点乘注意力和多头注意力直接使用上一篇Transformer \u0026amp; Self-Attention (多头)自注意力编码中的方法.\n然后就是feed forward layer, 在输出层之前加入了一个升维的中间层intermediate, 并应用激活函数(在这里是gelu), 末尾的输出网络没有激活函数, 只负责把输出映射回transformer的隐含层维度大小, 最后同样加上残差和正则化. 这种扩张-变换-压缩的范式, 是借鉴了mobilenet中的思路, 在需要使用ReLU的卷积层中，将channel数扩张到足够大，再进行激活，被认为可以降低激活层的信息损失。:\nwith tf.variable_scope(\u0026#34;ffn_1\u0026#34;): with tf.variable_scope(\u0026#34;intermediate\u0026#34;): intermediate_output = dense_layer_2d( attention_output, intermediate_size, create_initializer(initializer_range), intermediate_act_fn, num_attention_heads=num_attention_heads, name=\u0026#34;dense\u0026#34;) with tf.variable_scope(\u0026#34;output\u0026#34;): ffn_output = dense_layer_2d( intermediate_output, hidden_size, create_initializer(initializer_range), None, num_attention_heads=num_attention_heads, name=\u0026#34;dense\u0026#34;) ffn_output = dropout(ffn_output, hidden_dropout_prob) ffn_output = layer_norm(ffn_output + attention_output) return ffn_output 其中用到的dense_layer_2d就是一个基本的神经网络$y=f(Wx+b)$, 其中$f()$是激活函数:\ndef dense_layer_2d(input_tensor, output_size, initializer, activation, num_attention_heads=1, name=None): \u0026#34;\u0026#34;\u0026#34;A dense layer with 2D kernel. Args: input_tensor: Float tensor with rank 3. output_size: The size of output dimension. initializer: Kernel initializer. activation: Activation function. num_attention_heads: number of attention head in attention layer. name: The name scope of this layer. Returns: float logits Tensor. \u0026#34;\u0026#34;\u0026#34; del num_attention_heads # unused input_shape = get_shape_list(input_tensor) hidden_size = input_shape[2] with tf.variable_scope(name): w = tf.get_variable( name=\u0026#34;kernel\u0026#34;, shape=[hidden_size, output_size], initializer=initializer) b = tf.get_variable( name=\u0026#34;bias\u0026#34;, shape=[output_size], initializer=tf.zeros_initializer) ret = tf.einsum(\u0026#34;BFH,HO-\u0026gt;BFO\u0026#34;, input_tensor, w) ret += b if activation is not None: return activation(ret) else: return ret 一个完整的transformer模块, 核心是由多个attention_ffn_block堆叠而成, 同时注意设定reuse=tf.AUTO_REUSE来实现Cross-layer parameter sharing, 设定num_hidden_groups=1就可以让所有层都共享参数.\ndef transformer_model(input_tensor, attention_mask=None, hidden_size=768, num_hidden_layers=12, num_hidden_groups=12, num_attention_heads=12, intermediate_size=3072, inner_group_num=1, intermediate_act_fn=\u0026#34;gelu\u0026#34;, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, initializer_range=0.02, do_return_all_layers=False): \u0026#34;\u0026#34;\u0026#34;Multi-headed, multi-layer Transformer from \u0026#34;Attention is All You Need\u0026#34;. This is almost an exact implementation of the original Transformer encoder. See the original paper: https://arxiv.org/abs/1706.03762 Also see: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py Args: input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size]. attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length, seq_length], with 1 for positions that can be attended to and 0 in positions that should not be. hidden_size: int. Hidden size of the Transformer. num_hidden_layers: int. Number of layers (blocks) in the Transformer. num_hidden_groups: int. Number of group for the hidden layers, parameters in the same group are shared. num_attention_heads: int. Number of attention heads in the Transformer. intermediate_size: int. The size of the \u0026#34;intermediate\u0026#34; (a.k.a., feed forward) layer. inner_group_num: int, number of inner repetition of attention and ffn. intermediate_act_fn: function. The non-linear activation function to apply to the output of the intermediate/feed-forward layer. hidden_dropout_prob: float. Dropout probability for the hidden layers. attention_probs_dropout_prob: float. Dropout probability of the attention probabilities. initializer_range: float. Range of the initializer (stddev of truncated normal). do_return_all_layers: Whether to also return all layers or just the final layer. Returns: float Tensor of shape [batch_size, seq_length, hidden_size], the final hidden layer of the Transformer. Raises: ValueError: A Tensor shape or parameter is invalid. \u0026#34;\u0026#34;\u0026#34; if hidden_size % num_attention_heads != 0: raise ValueError( \u0026#34;The hidden size (%d) is not a multiple of the number of attention \u0026#34; \u0026#34;heads (%d)\u0026#34; % (hidden_size, num_attention_heads)) attention_head_size = hidden_size // num_attention_heads input_shape = get_shape_list(input_tensor, expected_rank=3) input_width = input_shape[2] all_layer_outputs = [] if input_width != hidden_size: prev_output = dense_layer_2d( input_tensor, hidden_size, create_initializer(initializer_range), None, name=\u0026#34;embedding_hidden_mapping_in\u0026#34;) else: prev_output = input_tensor with tf.variable_scope(\u0026#34;transformer\u0026#34;, reuse=tf.AUTO_REUSE): for layer_idx in range(num_hidden_layers): group_idx = int(layer_idx / num_hidden_layers * num_hidden_groups) with tf.variable_scope(\u0026#34;group_%d\u0026#34; % group_idx): with tf.name_scope(\u0026#34;layer_%d\u0026#34; % layer_idx): layer_output = prev_output for inner_group_idx in range(inner_group_num): with tf.variable_scope(\u0026#34;inner_group_%d\u0026#34; % inner_group_idx): layer_output = attention_ffn_block( layer_output, hidden_size, attention_mask, num_attention_heads, attention_head_size, attention_probs_dropout_prob, intermediate_size, intermediate_act_fn, initializer_range, hidden_dropout_prob) prev_output = layer_output all_layer_outputs.append(layer_output) if do_return_all_layers: return all_layer_outputs else: return all_layer_outputs[-1] Factorized Embedding实现 首先是需要embedding的因式分解, embedding_lookup输出的是V x E matrix, 其中E就是embedding_size:\ndef embedding_lookup(input_ids, vocab_size, embedding_size=128, initializer_range=0.02, word_embedding_name=\u0026#34;word_embeddings\u0026#34;, use_one_hot_embeddings=False): \u0026#34;\u0026#34;\u0026#34;Looks up words embeddings for id tensor. Args: input_ids: int32 Tensor of shape [batch_size, seq_length] containing word ids. vocab_size: int. Size of the embedding vocabulary. embedding_size: int. Width of the word embeddings. initializer_range: float. Embedding initialization range. word_embedding_name: string. Name of the embedding table. use_one_hot_embeddings: bool. If True, use one-hot method for word embeddings. If False, use `tf.nn.embedding_lookup()`. Returns: float Tensor of shape [batch_size, seq_length, embedding_size]. \u0026#34;\u0026#34;\u0026#34; # This function assumes that the input is of shape [batch_size, seq_length, # num_inputs]. # # If the input is a 2D tensor of shape [batch_size, seq_length], we # reshape to [batch_size, seq_length, 1]. if input_ids.shape.ndims == 2: input_ids = tf.expand_dims(input_ids, axis=[-1]) embedding_table = tf.get_variable( name=word_embedding_name, shape=[vocab_size, embedding_size], initializer=create_initializer(initializer_range)) if use_one_hot_embeddings: flat_input_ids = tf.reshape(input_ids, [-1]) one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size) output = tf.matmul(one_hot_input_ids, embedding_table) else: output = tf.nn.embedding_lookup(embedding_table, input_ids) input_shape = get_shape_list(input_ids) output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size]) return (output, embedding_table) 把embedding映射回隐层大小E x H, 依靠的是上面定义的transformer_model中的\nif input_width != hidden_size: prev_output = dense_layer_2d( input_tensor, hidden_size, create_initializer(initializer_range), None, name=\u0026#34;embedding_hidden_mapping_in\u0026#34;) ALBERT模型搭建 大体框架就是embeddings+encoder+pooler output, 其中encoder就是transformerblocks的堆叠:\nclass AlbertModel(object): \u0026#34;\u0026#34;\u0026#34;BERT model (\u0026#34;Bidirectional Encoder Representations from Transformers\u0026#34;). \u0026#34;\u0026#34;\u0026#34; def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=False, scope=None): \u0026#34;\u0026#34;\u0026#34;Constructor for AlbertModel. Args: config: `AlbertConfig` instance. is_training: bool. true for training model, false for eval model. Controls whether dropout will be applied. input_ids: int32 Tensor of shape [batch_size, seq_length]. input_mask: (optional) int32 Tensor of shape [batch_size, seq_length]. token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]. use_one_hot_embeddings: (optional) bool. Whether to use one-hot word embeddings or tf.embedding_lookup() for the word embeddings. scope: (optional) variable scope. Defaults to \u0026#34;bert\u0026#34;. Raises: ValueError: The config is invalid or one of the input tensor shapes is invalid. \u0026#34;\u0026#34;\u0026#34; config = copy.deepcopy(config) if not is_training: config.hidden_dropout_prob = 0.0 config.attention_probs_dropout_prob = 0.0 input_shape = get_shape_list(input_ids, expected_rank=2) batch_size = input_shape[0] seq_length = input_shape[1] if input_mask is None: input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32) if token_type_ids is None: token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32) with tf.variable_scope(scope, default_name=\u0026#34;bert\u0026#34;): with tf.variable_scope(\u0026#34;embeddings\u0026#34;): # Perform embedding lookup on the word ids. (self.word_embedding_output, self.output_embedding_table) = embedding_lookup( input_ids=input_ids, vocab_size=config.vocab_size, embedding_size=config.embedding_size, initializer_range=config.initializer_range, word_embedding_name=\u0026#34;word_embeddings\u0026#34;, use_one_hot_embeddings=use_one_hot_embeddings) # Add positional embeddings and token type embeddings, then layer # normalize and perform dropout. self.embedding_output = embedding_postprocessor( input_tensor=self.word_embedding_output, use_token_type=True, token_type_ids=token_type_ids, token_type_vocab_size=config.type_vocab_size, token_type_embedding_name=\u0026#34;token_type_embeddings\u0026#34;, use_position_embeddings=True, position_embedding_name=\u0026#34;position_embeddings\u0026#34;, initializer_range=config.initializer_range, max_position_embeddings=config.max_position_embeddings, dropout_prob=config.hidden_dropout_prob) with tf.variable_scope(\u0026#34;encoder\u0026#34;): # Run the stacked transformer. # `sequence_output` shape = [batch_size, seq_length, hidden_size]. self.all_encoder_layers = transformer_model( input_tensor=self.embedding_output, attention_mask=input_mask, hidden_size=config.hidden_size, num_hidden_layers=config.num_hidden_layers, num_hidden_groups=config.num_hidden_groups, num_attention_heads=config.num_attention_heads, intermediate_size=config.intermediate_size, inner_group_num=config.inner_group_num, intermediate_act_fn=get_activation(config.hidden_act), hidden_dropout_prob=config.hidden_dropout_prob, attention_probs_dropout_prob=config.attention_probs_dropout_prob, initializer_range=config.initializer_range, do_return_all_layers=True) self.sequence_output = self.all_encoder_layers[-1] # The \u0026#34;pooler\u0026#34; converts the encoded sequence tensor of shape # [batch_size, seq_length, hidden_size] to a tensor of shape # [batch_size, hidden_size]. This is necessary for segment-level # (or segment-pair-level) classification tasks where we need a fixed # dimensional representation of the segment. with tf.variable_scope(\u0026#34;pooler\u0026#34;): # We \u0026#34;pool\u0026#34; the model by simply taking the hidden state corresponding # to the first token. We assume that this has been pre-trained first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1) self.pooled_output = tf.layers.dense( first_token_tensor, config.hidden_size, activation=tf.tanh, kernel_initializer=create_initializer(config.initializer_range)) 再附上官网的API介绍：\n# Already been converted from strings into ids input_ids = tf.constant([[31, 51, 99], [15, 5, 0]]) input_mask = tf.constant([[1, 1, 1], [1, 1, 0]]) token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]]) config = modeling.AlbertConfig(vocab_size=32000, hidden_size=512, num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024) model = modeling.AlbertModel(config=config, is_training=True, input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids) label_embeddings = tf.get_variable(...) pooled_output = model.get_pooled_output() logits = tf.matmul(pooled_output, label_embeddings) ... 参考资料 https://github.com/google-research/ALBERT\n","permalink":"https://congchan.github.io/posts/a-lite-bertalbert-%E5%8E%9F%E7%90%86%E5%92%8C%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","summary":"\u003ch3 id=\"a-lite-bert\"\u003eA Lite BERT\u003c/h3\u003e\n\u003cp\u003eBERT(Devlin et al., 2019)的参数很多, 模型很大, 内存消耗很大, 在分布式计算中的通信开销很大.\u003c/p\u003e\n\u003cp\u003e但是BERT的高内存消耗边际收益并不高, 如果继续增大BERT-large这种大模型的隐含层大小, 模型效果不升反降.\u003c/p\u003e\n\u003cp\u003e针对这些问题, 启发于mobilenet, Alert使用了两种减少参数的方法来降低模型大小和提高训练速度, 分别是Factorized embedding parameterization和Cross-layer parameter sharing. 这些设计让ALBERT增加参数大小的边界收益远远大于BERT.\u003c/p\u003e\n\u003cp\u003e除此之外, 在句子关系任务上抛弃了bert的\u003ccode\u003ensp\u003c/code\u003e任务, 改为\u003ccode\u003esop\u003c/code\u003e任务.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e整体而言, ALBERT是当前众多BERT系列模型的集大成者, 其思路值得学习, 代码也写得很清楚. 下面仔细过一遍.\u003c/p\u003e\n\u003ch3 id=\"factorized-embedding-parameterization\"\u003eFactorized embedding parameterization\u003c/h3\u003e\n\u003cp\u003eBERT以及后续的XLNet(Yang et al., 2019), RoBERTa(Liu et al., 2019)等, WordPiece embedding的维度\u003ccode\u003eE\u003c/code\u003e是和隐层维度\u003ccode\u003eH\u003c/code\u003e绑定的. WordPiece embedding本意是学习context-independent的表达，而hidden-layer旨在学习context-dependent的表达。将WordPiece embedding大小\u003ccode\u003eE\u003c/code\u003e与隐层大小\u003ccode\u003eH\u003c/code\u003e解绑，可以更有效地利用建模所需的总模型参数.\u003c/p\u003e\n\u003cp\u003e从实用性的角度看, 这样可以减少词汇量对模型大小的影响. 在NLP中词汇量一般都很大, 所以这个解绑收益是很明显的.\u003c/p\u003e\n\u003cp\u003e具体的做法就是对embedding进行因式分解, 把非常大的单词embedding分解成两个小的矩阵, \u003ccode\u003eO(V × H)\u003c/code\u003e变成\u003ccode\u003eO(V × E + E × H)\u003c/code\u003e, 可以显著减少单词映射embedding的参数量. 这个在topic models一文中的隐变量模型中类似的思路体现.\u003c/p\u003e\n\u003ch3 id=\"cross-layer-parameter-sharing\"\u003eCross-layer parameter sharing\u003c/h3\u003e\n\u003cp\u003e各个 transformer blocks 所有参数共享, 这样参数不再随着模型层数加深而增大.\u003c/p\u003e","title":"A Lite BERT(AlBERT) 原理和源码解析"},{"content":"Entity Linking\nKnowledge Graph (知识图谱)：一种语义网络，旨在描述客观世界的概念实体及其之间的关系，有时也称为Knowledge Base (知识库)。 图谱由三元组构成：\u0026lt;实体1，关系，实体2\u0026gt; 或者 \u0026lt;实体，属性，属性值\u0026gt;； 例如：\u0026lt;姚明，plays-in，NBA\u0026gt;、\u0026lt;姚明，身高，2.29m\u0026gt;； 常见的KB有：Wikidata、DBpedia、YAGO。 Entity 实体：实体是知识图谱的基本单元，也是文本中承载信息的重要语言单位。 Mention 提及：自然文本中表达实体的语言片段。 应用方向\nQuestion Answering：EL是KBQA的刚需，linking到实体之后才能查询图数据库； Content Analysis：舆情分析、内容推荐、阅读增强； Information Retrieval：基于语义实体的搜索引擎，google搜索一些实体，右侧会出现wikipedia页面； Knowledge Base population：扩充知识库，更新实体和关系。 候选实体和消歧\nEntity linking system consists of two components:\ncandidate entity generation：从mention出发，找到KB中所有可能的实体，组成候选实体集 (candidate entities)； Entity Disambiguation：从candidate entities中，选择最可能的实体作为预测实体。 Entity Disambiguation (ED) 是最重要的部分\nFeatures Context-Independent Features： LinkCount：#(m-\u0026gt;e)，知识库中某个提及m指向实体e的次数； Entity Attributes：Popularity、Type； Context-Dependent Features： Textual Context：BOW, Concept Vector Coherence Between Entities：WLM、PMI、Jaccard Distance Context-Independent Features mention到实体的LinkCount、实体自身的一些属性（比如热度、类型等等）\nLinkCount作为一个先验知识，在消歧时，往往很有用 Context-Dependent Features 全局地进行entities的消歧实际上是一个NP-hard的问题，因此核心问题是如何更加快速有效地利用一致性特征\nLearning to Rank Methods：Point-wise、Pair-wise、List-wise。由于ED任务ground truth只有一个实体，一般都是用point-wise来做。输入是文本的context、mention、某个entity的一些attributes，输出mention指向该entity的置信度，以此rank，选出最可信的entity； Probabilistic Methods：Incorporate heterogeneous knowledge into a probabilistic model。结合不同信息，得到条件概率 $P(e|m,c)$，其中 c 是输入文本，e 为实体， m 是mention。比如用归一化的LinkCount信息，作为先验概率 $P(e|m)$ ； Graph-Based Approaches：maximize coherene between entities。利用图特征 (entity embedding、relation)，在消歧时，考虑全局消歧后实体的一致性； Deep Type Discovering Types for Entity Disambiguation\nHigh-level overview Our system uses the following steps:\nExtract every Wikipedia-internal link to determine, for each word, the set of conceivable entities it can refer to. For example, when encountering the link [jaguar](https://en.wikipedia.org/wiki/Jaguar) in a Wikipedia page, we conclude that https://en.wikipedia.org/wiki/Jaguar is one of the meanings of jaguar.\nWalk the Wikipedia category tree (using the Wikidata knowledge graph) to determine, for each entity, the set of categories it belongs to. For example, at the bottom of https://en.wikipedia.org/wiki/Jaguar_Cars’s Wikipedia page, are the following categories (which themselves have their own categories, such as Automobiles):\nPick a list of ~100 categories to be your “type” system, and optimize over this choice of categories so that they compactly express any entity. We know the mapping of entities to categories, so given a type system, we can represent each entity as a ~100-dimensional binary vector indicating membership in each category.\nUsing every Wikipedia-internal link and its surrounding context, produce training data mapping a word plus context to the ~100-dimensional binary representation of the corresponding entity, and train a neural network to predict this mapping. This chains together the previous steps: Wikipedia links map a word to an entity, we know the categories for each entity from step 2, and step 3 picked the categories in our type system.\nAt test time, given a word and surrounding context, our neural network’s output can be interpreted as the probability that the word belongs to each category. If we knew the exact set of category memberships, we would narrow down to one entity (assuming well-chosen categories). But instead, we must play a probabilistic 20 questions: use Bayes’ theorem to calculate the chance of the word disambiguating to each of its possible entities.\nUnlinkable Mention Prediction 拒识掉未知实体 NIL Threshold：通过一个置信度的阈值来卡一下； Binary Classification：训练一个二分类的模型，判断Top-rankeded Entity是否真的是文中的mention想要表达的实体； Rank with NIL：在rank的时候，在候选实体中加入NIL Entity。 一般就阈值卡一下就好了，不是太大的问题。但如果具体的场景是做KB Population且实体还不是很全的时候，就需要重点关注一下了。\nCandidate Entity Generation (CEG) CEG的方法都比较朴素\n最重要的方法：Name Dictionary ( {mention: entity} ) 哪些别名：首字母缩写、模糊匹配、昵称、拼写错误等。 构建方法： Wikipedia（Redirect pages, Disambiguation pages, Hyperlinks）； 基于搜索引擎：调google api，搜mention。若前m个有wiki entity，建立map； Heuristic Methods； 人工标注、用户日志。 对于每一个entity，紧凑而充分地配置别名，才能保证生成的candidate entites没有遗漏掉ground truth entity。\n具体的，要配置哪些别名，要用什么构建方法，往往取决于EL的使用场景。比如做百科问答或是通用文本的阅读增强，就很依赖于wikipedia和搜索引擎；但如果是某个具体的行业领域，就需要通过一些启发式的方法、用户日志、网页爬取，甚至人工标注的方法来构建Name Dictionary。\nReference 【知识图谱】实体链接：一份\u0026quot;由浅入深\u0026quot;的综述 Discovering Types for Entity Disambiguation ","permalink":"https://congchan.github.io/posts/entity-linking/","summary":"\u003cp\u003eEntity Linking\u003c/p\u003e\n\u003c!-- more --\u003e \n\u003cul\u003e\n\u003cli\u003eKnowledge Graph (知识图谱)：一种语义网络，旨在描述客观世界的概念实体及其之间的关系，有时也称为Knowledge Base (知识库)。\n\u003cul\u003e\n\u003cli\u003e图谱由三元组构成：\u003ccode\u003e\u0026lt;实体1，关系，实体2\u0026gt;\u003c/code\u003e 或者 \u003ccode\u003e\u0026lt;实体，属性，属性值\u0026gt;\u003c/code\u003e；\u003c/li\u003e\n\u003cli\u003e例如：\u003ccode\u003e\u0026lt;姚明，plays-in，NBA\u0026gt;\u003c/code\u003e、\u003ccode\u003e\u0026lt;姚明，身高，2.29m\u0026gt;\u003c/code\u003e；\u003c/li\u003e\n\u003cli\u003e常见的KB有：Wikidata、DBpedia、YAGO。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEntity 实体：实体是知识图谱的基本单元，也是文本中承载信息的重要语言单位。\u003c/li\u003e\n\u003cli\u003eMention 提及：自然文本中表达实体的语言片段。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e应用方向\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eQuestion Answering\u003c/strong\u003e：EL是KBQA的刚需，linking到实体之后才能查询图数据库；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContent Analysis\u003c/strong\u003e：舆情分析、内容推荐、阅读增强；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInformation Retrieval\u003c/strong\u003e：基于语义实体的搜索引擎，google搜索一些实体，右侧会出现wikipedia页面；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKnowledge Base population\u003c/strong\u003e：扩充知识库，更新实体和关系。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e候选实体和消歧\u003c/p\u003e\n\u003cp\u003eEntity linking system consists of two components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ecandidate entity generation：从mention出发，找到KB中所有可能的实体，组成候选实体集 (candidate entities)；\u003c/li\u003e\n\u003cli\u003eEntity Disambiguation：从candidate entities中，选择最可能的实体作为预测实体。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"entity-disambiguation-ed\"\u003eEntity Disambiguation (ED)\u003c/h1\u003e\n\u003cp\u003e是最重要的部分\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFeatures\n\u003cul\u003e\n\u003cli\u003eContext-Independent Features：\n\u003cul\u003e\n\u003cli\u003eLinkCount：#(m-\u0026gt;e)，知识库中某个提及m指向实体e的次数；\u003c/li\u003e\n\u003cli\u003eEntity Attributes：Popularity、Type；\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eContext-Dependent Features：\n\u003cul\u003e\n\u003cli\u003eTextual Context：BOW, Concept Vector\u003c/li\u003e\n\u003cli\u003eCoherence Between Entities：WLM、PMI、Jaccard Distance\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"context-independent-features\"\u003eContext-Independent Features\u003c/h2\u003e\n\u003cp\u003emention到实体的LinkCount、实体自身的一些属性（比如热度、类型等等）\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinkCount作为一个先验知识，在消歧时，往往很有用\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"context-dependent-features\"\u003eContext-Dependent Features\u003c/h2\u003e\n\u003cp\u003e全局地进行entities的消歧实际上是一个NP-hard的问题，因此核心问题是如何更加快速有效地利用一致性特征\u003c/p\u003e","title":"Entity Linking"},{"content":"知识图谱补全\n基于知识表示的方法 知识表示学习：对知识图谱中的实体和关系学习其低维度的嵌入式表示。\n常见的知识表示学习方法：主要是以 TransE 法及其变种为核心，针对空间映射等场景做的改进\n基于实体和关系的表示对缺失三元组进行预测；\n利用实体描述信息，可以解决开放域实体补全的问题；\n基于路径查找的方法 可使用基于路径查找的方法来处理这类多步推理问题。\n传统的路径查找方法主要是 PRA 方法（Path Ranking Algorithm）；但是这种方法对于包含较大规模的知识图谱来说，会由于路径数量爆炸式增长，导致特征空间急剧膨胀\n可以尝试用 embedding 的方式表示关系，对关系进行泛化，并基于此对知识的补全进行建模，以缓解路径数量过多导致的特征空间膨胀问题。\n给定实体对集合，利用 PRA 查找一定数量的路径； 路径计算过程中加入实体类型信息（减少长尾实体影响）； 使用 RNN 沿着路径进行向量化建模；RNN 模型参数在不同关系之间共享； 通过比较路径向量与待预测关系向量间的关联度来进行关系补全。 基于强化学习的方法 前面提到的两种方法，仍然存在若干的问题：\n需要基于 random walk 来查找路径； 而 random walk 算法在离散空间中运行，难以评价知识图谱中相似的实体和关系； 超级结点可能影响 random walk 算法运行速度。 强化学习方法：\n在连续空间中进行路径搜索； 通过引入多种奖励函数，使得路径查找更加灵活、可控。 DeepPath DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning\nxwhan/DeepPath\n任务：查找 Band of Brothers 和 English 之间的关系。 路径起点：Band of Brothers 状态：实体中的 embedding 动作：图谱中的关系； 奖励 Binary，是否到达终点 路径长度 路径多样性 策略网络：使用全连接网络。 DeepPath 方法仍然存在一些缺陷：知识图谱本身的不完善很可能对路径查找造成影响。\nCollaborative Policy Learning for Open Knowledge Graph Reasoning\n在路径查找过程中，通过抽取关系，将缺失的路径补全。\n","permalink":"https://congchan.github.io/posts/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8/","summary":"\u003cp\u003e知识图谱补全\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch1 id=\"基于知识表示的方法\"\u003e基于知识表示的方法\u003c/h1\u003e\n\u003cp\u003e知识表示学习：对知识图谱中的实体和关系学习其低维度的嵌入式表示。\u003c/p\u003e\n\u003cp\u003e常见的知识表示学习方法：主要是以 TransE 法及其变种为核心，针对空间映射等场景做的改进\u003c/p\u003e\n\u003cp\u003e基于实体和关系的表示对缺失三元组进行预测；\u003c/p\u003e\n\u003cp\u003e利用实体描述信息，可以解决开放域实体补全的问题；\u003c/p\u003e\n\u003ch1 id=\"基于路径查找的方法\"\u003e基于路径查找的方法\u003c/h1\u003e\n\u003cp\u003e可使用基于路径查找的方法来处理这类多步推理问题。\u003c/p\u003e\n\u003cp\u003e传统的路径查找方法主要是 PRA 方法（Path Ranking Algorithm）；但是这种方法对于包含较大规模的知识图谱来说，会由于路径数量爆炸式增长，导致特征空间急剧膨胀\u003c/p\u003e\n\u003cp\u003e可以尝试用 embedding 的方式表示关系，对关系进行泛化，并基于此对知识的补全进行建模，以缓解路径数量过多导致的特征空间膨胀问题。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e给定实体对集合，利用 PRA 查找一定数量的路径；\u003c/li\u003e\n\u003cli\u003e路径计算过程中加入实体类型信息（减少长尾实体影响）；\u003c/li\u003e\n\u003cli\u003e使用 RNN 沿着路径进行向量化建模；RNN 模型参数在不同关系之间共享；\u003c/li\u003e\n\u003cli\u003e通过比较路径向量与待预测关系向量间的关联度来进行关系补全。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"基于强化学习的方法\"\u003e基于强化学习的方法\u003c/h1\u003e\n\u003cp\u003e前面提到的两种方法，仍然存在若干的问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e需要基于 random walk 来查找路径；\u003c/li\u003e\n\u003cli\u003e而 random walk 算法在离散空间中运行，难以评价知识图谱中相似的实体和关系；\u003c/li\u003e\n\u003cli\u003e超级结点可能影响 random walk 算法运行速度。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e强化学习方法：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在连续空间中进行路径搜索；\u003c/li\u003e\n\u003cli\u003e通过引入多种奖励函数，使得路径查找更加灵活、可控。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"deeppath\"\u003eDeepPath\u003c/h2\u003e\n\u003cp\u003eDeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/xwhan/DeepPath\"\u003exwhan/DeepPath\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"/images/papers/paper7.png\" loading=\"lazy\" src=\"/images/papers/paper7.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e任务：查找 Band of Brothers 和 English 之间的关系。\u003c/li\u003e\n\u003cli\u003e路径起点：Band of Brothers\u003c/li\u003e\n\u003cli\u003e状态：实体中的 embedding\u003c/li\u003e\n\u003cli\u003e动作：图谱中的关系；\u003c/li\u003e\n\u003cli\u003e奖励\n\u003cul\u003e\n\u003cli\u003eBinary，是否到达终点\u003c/li\u003e\n\u003cli\u003e路径长度\u003c/li\u003e\n\u003cli\u003e路径多样性\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e策略网络：使用全连接网络。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDeepPath 方法仍然存在一些缺陷：知识图谱本身的不完善很可能对路径查找造成影响。\u003c/p\u003e","title":"知识图谱补全"},{"content":"Combining reinforcement learning and deep neural networks at scale. The algorithm was developed by enhancing a classic RL algorithm called Q-Learning with deep neural networks and a technique called experience replay.\nQ-Learning Q-Learning is based on the notion of a Q-function. The Q-function (a.k.a the state-action value function) of a policy $\\pi$，$Q^{\\pi}(s, a)$ ，measures the expected return or discounted sum of rewards obtained from state $s$ by taking action $a$ first and following policy $\\pi$ thereafter.\nThe optimal Q-function $Q^{*}(s, a)$ obeys the following Bellman optimality equation:\nThis means that the maximum return from state s and action a is the sum of the immediate reward $r$ and the return (discounted by $\\gamma$) obtained by following the optimal policy thereafter until the end of the episode(i.e., the maximum reward from the next state $s^{\\prime}$). The expectation is computed both over the distribution of immediate rewards $r$ and possible next states $s^{\\prime}$.\nEach sequence from the initial state and action to the end is called an episode.\n通过期望值来预估未来状态\n假设没有$\\gamma$ ，那么未来长期reward没有折损，会得到 sparse reward：因为没有折损，所有状态最后得到的值是一样的，模型无法获得差异信号\nIt is important to tune this hyperparameter to get optimum results. Successful values range from 0.9 to 0.99. A lower value encourages short-term thinking A higher value emphasizes long-term rewards The Bellman Equation was introduced by Dr. Richard Bellman (who\u0026rsquo;s known as the Father of dynamic programming) in 1954 in the paper: The Theory of Dynamic Programming.\nUse the Bellman optimality equation as an iterative update\n$$Q_{i + 1}(s, a) \\leftarrow E\\left[r+\\gamma \\max_{a^{\\prime}} Q_{i}(s^{\\prime}, a^{\\prime})\\right]$$this converges to the optimal Q-function, i.e $Q_{i} \\rightarrow Q^{*} \\text { as } i \\rightarrow \\infty$\n在深度学习之前，Bellman optimality equation 使用递归求解，在每层递归中，需要知道能使预期长期回报最大化的最佳操作是什么。也就是会遍历庞大的递归搜索树。\n在Non-deterministic情况下，BO函数变成\n$$Q(s, a) = R(s, a) + \\gamma \\sum_{s'}P(s, a, s') max_{a'}Q(s', a'))$$Temporal Difference Non-deterministic search can be very difficult to actually calculate the value of each state.\n用Temporal difference 来迭代更新每一个事件的Q-value. $\\alpha$是学习率\n$$Q_t(s, a) = Q_{t-1}(s, a) + \\alpha TD_t(a, s)$$假设t时间步选择了$(s', a')$，则Temporal difference是\n$$TD(a, s) = R(s, a) + \\gamma max_{a'}Q(s', a') - Q_{t-1}(s, a)$$由于Non-deterministic环境中存在的随机性，TD值一般不会为0，就可以随着每一时间步的推进更新Q-value\nDeep Q-Learning For most problems, it is impractical to represent the Q-function as a table containing values for each combination of s and a. Instead, we train a function approximator, such as a neural network with parameters $\\theta$, to estimate the Q-values, i.e. $Q(s, a ; \\theta) \\approx Q^{*}(s, a)$, by minimizing the following loss at each step i:\n其中 $y_i$是TD Target, $y_i - Q$ is the TD error. $s, a, r, s^{\\prime}$是可能的状态转移\nNote that the parameters from the previous iteration $\\theta_{i-1}$ are fixed and not updated. In practice we use a snapshot of the network parameters from a few iterations ago instead of the last iteration. This copy is called the target network.\nhttps://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n初始化一个网络，用于计算Q值，假设当前状态St，\n把St输入到Q，计算该状态下，各个动作的Q值 $Q(s)$ 选择能得到最大Q值的动作A, 需要更新当前状态St下的动作A的Q值：$Q(S,A)$, 执行A，输入到环境，往前一步，到达St+1; 把St+1输入Q网络，计算St+1下所有动作的Q值； 获得最大的Q值，用gamma 折损，加上奖励R作为更新目标； 计算损失 Q(S,A) 相当于有监督学习中的logits gamma * maxQ(St+1) + R 相当于有监督学习中的lables 用mse函数，得出两者的loss 用loss更新Q网络，缩小Q(S,A) 和目标。 不断循环以上步骤\nExperience Replay Prioritized Experience Replay\n经验池的技巧，就是如何存储样本及采样问题。由于玩Atari采集的样本是一个时间序列，样本之间具有连续性，如果每次得到样本就更新Q值，受样本分布影响，效果会不好。因此，一个很直接的想法就是把样本先存起来，然后随机采样如何？这就是Experience Replay的意思。按照脑科学的观点，人的大脑也具有这样的机制，就是在回忆中学习。\n反复试验，然后存储数据。数据存到一定程度，就每次随机采用数据，进行梯度下降！在DQN中增强学习Q-Learning算法和深度学习的SGD训练是同步进行的，通过Q-Learning获取无限量的训练样本，然后对神经网络进行训练。\nAction Selection Policies once we have the Q-values, how do decide which one to use?\nRecall that in simple Q-learning we just choose the action with the highest Q-value. With deep Q-learning we pass the Q-values through a softmax function. The reason that we don\u0026rsquo;t just use the highest Q-value comes down to an important concept in reinforcement learning: the exploration vs. exploitation dilemma.\nthere are others that could be used, and a few of the most common include:\nϵ greedy: selects the greedy action with probability 1- ϵ, and a random action with probability ϵ to ensure good coverage of the state-action space. ϵ soft Softmax References Deep Reinforcement Learning: Guide to Deep Q-Learning https://www.tensorflow.org/agents/tutorials/0_intro_rl https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf ","permalink":"https://congchan.github.io/posts/deep-q-networks/","summary":"\u003cp\u003eCombining reinforcement learning and deep neural networks at scale. The algorithm was developed by enhancing a classic RL algorithm called Q-Learning with deep neural networks and a technique called \u003cstrong\u003eexperience replay\u003c/strong\u003e.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"q-learning\"\u003eQ-Learning\u003c/h2\u003e\n\u003cp\u003eQ-Learning is based on the notion of a Q-function. The Q-function (a.k.a the state-action value function) of a policy $\\pi$，$Q^{\\pi}(s, a)$ ，measures the expected return or discounted sum of rewards obtained from state $s$ by taking action $a$ first and following policy $\\pi$ thereafter.\u003c/p\u003e","title":"Deep Q Networks"},{"content":"Adam Weight Decay in BERT 在看BERT(Devlin et al., 2019)的源码中优化器部分的实现时，发现有这么一段话\n# Just adding the square of the weights to the loss function is *not* # the correct way of using L2 regularization/weight decay with Adam, # since that will interact with the m and v parameters in strange ways. # # Instead we want ot decay the weights in a manner that doesn\u0026#39;t interact # with the m/v parameters. This is equivalent to adding the square # of the weights to the loss with plain (non-momentum) SGD. 其针对性地指出一些传统的Adam weight decay实现是错误的.\n优化器回顾 先回顾一下几个优化器.\nSGD和动量更新 SGD在所有参数上均采用全局且均等的学习率。\n# Vanilla update x += - learning_rate * dx 加入动量更新Momentum update一般都能得到更好的收敛速。动量更新可以从优化问题的物理角度出发来理解。损失函数可以解释为丘陵地形的高度（因此也可以解释为势能，U = mgh , 势能正比于高度）。\n随机数初始化参数等效于在某个位置将初始速度设置为零。优化过程就等同于模拟参数矢量（即粒子）在损失函数的丘陵地形上滚动的过程。\n由于作用在粒子上的力与势能的梯度有关（即$F = - \\nabla U$），因此粒子所感受到的力正好是损失函数的（负）梯度。此外$F = ma$，因此（负）梯度在这个视角下和中与粒子的加速度成比例。因此梯度直接影响的是速度，由速度来影响位置.\n# Momentum update v = mu * v - learning_rate * dx # integrate velocity x += v # integrate position 动量mu（一般取0.9）虽然叫动量，但其物理意义更像是摩擦系数. 它会衰减速度并降低系统的动能，避免粒子一直在山底震荡无法停止. 也就是在梯度方向有所改变的维度上的衰减速度. 同时可以在梯度方向不变的维度上维持速度，这样就可以加快收敛并减小震荡。\nAdaGrad, RMSprop和Adam 我们希望优化器算法可以对每个参数自适应地调整学习率. AdaGrad(Duchi et al.)独立地适应模型的每个参数:\n# Assume the gradient dx and parameter vector x cache += dx**2 x += - learning_rate * dx / (np.sqrt(cache) + eps) 变量cache跟踪每个参数的梯度平方和。然后，将其用于element-wise地正则化参数更新。接收高梯度的权重将降低其有效学习率，而接收较小或不经常更新的权重将提高其有效学习率。 每个参数的学习率会缩放各参数反比于其历史梯度平方值总和的平方根.\nRMSprop(Tieleman \u0026amp; Hinton, 2012)优化器也是一种自适应学习率方法, 不过没发表, 都是引用 slide 29 of Lecture 6 of Geoff Hinton’s Coursera class.\nRMSProp对Adagrad进行如下调整:\ncache = decay_rate * cache + (1 - decay_rate) * dx**2 x += - learning_rate * dx / (np.sqrt(cache) + eps) 使用了梯度平方的移动平均值, 避免激进的单调递减的学习率。 decay_rate一般取[0.9, 0.99, 0.999].\nAdam (Kingma \u0026amp; Ba, 2014)可以看做动量法和RMSprop的结合, 结合了AdaGrad处理稀疏梯度的能力和RMSProp处理不平稳目标函数的能力。简化的实现:\nm = beta1*m + (1-beta1)*dx v = beta2*v + (1-beta2)*(dx**2) x += - learning_rate * m / (np.sqrt(v) + eps) 看起来与RMSProp更新完全相同，只是使用了渐变m的“平滑”版本而不是原始（且可能是嘈杂的）梯度dx。文章建议值为eps = 1e-8, beta1 = 0.9, beta2 = 0.999\n在MNIST数据上做的简单对比实验: 引用cs231的图: Adam Weight Decay 和 L2正则化 以前在训练语言模型时, 发现精调的SGD比Adam得到的最终效果更好. 可见Adam的优势并不如原来文章所言. 在2017年的论文《Fixing Weight Decay Regularization in Adam》(后来更新第三版为Decoupled Weight Decay Regularization, Loshchilov 2017)[#refer]中提出了Adam Weight Decay的方法用于修复Adam的权重衰减错误。问题在于目前大多数DL框架的L2 regularization实现用的是weight decay的方式，而weight decay在与Adam共同使用的时候有互相耦合。\nL2 regularization: 给参数加上一个L2惩罚 $$ f_{t}^{r e g}(\\mathbb{\\theta})=f_{t}(\\mathbb{\\theta})+\\frac{\\lambda^{\\prime}}{2}\\|\\mathbb{\\theta}\\|_{2}^{2} $$ 用程序表达是:\nfinal_loss = loss + weight_decay_r * all_weights.pow(2).sum() / 2 Hanson \u0026amp; Pratt (1988)的Weight decay让weight $\\theta$以$\\lambda$的速率指数衰减: $$ \\theta_{t+1}=(1-\\lambda) \\theta_{t}-\\alpha \\nabla f_{t}\\left(\\theta_{t}\\right), $$ 在vanilla SGD中用程序表达是:\nw = w - lr * w.grad - lr * weight_decay_r * w 大部分库都使用第一个实现。不过实际上几乎总是通过在梯度上添加 weight_decay_r * w来实现，而不是实际更改损失函数。）\n在标准SGD的情况下，通过对衰减系数做变换，令$\\lambda^{\\prime}=\\frac{\\lambda}{\\alpha}$, L2正则则等价于Weight Decay. 但是其他情况下, 比如增加了momentum后, L2正则化和权重衰减并不等价。\nboth mechanisms push weights closer to zero, at the same rate\nfast ai的代码解释是, 在momentum SGD中使用L2正则就需要把weight_decay_r * w加到梯度中. 但是梯度不是直接在weights中减去, 而是要通过移动平均\nmoving_avg = alpha * moving_avg + (1-alpha) * (w.grad + weight_decay_r*w) 该移动平均值再乘以学习率，然后从weights中减去.\n而权重衰减则是:\nmoving_avg = alpha * moving_avg + (1-alpha) * w.grad w = w - lr * moving_avg - lr * wd * w 很明显二者会不同的.\n在自适应优化器Adam中情况类似, 主要体现在以下二者:\nthe sums of the gradient of the loss function the gradient of the regularizer (i.e., the L2 norm of the weights) 红色是Adam+L2 regularization的方式，梯度$g_t$的移动平均 $m_t$ 与梯度平方的移动平均 $v_t$ 都加入了$\\lambda \\theta_{t- 1}$\n如何解释这种不同? 直接引用文章原文:\nwith decoupled weight decay, only the gradients of the loss function are adapted (with the weight decay step separated from the adaptive gradient mechanism)\nWith L2 regularization both types of gradients are normalized by their typical (summed) magnitudes, and therefore weights x with large typical gradient magnitude s are regularized by a smaller relative amount than other weights.\ndecoupled weight decay regularizes all weights with the same rate λ, effectively regularizing weights x with large s more than standard L2 regularization\nBERT源码中的apply_gradients给出了修正方法:\ndef apply_gradients(self, grads_and_vars, global_step=None, name=None): \u0026#34;\u0026#34;\u0026#34;See base class.\u0026#34;\u0026#34;\u0026#34; assignments = [] for (grad, param) in grads_and_vars: if grad is None or param is None: continue param_name = self._get_variable_name(param.name) m = tf.get_variable( name=param_name + \u0026#34;/adam_m\u0026#34;, shape=param.shape.as_list(), dtype=tf.float32, trainable=False, initializer=tf.zeros_initializer()) v = tf.get_variable( name=param_name + \u0026#34;/adam_v\u0026#34;, shape=param.shape.as_list(), dtype=tf.float32, trainable=False, initializer=tf.zeros_initializer()) # Standard Adam update. next_m = ( tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad)) next_v = ( tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2, tf.square(grad))) update = next_m / (tf.sqrt(next_v) + self.epsilon) # Just adding the square of the weights to the loss function is *not* # the correct way of using L2 regularization/weight decay with Adam, # since that will interact with the m and v parameters in strange ways. # # Instead we want ot decay the weights in a manner that doesn\u0026#39;t interact # with the m/v parameters. This is equivalent to adding the square # of the weights to the loss with plain (non-momentum) SGD. if self._do_use_weight_decay(param_name): update += self.weight_decay_rate * param update_with_lr = self.learning_rate * update next_param = param - update_with_lr assignments.extend( [param.assign(next_param), m.assign(next_m), v.assign(next_v)]) return tf.group(*assignments, name=name) tensorflow v1 加入了修正, 但是后续的tf2就是很混乱找不到了.\nAdamWOptimizer = tf.contrib.opt.extend_with_decoupled_weight_decay(tf.train.AdamOptimizer) optimizer = AdamWOptimizer(weight_decay=weight_decay, learning_rate=deep_learning_rate) 参考资料 Devlin et al., 2019: https://github.com/google-research/BERT Duchi et al.: http://jmlr.org/papers/v12/duchi11a.html Tieleman \u0026amp; Hinton, 2012: csc321 Kingma \u0026amp; Ba, 2014: Adam: A Method for Stochastic Optimization cs231: https://cs231n.github.io/neural-networks-3/#sgd Wilson et al. (2017): Loshchilov 2017: Decoupled Weight Decay Regularization Hanson \u0026amp; Pratt (1988): Comparing biases for minimal network construction with back-propagation fast ai: https://www.fast.ai/2018/07/02/adam-weight-decay/ ","permalink":"https://congchan.github.io/posts/bert%E7%9A%84adam-weight-decay/","summary":"\u003ch1 id=\"adam-weight-decay-in-bert\"\u003eAdam Weight Decay in BERT\u003c/h1\u003e\n\u003cp\u003e在看BERT(\u003ca href=\"/posts/bert%E7%9A%84adam-weight-decay/#refer\"\u003eDevlin et al., 2019\u003c/a\u003e)的源码中优化器部分的实现时，发现有这么一段话\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Just adding the square of the weights to the loss function is *not*\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# the correct way of using L2 regularization/weight decay with Adam,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# since that will interact with the m and v parameters in strange ways.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e#\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Instead we want ot decay the weights in a manner that doesn\u0026#39;t interact\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# with the m/v parameters. This is equivalent to adding the square\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# of the weights to the loss with plain (non-momentum) SGD.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c!-- more --\u003e\n\u003cp\u003e其针对性地指出一些传统的Adam weight decay实现是错误的.\u003c/p\u003e","title":"BERT的Adam Weight Decay"},{"content":"What is Word Lattices?\nA word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)\n语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的k-best条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。\n在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。\n所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题：\n未登录词，易引起切分错误 粒度不可控 节点权重如何设定, 比如每夜总会加班涉及每夜和夜总会 因此我们需要把词典匹配方法和神经网络NER模型结合使用. 需要结合CRF模型的实体(term-标签)和基于领域字典匹配的Term(可以加上pos标签)，求解文本的分词+NER划分的最优解。最优解的评判标准就是概率模型, 如果把一句话当作各个term序列, 那么文本序列标签最优解就是序列的最大联合概率 $$ \\begin{aligned} \\arg \\max \\prod_i P(w_i) \u0026 = \\arg \\max \\log \\prod_i P(w_i)\\\\\\ \u0026 = \\arg \\max \\sum_i \\log P(w_i) \\end{aligned} $$ 这里$w_i$指文本的每个term.\n因为一句话中, NER模型和词典匹配的结果可能粒度不同或者互相交叉, 所以我们需要在所有组合中找出一个联合概率最大的组合.\n利用分词工具的词典匹配功能 词典匹配的方法很多, 比如使用Trie树匹配. 这里为了简便直接使用Jieba分词来模拟词典匹配, 因为其底层实现也是一种词典匹配. 为了检索词典中的词，jieba一开始采取的思路是构建前缀Trie树以缩短查询时间。Jieba用了两个dict，trie dict用于保存trie树，lfreq dict用于存储词 -\u0026gt; 词频. 后来Pull request 187提出把前缀信息也放到lfreq, 解决纯Python中Trie空间效率低下的问题. 引用部分说明如下:\n对于get_DAG()函数来说，用Trie数据结构，特别是在Python环境，内存使用量过大。经实验，可构造一个前缀集合解决问题。\n该集合储存词语及其前缀，如set(['数', '数据', '数据结', '数据结构'])。在句子中按字正向查找词语，在前缀列表中就继续查找，直到不在前缀列表中或超出句子范围。大约比原词库增加40%词条。\n建模 一个句子所有的分词和实体组合构成了有向无环图（Directed Acyclic Graph, DAG）$G=(V,E)$，一个词对应与DAG中的的一条边$e \\in E$，边的起点为词的初始字符，边的结点为词的结束字符。DAG可以用dict表示，key为边的起点，value为边的终点集合, 比如sentence = '微软银行收购tiktok'的DAG就是\n{0: [(0, \u0026#39;微\u0026#39;), (1, \u0026#39;微软\u0026#39;), (3, \u0026#39;微软银行\u0026#39;)], 1: [(1, \u0026#39;软\u0026#39;), (2, \u0026#39;软银\u0026#39;)], 2: [(2, \u0026#39;银\u0026#39;), (3, \u0026#39;银行\u0026#39;)], 3: [(3, \u0026#39;行\u0026#39;)], 4: [(4, \u0026#39;收\u0026#39;), (5, \u0026#39;收购\u0026#39;)], 5: [(5, \u0026#39;购\u0026#39;)], 6: [(6, \u0026#39;t\u0026#39;), (8, \u0026#39;tik\u0026#39;), (11, \u0026#39;tiktok\u0026#39;)], ...} 4 -\u0026gt; 5表示词'收购'。 这里面有的是词, 有的是实体如'微软'等. 我们可以给它们赋予不同的权重分值, 以强化我们的输出偏好. 一般以工程统计的频率作为权重.\n将词频的log值作为图$G$边的权值，将联合概率求解从连乘变为求和, 最大概率问题转换为最大分值路径问题；在上面的DAG中，节点0表示源节点，节点m-1表示尾节点；则$V=\\{0, \\cdots , m-1 \\}$，且DAG顶点的序号的顺序与图流向是一致的： $$v \u003e u, \\quad \\forall \\ (u,v) \\in E$$参考jieba.get_DAG()函数，我们修改一下DAG的格式，使其包含一些我们想要的信息，比如权重等等.\ndef get_DAG(self, sentence): self.check_initialized() DAG = {} N = len(sentence) for k in range(N): tmplist = [] i = k frag = sentence[k] while i \u0026lt; N and frag in self.FREQ: if self.FREQ[frag]: tmplist.append((i, self.get_weight(frag), \u0026#39;SEG\u0026#39;)) i += 1 frag = sentence[k:i + 1] if not tmplist: tmplist.append((k, self.get_weight(frag), \u0026#39;SEG\u0026#39;)) DAG[k] = tmplist return DAG get_weight可以在线的提取每个分词对应的权重\ndef get_weight(self, segment): return log(self.FREQ.get(segment) or 1) - self.logtotal 这里主要是涉及到热更新词和词频的考虑, 所以每一次都从头计算一遍, 但是在分布式中这些都可以通过工程设计规避掉.\n然后把NER的结果也按照类似Ditc{offest: [(end_id, weight, tag), ...]}格式加入到DAG中，就可以统一求解了。\n求解概率图 图的最大路径问题其实是最短路径的对称问题. 在图论中针对DAG的最短路径求解的经典算法是按照图节点的Topological顺序, 更新从每一个节点出发的所有边. 而计算Topological顺序需要DFS遍历所有节点和边.\n不过这里使用比较容易实现的动态规划方法, 直接计算最大分值路径. 如果用$d_i$表示源节点到节点$i$的最大路径的值，则有 $$d_i = \\max_{(j,i) \\in E} \\ \\{ d_j+w(j,i) \\}$$ 其中，$w(j,i)$表示词$c_j^i$的词频log值，若$j = i$就表示字符独立成词的词频log值。\n考虑到DAG是以Start -\u0026gt; [end0, end1, ...]的形式表达, 在定义状态时, 用$r_i$标记节点$i$到尾节点的最大路径的值, 这样可以从句子尾部往前计算, 保证考虑的每一个边不会往前越界. $$r_i = \\max_{(i,j) \\in E} \\ \\{ r_j+w(i,j) \\}$$根据上面设定的DAG的格式Dict{offest: [(end_id, weight, tag), ...]}, 则可以这样计算:\ndef calc(self, sentence, DAG, route={}): N = len(sentence) route[N] = (0, 0) for idx in xrange(N - 1, -1, -1): route[idx] = max((x[1] + route[x[0] + 1][0], x[0], x[2]) for x in DAG[idx]) return route 返回最大路径\ndef get_seg(self, sentence, route): N = len(sentence) x = 0 while x \u0026lt; N: y = route[x][1] + 1 word = sentence[x:y] yield (word, route[x][2]) x = y return segs 参考资料 https://github.com/fxsjy/jieba/ https://www.cnblogs.com/en-heng/p/6234006.html ","permalink":"https://congchan.github.io/posts/word-lattice/","summary":"\u003cp\u003eWhat is \u003ca href=\"http://www.statmt.org/moses/?n=Moses.WordLattices#:~:text=A%20word%20lattice%20is%20a%20directed%20acyclic%20graph,and%20edges%20labeled%20with%20a%20word%20and%20weight.\"\u003eWord Lattices\u003c/a\u003e?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)\u003c/p\u003e\u003c/blockquote\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/lattice.png\"\u003e\u003c/p\u003e\n\u003cp\u003e语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的\u003ccode\u003ek-best\u003c/code\u003e条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。\u003c/p\u003e\n\u003cp\u003e在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。\u003c/p\u003e\n\u003cp\u003e所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题：\u003c/p\u003e","title":"Word Lattice"},{"content":"NLP任务的难点 不像图像的普适性, 语言本身有其多样性, 如语境的偏移, 背景的变化, 人与人间的分歧, 这导致以下问题:\n有标注数据的通用性低 标注数据质量不稳定 现实世界的语言和使用场景不断更新, 导致模型的维护更新换代成本极高 \u0026hellip; 为了应对NLP的难点, 需要充分利用各种可用的监督信号，包括但不限于传统监督学习（supervision），自监督学习（self-supervised），弱监督(weak supervision)，迁移学习（transfer learning），多任务学习（multi-task learning, MTL）。\nNear-term improvements in NLP will be mostly about making clever use of \u0026ldquo;free\u0026rdquo; data.\n语言模型 - 经典的自监督学习模型 Lecun有给自监督学习下定义，但我个人对自监督的理解是，基于数据本身进行学习，让模型学习到数据隐含的特征。\n比如语言模型的根据前文预测下一个单词。\n最近的BERT丰富了玩法，提出了Mask language model，就是通过上下文预测掩码位置的单词，作为其核心学习任务；BERT的训练过程还应用了多任务学习，把 next sentence prediction 也作为任务之一一起学习。\n目前除了语言模型和句模型(next sentence)，是否还有其他任务?\nBaidu ERNIE: 引入了论坛对话类数据，利用 DLM（Dialogue Language Model）建模 Query-Response 对话结构，将对话 Pair 对作为输入，引入 Dialogue Embedding 标识对话的角色，利用 Dialogue Response Loss 学习对话的隐式关系。\nELMo vs GPT vs BERT 经典Word2vec表达是context free的，open a bank account和on the river bank的bank共用一个向量值[0.3, 0.2, -0.8, …]. 如指公司的苹果和指水果的苹果共用一个向量.\n解决方案：在文本语料中训练上下文表达contextual representations\n而 ELMo, GPT, 和 BERT 都着眼于contextual representations\nELMo : Deep Contextual Word Embeddings, 训练独立的left-to-right和right-to-left的LMs, 外加一个Word Embedding层, 作为预训练的词向量使用 OpenAI GPT : Improving Language Understanding by Generative Pre-Training. 使用 left-to-right Transformer LM, 然后在下游任务中fine-tune BERT : Bidirectional Encoder Representations from Transformers 但是, 为何2018年之前类似ELMo的contextual representations并不流行？\n因为好的预训练结果比有监督训练代价高1000倍甚至100,000倍。2013年微调好的二层512维度的LSTM sentiment analysis 有 80% 准确度, 训练时间 8 小时. 同期的相同结构的预训练模型需要训练一周, 准确率稍微好点, 80.5%.\n迁移学习的两种思路: Feature based 和 Fine-tune based ELMO区分不同同词不同意的方法是通过它的三层向量的加权组合(concat or sum whatever), 权重可以在下游任务中学习调整.\n而GPT和BERT是通过在下游任务中fine-tune模型参数, 同时也利用了transformer的self-attention机制解决共指消解 而这三个模型是一个不断进化的过程:\nELMO 独立地训练前后向LSTM, 每一个位置只能直接接收其左右相邻位置的信息, 而且因为实践上LSTM cell 能够记忆的距离很有限(能够记忆的信息也很有限), 这导致ELMO的全局语境理解能力很有限. GPT 是从左到右 BERT 放弃了\u0026quot;预测下一个词\u0026quot;的传统LM任务, 改用Mask-LM任务. BERT模型学习的应该不仅仅是contextual embeddings： 预测缺失的单词（或下一个单词）需要学习许多类型的语义理解features: 语法，语义，语用，共指等. 这说明预训练模型其实远远大于它所需要解决的任何特定下游任务\n迁移学习 迁移学习的主流思路是知识共享, 让模型在一种较为通用的数据上预训练, 然后把预训练的模型迁移到下游的具体任务中.\n迁移学习在图像领域大获成功（ImageNet + resnet），解决了分类这一图像领域的瓶颈。\n近年来涌现出ULMFit, ELMO，GPT, BERT这些优秀的预训练模型，但没有CV领域那么耀眼。主要原因是NLP目前没有单个明确的瓶颈，\nNLP需要多种多样的推理：逻辑，语言语义，情感，和视觉。 NLP要求长期短期结合的记忆 比较综合的语言理解任务是GLUE。\nBERT，GPT-2 等算法指明了一条可行的 NLP 在实际工业应用的可行路径：\n预训练：利用超大规模无监督数据预训练神经网络模型 (可选) 知识注入, 加入知识图谱的结构化信息, 如基于BERT的ERNIE 知识迁移，二个思路： 微调 Fine-tune 单任务/多任务学习 预训练 预训练阶段的核心是什么？\nresnet, BERT 和 GPT-2 告诉我们: 更大的数据规模，更多样性的数据，更高的数据质量。这三点的尺度上限都接近无穷大，所以天花板很高，未来模型的性能还有提升空间。 针对数据量大和多样性，我们有两种解决思路, 预训练阶段需要自监督或者无监督的任务，显而易见的任务是语言模型, ELMo, GPT, 和 BERT 都用到了这个任务. 使用弱监督(远程监督) 知识注入 百度的ERNIE的做法是: 基于百度的词库, 把BERT中对token level 的 mask 改进为 对 word level 的 mask.\n对于每一个序列, 我们需要进行 word level 的标记, 来区分各个 token 是否属于同一个词. 对序列进行掩码时, 不再是随机选择 token, 而是选择词 多任务学习 与单独训练模型相比，多任务学习在使用shared representation的同时并行地学习任务。通过shared representation在任务之间传递知识，可以提高特定任务模型的学习效率和预测准确性。\n有两种MT思路：\nHard parameter sharing：不同任务共享底层的神经网络层，但各个任务有自己特定任务的output layer。同时学习的任务越多，模型底层就越能找到捕捉所有任务的表达，而对单个任务过度拟合的可能性就越小。 Soft parameter sharing：每个任务有自己的模型自己的参数，然后对各个模型的参数之间的距离进行正则化，以鼓励参数趋近。 Hard parameter sharing的训练，目前至少有两种方式。\n交替地优化每个任务特定的task_loss[k]. 这种方法不需要各个任务的训练数据有任何对齐关联 联合优化total_loss=Σ(task_loss[k])。 这个方法要求各个任务的batch训练数据相同或者有key来对齐 除此之外, 第二种方法还方便我们为每个任务添加自适应权重(adaptive weight)，以获得更多task-sensitive learning。 弱（远程）监督 Snorkel MeTaL In Snorkel, the heuristics are called Labeling Functions (LFs). Here are some common types of LFs:\nHard-coded heuristics: usually regular expressions (regexes) Syntactics: for instance, Spacy’s dependency trees Distant supervision: external knowledge bases Noisy manual labels: crowdsourcing External models: other models with useful signals Reference Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale Data Programming: Creating Large Training Sets, Quickly Improving Language Understanding by Generative Pre-Training https://github.com/kweonwooj/papers/issues/114 Massive Multi-Task Learning with Snorkel MeTaL: Bringing More Supervision to Bear An Overview of Multi-Task Learning in Deep Neural Networks Training Complex Models with Multi-Task Weak Supervision https://nlp.stanford.edu/seminar/details/jdevlin.pdf Get 10x Speedup in Tensorflow Multi-Task Learning using Python Multiprocessing ","permalink":"https://congchan.github.io/posts/%E5%88%A9%E7%94%A8bert%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/","summary":"\u003ch3 id=\"nlp任务的难点\"\u003eNLP任务的难点\u003c/h3\u003e\n\u003cp\u003e不像图像的普适性, 语言本身有其多样性, 如语境的偏移, 背景的变化, 人与人间的分歧, 这导致以下问题:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e有标注数据的通用性低\u003c/li\u003e\n\u003cli\u003e标注数据质量不稳定\u003c/li\u003e\n\u003cli\u003e现实世界的语言和使用场景不断更新, 导致模型的维护更新换代成本极高\u003c/li\u003e\n\u003cli\u003e\u0026hellip;\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e为了应对NLP的难点, 需要充分利用各种可用的监督信号，包括但不限于传统监督学习（supervision），自监督学习（self-supervised），弱监督(weak supervision)，迁移学习（transfer learning），多任务学习（multi-task learning, MTL）。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNear-term improvements in NLP will be mostly about making clever use of \u0026ldquo;free\u0026rdquo; data.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"语言模型---经典的自监督学习模型\"\u003e语言模型 - 经典的自监督学习模型\u003c/h3\u003e\n\u003cp\u003eLecun有给自监督学习下定义，但我个人对自监督的理解是，基于数据本身进行学习，让模型学习到数据隐含的特征。\u003c/p\u003e\n\u003cp\u003e比如语言模型的根据前文预测下一个单词。\u003c/p\u003e\n\u003cp\u003e最近的BERT丰富了玩法，提出了Mask language model，就是通过上下文预测掩码位置的单词，作为其核心学习任务；BERT的训练过程还应用了多任务学习，把 next sentence prediction 也作为任务之一一起学习。\u003c/p\u003e\n\u003cp\u003e目前除了语言模型和句模型(\u003ccode\u003enext sentence\u003c/code\u003e)，是否还有其他任务?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBaidu ERNIE: 引入了论坛对话类数据，利用 DLM（Dialogue Language Model）建模 Query-Response 对话结构，将对话 Pair 对作为输入，引入 Dialogue Embedding 标识对话的角色，利用 Dialogue Response Loss 学习对话的隐式关系。\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"elmo-vs-gpt-vs-bert\"\u003eELMo vs GPT vs BERT\u003c/h3\u003e\n\u003cp\u003e经典Word2vec表达是context free的，\u003ccode\u003eopen a bank account\u003c/code\u003e和\u003ccode\u003eon the river bank\u003c/code\u003e的\u003ccode\u003ebank\u003c/code\u003e共用一个向量值\u003ccode\u003e[0.3, 0.2, -0.8, …]\u003c/code\u003e. 如指公司的\u003ccode\u003e苹果\u003c/code\u003e和指水果的\u003ccode\u003e苹果\u003c/code\u003e共用一个向量.\u003c/p\u003e","title":"利用bert进行迁移学习"},{"content":"注意力机制的原理是计算query和每个key之间的相关性$\\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。\n注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章Attention is all you need提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。\n近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。\n因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。\n所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用self-attention 自注意力机制(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时Query, Key和Value都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。\nTransformer总体架构 与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:\nTransformer使用注意力来编码(不需要LSTM/CNN之类的)。 引入自注意力机制 Multi-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。 Transformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。\nTransformer的编码阶段概括起来就是：\n首先为每个单词生成初始表达或embeddings。这些由空心圆表示。 然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。 Encoder的self-attention中, 所有Key, Value和Query都来自同一位置, 即上一层encoder的输出。\n解码器类似，所有Key, Value和Query都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前query位置之前的部分。生成Query时, 不仅关注前一步的输出，还参考编码器的最后一层输出。\nN = 6, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。\n编码器:\nStage 1 - 输入编码: 序列的顺序信息是非常重要的。由于没有循环，也没有卷积，因此使用“位置编码”表示序列中每个标记的绝对（或相对）位置的信息。 positional encodings $\\oplus$ embedded input Stage 2 – Multi-head self-attention 和 Stage 3 – position-wise FFN. 两个阶段都是用来残差连接, 接着正则化输出层 Stage1_out = Embedding512 + TokenPositionEncoding512 Stage2_out = layer_normalization(multihead_attention(Stage1_out) + Stage1_out) Stage3_out = layer_normalization(FFN(Stage2_out) + Stage2_out) out_enc = Stage3_out 解码器的架构类似，但它在第3阶段采用了附加层, 在输出层上的 mask multi-head attention:\nStage 1 – 输入解码: 输入 output embedding，偏移一个位置以确保对位置i的预测仅取决于\u0026lt; i的位置。 def shift_right_3d(x, pad_value=None): \u0026#34;\u0026#34;\u0026#34;Shift the second dimension of x right by one.\u0026#34;\u0026#34;\u0026#34; if pad_value is None: shifted_targets = tf.pad(x, [[0, 0], [1, 0], [0, 0]])[:, :-1, :] else: shifted_targets = tf.concat([pad_value, x], axis=1)[:, :-1, :] return shifted_targets Stage 2 - Masked Multi-head self-attention: 需要有一个mask来防止当前位置i的生成任务看到后续\u0026gt; i位置的信息。 # 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html def subsequent_mask(size): \u0026#34;Mask out subsequent positions.\u0026#34; attn_shape = (1, size, size) subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\u0026#39;uint8\u0026#39;) return torch.from_numpy(subsequent_mask) == 0 # # The attention mask shows the position each tgt word (row) is allowed to look at (column). # Words are blocked for attending to future words during training. plt.figure(figsize=(5,5)) plt.imshow(subsequent_mask(20)[0]) 阶段2,3和4同样使用了残差连接，然后在输出使用归一化层。\nStage1_out = OutputEmbedding512 + TokenPositionEncoding512 Stage2_Mask = masked_multihead_attention(Stage1_out) Stage2_Norm1 = layer_normalization(Stage2_Mask) + Stage1_out Stage2_Multi = multihead_attention(Stage2_Norm1 + out_enc) + Stage2_Norm1 Stage2_Norm2 = layer_normalization(Stage2_Multi) + Stage2_Multi Stage3_FNN = FNN(Stage2_Norm2) Stage3_Norm = layer_normalization(Stage3_FNN) + Stage2_Norm2 out_dec = Stage3_Norm 可以利用开源的Tensor2Tensor，通过调用几个命令来训练Transformer网络进行翻译和解析。\n通过Self Attention对比Attention有什么增益呢？可以看到，自注意力算法可以捕获同一个句子中单词之间的语义特征, 比如共指消解（coreference resolution），例如句子中的单词“it”可以根据上下文引用句子的不同名词。除此之外, 理论上也可以捕捉一些语法特征. ![](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/CoreferenceResolution.png \u0026ldquo;Co-reference resolution. 两边的\u0026quot;it\u0026quot;指向不同的词. Adopted from Google Blog.\u0026rdquo;)\n其实在LSTM_encoder-LSTM_decoder架构上的Attention也可以做到相同的操作, 但效果却不太好. 问题可能在于此时的Attention处理的不是纯粹的一个个序列编码, 而是经过LSTM(复杂的门控记忆与遗忘)编码后的包含前面时间步输入信息的一个个序列编码, 这个导致Attention的软寻址难度增大. 而现在是2019年, 几乎主流的文本编码方案都转投Transformer了, 可见单纯利用self-attention编码其实效率更高.\nAttention Vaswani, 2017明确定义了使用的注意力算法\n$$\\begin{eqnarray} Attention (Q,K,V) = softmax \\Big( \\frac{QK^T}{\\sqrt{d_k}} \\Big) V \\end{eqnarray},$$其中$\\mathbb{Q}\\in\\mathbb{R}^{n\\times d_k}, \\mathbb{K}\\in\\mathbb{R}^{m\\times d_k}, \\mathbb{V}\\in\\mathbb{R}^{m\\times d_v}$. 这就是传统的Scaled Dot-Product Attention, 把这个Attention理解为一个神经网络层，将$n\\times d_k$的序列$Q$编码成了一个新的$n\\times d_v$的序列。因为对于较大的$d_k$，内积会数量级地放大, 太大的话softmax可能会被推到梯度消失区域, softmax后就非0即1(那就是hardmax), 所以$q \\cdot k = \\sum_{i=1}^{d_k}q_i k_i$按照比例因子$\\sqrt{d_k}$缩放.\nBERT/ALBERT中的点积attention实现:\n# dot_product_attention from bert implementation def dot_product_attention(q, k, v, bias, dropout_rate=0.0): \u0026#34;\u0026#34;\u0026#34;Dot-product attention. Args: q: Tensor with shape [..., length_q, depth_k]. k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must match with q. v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must match with q. bias: bias Tensor (see attention_bias()) dropout_rate: a float. Returns: Tensor with shape [..., length_q, depth_v]. \u0026#34;\u0026#34;\u0026#34; logits = tf.matmul(q, k, transpose_b=True) # [..., length_q, length_kv] logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1]))) if bias is not None: # `attention_mask` = [B, T] from_shape = get_shape_list(q) if len(from_shape) == 4: broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32) elif len(from_shape) == 5: # from_shape = [B, N, Block_num, block_size, depth]# broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], from_shape[3], 1], tf.float32) bias = tf.matmul(broadcast_ones, tf.cast(bias, tf.float32), transpose_b=True) # Since attention_mask is 1.0 for positions we want to attend and 0.0 for # masked positions, this operation will create a tensor which is 0.0 for # positions we want to attend and -10000.0 for masked positions. adder = (1.0 - bias) * -10000.0 # Since we are adding it to the raw scores before the softmax, this is # effectively the same as removing these entirely. logits += adder else: adder = 0.0 attention_probs = tf.nn.softmax(logits, name=\u0026#34;attention_probs\u0026#34;) attention_probs = dropout(attention_probs, dropout_rate) return tf.matmul(attention_probs, v) # 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html def attention(query, key, value, mask=None, dropout=0.0): \u0026#34;Compute \u0026#39;Scaled Dot Product Attention\u0026#39;\u0026#34; d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) \\ / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = F.softmax(scores, dim = -1) # (Dropout described below) p_attn = F.dropout(p_attn, p=dropout) return torch.matmul(p_attn, value), p_attn 这只是注意力的一种形式，还有其他比如query跟key的运算方式是拼接后再内积一个参数向量，权重也不一定要归一化，等等。\nSelf-Attention (SA) 在实际的应用中, 不同的场景的$Q,K,V$是不一样的, 如果是SQuAD的话，$Q$是文章的向量序列，$K=V$为问题的向量序列，输出就是Aligned Question Embedding。\nGoogle所说的自注意力(SA), 就是$Attention(\\mathbb{X},\\mathbb{X},\\mathbb{X})$, 通过在序列自身做Attention，寻找序列自身内部的联系。Google论文的主要贡献之一是它表明了SA在序列编码部分是相当重要的，甚至可以替代传统的RNN(LSTM), CNN, 而之前关于Seq2Seq的研究基本都是关注如何把注意力机制用在解码部分。\n编码时，自注意力层处理来自相同位置的输入$queries, keys, value$，即编码器前一层的输出。编码器中的每个位置都可以关注前一层的所有位置.\n在解码器中，SA层使每个位置能够关注解码器中当前及之前的所有位置。为了保持 auto-regressive 属性，需要阻止解码器中的向左信息流, 所以要在scaled dot-product attention层中屏蔽（设置为-∞）softmax输入中与非法连接相对应的所有值.\n作者使用SA层而不是CNN或RNN层的动机是:\n最小化每层的总计算复杂度: SA层通过$O(1)$数量的序列操作连接所有位置. ($O(n)$ in RNN) 最大化可并行化计算：对于序列长度$n$ \u0026lt; representation dimensionality $d$（对于SOTA序列表达模型，如word-piece, byte-pair）。对于非常长的序列$n \u003e d$, SA可以仅考虑以相应输出位置为中心的输入序列中的某个大小$r$的邻域，从而将最大路径长度增加到$O(n/r)$ 最小化由不同类型层组成的网络中任意两个输入和输出位置之间的最大路径长度。任何输入和输出序列中的位置组合之间的路径越短，越容易学习长距离依赖。 Multi-head Attention Transformer的SA将关联输入和输出序列中的（特别是远程）位置的计算量减少到$O(1)$。然而，这是以降低有效分辨率为代价的，因为注意力加权位置被平均了。为了弥补这种损失, 文章提出了 Multi-head Attention:\n$h=8$ attention layers (“heads”): 将key $K$ 和 query $Q$ 线性投影到 $d_k$ 维度, 将value $V$ 投影到$d_v$维度, (线性投影的目的是减少维度) $$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\\dots,h$$ 投影是参数矩阵$W^Q_i, W^K_i\\in\\mathbb{R}^{d_{model}\\times d_k}, W^V_i\\in\\mathbb{R}^{d_{model}\\times d_v}$ $d_k=d_v=d_{model}/h = 64$ 每层并行地应用 scaled-dot attention(用不同的线性变换), 得到$d_v$维度的输出 把每一层的输出拼接在一起 $Concat(head_1,\\dots,head_h)$ 再线性变换上一步的拼接向量$MultiHeadAttention(Q,K,V) = Concat(head_1,\\dots,head_h) W^O$, where $W^0\\in\\mathbb{R}^{d_{hd_v}\\times d_{model}}$ 因为Transformer只是把原来$d_{model}$维度的注意力函数计算并行分割为$h$个独立的$d_{model}/h$维度的head, 所以计算量相差不大.\nBERT/ALBERT中的multi-head attention层实现:\ndef attention_layer(from_tensor, to_tensor, attention_mask=None, num_attention_heads=1, query_act=None, key_act=None, value_act=None, attention_probs_dropout_prob=0.0, initializer_range=0.02, batch_size=None, from_seq_length=None, to_seq_length=None): \u0026#34;\u0026#34;\u0026#34;Performs multi-headed attention from `from_tensor` to `to_tensor`. Args: from_tensor: float Tensor of shape [batch_size, from_seq_length, from_width]. to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width]. attention_mask: (optional) int32 Tensor of shape [batch_size, from_seq_length, to_seq_length]. The values should be 1 or 0. The attention scores will effectively be set to -infinity for any positions in the mask that are 0, and will be unchanged for positions that are 1. num_attention_heads: int. Number of attention heads. query_act: (optional) Activation function for the query transform. key_act: (optional) Activation function for the key transform. value_act: (optional) Activation function for the value transform. attention_probs_dropout_prob: (optional) float. Dropout probability of the attention probabilities. initializer_range: float. Range of the weight initializer. batch_size: (Optional) int. If the input is 2D, this might be the batch size of the 3D version of the `from_tensor` and `to_tensor`. from_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `from_tensor`. to_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `to_tensor`. Returns: float Tensor of shape [batch_size, from_seq_length, num_attention_heads, size_per_head]. Raises: ValueError: Any of the arguments or tensor shapes are invalid. \u0026#34;\u0026#34;\u0026#34; from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) to_shape = get_shape_list(to_tensor, expected_rank=[2, 3]) size_per_head = int(from_shape[2]/num_attention_heads) if len(from_shape) != len(to_shape): raise ValueError( \u0026#34;The rank of `from_tensor` must match the rank of `to_tensor`.\u0026#34;) if len(from_shape) == 3: batch_size = from_shape[0] from_seq_length = from_shape[1] to_seq_length = to_shape[1] elif len(from_shape) == 2: if (batch_size is None or from_seq_length is None or to_seq_length is None): raise ValueError( \u0026#34;When passing in rank 2 tensors to attention_layer, the values \u0026#34; \u0026#34;for `batch_size`, `from_seq_length`, and `to_seq_length` \u0026#34; \u0026#34;must all be specified.\u0026#34;) # Scalar dimensions referenced here: # B = batch size (number of sequences) # F = `from_tensor` sequence length # T = `to_tensor` sequence length # N = `num_attention_heads` # H = `size_per_head` # `query_layer` = [B, F, N, H] q = dense_layer_3d(from_tensor, num_attention_heads, size_per_head, create_initializer(initializer_range), query_act, \u0026#34;query\u0026#34;) # `key_layer` = [B, T, N, H] k = dense_layer_3d(to_tensor, num_attention_heads, size_per_head, create_initializer(initializer_range), key_act, \u0026#34;key\u0026#34;) # `value_layer` = [B, T, N, H] v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head, create_initializer(initializer_range), value_act, \u0026#34;value\u0026#34;) q = tf.transpose(q, [0, 2, 1, 3]) k = tf.transpose(k, [0, 2, 1, 3]) v = tf.transpose(v, [0, 2, 1, 3]) if attention_mask is not None: attention_mask = tf.reshape( attention_mask, [batch_size, 1, to_seq_length, 1]) # \u0026#39;new_embeddings = [B, N, F, H]\u0026#39; new_embeddings = dot_product_attention(q, k, v, attention_mask, attention_probs_dropout_prob) return tf.transpose(new_embeddings, [0, 2, 1, 3]) 可以看到k和v都是to_tensor.\n# 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \u0026#34;Take in model size and number of heads.\u0026#34; super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.p = dropout self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None def forward(self, query, key, value, mask=None): if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model =\u0026gt; h x d_k query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention(query, key, value, mask=mask, dropout=self.p) # 3) \u0026#34;Concat\u0026#34; using a view and apply a final linear. x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) return self.linears[-1](x) NMT中Transformer以三种不同的方式使用Multi-head Attention：\n在encoder-decoder attention层中，queries来自前一层decoder层，并且 memory keys and values 来自encoder的输出。这让decoder的每个位置都可以注意到输入序列的所有位置。这其实还原了典型的seq2seq模型里常用的编码器 - 解码器注意力机制（例如Bahdanau et al., 2014或Conv2S2）。 编码器本身也包含了self-attention layers。在self-attention layers中，所有 keys, values and queries 来自相同的位置，在这里是编码器中前一层的输出。这样，编码器的每个位置都可以注意到前一层的所有位置。 with tf.variable_scope(\u0026#34;attention_1\u0026#34;): with tf.variable_scope(\u0026#34;self\u0026#34;): attention_output = attention_layer( from_tensor=layer_input, to_tensor=layer_input, attention_mask=attention_mask, num_attention_heads=num_attention_heads, attention_probs_dropout_prob=attention_probs_dropout_prob, initializer_range=initializer_range) 类似地，解码器中的 self-attention layers 允许解码器的每个位置注意到解码器中包括该位置在内的所有前面的位置（有mask屏蔽了后面的位置）。需要阻止解码器中的向左信息流以保持自回归属性(auto-regressive 可以简单理解为时序序列的特性, 只能从左到右, 从过去到未来)。我们通过在scaled dot-product attention层中屏蔽（设置为-∞）softmax输入中与非法连接相对应的所有值来维持该特性。 Position-wise Feed-Forward Networks 在编码器和解码器中，每个层都包含一个全连接的前馈网络(FFN)，FFN 分别应用于每个位置，使用相同的两个线性变换和一个ReLU $$FFN(x) = max(0, xW_1+b_1) W_2 + b_2$$ 虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。它的工作方式类似于两个内核大小为1的卷积层. 输入/输出维度是$d_{model}=512$, 内层的维度$d_{ff} = 2048$.\n# 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html class PositionwiseFeedForward(nn.Module): \u0026#34;Implements FFN equation.\u0026#34; def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() # Torch linears have a `b` by default. self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) Positional Encoding 在解码时序信息时，LSTM模型通过时间步的概念以输入/输出流一次一个的形式编码的. 而Transformer选择把时序编码为正弦波。这些信号作为额外的信息加入到输入和输出中以表达时序信息.\n这种编码使模型能够感知到当前正在处理的是输入（或输出）序列的哪个部分。位置编码可以学习或者使用固定参数。作者进行了测试（PPL，BLEU），显示两种方式表现相似。文中作者选择使用固定的位置编码参数:\n$$ \\begin{eqnarray} PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \\end{eqnarray} $$ $$ \\begin{eqnarray} PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\\end{eqnarray} $$ 其中$pos$是位置，$i$是维度。\n也就是说，位置编码的每个维度对应于正弦余弦曲线的拼接。波长形成从2π到10000⋅2π的几何级数。选择这个函数，是因为假设它能让模型容易地学习相对位置，因为对于任意固定偏移$k$，$PE_{pos + k}$可以表示为$PE_{pos}$的线性函数。\n# 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html class PositionalEncoding(nn.Module): \u0026#34;Implement the PE function.\u0026#34; def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(\u0026#39;pe\u0026#39;, pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) return self.dropout(x) 位置编码将根据位置添加正弦余弦波。每个维度的波的频率和偏移是不同的。\nplt.figure(figsize=(15, 5)) pe = PositionalEncoding(20, 0) y = pe.forward(Variable(torch.zeros(1, 100, 20))) plt.plot(np.arange(100), y[0, :, 4:8].data.numpy()) plt.legend([\u0026#34;dim %d\u0026#34;%p for p in [4,5,6,7]]) 直观的理解是，将这些值添加到embedding中，一旦它们被投影到$Q / K / V$向量和dot product attention中，就给embedding向量之间提供了有意义的相对距离。\nShared-Weight Embeddings and Softmax 与其他序列转导模型类似，使用可学习的Embeddings将 input tokens and output tokens 转换为维度$d_{model}$的向量。通过线性变换和softmax函数将解码器的输出向量转换为预测的token概率。在Transformer模型中，两个嵌入层和pre-softmax线性变换之间共享相同的权重矩阵，在Embeddings层中，将权重乘以$\\sqrt{d_{\\text{model}}}$. 这些都是当前主流的操作。\n# 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) 启发 作者已经进行了一系列测试（论文表3），其中他们讨论N = 6层的建议，模型大小为512，基于h = 8个heads，键值维度为64，使用100K步。\n还指出，由于模型质量随着$d_k$（行B）的减小而降低，因此可以进一步优化点积兼容性功能。\n其声称提出的固定正弦位置编码，与学习到的位置编码相比，产生几乎相等的分数。\n算法适合哪些类型的问题？ 序列转导（语言翻译） 语法选区解析的经典语言分析任务 syntactic constituency parsing 共指消解 coreference resolution 参考资料 https://research.googleblog.com/2017/08/transformer-novel-neural-network.html https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/ http://nlp.seas.harvard.edu/2018/04/03/attention.html\n","permalink":"https://congchan.github.io/posts/transformer-self-attention-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BC%96%E7%A0%81/","summary":"\u003cp\u003e注意力机制的原理是计算query和每个key之间的相关性$\\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eAttention is all you need\u003c/a\u003e提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。\u003c/p\u003e\n\u003cp\u003e近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。\u003c/p\u003e\n\u003cp\u003e因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。\u003c/p\u003e\n\u003cp\u003e所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用\u003cstrong\u003eself-attention 自注意力机制\u003c/strong\u003e(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时\u003ccode\u003eQuery\u003c/code\u003e, \u003ccode\u003eKey\u003c/code\u003e和\u003ccode\u003eValue\u003c/code\u003e都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。\u003c/p\u003e\n\u003ch3 id=\"transformer总体架构\"\u003eTransformer总体架构\u003c/h3\u003e\n\u003cp\u003e与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTransformer使用注意力来编码(不需要LSTM/CNN之类的)。\u003c/li\u003e\n\u003cli\u003e引入自注意力机制\u003c/li\u003e\n\u003cli\u003eMulti-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTransformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/transform20fps.gif\" title=\"image from: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\"\u003e\u003c/p\u003e\n\u003cp\u003eTransformer的编码阶段概括起来就是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e首先为每个单词生成初始表达或embeddings。这些由空心圆表示。\u003c/li\u003e\n\u003cli\u003e然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eEncoder的self-attention中, 所有\u003ccode\u003eKey\u003c/code\u003e, \u003ccode\u003eValue\u003c/code\u003e和\u003ccode\u003eQuery\u003c/code\u003e都来自同一位置, 即上一层encoder的输出。\u003c/p\u003e\n\u003cp\u003e解码器类似，所有\u003ccode\u003eKey\u003c/code\u003e, \u003ccode\u003eValue\u003c/code\u003e和\u003ccode\u003eQuery\u003c/code\u003e都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前\u003ccode\u003equery\u003c/code\u003e位置之前的部分。生成\u003ccode\u003eQuery\u003c/code\u003e时, 不仅关注前一步的输出，还参考编码器的最后一层输出。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/transformer.png\" title=\"单层编码器（左）和解码器（右），由 N = 6 个相同的层构建。\"\u003e\n\u003ccode\u003eN = 6\u003c/code\u003e, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。\u003c/p\u003e","title":"Transformer \u0026 Self-Attention (多头)自注意力编码"},{"content":"序列标注（Sequence Labeling） 序列标注任务是指根据观察得到的序列（如一个句子）, 推断出序列每个元素（单词）对应的标注。\n具体的任务包括分词(Segmentation), 词性标注（Part-of-Speach tagging, POS）, 实体识别(Named Entity Recognition, NER), 等等. 所谓POS, 就是对于一个句子, 如Bob drank coffee at Starbucks, 标注可能为Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN).\n除此之外, 还有其他涉及到需要根据观察序列推断隐含状态的问题, 这种问题的特点是每一个位置的标签都不是独立的, 而是和上下文相关依存的, 可以用序列标注的思路来处理.\n单个分类器仅能预测单个类变量，但是序列标注基于概率图模型, 图模型(Graphical Models)的真正功能在于它们能够对许多有相互依赖的变量进行建模。最简单的依赖关系可以描述为一种线性链(Linear Chain), 也就是后续介绍到的隐马尔可夫模型(Hidden Markov Model, HMM)用到的假设.\n概率图模型 Graphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。\nG = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立. 从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为P(当前节点 | 父节点).\n而马尔可夫网络则是无向图. 无向图形模型是指一整个家族的概率分布，每个概率分布都根据给定的因子集合进行因式分解。一般用random field来指代无向图中定义的特定分布. 数学上表达无向图, 指给定子集$\\\\{Y_a \\\\}_{a=1}^A$, 对于所有$\\mathbf{y}_a$和任何因子选项$\\mathcal{F}=\\\\{\\Psi_a\\\\}$, $\\Psi_a(\\mathbf{y}_a) \\geq 0$, 无向图定义的各个分布可以写成:\n$$p(\\mathbf{y})=\\frac{1}{Z} \\prod_{a=1}^A \\Psi_{a}\\left(\\mathbf{y}_{a}\\right)$$Z是正则项用于保证分布$p$和为$1$ $$Z=\\sum_{\\mathbf{y}} \\prod_{a=1}^{A} \\Psi_{a}\\left(\\mathbf{y}_{a}\\right)$$Markov Net 包含了一组具有马尔可夫性质的随机变量. **马尔可夫随机场(Markov Random Fields, MRF)**是由参数$λ=(S, π, A)$表示, 其中S是状态的集合，π是初始状态的概率, A是状态间的转移概率。一阶马尔可夫链就是假设t时刻的状态只依赖于前一时刻的状态，与其他时刻的状态和观测无关。这个性质可以用于简化概率链的计算。使用类似性质作为假设的模型还有Bi-gram语言模型等.\n朴素贝叶斯分类器与隐马尔可夫模型 朴素贝叶斯分类器(NBs)假设条件独立性(朴素贝叶斯假设, Hand and Yu, 2001)：$p(x_i | y, x_j) = p(x_i | y)$, 在给定目标值 y 时，x的属性值之间相互条件独立。这样, 计算可以简化为 $$p(y | \\overrightarrow{x}) \\propto p(y, \\overrightarrow{x}) = p(y) \\prod_{i=1} p(x_i | y).$$朴素贝叶斯模型只考虑了单个输出变量y。如果要为一个观察序列$\\overrightarrow{x} =(x_1, ..., x_n)$预测对应的分类序列$\\overrightarrow{y} =（y_1, ..., y_n)$ ，一个简单的序列模型可以表示为多个NBs的乘积。此时不考虑序列单个位置之间的相互依赖。\n$$p(\\overrightarrow{y}, \\overrightarrow{x}) = \\prod^n_{i=1} p(y_i) p(x_i | y_i).$$ 此时每个观察值$x_i$仅取决于对应序列位置的类变量$y_i$。由于这种独立性假设，从一个步骤到另一个步骤的转换概率不包括在该模型中。然而这种假设在实践中几乎不会符合，这导致这种模型的性能很有限。\n因此，比较合理的假设是观测序列在连续相邻位置间的状态存在依赖。要模拟这种依赖关系, 就要引入状态转移概率$p(y_i | y_{i-1})$, 由此引出著名的隐马尔可夫模型 Hidden Markov model, HMM, Rabiner (1989).\nHMM参数$λ = (Y, X, π, A, B)$ ，其中Y是隐状态（输出变量）的集合，X是观察值（输入）集合，π是初始状态的概率，A是状态转移概率矩阵$p(y_i | y_{i-1})$，B是输出观察值概率矩阵$p(x_i | y_{i})$。在POS任务中, X就是观察到的句子, Y就是待推导的标注序列, 因为词性待求的, 所以人们称之为隐含状态.\n概括一下HMM设定的假设:\nMarkov assumption：假设每个状态仅依赖于其前一个状态, $p(y_t|y_{t−1})$ Stationarity assumption：状态的转换概率与转换发生的实际时间（实际输入）无关 Output independence assumption: 假设每一个观察值x仅依赖于当前状态值y, $p(x_t|y_t)$, 而与前面的观察值无关。 那么状态序列y和观察序列x的联合概率可以分解为 $$p(\\mathbf{y}, \\mathbf{x})=\\prod_{t=1}^{\\mathrm{T}} p\\left(y_{t} | y_{t-1}\\right) p\\left(x_{t} | y_{t}\\right)$$总的来说, 隐马尔可夫模型（HMM）是具有随机状态转移和观测值的有限状态自动机（Rabiner，1989）。自动机对概率生成过程进行建模: 先从某个初始状态开始，发射(emit)该状态生成的观察值，再转移到下一个状态，再发射另一个观察值，以此类推，直到达到指定的最终状态，从而产生一系列观察值。\nHMM作为生成式的概率模型, 对观察特征的条件独立约束非常严格. 而且为了定义观察值序列和序列标记的联合概率，生成模型需要枚举所有可能的观察序列. 对于表示多个相互作用的特征或观测值的长距离相关性, 这种枚举是不切实际的，因为此类模型的inference很棘手。但很多任务往往受益于这种相互作用、相互交叉重叠的特征，比如除了传统的单词自身外，还有大小写，单词结尾，词性，格式，在页面上的位置以及WordNet中的节点成员身份等等。\n除此之外, 大部分文本任务是根据给定的观察序列（如纯文本）来预测对应的状态序列，也就是判别问题。换句话说，HMM不恰当地用了生成联合概率的模型去判别问题。\n隐马尔可夫模型与最大熵马尔可夫模型 最大熵马尔可夫模型(Maximum Entropy Markov Models, MEMM)跟HMM的生成式概率图不同，MEMM对当前状态的判断依赖于前一时间步的状态和当前观察值的状态。\n首先所谓\u0026quot;熵\u0026quot;就是信息论中的概念:\nEntropy: the uncertainty of a distribution.\n量化Entropy: surprise. Entropy = expected surprise\nEvent $x$, Probability $p_x$, \u0026ldquo;Surprise\u0026rdquo; $log(1/p_x)$, Entropy: $$ \\begin{aligned} \u0026H(p)=E_{p}\\left[\\log \\frac{1}{p_{x}}\\right]\\\\ \u0026\\mathrm{H}(p)=-\\sum_{x} p_{x} \\log p_{x} \\end{aligned} $$熵最大的分布就是均匀分布，也就是每一个选项都一样，等于什么信息都没给。如果给了额外的信息，如约束，特征之后，熵就可以降低。\n“最大熵”是指遵循最大熵原则：\nmodel all that is known and assume nothing about that which is unknown.\n也就说, 如果给定了一组事实，我们最好选择一个符合这些事实的模型，剩余情况则尽可能地一视同仁不做任何区别对待。最佳的模型是符合训练数据衍生的约束条件的模型，同时尽可能少地做假设，也就是少做承诺，也就避免过拟合。\nMEMM 把HMM中的转移概率和发射概率替换为一个概率：当前状态$s$依赖于前一个状态$s^{\\prime}$和当前观察值$o$, $\\mathrm{P}\\left(s | s^{\\prime}, o\\right)$\nMEMM的训练思路是这样: 对每个状态$s^{\\prime}$, 将训练数据分为\u0026lt;观察-目标状态\u0026gt;对 $$, 也就是把 $\\mathrm{P}\\left(s | s^{\\prime}, o\\right)$ 分成 $|S|$ 个分别训练的exponential model $\\mathrm{P}_{s^{\\prime}}(s | o)=\\mathrm{P}\\left(s | s^{\\prime}, o\\right)$, 再通过最大化熵来训练exponential models, 换种说法叫logistic regression classifier.\n用的约束条件是学习分布中每个特征$a$的期望值与训练数据的观测序列上每个特征的期望值相同. 满足这些约束的最大熵分布（Della Pietra，Della Pietra和Lafferty，1997）是唯一的，与最大似然分布一致，对每一个位置的状态$s^{\\prime}$, 具有指数形式： $$ P_{s^{\\prime}}(s | o)=\\frac{1}{Z\\left(o, s^{\\prime}\\right)} \\exp \\left(\\sum_{a} \\lambda_{a} f_{a}(o, s)\\right) $$其中$\\lambda_{a}$是待估计的参数, $Z\\left(o, s^{\\prime}\\right)$是归一化因子 $$ Z\\left(o, s^{\\prime}\\right)=\\sum_{s \\in S} P\\left(s | s^{\\prime}, o\\right) $$ $S$是标签集.\n如果把问题简化为线性的相邻依赖, 那么每一个状态$s_{i}$仅依赖于前一个状态$s_{i-1}$. 用$Y$表达标签序列, 用$X$表达观察序列, 那么 $$P\\left(y_{1}, y_{2}, \\ldots, y_{n} | \\mathbb{x}\\right)=P\\left(y_{1} | \\mathbb{x}\\right) P\\left(y_{2} | \\mathbb{x}, y_{1}\\right) P\\left(y_{3} | \\mathbb{x}, y_{2}\\right) \\ldots P\\left(y_{n} | \\mathbb{x}, y_{n-1}\\right)$$ 其中 $$P\\left(y_{1} | \\mathbb{x}\\right)=\\frac{e^{f\\left(y_{1} ; \\mathbb{x}\\right)}}{\\sum_{y_{1} \\in S} e^{f\\left(y_{k} ; \\mathbb{x}\\right)}}, \\quad P\\left(y_{k} | \\mathbb{x}, y_{k-1}\\right)=\\frac{e^{g\\left(y_{k-1}, y_{k}\\right)+f\\left(y_{k} ; \\mathbb{x}\\right)}}{\\sum_{y_{k} \\in S} e^{g\\left(y_{k-1}, y_{k}\\right)+f\\left(y_{k} ; \\mathbb{x}\\right)}}$$ 则 $$P(\\mathbb{y} | \\mathbb{x})=\\frac{e^{f\\left(y_{1} ; \\mathbb{x}\\right)+g\\left(y_{1}, y_{2}\\right)+f\\left(y_{2} ; \\mathbb{x}\\right)+\\cdots+g\\left(y_{n-1}, y_{n}\\right)+f\\left(y_{n} ; \\mathbb{x}\\right)}}{\\left(\\sum_{y_{1} \\in S} e^{f\\left(y_{1} ; \\mathbb{x}\\right)}\\right)\\left(\\sum_{y_{2} \\in S} e^{g\\left(y_{1}, y_{2}\\right)+f\\left(y_{2} ; \\mathbb{x}\\right)}\\right) \\cdots\\left(\\sum_{y_{n} \\in S} e^{g\\left(y_{n-1}, y_{n}\\right)+f\\left(y_{n} ; \\mathbb{x}\\right)}\\right)}$$ MEMM将整体的概率分布分解为每一个时间步的分布之积，所以算loss只需要把每一步的交叉熵求和。只需要每一步单独执行softmax，所以MEMM是完全可以并行的，速度跟直接逐步Softmax基本一样。\n虽然MEMM能克服HMM的很多弱点, 但是MEMM自身也有一个 label bias 问题, 就是标签偏差, 离开给定状态的转移仅相互对比，而不是与全局所有其他转移对比。转移分数是分别对每个状态的归一化, 这意味到达一个状态的所有质量必须在可能的后续状态之间分配。观察值可以影响哪个目标状态获得质量，但无法影响多少质量可以被传递。这会导致模型偏向输出选择较少的状态, 比如极端情况下, 在训练集中某个状态$s_a$只发现了有一种可能的转移$s_b$, 那么状态$s_a$别无选择，只能将所有质量传递给它的唯一的 transition output $s_b$。\n随机场 随机场, 可以看成是一组随机变量的集合（这组随机变量对应同一个样本空间）。当给每一个位置按照某种分布随机赋予一个值之后，其全体就叫做随机场。这些随机变量之间可能有依赖关系，一般来说，也只有当这些变量之间有依赖关系的时候，我们将其单独拿出来看成一个随机场才有实际意义。\n如果给定的MRF中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个MRF的分布，也就是条件分布，那么这个MRF就称为 Conditional random fields (CRF)。它的条件分布形式完全类似于MRF的分布形式，只不过多了一个观察集合X。所以, CRF本质上是给定了条件(观察值observations)集合的MRF.\n1.特征函数的选择: 特征函数的选取直接关系模型的性能。 2.参数估计: 从已经标注好的训练数据集学习条件随机场模型的参数，即各特征函数的权重向量λ。 3.模型推断: 在给定条件随机场模型参数λ下，预测出最可能的状态序列。\nMEMM和CRF 在CRF的序列标注问题中，我们要计算的是条件概率 $$ P\\left(y_{1}, \\ldots, y_{n} \\mid \\mathbb{x}\\right), \\quad \\mathbb{x}=\\left(x_{1}, \\ldots, x_{n}\\right) $$CRF和MEMM的关键区别在于，MEMM使用每个状态的指数模型来确定给定当前状态的下一状态的条件概率，而CRF则使用一个指数模型来表示整个标签序列的联合概率, 这个概率条件依赖于给定的完整观察序列。二者区别仅在于分母（也就是归一化因子）的计算方式不同，CRF的是全局归一化的，而MEMM的是局部归一化的. 也就是说CRF是一个以观测序列$X$为全局条件的随机场. 存在函数$f(y_1,\\dots,y_n;\\mathbb{x})$，使得 $$ P(y_1,\\dots,y_n|\\mathbb{x})=\\frac{1}{Z(\\mathbb{x})}\\exp\\Big(f(y_1,\\dots,y_n;\\mathbb{x})\\Big) $$可以得到对应得概率是 $$P(\\mathbb{y} | \\mathbb{x})=\\frac{e^{f\\left(y_{1}, y_{2}, \\ldots, y_{n} ; \\mathbb{x}\\right)}}{\\sum_{y_{1}, y_{2}, \\ldots, y_{n} \\in S^n} e^{f\\left(y_{1}, y_{2}, \\ldots, y_{n} ; \\mathbb{x}\\right)}}$$ CRF的计算困难之处在于上式的分母项包含了所有可能路径$S^n$的求和，搜索空间非常庞大.\n因此做出一些简化，假设输出之间的关联仅发生在相邻位置，并且关联是指数加性的:\n$$\\begin{aligned} f\\left(y_{1}, y_{2}, \\ldots, y_{n} ; \\mathbb{x}\\right) \u0026=f\\left(y_{1} ; \\mathbb{x}\\right)+g\\left(y_{1}, y_{2};\\mathbb{x}\\right)+\\cdots+g\\left(y_{n-1}, y_{n};\\mathbb{x}\\right)+f\\left(y_{n} ; \\mathbb{x}\\right) \\\\\\\\ \u0026=f\\left(y_{1} ; \\mathbb{x}\\right)+\\sum_{k=2}^{n}\\left(g\\left(y_{k-1}, y_{k};\\mathbb{x}\\right)+f\\left(y_{k} ; \\mathbb{x}\\right)\\right) \\end{aligned}\\tag{1}$$只需要对每一个标签和每一个相邻标签对分别打分，然后将所有打分结果求和得到总分。\nLinear Chain CRF 尽管CRF已经做了一些简化假设，但一般来说，(1)式所表示的概率模型还是过于复杂，难以求解。于是考虑到当前深度学习模型中，RNN或者层叠CNN等模型已经能够比较充分捕捉各个$y$与输入$x$的联系，因此，我们不妨考虑函数$g$跟$x$无关，那么 $$\\begin{aligned} f\\left(y_{1}, y_{2}, \\ldots, y_{n} ; \\mathbb{x}\\right) \u0026=h\\left(y_{1} ; \\mathbb{x}\\right)+g\\left(y_{1}, y_{2}\\right)+\\cdots+g\\left(y_{n-1}, y_{n}\\right)+h\\left(y_{n} ; \\mathbb{x}\\right) \\\\\\\\ \u0026=h\\left(y_{1} ; \\mathbb{x}\\right)+\\sum_{k=2}^{n}\\left(g\\left(y_{k-1}, y_{k}\\right)+h\\left(y_{k} ; \\mathbb{x}\\right)\\right) \\end{aligned}$$ 其中$g\\left(y_{k-1}, y_{k}\\right)$是一个有限的、待训练的参数矩阵，而单标签的打分函数$h(y_i;\\mathbb{x})$我们可以通过RNN或者CNN来建模。因此，该模型是可以建立的，其中概率分布变为 $$ P(y_1,\\dots,y_n|\\mathbb{x})=\\frac{1}{Z(\\mathbb{x})}\\exp\\left(h(y_1;\\mathbb{x})+\\sum_{k=1}^{n-1}\\Big[g(y_k,y_{k+1})+h(y_{k+1};\\mathbb{x})\\Big]\\right)\\tag{2} $$直接引用(Sutton, C. 2010)的定义: 在CRF中，首先需要定义特征函数.\n然后为每个特征函数$f_{j}$分配权重$\\lambda_j$, 权重是从数据中学习而来. 对$j$个特征方程求和, 对序列每个位置$i$求和:\n$$ score(y | x) = \\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(s, i, y_i, y_{i-1})$$CRF的每个特征函数都是一个输入的函数, 对应的输出是一个实数值（只是0或1）。例如, 选择特征函数$f_1(x, i, y_i, y_{i-1}) = 1$, 当且仅当$y_i = ADVERB$, 且第i个单词以“-ly”结尾; 否则为0. 如果与此特征相关的权重$\\lambda_j$很大且为正，那么这个特征等同于说模型倾向于把以-ly结尾的单词标记为ADVERB。\n通过指数化和归一化把这些得分转换为概率值: $$p(y | x) = \\frac{exp\\left\\(score(y|x)\\right\\)}{\\sum_{y^\\prime} exp\\left\\(score(y^\\prime|x)\\right\\)} = \\frac{exp\\left\\(\\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(x, i, y_i, y_{i-1})\\right\\)}{\\sum_{y^{\\prime}} exp\\left\\(\\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(x, i, y^\\prime_i, l^\\prime_{i-1})\\right\\)} $$Linear-Chain CRF特征函数的定义非常灵活, 不同的形式用于不同的功能. 比如对于HMM而言, 不管输入怎么变, 状态转换$transition(i, j)$的分值是一样的$\\log p (y_t = j | y_{t−1} = i)$; 那么此时在CRF中, 我们通过增加这样一个特征$1_{\\\\{y_{t}=j\\\\}} 1_{\\\\{y_{t-1}=1\\\\}} 1_{\\\\{x_{t}=o\\\\}}$, 使$transition(i, j)$分值依赖于当前的观察序列:\n这种特征常常用于文本处理中, 比如:\n一个句子提供观察值$x_{i-1, i}$ 单词的标签$y_{i-1, i}$ 需要指出的是在线性链CRF的定义中每个feature的依赖值并不仅限于当前和上一时间步的观察值. 事实上, 因为CRF并不表达变量之间的依赖关系, 我们可以让因子$\\Psi_{t}$依赖于整个观察向量$x$并保持线性图结构, 这时候的特征函数就是$f_{k}\\left(y_{t}, y_{t-1}, \\mathbf{x}\\right)$, 可以自由检视所有输入变量$x$, 这个特性可以拓展到所有CRFs而不仅限于线性链CRF.\nCRF既具有判别式模型的优点，又考虑到长距离上下文标记间的转移概率，以序列化形式进行全局参数优化和解码的特点，解决了其他判别式模型(如MEMM)难以避免的标记偏见问题。\n隐马尔可夫模型和Linear-Chain CRF的联系 HMM的生成式概率模型是$p(y,x)$, 它的条件概率$p(y|x)$本质上就是选取了特定特征函数的CRF. HMM和CRF的对应关系类似于Naive-Bayes和Logistics regression, 都是生成式和判别式的对比. HMM则采用生成式方法进行标签生成, CRF将各种特征组合在一起判断标签. HMM可以推演出特定形式的CRF. 把上式的HMM改写成如下形式:\n$$ \\begin{aligned} p(y, x)=\u0026 \\frac{1}{Z} \\prod_{t=1}^T \\exp \\left( \\sum_{i, j \\in S} \\theta_{i j} 1_{\\\\{y_t=i\\\\}} 1_{\\\\{y_\\{t-1}=j\\\\}} \\right. \\\\\\\\ \u0026\\left.+ \\sum_{i \\in S} \\sum_{o \\in O} \\mu_{o i} 1_{\\\\{y_{t}=i\\\\}} 1_{\\\\{x_{t}=o\\\\}} \\right) \\end{aligned} $$其中$\\theta=\\\\{\\theta_{i j}, \\mu_{o i}\\\\}$是分布的实参数, $Z$是常数正则项. $$ \\begin{aligned} \\theta_{i j} \u0026=\\log p\\left(y^{\\prime}=i | y=j\\right) \\\\\\\\ \\mu_{o i} \u0026=\\log p(x=o | y=i) \\\\\\\\ Z \u0026=1 \\end{aligned} $$HMM是生成式的, 借鉴Naive Bayes 到 logistics regression的方式, 通过引入特征函数这个概念, $f_{k}\\left(y_{t}, y_{t-1}, x_{t}\\right)$, 对于每一个$(i, j)$跳转, 加入特征函数$f_{i j}\\left(y, y^{\\prime}, x\\right)=1_{\\\\{y=i\\\\}} 1_{\\\\{y^{\\prime}=j\\\\}}$, 对于每一个状态-观察值对$(i,o)$, 加入特征函数$f_{i o}\\left(y, y^{\\prime}, x\\right)=1_{\\\\{y=i\\\\}} \\mathbf{1}_{\\\\{x=o\\\\}}$. 以上特征函数统一表示为$f_k$, 那么可以进一步把HMM写成:\n$$p(\\mathbf{y}, \\mathbf{x})=\\frac{1}{Z} \\prod_{t=1}^{T} \\exp \\left\\(\\sum_{k=1}^{K} \\theta_{k} f_{k}\\left(y_{t}, y_{t-1}, x_{t}\\right)\\right\\)$$可以得出条件概率$p(y|x)$\n$$p(\\mathbf{y} | \\mathbf{x})=\\frac{p(\\mathbf{y}, \\mathbf{x})}{\\sum_{\\mathbf{y}^{\\prime}} p\\left(\\mathbf{y}^{\\prime}, \\mathbf{x}\\right)}=\\frac{\\prod_{t=1}^{T} \\exp \\left\\(\\sum_{k=1}^{K} \\theta_{k} f_{k}\\left(y_{t}, y_{t-1}, x_{t}\\right)\\right\\)}{\\sum_{\\mathbf{y}^{\\prime}} \\prod_{t=1}^{T} \\exp \\left\\(\\sum_{k=1}^{K} \\theta_{k} f_{k}\\left(y_{t}^{\\prime}, y_{t-1}^{\\prime}, x_{t}\\right)\\right\\)}$$所以当联合概率$p(y,x)$以HMM的形式因式分解, 则关联的条件分布$p(y|x)$就是一种特定形式的linear-chain CRF，即一种仅使用当前单词自身作为特征的CRF. 通过恰当地设置特征函数, 可以从CRF中构建出一个HMM. 在CRF的对数线性形式中, 设置权重为对应HMM(取对数后)的二元转换和发射概率: $\\log p(s,o) = \\log p(s_0) + \\sum_i \\log p(s_i | s_{i-1}) + \\sum_i \\log p(o_i | s_i)$\n对于每个状态pair$\\left(y_{i-1}, y_i\\right)$, 对应HMM的每个状态转换概率$p(s_i = y_i | s_{i-1} = y_{i-1})$, CRF定义一组特征函数为$f_{y_{i-1},y_i}(o, i, s_i, s_{i-1}) = 1$ 如果 $s_i = y_i$ 且 $s_{i-1} = y_{i-1}$, 为这些特征赋予权重$g_{y_{i-1},y_i} = \\log p(s_i = y_i | s_{i-1} = y_{i-1})$ 对于每个状态-观察值pair, 对应HMM的每个发射概率$p(o_i = x | s_{i} = y_i)$, CRF定义一组特征函数为$f_{x,y}(o, i, s_i, s_{i-1}) = 1$ 如果 $o_i = x$ 且 $s_i = y_i$, 赋予权重$w_{x,y} = \\log p(o_i = x | s_i = y)$. 如此, CRF计算的似然$p(y|x)$就精确地正比于对应的HMM, 也就是说, 任意的HMM都可以由CRF表达出来.\nCRF比HMM更强大, 更泛用\nCRF可以定义更广泛的特征函数：HMM受限于相邻位置的状态转换（二元转换）和发射概率函数，迫使每个单词仅依赖于当前标签，并且每个标签仅依赖于前一个标签。而CRF可以使用更多样的全局特征。例如，如果句子的结尾包含问号，则可以给给CRF模型增加一个特征函数，记录此时将句子的第一个单词标记为VERB的概率。这使得CRF可以使用长距离依赖的特征。 CRF可以有任意的权重值：HMM的概率值必须满足特定的约束， $0 \u003c= p(o_i | s_i) \u003c= 1, \\sum_o p(o_i = x | y_1) = 1)$, 而CRF的权重值是不受限制的。 CRF与Logistic Regression CRF的概率计算与Logistic Regression (LR)的形式类似， $$CRF: p(l | s) = \\frac{exp \\left\\(\\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(s, i, l_i, l_{i-1})\\right\\)}{\\sum_{l’} exp\\left\\(\\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(s, i, l^\\prime_i, l^\\prime_{i-1})\\right\\)}$$$$LR: P(y|x) = \\frac{\\exp \\bigg( \\sum\\limits_{i=1}^{N} w_{i} \\cdot f_{i}(x,y) \\bigg)} {\\sum\\limits_{y' \\in Y} \\exp \\bigg( \\sum\\limits_{i=1}^{N} w_{i} \\cdot f_{i}(x,y') \\bigg)}$$ 在LR中, $f_i(y, x)$是一个特征，$w_i$是与该特征相关的权重。提取的特征是二元特征，取值0或1，通常称为指示函数。这些特征中的每一个都由与输入$x$和分类$y$相关联的函数计算。\n实际上，CRF基本上就是逻辑回归的序列化：与逻辑回归是用于分类的对数线性模型不同，CRF是标签序列的对数线性模型。\nCRF模型训练 如何通过数据训练CRF模型, 估计特征函数的权重? 利用极大似然估计（Maximum Likelihood Estimation，MLE)和梯度优化(gradient descent).\n$\\log p(l | s)$相对于参数$λ_i$的梯度为:\n$$\\frac{\\partial}{\\partial w_j} \\log p(l | s) = \\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \\sum_{l’} p(l’ | s) \\sum_{j = 1}^m f_i(s, j, l^\\prime_j, l^\\prime_{j-1})$$导数的第一项是真实标签下的特征$f_i$的贡献，第二项是当前模型下特征$f_i$的期望贡献。\n对于一堆训练样例（句子和相关的词性标签）。随机初始化CRF模型的权重。要将这些随机初始化的权重转移到正确的权重，对于每个训练示例:\n遍历每个特征函数$f_i$，计算训练示例相对于$λ_i$的对数概率的梯度 以learning rate $\\alpha$的速率沿梯度方向不断修正$λ_i$: $$\\lambda_i = \\lambda_i + \\alpha [\\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \\sum_{l’} p(l’ | s) \\sum_{j = 1}^m f_i(s, j, l^\\prime_j, l^\\prime_{j-1})]$$ 重复这些训练步骤，直到满足停止条件（例如，更新低于某个阈值）。 CRF的缺点是模型训练时收敛速度比较慢.\n训练后的CRF模型, 可以用于预测一个未标记序列的最大可能标记. 我们需要每个标记的概率$p(l | s)$, 对于大小为k的标签集和长度为m的句子, 需要比较的$p(l | s)$组合有$k^m$种. 但是计算时, 可以利用动态规划的方法, 原理类似于Viterbi算法.\nTensorflow实现的CRF就是线性链CRF $$\\begin{aligned}\u0026P_Q(a_1,a_2,\\dots,a_n)\\\\ =\u0026\\frac{1}{Z} \\exp \\Big[f(a_1;Q)+g(a_1, a_2;Q) + f(a_2;Q) +\\dots + g(a_{n-1}, a_n;Q) + f(a_n;Q)\\Big] \\end{aligned}$$ 所谓线性链，就是直接认为函数$g$实际上跟$Q$没关系，即对于任何的输入文本，$g(a_{k-1},a_k)$是个常数矩阵。剩下的则跟逐标签softmax的情形差不多了，认为$f(a_k;Q)\\equiv f(a_k;q_k)$. 相对于逐标签softmax，CRF只是换了一个loss，多了一个转移矩阵，并且解码的时候需要用到viterbi算法。按照极大似然的思想，loss应该取为： $$\\begin{aligned} \u0026-\\log P_Q(a_1,a_2,\\dots,a_n)\\\\ =\u0026 - \\sum_{k=1}^n f(a_k;q_k) - \\sum_{k=2}^n g(a_{k-1},a_k) + \\log Z \\end{aligned}$$如果前面模型用BERT或者BiLSTM来得到特征$q_k$，那么就得到了序列标注任务中的Encoder-CRF了。\nCRF中文命名实体识别 比如中文命名实体识别任务, 假如需要判断人名、地名、组织名三类命名实体.\n对于人名, 通过一些模板来筛选特征。模板是对上下文的特定位置和特定信息的考虑, 适用于人名的特征模板:\n人名的指界词：主要包括称谓词、动词和副词等，句首位置和标点符号也可。根据指界词与人名共现的概率的大小，将人名的左右指界词各分为两级，生成4个人名指界词列表： 人名识别特征的原子模板，每个模板都只考虑了一种因素： 当特征函数取特定值时，特征模板被实例化, 就可以得到具体的特征。比如当前词的前一个词 $w_{i-1}$ 在人名1级左指界词列表中出现, $f_i(x, y) = 1, if: PBW1(w_{i-1}) = true, y = PERSON$\n类似的，做地名、组织名的特征提取和选择，并将其实例化，得到所有的特征函数。\n评测指标: 召回 recall = $ \\frac{正确识别的命名实体首部（尾部）的个数}{标准结果中命名实体首部（尾部）的的总数} \\times 100\\%$\n精确率 precision = $ \\frac{正确识别的命名实体首部（尾部）的个数}{识别出的命名实体首部（尾部）的总数} \\times 100\\%$\nF1 = $ \\frac{2 \\times precision \\times recall}{precision + recall}$\n谈谈生成式模型和判别式模型 从朴素贝叶斯, 到HMM; 从Logistic Regression到CRF, 这些概率图模型有如下转换关系: 而在朴素贝叶斯与Logistic Regression, 以及HMM和CRF之间, 又有生成式和判别式的区别.\n生成式模型描述标签向量y如何有概率地生成特征向量x, 即尝试构建x和y的联合分布$p(y, x)$, 典型的模型有HMM，贝叶斯模型，MRF。生成式模型 而判别模型直接描述如何根据特征向量x判断其标签y, 即尝试构建$p(y | x)$的条件概率分布, 典型模型如如LR, SVM，CRF，MEMM等. 不构建$p(x)$是因为分类时用不到. 原则上，任何类型的模型都可以使用贝叶斯规则转换为另一种类型，但实际上这些方法是不同的. 生成模型和判别模型都描述了$p(y, x)$的概率分布，但努力的方向不同。生成模型，例如朴素贝叶斯分类器和HMM，是一类可以因式分解为$p(y, x) = p(y)p(x|y)$的联合分布, 也就是说，它们描述了如何为给定标签的特征采样或“生成”值。生成式模型从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，不关心判别边界。生成式模型的优点是: • 实际上带的信息要比判别模型丰富， 研究单类问题比判别模型灵活性强 • 能更充分的利用先验知识 • 模型可以通过增量学习得到 缺点也很明显: • 学习过程比较复杂; • 在目标分类问题中准确度不高\n而判别式模型, 比如 LR, 是一系列条件分布$p(y | x)$. 也就是说，分类规则是直接建模的。原则上，判别模型也可通过为输入提供边际分布$p(x)$来获得联合分布$p(y, x)$，但很少需要这样。条件分布$p(y | x)$不包括$p(x)$的信息，在分类任务中其实无论如何也用不到。其次，对$p(x)$建模的困难之处在于它通常包含很多建模难度较高的有高度依赖性的特征。判别式模型寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。优点是: • 分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。 • 能清晰的分辨出多类或某一类与其他类之间的差异特征 • 在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好 •适用于较多类别的识别 缺点是：• 不能反映训练数据本身的特性。• 能力有限，可以分类, 但无法把整个场景描述出来。\n参考资料 An Introduction to Conditional Random Fields, Sutton, C., \u0026amp; McCallum, A. (2011) http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/ Classical probabilistic models and conditional random fields https://kexue.fm/archives/5542 McCallum, A. (1909). Maximum Entropy Markov Models for Information Extraction and Segmentation. Berichte Der Deutschen Chemischen Gesellschaft, 42(1), 310–317. https://doi.org/10.1002/cber.19090420146 ","permalink":"https://congchan.github.io/posts/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","summary":"\u003ch2 id=\"序列标注sequence-labeling\"\u003e序列标注（Sequence Labeling）\u003c/h2\u003e\n\u003cp\u003e序列标注任务是指根据观察得到的序列（如一个句子）, 推断出序列每个元素（单词）对应的标注。\u003c/p\u003e\n\u003cp\u003e具体的任务包括分词(Segmentation), 词性标注（Part-of-Speach tagging, POS）, 实体识别(Named Entity Recognition, NER), 等等. 所谓POS, 就是对于一个句子, 如\u003ccode\u003eBob drank coffee at Starbucks\u003c/code\u003e, 标注可能为\u003ccode\u003eBob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN)\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e除此之外, 还有其他涉及到需要根据观察序列推断隐含状态的问题, 这种问题的特点是每一个位置的标签都不是独立的, 而是和上下文相关依存的, 可以用序列标注的思路来处理.\u003c/p\u003e\n\u003cp\u003e单个分类器仅能预测单个类变量，但是序列标注基于概率图模型, 图模型(Graphical Models)的真正功能在于它们能够对许多有相互依赖的变量进行建模。最简单的依赖关系可以描述为一种线性链(Linear Chain), 也就是后续介绍到的隐马尔可夫模型(Hidden Markov Model, HMM)用到的假设.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"概率图模型\"\u003e概率图模型\u003c/h2\u003e\n\u003cp\u003eGraphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。\u003c/p\u003e\n\u003cp\u003eG = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立.\n\u003cimg loading=\"lazy\" src=\"/images/probabilistic_graphical_models.png\" title=\"image from: Probabilistic Graphical Models Principles and Techniques\"\u003e 从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为\u003ccode\u003eP(当前节点 | 父节点)\u003c/code\u003e.\u003c/p\u003e","title":"概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归"},{"content":"The Line Patterns Recognition A basic but important application of pattern recognition is to recognize line patterns in a given set of points. http://coursera.cs.princeton.edu/algs4/assignments/collinear.html. This blog will give a breif introduction to this problem and provide an enfficient solution. Codes available in algs4/collinear/src/\nThe problem could be described as: Given a set of n distinct points in the plane, find every (maximal) line segment that connects a subset of 4 or more of the points..\nPoint data type. an immutable data type Point that represents a point in the plane by implementing the following API:\npublic class Point implements Comparable\u0026lt;Point\u0026gt; { public Point(int x, int y) // constructs the point (x, y) public void draw() // draws this point public void drawTo(Point that) // draws the line segment from this point to that point public String toString() // string representation public int compareTo(Point that) // compare two points by y-coordinates, breaking ties by x-coordinates public double slopeTo(Point that) // the slope between this point and that point public Comparator\u0026lt;Point\u0026gt; slopeOrder() // compare two points by slopes they make with this point } Line segment data type. To represent line segments in the plane, use the data type LineSegment.java, which has the following API:\npublic class LineSegment { public LineSegment(Point p, Point q) // constructs the line segment between points p and q public void draw() // draws this line segment public String toString() // string representation } Apparently if using brute force, the order of growth of the running time of the program will be $n^4$ in the worst case.\nA faster, sorting-based solution: Given a point p, the following method determines whether p participates in a set of 4 or more collinear points.\nThink of p as the origin. For each other point q, determine the slope it makes with p. Sort the points according to the slopes they makes with p. Check if any 3 (or more) adjacent points in the sorted order have equal slopes with respect to p. If so, these points, together with p, are collinear. Solution There are two key points to get the order of growth of the running time to be $n^2\\log n$ in the worst case, with space proportional to n plus the number of line segments returned, and work properly even if the input has 5 or more collinear points.\nStable sort: Arrays.sort() is guaranteed to be stable, so equal elements will not be reordered as a result of the sort. So the input points array is already sorted by points natural order once we sort the element at the first valid check step. To avoid duplicate line segments, we need to check if new found collinear points pairs already exist in the LineSegment. If we loop over the LineSegment everytime we have a new line segments to check, this will results in large run time that will not satisfy the requirement. Instead, we need to make use of the inner features of the line patterns: Since the every possible segment is created by points it contains, and we iterate through the sorted Points array to find segment so every non-duplicate new segment is guaranteed to be created from its smallest point member any duplicate segment is created later by its other member other than the smallest ","permalink":"https://congchan.github.io/posts/find-all-collinear-points-a-pattern-recognition-problem/","summary":"\u003ch2 id=\"the-line-patterns-recognition\"\u003eThe Line Patterns Recognition\u003c/h2\u003e\n\u003cp\u003eA basic but important application of pattern recognition is to recognize line patterns in a given set of points. \u003ca href=\"http://coursera.cs.princeton.edu/algs4/assignments/collinear.html\"\u003ehttp://coursera.cs.princeton.edu/algs4/assignments/collinear.html\u003c/a\u003e. This blog will give a breif introduction to this problem and provide an enfficient solution. Codes available in \u003ca href=\"https://github.com/congchan/algs4\"\u003ealgs4/collinear/src/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe problem could be described as: Given a set of n distinct points in the plane, find every (maximal) line segment that connects a subset of 4 or more of the points.\u003cimg loading=\"lazy\" src=\"https://coursera.cs.princeton.edu/algs4/assignments/collinear/lines2.png\" title=\"image from: http://coursera.cs.princeton.edu/algs4/\"\u003e.\u003c/p\u003e","title":"Find All Collinear Points - A Pattern Recognition Problem"},{"content":"This blog explains an apllication of randomized queue algorithms.\nPermutation client memory challenge A client program Permutation.java that takes an integer k as a command-line argument; reads in a sequence of strings from standard input using StdIn.readString(); and prints exactly k of them, uniformly at random. Print each item from the sequence at most once.\nMore detail could be found at programming assignment specification and checklist, codes available in algs4/queues/src/.\nRandomized queue For a randomized queue, the item removed is chosen uniformly at random from items in the data structure.\nEach iterator must return the items in uniformly random order. The order of two or more iterators to the same randomized queue must be mutually independent; each iterator must maintain its own random order.\nAPI:\npublic class RandomizedQueue\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { public RandomizedQueue() {} // construct an empty randomized queue public boolean isEmpty() {} // is the randomized queue empty? public int size() {} // return the number of items on the randomized queue public void enqueue(Item item) {} // add the item public Item dequeue() {} // remove and return a random item public Item sample() {} // return a random item (but do not remove it) public Iterator\u0026lt;Item\u0026gt; iterator() {} // return an independent iterator over items in random order public static void main(String[] args) {} // unit testing (optional) } Solution The bonu point is to use only one RandomizedQueue object of maximum size at most k.\nMore specifically, as the program sees a sequence of input, one at a time, the programe could only keep k items in memory, and they should be selected at random from the sequence. If we know the total number of items (n), then the solution is easy: select ten distinct indices i between 1 and n with equal probability, and keep the i-th elements. The challenge is that we do not know the input sequence length in advance.\nThe idea is when reading in input strings one by one, we maintain the RandomizedQueue with size at most k on the fly. If the RandomizedQueue is full, then we need to decide whether a new input string should be accepted or not. If it should be accepted, then one of the old elements must be kicked out of the queues. The key point here is how to make the decision.\nThe algorithms explain the mechanism: For a loop over n, swap item [n] with a random item in the range [0] through [n]. We store only the first k elements ([0 : k-1]) as that are all we need. Afterwards, when we get a new string (index [n]), we\u0026rsquo;ll swap it with one of the first k strings for a given probability P, otherwise just discard it.\nThe Reservoir sampling algorithms could solve the problem:\nReservoir sampling is a family of randomized algorithms for randomly choosing a sample of k items from a list S containing n items, where n is either a very large or unknown number. Typically n is large enough that the list doesn\u0026rsquo;t fit into main memory.\nKeep the first k items in memory. When the i-th item arrives (for $i\u003ek$): with probability $k/i$, keep the new item (discard an old one, selecting which to replace at random, each with chance $1/k$) with probability $1-k/i$, keep the old items (ignore the new one) code available: https://github.com/congchan/algs4/tree/master/queues/src\nIn our case of implementation, for a loop over n, swap item [n] with a random item in the range [0] through [n]. We store only the first k elements ([0 : k-1]) as that are all we need. Afterwards, when we get a new string (index [n]), we\u0026rsquo;ll swap it with one of the first k strings for a given probability P, otherwise just discard it.\nThere is another shuffle method called Fisher–Yates shuffle and its $O(n)$ version called Knuth shuffle which could shuffle a given sequence.\nTest report:\nCorrectness: 43/43 tests passed Memory: 106/105 tests passed Timing: 136/136 tests passed Aggregate score: 100.10% [Compilation: 5%, API: 5%, Findbugs: 0%, PMD: 0%, Checkstyle: 0%, Correctness: 60%, Memory: 10%, Timing: 20%] Test 3 (bonus): check that maximum size of any or Deque or RandomizedQueue object created is equal to k * filename = tale.txt, n = 138653, k = 5 * filename = tale.txt, n = 138653, k = 50 * filename = tale.txt, n = 138653, k = 500 * filename = tale.txt, n = 138653, k = 5000 * filename = tale.txt, n = 138653, k = 50000 ==\u0026gt; passed Total: 3/2 tests passed! ","permalink":"https://congchan.github.io/posts/randomized-queue-with-reservoir-sampling/","summary":"\u003cp\u003eThis blog explains an apllication of randomized queue algorithms.\u003c/p\u003e\n\u003ch2 id=\"permutation-client-memory-challenge\"\u003ePermutation client memory challenge\u003c/h2\u003e\n\u003cp\u003eA client program \u003ccode\u003ePermutation.java\u003c/code\u003e that takes an integer k as a command-line argument; reads in a sequence of strings from standard input using \u003ccode\u003eStdIn.readString()\u003c/code\u003e; and prints exactly k of them, uniformly at random. Print each item from the sequence at most once.\u003c/p\u003e\n\u003cp\u003eMore detail could be found at programming assignment \u003ca href=\"http://coursera.cs.princeton.edu/algs4/assignments/queues.html\"\u003especification\u003c/a\u003e and \u003ca href=\"http://coursera.cs.princeton.edu/algs4/checklists/queues.html\"\u003echecklist\u003c/a\u003e, codes available in \u003ca href=\"https://github.com/congchan/algs4/tree/master/queues/src\"\u003ealgs4/queues/src/\u003c/a\u003e.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"randomized-queue\"\u003eRandomized queue\u003c/h3\u003e\n\u003cp\u003eFor a randomized queue, the item removed is chosen \u003cstrong\u003euniformly\u003c/strong\u003e at random from items in the data structure.\u003c/p\u003e","title":"Randomized Queue with Reservoir Sampling"},{"content":"本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.\n注意力机制应用于阅读理解 The Standford question and answer dataset (SQuAD) 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。\n由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。\n但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。\n虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用全局注意力机制中提到的 $$ \\begin{aligned} score_{general}(t' t) \u0026= s^\\top_{t'} W_\\alpha h_t, \\\\\\ \\end{aligned} $$ 就是用一个交互矩阵$W_\\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 Bilinear.\nclass Attention(object): def forwards_bilinear(self, hc, hq, hc_mask, hq_mask, max_context_length_placeholder, max_question_length_placeholder, is_train, keep_prob): \u0026#39;\u0026#39;\u0026#39;combine context hidden state(hc) and question hidden state(hq) with global attention bilinear score = hc.T *W *hq \u0026#39;\u0026#39;\u0026#39; d_en = hc.get_shape().as_list()[-1] # (BS, MPL, MQL) interaction_weights = tf.get_variable(\u0026#34;W_interaction\u0026#34;, shape=[d_en, d_en]) hc_W = tf.reshape(tf.reshape(hc, shape=[-1, d_en]) @ interaction_weights, shape=[-1, max_context_length_placeholder, d_en]) # (BS, MPL, HS * 2) @ (BS, HS * 2, MCL) -\u0026gt; (BS ,MCL, MQL) score = hc_W @ tf.transpose(hq, [0, 2, 1]) # Create mask (BS, MPL) -\u0026gt; (BS, MPL, 1) -\u0026gt; (BS, MPL, MQL) hc_mask_aug = tf.tile(tf.expand_dims(hc_mask, -1), [1, 1, max_question_length_placeholder]) hq_mask_aug = tf.tile(tf.expand_dims(hq_mask, -2), [1, max_context_length_placeholder, 1]) hq_mask_aug = hc_mask_aug \u0026amp; hq_mask_aug score = softmax_mask_prepro(score, hq_mask_aug) # (BS, MPL, MQL) alignment_weights = tf.nn.softmax(score) # (BS, MPL, MQL) @ (BS, MQL, HS * 2) -\u0026gt; (BS, MPL, HS * 2) context_aware = tf.matmul(alignment_weights, hq) concat_hidden = tf.concat([context_aware, hc], axis=2) concat_hidden = tf.cond(is_train, lambda: tf.nn.dropout(concat_hidden, keep_prob), lambda: concat_hidden) # (HS * 4, HS * 2) Ws = tf.get_variable(\u0026#34;Ws\u0026#34;, shape=[d_en * 2, d_en]) attention = tf.nn.tanh(tf.reshape(tf.reshape(concat_hidden, [-1, d_en * 2]) @ Ws, [-1, max_context_length_placeholder, d_en])) return (attention) def _similarity_matrix(self, hq, hc, max_question_length, max_context_length, question_mask, context_mask, is_train, keep_prob): def _flatten(tensor, keep): fixed_shape = tensor.get_shape().as_list() start = len(fixed_shape) - keep # Calculate (BS * MCL * MQL) left = reduce(mul, [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start)]) # out_shape is simply HS * 2 out_shape = [left] + [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start, len(fixed_shape))] # (BS * MCL * MQL, HS * 2) flat = tf.reshape(tensor, out_shape) return (flat) def _reconstruct(tensor, ref, keep): ref_shape = ref.get_shape().as_list() tensor_shape = tensor.get_shape().as_list() ref_stop = len(ref_shape) - keep tensor_start = len(tensor_shape) - keep # [BS, MCL, MQL] pre_shape = [ref_shape[i] or tf.shape(ref)[i] for i in range(ref_stop)] # [1] keep_shape = [tensor_shape[i] or tf.shape(tensor)[i] for i in range(tensor_start, len(tensor_shape))] # pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))] # keep_shape = tensor.get_shape().as_list()[-keep:] # [BS, MCL, MQL, 1] target_shape = pre_shape + keep_shape out = tf.reshape(tensor, target_shape) out = tf.squeeze(out, [len(args[0].get_shape().as_list()) - 1]) return (out) # (BS, MCL, MQL, HS * 2) d = hq.get_shape().as_list()[-1] logging.debug(\u0026#34;d is: {}\u0026#34;.format(d)) hc_aug = tf.tile(tf.reshape(hc, shape=[-1, max_context_length, 1, d]), [1, 1, max_question_length, 1]) # (BS, MCL, MQL, HS * 2) hq_aug = tf.tile(tf.reshape(hq, shape=[-1, 1, max_question_length, d]), [1, max_context_length, 1, 1]) # [(BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2)] args = [hc_aug, hq_aug, hc_aug * hq_aug] # [(BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2)] args_flat = [_flatten(arg, 1) for arg in args] args_flat = [tf.cond(is_train, lambda: tf.nn.dropout(arg, keep_prob), lambda: arg) for arg in args_flat] d_concat = d * 3 W = tf.get_variable(\u0026#34;W\u0026#34;, shape=[d_concat, 1]) b = tf.get_variable(\u0026#34;b\u0026#34;, shape=[1]) # Calculating a(h, u) = w_s^(t)[h; u; h * u] # (BS * MCL * MQL, HS * 6) @ (HS * 6, 1) + (1) -\u0026gt; (BS * MCL * MQL, 1) res = tf.concat(args_flat, 1) @ W + b # (BS * MCL * MQL, 1) -\u0026gt; (BS, MCL, MQL) similarity_matrix = _reconstruct(res, args[0], 1) logging.debug(\u0026#34;similiarity_matrix after reconstruct: {}\u0026#34;.format(similarity_matrix.get_shape())) context_mask_aug = tf.tile(tf.expand_dims(context_mask, 2), [1, 1, max_question_length]) question_mask_aug = tf.tile(tf.expand_dims(question_mask, 1), [1, max_context_length, 1]) mask_aug = context_mask_aug \u0026amp; question_mask_aug similarity_matrix = softmax_mask_prepro(similarity_matrix, mask_aug) return (similarity_matrix) Bi-Directional Attention Flow lSeo et al. (2016)针对SQuAD提出了一个另一种更复杂的注意力机制, Bi-Directional Attention Flow (BiDAF)。 BiDAF顾名思义那个就是问题与段落的双向的注意力机制, 分别是 Context-to-query (C2Q) attention 和 Query-to-context (Q2C) attention. 两者都是基于传统的段落的背景向量 $H$ 与问题的背景向量 $U$ 间相似矩阵(similarity matrix) $S \\in \\mathbb{R^{T×J}}$衍生出来的. $$ S_{tj} = \\alpha(H_{:t}, U_{:j}) \\in R \\\\\\ \\alpha(h, u) = w^{\\top}_{(S)}[h; u; h \\odot u] $$ Where $S_{tj}$ indicates the similarity between t-th context word and j-th query word, $\\alpha$ is a trainable scalar function that encodes the similarity between its two input vectors, $H_{:t}$ is t-th column vector of H, and $U_{:j}$ is j-th column vector of U, $w_{(S)} \\in R^{6d}$ is a trainable weight vector, $[;]$ is vector concatenation across row.\n相似矩阵S被用于计算两种方向的注意力向量.\nContext-to-query (C2Q) attention signifies which query words are most relevant to each context word\n$$ \\tilde{U_{:t}} = \\sum_j \\alpha_{tj} U_{:j} \\\\\\ \\alpha_t = softmax(S_{t:}) $$ 其中 $\\alpha_t \\in R^J 表示$t$段落词对各个问题词的注意力权重\nQuery-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query.\n对段落的注意力权重为: $$ b = softmax(max_{col}(S)) \\in R^T $$ 其中$max_{col}$是在每行选出最大值. 然后对段落背景向量进行注意力加权: $$ \\tilde{h} = \\sum_t b_t H_{:t} \\in R^{2d} $$ 这个$\\tilde{h}$向量指的是在query眼里最重要的段落次的加权求和. 因为$\\tilde{h}$是在每一个内去最大值, 所以还需要从新把$\\tilde{h}$的值在每一个铺开$T$次得到一个$\\tilde{H} \\in R^{2dxT}$向量以方便后续的计算.\n最后, 段落的embeddings向量和注意力向量结合为$G$, $G$的每一列向量可以理解为每个段落词的 query-aware representation: $$ G_{:t} = \\beta(H_{:t}, \\tilde{U_{:t}}, \\tilde{H_{:t}}) \\in R^{d_G} $$ where $G_{:t}$ is the t-th column vector (corresponding to t-th context word), β is a trainable vector function that fuses its (three) input vectors, and $d_G$ is the output dimension of the β function.\nβ 函数可以是任意的神经网络, 但是文章中指出使用简单的函数如 $\\beta(h, \\tilde{u}, \\tilde{h}) = [h; \\tilde{u}; h \\odot \\tilde{u}; h \\odot \\tilde{h}] \\in R^{8dxT}$ (i.e., dG = 8d) 表现已经很好了。\nclass Attention(object): def forwards_complex(self, hc, hq, hc_mask, hq_mask, max_context_length_placeholder, max_question_length_placeholder, is_train, keep_prob): \u0026#39;\u0026#39;\u0026#39;combine context hidden state(hc) and question hidden state(hq) with attention measured similarity = hc : hq : hc.T * hq \u0026#39;\u0026#39;\u0026#39; s = self._similarity_matrix(hq, hc, max_question_length_placeholder, max_context_length_placeholder, hq_mask, hc_mask, is_train, keep_prob) # C2Q # (BS, MCL, MQL) weights_c2q = tf.nn.softmax(s) # (BS, MCL, MQL) @ (BS, MQL, HS * 2) -\u0026gt; (BS, MCL, HS * 2) query_aware = weights_c2q @ hq # Q2C # (BS, MCL, MQL) -\u0026gt; (BS, MCL) # We are effectively looking through all the question words j\u0026#39;s to some context word i and finding the # maximum of those context words score_q2c = tf.reduce_max(s, axis=-1) # (BS, MCL) weights_q2c = tf.expand_dims(tf.nn.softmax(score_q2c), -1) # (BS, HS) context_aware = tf.reduce_sum(tf.multiply(weights_q2c, hc), axis=1) # (BS, MCL, HS * 2) context_aware = tf.tile(tf.expand_dims(context_aware, 1), [1, max_context_length_placeholder, 1]) # [(BS, MCL, HS * 2), (BS, MCL, HS * 2), (BS, MCL, HS * 2), (BS, MCL, HS * 2)] biattention = tf.nn.tanh(tf.concat([hc, query_aware, hc * query_aware, hc * context_aware], 2)) return (biattention) def _similarity_matrix(self, hq, hc, max_question_length, max_context_length, question_mask, context_mask, is_train, keep_prob): def _flatten(tensor, keep): fixed_shape = tensor.get_shape().as_list() start = len(fixed_shape) - keep # Calculate (BS * MCL * MQL) left = reduce(mul, [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start)]) # out_shape is simply HS * 2 out_shape = [left] + [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start, len(fixed_shape))] # (BS * MCL * MQL, HS * 2) flat = tf.reshape(tensor, out_shape) return (flat) def _reconstruct(tensor, ref, keep): ref_shape = ref.get_shape().as_list() tensor_shape = tensor.get_shape().as_list() ref_stop = len(ref_shape) - keep tensor_start = len(tensor_shape) - keep # [BS, MCL, MQL] pre_shape = [ref_shape[i] or tf.shape(ref)[i] for i in range(ref_stop)] # [1] keep_shape = [tensor_shape[i] or tf.shape(tensor)[i] for i in range(tensor_start, len(tensor_shape))] # pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))] # keep_shape = tensor.get_shape().as_list()[-keep:] # [BS, MCL, MQL, 1] target_shape = pre_shape + keep_shape out = tf.reshape(tensor, target_shape) out = tf.squeeze(out, [len(args[0].get_shape().as_list()) - 1]) return (out) # (BS, MCL, MQL, HS * 2) d = hq.get_shape().as_list()[-1] logging.debug(\u0026#34;d is: {}\u0026#34;.format(d)) hc_aug = tf.tile(tf.reshape(hc, shape=[-1, max_context_length, 1, d]), [1, 1, max_question_length, 1]) # (BS, MCL, MQL, HS * 2) hq_aug = tf.tile(tf.reshape(hq, shape=[-1, 1, max_question_length, d]), [1, max_context_length, 1, 1]) # [(BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2)] args = [hc_aug, hq_aug, hc_aug * hq_aug] # [(BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2)] args_flat = [_flatten(arg, 1) for arg in args] args_flat = [tf.cond(is_train, lambda: tf.nn.dropout(arg, keep_prob), lambda: arg) for arg in args_flat] d_concat = d * 3 W = tf.get_variable(\u0026#34;W\u0026#34;, shape=[d_concat, 1]) b = tf.get_variable(\u0026#34;b\u0026#34;, shape=[1]) # Calculating a(h, u) = w_s^(t)[h; u; h * u] # (BS * MCL * MQL, HS * 6) @ (HS * 6, 1) + (1) -\u0026gt; (BS * MCL * MQL, 1) res = tf.concat(args_flat, 1) @ W + b # (BS * MCL * MQL, 1) -\u0026gt; (BS, MCL, MQL) similarity_matrix = _reconstruct(res, args[0], 1) logging.debug(\u0026#34;similiarity_matrix after reconstruct: {}\u0026#34;.format(similarity_matrix.get_shape())) context_mask_aug = tf.tile(tf.expand_dims(context_mask, 2), [1, 1, max_question_length]) question_mask_aug = tf.tile(tf.expand_dims(question_mask, 1), [1, max_context_length, 1]) mask_aug = context_mask_aug \u0026amp; question_mask_aug similarity_matrix = softmax_mask_prepro(similarity_matrix, mask_aug) return (similarity_matrix) 数据处理 内容段落摘自维基百科文章中的536篇文章，包含107,785对问题和答案，这使得SQuAD显着大于以前任何人类标注的数据集。在该数据集中，80％的数据用于训练，10％用于验证, 剩余10％用于测试。在训练集中，进一步划分出5％用于训练时的验证。\n与其他问答数据集相比，SQUAD具有比较独特的特征，所有答案都是出自相应的上下文中。对于每一个段落, 众包人员生成几个问题，并选择原段落中的一小段作为答案. 答案由两个index组成, 对应答案在段落中的起始位置。因此，SQuAD数据集的答案可能比其他以单个单词和实体为答案为主的数据集长得多。实例:\nQuestion: Why was Tesla returned to Gospic?\nContext paragraph: On 24 March 1879, Tesla was returned to Gospicunder police guard for not having a residence permit\u0026hellip;\nAnswer: {12, 16}\nEmbedding 词向量使用预训练好的 Glove embedding.\nGlove is a log-bilinear regression model that combines the advantages of global matrix factorization and local context window methods.\ndef load_glove_embeddings(embed_path): logger.info(\u0026#34;Loading glove embedding...\u0026#34;) glove = np.load(embed_path)[\u0026#39;glove\u0026#39;] logger.info(\u0026#34;Dimension: {}\u0026#34;.format(glove.shape[1])) logger.info(\u0026#34;Vocabulary: {}\u0026#34; .format(glove.shape[0])) return glove embeddings = load_glove_embeddings(embed_path) class Model(metaclass=ABCMeta): ... @abstractmethod def setup_embeddings(self): pass def setup_embeddings(self): \u0026#34;\u0026#34;\u0026#34; Loads distributed word representations based on placeholder tokens :return: embeddings representaion of question and context. \u0026#34;\u0026#34;\u0026#34; with tf.variable_scope(\u0026#34;embeddings\u0026#34;): if self.config.RE_TRAIN_EMBED: embeddings = tf.get_variable(\u0026#34;embeddings\u0026#34;, initializer=self.embeddings) else: embeddings = tf.cast(self.embeddings, dtype=tf.float32) question_embeddings = tf.nn.embedding_lookup(embeddings, self.question_placeholder) question_embeddings = tf.reshape(question_embeddings, shape = [-1, self.max_question_length_placeholder, self.config.embedding_size]) context_embeddings = tf.nn.embedding_lookup(embeddings, self.context_placeholder) context_embeddings = tf.reshape(context_embeddings, shape = [-1, self.max_context_length_placeholder, self.config.embedding_size]) return question_embeddings, context_embeddings 模型 整体的模型由Embedding层，Encodr层，Attention层，Decoder层组成\nEncoder 编码器就是一个双向GRU层:\nclass Encoder(object): \u0026#34;\u0026#34;\u0026#34; In a generalized encode function, you pass in your inputs, masks, and an initial hidden state input into this function. :param inputs: Symbolic representations of your input :param masks: this is to make sure tf.nn.dynamic_rnn doesn\u0026#39;t iterate through masked steps :param encoder_state_input: (Optional) pass this as initial hidden state to tf.nn.dynamic_rnn to build conditional representations :return: outputs: The RNN output Tensor an encoded representation of your input. It can be context-level representation, word-level representation, or both. state: The final state. \u0026#34;\u0026#34;\u0026#34; def __init__(self, state_size): self.state_size = state_size def encode(self, inputs, masks, initial_state_fw=None, initial_state_bw=None, reuse=False, keep_prob = 1.0): return BiGRU_layer(inputs, masks, self.state_size, initial_state_fw, initial_state_bw, reuse, keep_prob) def BiGRU_layer(inputs, masks, state_size, initial_state_fw=None, initial_state_bw=None, reuse = False, keep_prob=1.0): \u0026#39;\u0026#39;\u0026#39; Wrapped BiGRU_layer for reuse\u0026#39;\u0026#39;\u0026#39; # \u0026#39;outputs\u0026#39; is a tensor of shape [batch_size, max_time, cell_state_size] cell_fw = tf.contrib.rnn.GRUCell(state_size, reuse = reuse) cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob) cell_bw = tf.contrib.rnn.GRUCell(state_size, reuse = reuse) cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob) sequence_length = tf.reduce_sum(tf.cast(masks, \u0026#39;int32\u0026#39;), axis=1) sequence_length = tf.reshape(sequence_length, [-1,]) # Outputs Tensor shaped: [batch_size, max_time, cell.output_size] (outputs_fw, outputs_bw), (final_state_fw, final_state_bw) = tf.nn.bidirectional_dynamic_rnn( cell_fw = cell_fw,\\ cell_bw = cell_bw,\\ inputs = inputs,\\ sequence_length = sequence_length, initial_state_fw = initial_state_fw,\\ initial_state_bw = initial_state_bw, dtype = tf.float32) outputs = tf.concat([outputs_fw, outputs_bw], 2) return outputs, final_state_fw, final_state_bw Decoder 解码器也包含一个双向GRU层，输出的状态分别由两个softmax分类器计算出预测的答案的 start 和 end index 位置:\nclass Decoder(object): \u0026#34;\u0026#34;\u0026#34; takes in a knowledge representation and output a probability estimation over all paragraph tokens on which token should be the start of the answer span, and which should be the end of the answer span. :param knowledge_rep: it is a representation of the paragraph and question, decided by how you choose to implement the encoder :return: (start, end) \u0026#34;\u0026#34;\u0026#34; def __init__(self, output_size, state_size=None): self.output_size = output_size self.state_size = state_size def decode(self, knowledge_rep, mask, max_input_length, keep_prob = 1.0): \u0026#39;\u0026#39;\u0026#39;Decode with BiGRU\u0026#39;\u0026#39;\u0026#39; with tf.variable_scope(\u0026#39;Modeling\u0026#39;): outputs, _, _ = BiGRU_layer(knowledge_rep, mask, self.state_size, keep_prob=keep_prob) with tf.variable_scope(\u0026#34;start\u0026#34;): start = self.get_logit(outputs, max_input_length) start = softmax_mask_prepro(start, mask) with tf.variable_scope(\u0026#34;end\u0026#34;): end = self.get_logit(outputs, max_input_length) end = softmax_mask_prepro(end, mask) return (start, end) def get_logit(self, inputs, max_inputs_length): \u0026#39;\u0026#39;\u0026#39; Get the logit (-inf, inf). \u0026#39;\u0026#39;\u0026#39; d = inputs.get_shape().as_list()[-1] assert inputs.get_shape().ndims == 3, (\u0026#34;Got {}\u0026#34;.format(inputs.get_shape().ndims)) inputs = tf.reshape(inputs, shape = [-1, d]) W = tf.get_variable(\u0026#39;W\u0026#39;, initializer=tf.contrib.layers.xavier_initializer(), shape=(d, 1), dtype=tf.float32) pred = tf.matmul(inputs, W) pred = tf.reshape(pred, shape = [-1, max_inputs_length]) tf.summary.histogram(\u0026#39;logit\u0026#39;, pred) return pred 搭建整个系统 在整个QASystem类中初始化这些功能层:\nclass QASystem(Model): def __init__(self, embeddings, config): \u0026#34;\u0026#34;\u0026#34; Initializes System \u0026#34;\u0026#34;\u0026#34; self.embeddings = embeddings self.config = config self.encoder = Encoder(config.encoder_state_size) self.decoder = Decoder(output_size=config.output_size, state_size = config.decoder_state_size) self.attention = Attention() # ==== set up placeholder tokens ======== self.context_placeholder = tf.placeholder(tf.int32, shape=(None, None)) self.context_mask_placeholder = tf.placeholder(tf.bool, shape=(None, None)) self.question_placeholder = tf.placeholder(tf.int32, shape=(None, None)) self.question_mask_placeholder = tf.placeholder(tf.bool, shape=(None, None)) self.answer_start_placeholder = tf.placeholder(tf.int32) self.answer_end_placeholder = tf.placeholder(tf.int32) self.max_context_length_placeholder = tf.placeholder(tf.int32) self.max_question_length_placeholder = tf.placeholder(tf.int32) self.dropout_placeholder = tf.placeholder(tf.float32) # ==== assemble pieces ==== with tf.variable_scope(self.config.which_model, initializer=tf.uniform_unit_scaling_initializer(1.0)): self.question_embeddings, self.context_embeddings = self.setup_embeddings() self.preds = self.setup_system() self.loss = self.setup_loss(self.preds) self.f1_train = tf.Variable(0., tf.float64) self.EM_train = tf.Variable(0., tf.float64) self.f1_val = tf.Variable(0., tf.float64) self.EM_val = tf.Variable(0., tf.float64) tf.summary.scalar(\u0026#39;f1_train\u0026#39;, self.f1_train) tf.summary.scalar(\u0026#39;EM_train\u0026#39;, self.EM_train) tf.summary.scalar(\u0026#39;f1_val\u0026#39;, self.f1_val) tf.summary.scalar(\u0026#39;EM_val\u0026#39;, self.EM_val) # ==== set up training/updating procedure ==== \u0026#39;\u0026#39;\u0026#39; With gradient clipping\u0026#39;\u0026#39;\u0026#39; opt_op = get_optimizer(self.config.optimizer, self.loss, config.max_gradient_norm, config.learning_rate) if config.exdma_weight_decay is not None: self.train_op = self.build_exdma(opt_op) else: self.train_op = opt_op self.merged = tf.summary.merge_all() 把各个功能层搭建成一个完整的模型:\ndef setup_system(self): \u0026#34;\u0026#34;\u0026#34; Connect all parts of your system here: After your modularized implementation of encoder and decoder you should call various functions inside encoder, decoder here to assemble your reading comprehension system! context: [None, max_context_length, d] question: [None, max_question_length, d] :return: \u0026#34;\u0026#34;\u0026#34; d = self.context_embeddings.get_shape().as_list()[-1] \u0026#39;\u0026#39;\u0026#39;Step 1: encode context and question, respectively, with independent weights e.g. hq = encode_question(question) # get U (d*J) as representation of q e.g. hc = encode_context(context, q_state) # get H (d*T) as representation of x \u0026#39;\u0026#39;\u0026#39; with tf.variable_scope(\u0026#39;question\u0026#39;): hq, question_state_fw, question_state_bw = \\ self.encoder.BiGRU_encode(self.question_embeddings, self.question_mask_placeholder, keep_prob = self.dropout_placeholder) if self.config.QA_ENCODER_SHARE: hc, context_state_fw, context_state_bw =\\ self.encoder.BiGRU_encode(self.context_embeddings, self.context_mask_placeholder, initial_state_fw = question_state_fw, initial_state_bw = question_state_bw, reuse = True, keep_prob = self.dropout_placeholder) if not self.config.QA_ENCODER_SHARE: with tf.variable_scope(\u0026#39;context\u0026#39;): hc, context_state_fw, context_state_bw =\\ self.encoder.BiGRU_encode(self.context_embeddings, self.context_mask_placeholder, initial_state_fw = question_state_fw, initial_state_bw = question_state_bw, keep_prob=self.dropout_placeholder) d_Bi = self.config.encoder_state_size*2 assert hc.get_shape().as_list() == [None, None, d_Bi], ( \u0026#34;Expected {}, got {}\u0026#34;.format([None, self.max_context_length_placeholder, self.config.encoder_state_size], hc.get_shape().as_list())) assert hq.get_shape().as_list() == [None, None, d_Bi], ( \u0026#34;Expected {}, got {}\u0026#34;.format([None, self.max_question_length_placeholder, self.config.encoder_state_size], hq.get_shape().as_list())) \u0026#39;\u0026#39;\u0026#39;Step 2: combine context hidden state(hc) and question hidden state(hq) with attention measured similarity = hc.T * hq Context-to-query (C2Q) attention signifies which query words are most relevant to each P context word. attention_c2q = softmax(similarity) hq_hat = sum(attention_c2q*hq) Query-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query. attention_q2c = softmax(similarity.T) hc_hat = sum(attention_q2c*hc) combine with β activation: β function can be an arbitrary trainable neural network g = β(hc, hq, hc_hat, hq_hat) \u0026#39;\u0026#39;\u0026#39; # concat[h, u_a, h*u_a, h*h_a] attention = self.attention.forwards_bilinear(hc, hq, self.context_mask_placeholder, self.question_mask_placeholder, max_context_length_placeholder = self.max_context_length_placeholder, max_question_length_placeholder = self.max_question_length_placeholder, is_train=(self.dropout_placeholder \u0026lt; 1.0), keep_prob=self.dropout_placeholder) d_com = d_Bi*4 \u0026#39;\u0026#39;\u0026#39;Step 3: decoding \u0026#39;\u0026#39;\u0026#39; with tf.variable_scope(\u0026#34;decoding\u0026#34;): start, end = self.decoder.BiGRU_decode(attention, self.context_mask_placeholder, self.max_context_length_placeholder, self.dropout_placeholder) return start, end ","permalink":"https://congchan.github.io/posts/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-lstm%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E6%96%AF%E5%9D%A6%E7%A6%8F%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86-squad/","summary":"\u003cp\u003e本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"注意力机制应用于阅读理解\"\u003e注意力机制应用于阅读理解\u003c/h2\u003e\n\u003cp\u003eThe Standford question and answer dataset \u003ca href=\"https://rajpurkar.github.io/SQuAD-explorer/\"\u003e(SQuAD)\u003c/a\u003e 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。\u003c/p\u003e\n\u003cp\u003e由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。\u003c/p\u003e\n\u003cp\u003e但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。\u003c/p\u003e\n\u003cp\u003e虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用\u003ca href=\"%5Cattention#%E5%85%A8%E5%B1%80%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6\"\u003e全局注意力机制\u003c/a\u003e中提到的\n\u003c/p\u003e\n$$\n\\begin{aligned}\n    score_{general}(t' t) \u0026= s^\\top_{t'} W_\\alpha h_t, \\\\\\\n\\end{aligned}\n$$\u003cp\u003e\n就是用一个交互矩阵$W_\\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 \u003cstrong\u003eBilinear\u003c/strong\u003e.\u003c/p\u003e","title":"机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)"},{"content":"时序决策 以经典的Atari游戏为例，agent在t时刻观测一段包含M个帧的视频$s_t = (x_{t-M+1}, ..., x_t) \\in S$, 然后agent做决策, 决策是选择做出一个动作 $a_t \\in A = \\{ 1, ..., |A| \\}$(A为可选的离散动作空间 ), 这个动作会让agent获得一个奖励$r_t$.\n这就是时序决策过程, 是一个通用的决策框架，可以建模各种时序决策问题，例如游戏，机器人等. Agent 观察环境，基于policy $\\pi\\left(a_{t} \\mid s_{t}\\right)$ 做出响应动作，其中 $s_{t}$是当前环境的观察值(Observation 是环境State对Agent可见的部分)。Action会获得新的 Reward $r_{t+1}$, 以及新的环境反馈 $s_{t+1}$.\nNote: It is important to distinguish between the state of the environment and the observation, which is the part of the environment state that the agent can see, e.g. in a poker game, the environment state consists of the cards belonging to all the players and the community cards, but the agent can observe only its own cards and a few community cards. In most literature, these terms are used interchangeably and observation is also denoted as .\nAgent的目标是通过优化 policy来最大化期望奖励(未来的奖励相对于当前时间需要打折, 也就是贴现, 跟未来现金流贴现一个道理), 称之为 discounted return $R_t = \\sum_{\\tau=t}^{\\infty} \\gamma^{\\tau-t} r_{\\tau}$, $\\gamma \\in [0, 1]$就是贴现率.\n定义一个值 $Q^\\pi(s, a)$, 用于表示一个 state-action pair $(s, a)$ 的价值， $Q^{\\pi}(s,a)=E{\\[R_t|s_t=s, a_t=a, \\pi\\]}$ 定义$V^\\pi(s)$用于表示状态$s$的价值 $V^{\\pi}(s)=E_{a \\sim \\pi(s)}\\[Q^{\\pi}(s, a)\\]$ 为了计算Q值， 需要利用动态规划递归求解\n$$Q^{\\pi}(s, a)=E_{s^{\\prime}}\\[r+\\gamma E_{a^{\\prime} \\sim \\pi\\left(s^{\\prime}\\right)}\\[Q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\] \\mid s, a, \\pi\\]$$最优的Q值就是 $Q^∗(s, a) = \\max_{\\pi} Q^\\pi(s, a)$, 假设每次都选择能让当前Q最大的动作(这种方式是deterministic policy, 其他的还有Stochastic policies), $a = argmax_{a' \\in A} Q^∗(s, a')$, 那么$V^∗(s) = \\max_a Q^∗(s, a)$, 由此引出最优 $Q^{*}(s, a)$ 满足Bellman optimality equation\n$$Q^*(s, a)=E_{s'}\\[r+\\gamma \\max _{a^{\\prime}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\]$$The Cartpole Environment The Cartpole Environment 是 RL中的Hello World. The environment simulates balancing a pole on a cart. The agent can nudge the cart left or right; these are the actions. It represents the state with a position on the x-axis, the velocity of the cart, the velocity of the tip of the pole and the angle of the pole (0° is straight up). The agent receives a reward of 1 for every step taken. The episode ends when the pole angle is more than ±12°, the cart position is more than ±2.4 (the edge of the display) or the episode length is greater than 200 steps. To solve the environment you need an average reward greater than or equal to 195 over 100 consecutive trials.\nObservation $s_{t}$: 4D vector [position, velocity, angle, angular velocity]\nActions $a_{t}$: push the cart right (+1) or left (-1).\nReward $r_{t+1}$:\n1 for every timestep that the pole remains upright. The episode ends when one of the following is true: the pole tips over some angle limit the cart moves outside of the world edges 200 time steps pass. Goal: Learn policy $\\pi\\left(a_{t} \\mid s_{t}\\right)$ to maximize the sum of rewards in an episode $\\sum_{t=0}^{T} \\gamma^{t} r_{t}$.\n$\\gamma$ is a discount factor in [0, 1] that discounts future rewards relative to immediate rewards. This parameter helps us focus the policy, making it care more about obtaining rewards quickly.\nMarkov Decision Processes MDP框架用于表达agent的学习过程，包含actions-rewards\nA Markov Decision Process is defined by 5 components:\nA set of possible states An initial state A set of actions A transition model probability of transition $P(s'|s, a)$ A reward function: $R(s'|s, a)$ Discount $\\gamma$: In this regard, the discount factor for a Markov Decision Process plays a similar role to a discount factor in Finance as it reflects the time value of rewards. This means that it is preferable to get a larger reward now and a smaller reward later, than it is to get a small reward now and a larger reward later due to the value of time. Markov process from Wikipedia:\nA stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process.\nMarkov decision process:\nA Markov decision process provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\nthere are two possible types of environments:\nThe first is an environment that is completely observable, in which case its dynamics can be modeled as a Markov Process. Markov processes are characterized by a short-term memory, meaning the future depends not on the environments whole history, but instead only on the current state. The second type is a partially observed environment where some variables are not observable. These situations can be modeled using dynamic latent variable models, for example, using Hidden Markov models. Decision Policies Since the problem needs to be solved now, but the actions will be performed in the future, we need to define a decision policy.\nThe decision policy is a function that takes the current state S and translates it into an action A.\nDeterministic policies:\nAlways gives the same answer for a given state In general, it can depend on previous states and actions For a MDP, deterministic policies depend only on the current state, because state transitions also depend only on the current state Stochastic (randomized) policies:\nGeneralize deterministic policies For a MDP with known transition probabilities, we only need to consider deterministic policies If the transition probability is not known, randomization of actions allow exploration for a better estimation of the model Stochastic policies may work better than deterministic policies for a Partially Observed MDP (POMDP) Exploration-Exploitation Dilemma This concept is specific to reinforcement learning and does not arise in supervised or unsupervised Learning.\nExploration means the agent is exploring potential hypotheses for how to choose actions, which inevitably will lead to some negative reward from the environment. Exploitation means how the agent exploits the limited knowledge about what it has already learned This is referred to as a dilemma because at each time-step, the agent must decide whether it should explore or exploit in this state - but it can\u0026rsquo;t do both at once.\nReinforcement learning should ideally combine both exploration and exploitation, for example by switching between each one at different time steps.\nQ和V转换 $V$跟策略有很大关系，计算过程是：\n从$S_i$出发，多次采样; 每个采样按照当前的 策略 选择行为$A_{i+1}$; 每个采样一直走到最终状态，并计算一路上获得的所有奖励总和; 计算每个采样获得的平均值, 这个平均值就是要求的$V$值。 $Q$的计算过程和$V$差不多，但是跟策略没有直接关系，而是与环境的状态转移概率相关，而环境的状态转移概率是不变的。\n可以把采样过程形象化为有Markov过程生成的树，每个状态和动作都是一个树节点，而树的叶子节点就是结束状态。状态节点和动作节点是分层相隔的，所以Q和V可以相互换算，即每一层的Q可以由下一层的V计算出来，反之亦然。\nWhat is the Q function and what is the V function in reinforcement learning?\n$$\\begin{align} v_{\\pi}(s)\u0026=E{\\[G_t|S_t=s\\]} \\\\\\\\ \u0026=\\sum_{g_t} p(g_t|S_t=s)g_t \\\\\\\\ \u0026= \\sum_{g_t}\\sum_{a}p(g_t, a|S_t=s)g_t \\\\\\\\ \u0026= \\sum_{a}p(a|S_t=s)\\sum_{g_t}p(g_t|S_t=s, A_t=a)g_t \\\\\\\\ \u0026= \\sum_{a}p(a|S_t=s)E{\\[G_t|S_t=s, A_t=a\\]} \\\\\\\\ \u0026= \\sum_{a}p(a|S_t=s)q_{\\pi}(s,a) \\end{align}$$一个状态的V值，就是这个状态下的所有动作的Q值$q_{\\pi}(s,a)$ 在策略$p(a|S_t=s)$下的期望。\nif we have a deterministic policy， then $v_{\\pi}(s)=q_{\\pi}(s,\\pi(s))$\n$$⁍$$\n实际应用中，我们更多会从V到V。把Q代入得到\nReference What is Reinforcement Learning? A Complete Guide for Beginners https://datascience.stackexchange.com/questions/9832/what-is-the-q-function-and-what-is-the-v-function-in-reinforcement-learning ","permalink":"https://congchan.github.io/posts/value-based-reinforcement-learning/","summary":"\u003ch1 id=\"时序决策\"\u003e\u003cstrong\u003e时序决策\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003e以经典的Atari游戏为例，agent在t时刻观测一段包含M个帧的视频$s_t = (x_{t-M+1}, ..., x_t) \\in S$, 然后agent做决策, 决策是选择做出一个动作 $a_t \\in A  = \\{ 1, ..., |A| \\}$(A为可选的离散动作空间 ), 这个动作会让agent获得一个奖励$r_t$.\u003c/p\u003e\n\u003cp\u003e这就是\u003cstrong\u003e时序决策过程,\u003c/strong\u003e 是一个通用的决策框架，可以建模各种\u003cstrong\u003e时序决策\u003c/strong\u003e问题，例如游戏，机器人等. Agent 观察环境，基于policy $\\pi\\left(a_{t} \\mid s_{t}\\right)$ 做出响应动作，其中 $s_{t}$是当前环境的观察值(Observation 是环境State对Agent可见的部分)。Action会获得新的 Reward $r_{t+1}$, 以及新的环境反馈 $s_{t+1}$.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: It is important to distinguish between the \u003cstrong\u003estate\u003c/strong\u003e of the environment and the \u003cstrong\u003eobservation\u003c/strong\u003e, which is the part of the environment state that the agent can see, e.g. in a poker game, the environment state consists of the cards belonging to all the players and the community cards, but the agent can observe only its own cards and a few community cards. In most literature, these terms are used interchangeably and observation is also denoted as .\u003c/p\u003e","title":"Value-based Reinforcement Learning"},{"content":"注意力机制如何起源的 神经网络中的注意力机制启发自人类的视觉注意力机制，能够（高分辨率地）聚焦于图像中需要重点关注的目标区域（节省大脑资源），同时（低分辨率地）感知周围的图像，然后随着时间的推移调整焦点（状态调整）。\n在神经网路中，注意力机制是为了解决什么问题？\n在深度学习还没流行的时候, 传统的算法早已应用了注意力机制的思想.\n比如一个非线性回归问题，对于代表位置的输入变量${x_1, ..., x_m}$ 和 代表位置对应的输出值${y_1, ..., y_m}$, 如何预测新的$x_n$对应的输出? Baseline 就是求均值, $$\\frac{1}{m} \\sum_{i=1}^{m} y_i$$ 当然更好的方案(Watson, Nadaraya, 1964)是根据不同的输入$x_i$给与$y_i$不同的权重, $$y = \\sum_{i=1}^{m} \\alpha(x, x_i) y_i $$这里$x$代表一个新的输入(作为query), 根据$x$和已有的位置$x_i$(作为key)进行某种运算, 得到$x_i$对应的输出$y_i$(作为value)的权重. 如果每一个权重都是一个Guassians分布, 并正则化, 则一个加权的回归预测模型就是:\n$$f(x) = \\sum_i y_i \\frac{k(x_i, x)}{\\sum_j k(x_j, x)}$$这个算法的\u0026quot;深度学习\u0026quot;版本, 就是其权重是通过优化器(如sgd)学习得来, 并且把平均运算改为加权池化(weighted pooling).\n如何简单直观地理解注意力机制 虽然注意力机制一开始被应用于图像识别领域，但是后来推广到神经机器翻译(NMT)中(Seq2Seq for Machine Translation, Sutskever, Vinyals, Le ‘14). NMT也是注意力机制在NLP领域最早最成功的应用之一.\n在上图中，Echt，Dicke和Kiste词被送到编码器中，并且在特殊信号（未显示）之后，解码器开始生成翻译后的句子。解码器不断生成单词，直到产生特殊的句子结尾标记(如\u0026lt;eos\u0026gt;)。也就是说解码器仅根据最后一个隐含状态$h_3$来生成序列. 假如这个句子很短, 那么效果其实是很好的.\n不过对于比较长的句子, 那么这个架构的弱点就暴露无疑了.\n首先, 编码器能否把句子的所有信息(语言学上的和常识等知识)都理解/捕捉到? 其次, 受限于目前的实现技术(主要是硬件), 单个隐含状态(如$h_3$这个向量)的维度大小是有限的, 而句子长度以及语言的组合情况是无限的, 单靠$h_3$自身是存储信息能力是有限的. 再者, 解码器是否有足够的解码能力从一个隐含状态中解码出所有的信息? 虽然大部分句子是相对紧凑的, 但语言有个特点, 就是一个词有可能和前面好几步之外的词有联系, 比如一些指代词用于指代文本最开头出现的名词; 语义上, 某个句子的理解, 可能依赖于前面多个句子; 当然往大了说, 要理解一篇文章或一本书, 我们通常需要理解并联系多个段落, 多个章节. 这种现象称之为语言的长距离依赖(long-term dependency), 在一般性的序列数据中, 这个现象称之为的Long-range dependence(LRD). 即使是使用了LSTM这种理论上可以克服长距离依赖问题地网络, 也无法很好的克服语言的长距离依赖问题, 究其原因, 除了LSTM自身的局限性之外, 更主要是深度学习的梯度学习方法的局限性(在梯度反向传播中, 会出现梯度消失).\n在没有更好地参数学习方法替代, 以及隐含层容量有限的前提下, 注意力机制通过为各个时间步的词分配注意力, 从理论上赋予了模型回望源头任意时间步的能力. 注意力机制自身包含的参数是一般神经网络的重要补充, 而它的机能也一定程度上解决了梯度消失的问题.\n注意力机制在NMT的具体作用过程是这样, 训练过程中, 给定一对输入序列知识就是力量\u0026lt;end\u0026gt;和输出序列Knowledge is power \u0026lt;end\u0026gt;。解码器可以在输出序列的时间步1(当前时间步就是一个query), 使用集中编码了知识信息的背景变量来生成Knowledge，在时间步2使用更集中编码了就是的信息的背景变量来生成is，依此类推。这看上去就像是在解码器的每一时间步对输入序列中不同时间步编码的信息分配不同的注意力。这样注意力矩阵参数就编码了这种\u0026quot;注意力\u0026quot;, 同时也更好的协助其他网络部件学习参数. 在预测阶段的每一个时间步, 注意力也参与其中.\n一个经典的(目前也还在不断发展的)NLP问题是文本编码, 即把非结构化的文本, 映射为结构化的数字/向量. 较早有纯统计的Bag of words(Salton \u0026amp; McGill, 1986), 后期发展出了经典的Word2Vec(Mikolov et al., 2013). 现在主流的神经网络文本编码方法是Word2Vec, fasttext, rnn(lstm/gru)等, 核心思想是把文本中的每个字符都映射到一个embedding向量空间中, 全部加在一起得到整个文本的向量表示, $f(x)=\\rho \\bigg( \\sum_{i=1}^n \\phi(x_i) \\bigg)$, 再拿去给后续的网络做分类等任务. 这种算法的缺陷是, 最终编码出来的向量, 会偏向统计频率高的元素, 这导致其在很多实际应用中表现不好, 比如情感分析中, 很多转折句, 前后态度是反转的, 但核心是转折后的部分.\nThey respect you, they really do, but you have to\u0026hellip; Why are you laughing?\n如何让编码模型重点关注句子的关键部分呢? 这得分情况, 一种如这样 They respect you, they really do, but you have to... Why are you laughing?, 整个句子的意思, 是着重于but后面的部分. 亦或者如Wang et al, ’16中提到的“The appetizers are ok, but the service is slow.”, 一个句子中其实分为两个意思, 对于外观口味，评价为积极，而对于服务，评价为消极。\n那么这个时候就需要用注意力机制来给句子的编码分配权重了, $$f(x)=\\rho \\bigg( \\sum_{i=1}^{n} \\alpha(w_i, X) \\phi(x_i) \\bigg)$$ 通过注意力机制，我们不再需要竭尽全力把完整的句子输入编码为固定长度的向量，二十允许解码器在输出生成的每个步骤中“关注”源语句的不同部分。\n所以 Attention 在神经网络模型中的作用就是改进池化(pooling): 没有Attention的池化:\n$$f(X)=\\rho \\bigg( \\sum_{x \\in X} \\phi(x) \\bigg)$$ 有Attention后: $$f(X)=\\rho \\bigg( \\sum_{x \\in X} \\alpha(x, w) \\phi(x) \\bigg)$$如何表达注意力机制 把Attention机制从encoder-decoder架构中抽象出来理解, 如下图: 注意力三个核心组件是:\nQuery: decoder当前待解码的输出. 如果是seq2seq模型, 那就是当前解码器待生成的时间步(用前一时间步的解码器隐含状态来表达). Key-Value: 每个key(输入序列经过编码后的隐含状态)都对应一个value. 在文本任务中, 一般Key和Value一样, 都是输入序列的编码。 Query和Key的相关性: $\\alpha(q, k)$, 告诉模型如何根据Query和各个Key的相关性来分配权重. 计算注意力的主要步骤:\n计算Query和每个key之间的相关性$\\alpha_c(q,k_i)$, 常用的相关性函数包括点积(Scaled Dot-Product Attention)，拼接，神经网路等 归一化(如softmax)后获得分配权重${\\theta_1, ..., \\theta_k}$ 计算Value的加权平均值, 作为Attention输出值. $$\\begin{eqnarray} A(q, \\{(k,v)\\}) \\xrightarrow[\\text{output}]{\\text{maps as}} \\sum_{i=1}^k{\\overbrace{\\alpha_c(q,k_i)}^{\\theta_i}}v_i, q \\in Q, k \\in K, v \\in V \\end{eqnarray}$$在编码器-解码器架构中，Query通常是解码器的隐含状态。而Key和Value，都是编码器的隐含状态。加权求和作为输出: $$\\begin{eqnarray} out = \\sum_{i=1}^k \\theta_i v_i = \\sum_{i=1}^k \\theta_i h(x_i) \\end{eqnarray}$$Attention和Memory对比 从上面的描述看Attention更像是对神经网络(如LSTM等)的记忆功能的改进. 也就是说, 注意力机制只是让网络能够访问其内存，比如编码器的隐含状态. 网络选择从内存中检索什么，并给与不同程度的“关注度”. 换句话说, 何种内存访问机制是soft的，网络将检索所有内存并加权组合。使soft的内存访问好处是可以使用反向传播轻松地进行端对端网络训练（当然也有其他办法-采样方法来计算梯度）。\n而另一方面, 更复杂的Memory机制的研究也发展地如火如荼。比如End-To-End Memory Networks(Sainbayar 2015)中提到的网络结构, 允许网络在输出之前多次读取相同的输入序列，并在每个步骤中更新内存。可以应用于, 例如通过对输入故事进行多个推理步骤来回答问题。 Joe went to the kitchen. Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk. Joe went to the bathroom.\nWhere is the milk?\n此时, 当网络参数以某种方式绑定在一起时，这个内存结构就和上面介绍的注意力机制一样了，只是它在内存上进行了多次跃迁（因为它试图集成来自多个句子的信息）。\n在这种情境下, 注意力机制也可以很灵活地应用, 比如分别在字符级使用注意力机制来编码单词, 在单词级上编码句子, 在句子级上编码段落, 即 Hierarchical attention. Neural Turing Machines(Graves et al., ‘14)的思想也是在内存机制上, 通过将神经网络和外部存储资源耦合来扩展神经网络的功能，这些资源可以通过注意力机制与之交互。组合后的系统类似于图灵机或冯·诺依曼架构，具有端到端的可微分性(因此可以通过梯度下降来进行训练)。除此之外, 神经图灵机但具有更复杂的寻址类型，既可以使用基于内容的寻址（如上下文），也可以使用基于位置的寻址，从而使网络可以学习寻址模式以执行简单的计算机程序，例如排序算法。\n这里并不是要给出Attention和Memory机制的确切的定义区别(也给不了, 有的人觉得二者就是一个东西, 比如有人就称Attention其实软寻址, 应该称为Soft Attention), 而是从主流角度给出类比和解读.\n实战案例: 注意力机制应用到机器翻译中 还是以机器翻译为例: 对于解码器的每一个时间步$t'$, 生成一个背景向量$c_{t'}$来捕捉相关的解码器信息, 以用于预测输出目标值$y_{t'}$. 解码器在时间步 $t'$ 的隐藏状态 $$s_{t'} = g(y_{t'-1}, c_{t'}, s_{t'-1}).$$ 令编码器在时间 $t$ 的隐藏状态为 $h_t$，且总时间步数为 $T$。解码器在时间步 $t'$ 的背景变量为 $$c_{t'} = \\sum_{t=1}^T A{t' t} h_t,$$ 其中 $A{t' t}$ 是注意力分配的权重，用于给定解码器的当前隐藏状态 $s_{t'}$，对编码器中不同时间步的隐藏状态$h_t$求加权平均。\n$$A{t' t} = align(s_{t'}, h_t) = \\frac{\\exp(score(t' t))}{ \\sum_{t=1}^T \\exp(score(t' t)) },$$ 其中 $score(t' t) \\in \\mathbb{R}$ 的计算为 $$score(t' t) = \\alpha(s_{t' - 1}, h_t).$$上式中的score打分函数 $score(t' t)$ 有多种设计方法。Bahanau 等使用了MLP感知机：\n$$e_{t't} = v^\\top \\tanh(W_s s_{t' - 1} + W_h h_t),$$其中 $v$、$W_s$、$W_h$ 以及编码器与解码器中的各个权重和偏差都是模型参数。\nBahanau 等在编码器和解码器中分别使用了门控循环单元GRU。在解码器中，我们需要对门控循环单元的设计稍作修改。解码器在 $t' $ 时间步的隐藏状态为\n$$s_{t'} = z_{t'} \\odot s_{t'-1} + (1 - z_{t'}) \\odot \\tilde{s}_{t'},$$其中的重置门、更新门和候选隐含状态分别为\n$$ \\begin{aligned} r_{t'} \u0026= \\sigma(W_{yr} y_{t'-1} + W_{sr} s_{t' - 1} + W_{cr} c_{t'} + b_r), \\\\\\ z_{t'} \u0026= \\sigma(W_{yz} y_{t'-1} + W_{sz} s_{t' - 1} + W_{cz} c_{t'} + b_z),\\\\\\ \\tilde{s_{t'}} \u0026= \\text{tanh}(W_{ys} y_{t'-1} + W_{ss} (s_{t' - 1} \\odot r_{t'}) + W_{cs} c_{t'} + b_s). \\end{aligned} $$然后，给定目标(解码器)隐藏状态$h_{t'}$, 以及背景向量$c_{t'}$, 通过使用简单的并联层合并这两个向量的信息, 来生成所谓的注意力隐藏状态:\n$$\\tilde{h_{t'}} = \\tanh(W_c[c_{t'} : h_{t'}]) $$这个注意力向量 $\\tilde{h_t}$ 之后会通过一个softmax层来生成预测的概率分布.\n延伸阅读:全局注意力机制 Effective Approaches to Attention-based Neural Machine Translation(Luong et al. 2015)对应用于NMT的注意力机制进行了一定的总结：分为全局（global）和局部（local）注意力机制。区别在于“注意力”是放在所有源位置或仅放置在少数源位置。\nThe idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector $c_t$.\n两种注意力机制区别就在于如何生成背景向量$c_{t'}$.\nLuong et al. (2015) 给出了几种打分函数的计算\n$$ \\begin{aligned} score_{dot}(t' t) \u0026= s^\\top_{t'}h_t \\\\\\ score_{general}(t' t) \u0026= s^\\top_{t'} W_\\alpha h_t, \\\\\\ score_{concat}(t' t) \u0026= v^\\top_\\alpha \\tanh (W_\\alpha[s_{t'} : h_t]) \\end{aligned} $$参考资料 Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 Wang, Yequan, Minlie Huang, and Li Zhao. \u0026ldquo;Attention-based LSTM for aspect-level sentiment classification.\u0026rdquo; Proceedings of the 2016 conference on empirical methods in natural language processing. 2016. Yang, Zichao, et al. \u0026ldquo;Hierarchical attention networks for document classification.\u0026rdquo; Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/ 目前主流的attention方法都有哪些？ - 张俊林的回答 - 知乎 https://www.zhihu.com/question/68482809/answer/264632289 https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis Attention in Deep Learning, Elex Smola, ICML 2019, Long Beach, CA ","permalink":"https://congchan.github.io/posts/%E4%BB%8E%E5%A4%B4%E7%90%86%E8%A7%A3%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","summary":"\u003ch3 id=\"注意力机制如何起源的\"\u003e注意力机制如何起源的\u003c/h3\u003e\n\u003cp\u003e神经网络中的注意力机制启发自人类的\u003cstrong\u003e视觉注意力机制\u003c/strong\u003e，能够（高分辨率地）聚焦于图像中需要重点关注的目标区域（节省大脑资源），同时（低分辨率地）感知周围的图像，然后随着时间的推移调整焦点（状态调整）。\u003c/p\u003e\n\u003cp\u003e在神经网路中，注意力机制是为了解决什么问题？\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e在深度学习还没流行的时候, 传统的算法早已应用了注意力机制的思想.\u003c/p\u003e\n\u003cp\u003e比如一个非线性回归问题，对于代表位置的输入变量${x_1, ..., x_m}$ 和 代表位置对应的输出值${y_1, ..., y_m}$, 如何预测新的$x_n$对应的输出? Baseline 就是求均值, \u003c/p\u003e\n$$\\frac{1}{m} \\sum_{i=1}^{m} y_i$$\u003cp\u003e 当然更好的方案(Watson, Nadaraya, 1964)是根据不同的输入$x_i$给与$y_i$不同的权重, \u003c/p\u003e\n$$y = \\sum_{i=1}^{m} \\alpha(x, x_i) y_i $$\u003cp\u003e这里$x$代表一个新的输入(作为\u003cstrong\u003equery\u003c/strong\u003e), 根据$x$和已有的位置$x_i$(作为\u003cstrong\u003ekey\u003c/strong\u003e)进行某种运算, 得到$x_i$对应的输出$y_i$(作为\u003cstrong\u003evalue\u003c/strong\u003e)的权重. 如果每一个权重都是一个Guassians分布, 并正则化, 则一个\u003cstrong\u003e加权的回归预测模型\u003c/strong\u003e就是:\u003c/p\u003e\n$$f(x) = \\sum_i y_i \\frac{k(x_i, x)}{\\sum_j k(x_j, x)}$$\u003cp\u003e这个算法的\u0026quot;深度学习\u0026quot;版本, 就是其权重是通过优化器(如sgd)学习得来, 并且把平均运算改为\u003cstrong\u003e加权池化(weighted pooling)\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"如何简单直观地理解注意力机制\"\u003e如何简单直观地理解注意力机制\u003c/h3\u003e\n\u003cp\u003e虽然注意力机制一开始被应用于图像识别领域，但是后来推广到神经机器翻译(NMT)中(\u003ccode\u003eSeq2Seq for Machine Translation, Sutskever, Vinyals, Le ‘14\u003c/code\u003e). NMT也是注意力机制在NLP领域最早最成功的应用之一.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"http://www.wildml.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-10.39.06-AM.png\" title=\"一个典型的seq2seq(encoder-decoder)翻译模型, 向量h表示编码器的内部状态\"\u003e\n在上图中，\u003ccode\u003eEcht\u003c/code\u003e，\u003ccode\u003eDicke\u003c/code\u003e和\u003ccode\u003eKiste\u003c/code\u003e词被送到编码器中，并且在特殊信号（未显示）之后，解码器开始生成翻译后的句子。解码器不断生成单词，直到产生特殊的句子结尾标记(如\u003ccode\u003e\u0026lt;eos\u0026gt;\u003c/code\u003e)。也就是说解码器仅根据最后一个隐含状态$h_3$来生成序列. 假如这个句子很短, 那么效果其实是很好的.\u003c/p\u003e\n\u003cp\u003e不过对于比较长的句子, 那么这个架构的弱点就暴露无疑了.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e首先, 编码器能否把句子的所有信息(语言学上的和常识等知识)都理解/捕捉到?\u003c/li\u003e\n\u003cli\u003e其次, 受限于目前的实现技术(主要是硬件), 单个隐含状态(如$h_3$这个向量)的维度大小是有限的, 而句子长度以及语言的组合情况是无限的, 单靠$h_3$自身是存储信息能力是有限的.\u003c/li\u003e\n\u003cli\u003e再者, 解码器是否有足够的解码能力从一个隐含状态中解码出所有的信息?\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e虽然大部分句子是相对紧凑的, 但语言有个特点, 就是一个词有可能和前面好几步之外的词有联系, 比如一些指代词用于指代文本最开头出现的名词; 语义上, 某个句子的理解, 可能依赖于前面多个句子; 当然往大了说, 要理解一篇文章或一本书, 我们通常需要理解并联系多个段落, 多个章节. 这种现象称之为语言的长距离依赖(\u003cstrong\u003elong-term dependency\u003c/strong\u003e), 在一般性的序列数据中, 这个现象称之为的Long-range dependence(LRD). 即使是使用了LSTM这种理论上可以克服长距离依赖问题地网络, 也无法很好的克服语言的长距离依赖问题, 究其原因, 除了LSTM自身的局限性之外, 更主要是深度学习的梯度学习方法的局限性(在梯度反向传播中, 会出现梯度消失).\u003c/p\u003e","title":"从头理解注意力机制"},{"content":"Union-find applications: Percolation Problem discriptions\nPercolation data type. To model a percolation system, create a data type Percolation with the following API:\npublic class Percolation { public Percolation(int n); // create n-by-n grid, with all sites blocked public void open(int row, int col); // open site (row, col) if it is not open already public boolean isOpen(int row, int col); // is site (row, col) open? public boolean isFull(int row, int col); // is site (row, col) full? public int numberOfOpenSites(); // number of open sites public boolean percolates(); // does the system percolate? } Monte Carlo simulation. To estimate the percolation threshold, consider the following computational experiment:\nInitialize all sites to be blocked. Repeat the following until the system percolates: Choose a site uniformly at random among all blocked sites. Open the site. The fraction of sites that are opened when the system percolates provides an estimate of the percolation threshold. Codes available at algs4/Percolation/src/\nThe back wash issue My solution inspired from this post https://www.sigmainfy.com/blog/avoid-backwash-in-percolation.html, with some improvements:\nUsing one WeightedQuickUnionUF(n * n) objects to track each site\u0026rsquo;s parent. Use a byte[n * n] to store the each site\u0026rsquo;s state. There are four possible states, represented as BLOCKED: 0b000 OPEN: 0b001 CONNECT_TO_BOTTOM: 0b010 CONNECT_TO_TOP: 0b100 With byte operation |, we enable sites to have mixture of states. open(row, col): to open the current site cur, we need to find out its four possible neibourghs (up, down, left, right, if exist); use find() to return the neibourghs\u0026rsquo; parents (upParent, etc..), use union() to connect cur and its neibourghs; Fianally, update cur\u0026rsquo;s new parent newParent\u0026rsquo;s state with the combination of cur\u0026rsquo;s parent state and the neibourghs\u0026rsquo; parents states. in totalm, there involves 4 union() and 5 find() API calls at most but the time complexity is still $\\Theta(\\lg N)$ isOpen(): $\\in \\Theta(1)$ by checking the byte[n * n]. isFull(): $\\in \\Theta(1)$, use one call find() API and thus is $\\in \\Theta (\\lg N)$ percolates(): use a boolean isPercolates as mark, for any new open site that becomes both CONNECT_TO_BOTTOM and CONNECT_TO_TOP, we could mark the model as percolates. Estimated student memory = 9.00 n^2 + 0.00 n + 160.00 (R^2 = 1.000) Test 2 (bonus): check that total memory \u0026lt;= 11 n^2 + 128 n + 1024 bytes ==\u0026gt; passed ","permalink":"https://congchan.github.io/posts/percolations-problem/","summary":"\u003ch3 id=\"union-find-applications-percolation\"\u003eUnion-find applications: Percolation\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://coursera.cs.princeton.edu/algs4/assignments/percolation.html\"\u003eProblem discriptions\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePercolation data type. To model a percolation system, create a data type Percolation with the following API:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003ePercolation\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ePercolation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"c1\"\u003e// create n-by-n grid, with all sites blocked\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003erow\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecol\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// open site (row, col) if it is not open already\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eisOpen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003erow\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecol\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"c1\"\u003e// is site (row, col) open?\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eisFull\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003erow\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecol\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"c1\"\u003e// is site (row, col) full?\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e     \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003enumberOfOpenSites\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e       \u003c/span\u003e\u003cspan class=\"c1\"\u003e// number of open sites\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003epercolates\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e              \u003c/span\u003e\u003cspan class=\"c1\"\u003e// does the system percolate?\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eMonte Carlo simulation. To estimate the percolation threshold, consider the following computational experiment:\u003c/p\u003e","title":"Percolations problem"},{"content":"爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh\nReferences: Accelerated natural language processing ANLP revision guide Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning\n概率模型 Probability Model 概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率\n估算概率（Probability Estimation）一般使用最大似然估计（MLE，相关频率）：\n$$p(x_i) = \\frac{Count(x_i)}{\\sum_{i=0}^nCount(x_i)}$$平滑Smoothing 一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。\nLanguage modeling To compute the probability of sentence /sequence of words $P(w_1, w_2, w_3...)$, or to predict upcomming words $P(w|w_1, w_2, w_3...)$\u0026hellip; a language model is also a probability model.\nProbability computation makes use of chain rule of probability, the products of a sequence of conditional probability.\n$$P(w_{1:n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})P(w_4|w_{1:3})...P(w_n|w_{1:n-1})$$But the last term based on the entire sentence is very difficult to compute. So it is simplified by Markov Assumption: approximate the conditional probability by only accounting several prefixes, a one-order Markov assumption simplifies as P(the| water is so transparent that) ≈ P(the| that) $$\\begin{align} P(w_{1:n}) \u0026= \\prod_{i=1}^n P(w_i | w_1, ..., w_{i-1}) \\\\\\\\ \u0026\\propto \\prod_{i=1}^n P(w_i | w_{i-k}, ..., w_{i-1}) \\end{align}$$ Evaluation: Perplexity\nPerplexity Intuition based on Shannon game: The best language model is one that best predicts an unseen test set(e.g. next word), gives the highest $P(sentence)$ to the word that actually occurs.\nDefinition: Perplexity is the inverse probability of the test set, normalized by the number of words(lie between 0-1). Normalize the log probability of all the test sentences: $$\\frac{1}{M} \\log_2 \\prod_{i=1}^m p(x^{(i)}) = \\frac{1}{M} \\sum_{i=1}^m \\log_2 p(x^{(i)})$$ Then transform to perplexity: $$Perplexity = 2^{-\\frac{1}{M} \\sum_{i=1}^m \\log_2 p(x^{(i)})}$$ So minimizing perplexity is the same as maximizing probability\nBad approximation: unless the test data looks just like the training data, so generally only useful in pilot experiments.\nN-Gram Language Model N-Gram语言模型是基于N-1阶马尔可夫假设且由MLE估算出的LM。N-GramLM 预测下一个单词出现概率仅条件于前面的(N-1)个单词, 以The students opened their books为例:\nBi-gram: 统计$P(w_{i}=m|w_{i-1})$, P(students | the), P(opened | students), \u0026hellip;, 属于马尔可夫一阶模型, 即当前t时间步的状态仅跟t-1相关. Tri-gram: P(students | \u0026lt;/s\u0026gt; The), P(opened | The students), 马尔可夫二阶模型 Four-gram: 依此类推 特殊的Uni-gram: 统计$P(w_i)$, P(the), P(students), \u0026hellip;, 此时整个模型退化为词袋模型, 不再属于马尔可夫模型, 而是基于贝叶斯假设, 即各个单词是条件独立的. 所以一般N-gram是指N\u0026gt;1的.\nHow to estimate theparameter? Maximum likelyhood estimate：\n$$P(w_{i}=m|w_{i-n:i-1}) = \\frac{Count(w_{i-n:i})}{Count(w_{i-n:i-1})}$$In practice, use log space to avoid underflow, and adding is faster than multiplying.\nInsufficient: To catch long-distance dependencies, the n has to be very large, that asks for very large memory requirement N-grams only work well for word prediction if the test corpus looks like the training corpus. Sparsity: Zero count of gram, means zero probability? No. To deal with 0 probability, commonly use Kneser-Ney smoothing, for very large N-grams like web, use stupid backoff. Add Alpha Smoothing Assign equal probability to all unseen events. Applied in text classification, or domains where zeros probability is not common. Backoff Smoothing Use information from lower order N-grams (shorter histories) Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram. Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. Interpolation Smoothing Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts Simple interpolation: $P(w_3|w_1, w_2) = \\lambda_1 P(w_3|w_1, w_2) + \\lambda_2 P(w_3|w_2) + \\lambda_3 P(w_3), \\sum \\lambda = 1$. λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data Kneser-Ney Smoothing Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams. Use a better estimate for probabilities of lower-order unigrams, the continuation probability, $P_{continuatin}(w)$ is how likely is w to appear as a novel continutaion. For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w. Every bigram type was a novel continuation the first time it was seen. normalized by the total number of word bigram types. To lower the probability of some fix bigram like \u0026ldquo;San Franscio\u0026rdquo; For general N-gram, Naive Bayes Classifier Application: Text classification, to classify a text, we calculate each class probability given the test sequence, and choose the biggest one. Evaluation: precision, recall, F-measure Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了 Text Classification Or text categorization, method is not limited to NB, see lab7. Spam email, gender/authorship/language identification, sentiments analysis,(opinion extraction, subjectivity analysis)\u0026hellip;\nSentiments Analysis For sentiment(or other text classification), word occurrence may matter more than word frequency. Thus it often improves performance to clip the word counts in each document at 1. This variant binary NB is called binary multinominal naive Bayes or binary NB. Remove duplicates in each data sample - bag of words representation, boolean features. Binarized seems to work better than full word counts. Deal with negation: like, not like, A very simple baseline that is commonly used in sentiment to deal with negation is during text normalization to prepend the prefix NOT_ to every word after a token of logical negation Sentiment lexicons: lists of words that are preannotated with positive or negative sentiment. To deal with insufficient labeled training data. A common way to use lexicons in the classifier is to use as one feature the total count of occurrences of any words in the positive lexicon, and as a second feature the total count of occurrences of words in the negative lexicon. Using just two features results in classifiers that are much less sparse to small amounts of training data, and may generalize better. See lab8. Naive Bayes Assumptions Bags of words: a set of unordered words/features with its frequency in the documents, their order was ignored. Conditional independence: the probabilities $P(w|C)$ are independence given the class, thus a sequence of words(w1,w2,w3\u0026hellip;) probability coculd be estimate via prducts of each $P(w_i|C)$ by walking through every pisition of the sequence, noted that the orders in the sequence does not matter. Naive Bayes Training Each classes\u0026rsquo; prior probability P(C) is the percentage of the classes in the training set. For the test set, its probability as a class j, is the products of its sequence probability $P(w_1, w_2, w_3...|C_j)$ and $P(C_j)$, normalized by the sequence probability $P(w_1, w_2, w_3...)$, which could be calculated by summing all $P(w_1, w_2, w_3...|C_j)\\*P(C_j)$. The joint features probability $P(w_1, w_2, w_3...|C)$ of each class is calculated by naively multiplying each word\u0026rsquo;s MLE given that class. In practice, to deal with 0 probability, we dun use MLE, instead we use add alpha smoothing. Why 0 probability matters? Because it makes the whole sequence probability $P(w_1, w_2, w_3...|C)$ 0, then all the other features as evidence for the class are eliminated too. How: first extract all the vocabulary V in the training set. Then, for each feature/word k, its add alpha smoothing probability estimation within a class j is $(Njk + \\alpha)/(N_j+V\\*\\alpha)$. This is not the actual probability, but just the numerator. Naive Bayes Relationship to Language Modelling When using all of the words as features for naive bayes, then each class in naive bayes is a unigram languange model. For each word, assign probability $P(word|C)$, For each sentence, assign probability $P(S|C) = P(w_1, w_2, w_3...|C)$ Running multiple languange models(classes) to assign probabilities, and pick out the highest language model. Hidden Markov Model The HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), they compute a probability distribution over possible sequences of labels and choose the best label sequence.\nHMM参数$λ= (Y, X, π, A, B)$ :\nY是隐状态（输出变量）的集合 X是观察值（输入）集合 Initial probability π Transition probability matrix A, $P(Tag_{i+1} | Tag_{i})$ Emission probability B, $P(Word | Tag)$ Application: part-of-speech tagging, name entity recognition(NEr), parse tree, speech recognition\nHidden?: these tags, trees or words is not observed(hidden). b比如在POS任务中, X就是观察到的句子, Y就是待推导的标注序列, 因为词性待求的, 所以人们称之为隐含状态.\nThe three fundamental problems of HMM:\ndecoding: discover the best hidden state sequence via Viterbi algorithm. Probability of the observation: Given an HMM with know parameters λ and an observation sequence O, determine the likelihood $P(O| \\lambda)$ (a language model regardless of tags) via Forward algorithm Learning (training): Given only the observed sequence, learn the best(MLE) HMM parameters λ via forward-backward algorithm, thus training a HMM is an unsupervised learning task. 算法:\n前向算法和后向算法解决如何计算似然$P(O| \\lambda)$的问题 Viterbi算法解决HMM 解码问题. 这些算法都是动态规划算法 HMM的缺陷是其基于观察序列中的每个元素都相互条件独立的假设。即在任何时刻观察值仅仅与状态（即要标注的标签）有关。对于简单的数据集，这个假设倒是合理。但大多数现实世界中的真实观察序列是由多个相互作用的特征和观察序列中较长范围内的元素之间的依赖而形成的。而条件随机场(conditional random fiel, CRF)恰恰就弥补了这个缺陷.\n同时, 由于生成模型定义的是联合概率，必须列举所有观察序列的可能值，这对多数领域来说是比较困难的。\nPart-of-speech Tagging Part-of-speech(POS), word classes, or syntactic categories, a description of eight parts-of-speech: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner. noun 名詞 (代號 n. ) pronoun 代名詞 (代號 pron. ) verb 動詞 (代號 v. ) adjective 形容詞 (代號 adj. ) adverb 副詞 (代號 adv. ) preposition 介系詞 (代號 prep. ) conjunction 連接詞 (代號 conj. ) interjection 感歎詞 (代號 int. ) Motivation: Use model to find the best tag sequence T for an untagged sentence S: argmax $P(T|S)$ -\u0026gt; argmax $P(S|T)\\*P(T)$, where P(T) is the transition (prior) probabilities, $P(S|T)$ is the emission (likelihood) probabilities. Parts-of-speech can be divided into two broad supercategories: closed class types and open class types Search for the best tag sequence: Viterbi algorithm evaluation: tag accuracy 使用HMM处理POS代码\nTransition Probability Matrix Tags or states Each (i,j) represent the probability of moving from state i to j When estimated from sequences, should include beginning \u0026lt;s\u0026gt; and end \u0026lt;/s\u0026gt; markers. Tag transition probability matrix: the probability of tag i followed by j Emission Probability Also called observation likelihoods, each expressing the probability of an observation j being generated from a states i. Word/symbol Penn Treebank Forward Algorithm Compute the likelihood of a particular observation sequence. Implementation is almost the same as Viterbi. Yet Viterbi takes the max over the previous path probabilities whereas the forward algorithm takes the sum. Viterbi Algorithm Decoding task: the task of determining which sequence of variables is the underlying source of some sequence of observations.\nViterbi的实现参考HMM POS Tagging\nIntuition: The probability of words $w_1$ followed by $w_2$ with tag/state i and j (i,j is index of all Tags), is the chain rule of the probability of i followed by j and the probability of i output $w_i$ $P(w_1 | i)$ and $P(w_2 |j)$, then choose the maximum from all the possible i j. Then using chain rule to multiply the whole sequence of words.\nThe value of each cell $Vt(j)$ is computed by recursively taking the most probable path that could lead us to this cell from left columns to right. See exampls in tutorial 2 Since HMM based on Markov Assumptions, so the present column $V_t$ is only related with the nearby left column $V_{t-1}$. HMM Training 给定观察序列$X = x_1, x_2, ..., x_t$ ，训练调整模型参数λ, 使$p(X | \\lambda)$最大: Baum-Welch算法 (Forward-backward algorithm)\ninputs: just the observed sequence output: the converged λ(A,B). For each interation k until λ converged: Compute expected counts using λ(k-1) Set λ(k) using MLE on the expected counts. 经常会得到局部最优解.\nContext-free Grammar CFG(phrase-structure grammar) consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered toLexicon gether, and a lexicon of words and symbols.\nConstituency Phrase structure, organizes words into nested constituents. Groups of words behaving as a single units, or constituents.\nNoun phrase(NP), a sequence of words surrounding at least one noun. While the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase Preposed or Postposed constructions. While the entire phrase can be placed differently, the individual words making up the phrase cannot be. Fallback: In languages with free word order, phrase structure (constituency) grammars don’t make as much sense. Headed phrase structure: many phrase has head, VP-\u0026gt;VB, NP-\u0026gt;NN, the other symbols excepct the head is modifyer. Probabilistic Context-free Grammar PCFG(Stochastic Context-Free Grammar SCFG (SCFG)), a probabilistic augmentation of context-free grammars in which each rule is associated with a probability.\nG = (T,N,S,R,P) T, N: Terminal and Non-terminal S: starts symbol R: Derive rule/grammar, N -\u0026gt; N/C P: a probability function, for a given N, ΣP(N-\u0026gt;Ni/Ci)=1. Normally P(S-\u0026gt;NP VP)=1, because this is the only rule for S. PCFG could generates a sentence/tree, thus it is a language model, assigns a probability to the string of words constituting a sentence The probability of a tree t is the product of the probabilities of the rules used to generate it. The probability of the string s is the sum of the probabilities of the trees/parses which have that string as their yield. The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees for the sentence. Application: Probabilistic parsing Shortage: lack the lexicalization of a trigram model, i.e only a small fraction of the rules contains information about words. To solve this problem, use lexicalized PCFGs Lexicalization of PCFGs The head word of phrase gives a good representation of the phrase\u0026rsquo;s structure and meaning Puts the properties of words back into a PCFG Word to word affinities are useful for certain ambiguities, because we know the probability of rule with words and words now, e.g. PP attachment ambiguity Recursive Descent Parsing It is a top-down, depth-first parser:\nBlindly expand nonterminals until reaching a terminal (word). If multiple options available, choose one but store current state as a backtrack point (in a stack to ensure depth-first.) If terminal matches next input word, continue; else, backtrack Can be massively inefficient (exponential in sentence length) if faced with local ambiguity Can fall into infinite loop CKY Parsing Well-formed substring table: For parsing, subproblems are analyses of substrings, memoized in well-formed substring table(WFST, chart).\nChart entries are indexed by start and end positions in the sentence, and correspond to: either a complete constituent (sub-tree) spanning those positions (if working bottom-up), or a prediction about what complete constituent might be found (if working top-down). The chart is a matrix where cell [i, j] holds information about the word span from position i to position j: The root node of any constituent(s) spanning those words Pointers to its sub-constituents (Depending on parsing method,) predictions about what constituents might follow the substring. Probability CKY parsing: Dependency Parsing Motivation: context-free parsing algorithms base their decisions on adjacency; in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective); we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present). Approach: Transition-based dependency parsing Dependency Syntax Dependency structure shows which words depend on (modify or are arguments of) which other words.\nA fully lexicalized formalism without phrasal constituents and phrase-structure rules: binary, asymmetric grammatical relations between words. More specific, head-dependent relations, with edges point from heads to their dependents. Motivation: In languages with free word order, phrase structure (constituency) grammars don’t make as much sense. E.g. we may need both S → NP VP and S → VP NP, but could not tell too much information simply looking at the rule. Dependencies: Identifies syntactic relations directly. The syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words. Relation between phrase structure and dependency structure Convert phrase structure annotations to dependencies via head rules. (Convenient if we already have a phrase structure treebank.): For a given lexicalized constituency parse(CFG tree), remove the phrasal categories, remove the (duplicated) terminals, and collapse chains of duplicates. The closure of dependencies give constituency from a dependency tree Transition-based Dependency Parsing transition-based systems use supervised machine learning methods to train classifiers that play the role of the oracle. Given appropriate training data, these methods learn a function that maps from configurations to transition operators(actions).\nBottom up Like shift-reduce parsing, but the \u0026lsquo;reduce\u0026rsquo; actions are specialized to create dependencies with head on left or right. configuration：consists of a stack, an input buffer of words or tokens, and a set of relations/arcs, a set of actions. How to choose the next action: each action is predicted by a discriminative classifier(often SVM, could be maxent) over each legal move. features: a sequence of the correct (configuration, action) pairs f(c ; x). Evaluation: accuracy (# correct dependencies with or ignore label)). Dependency Tree Dependencies from a CFG tree using heads, must be projective: There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words. But dependency theory normally does allow non-projective structures to account for displaced constituents. Bounded and Unbounded Dependencies Unbounded dependency could be considered as long distance dependency\nLong-distance dependencies: contained in wh-non-subject-question, \u0026ldquo;What flights do you have from Burbank to Tacoma Washington?\u0026rdquo;, the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the VP. Noisy Channel Model: The intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel. a probability model using Bayesian inference, input -\u0026gt; noisy/errorful encoding -\u0026gt; output, see an observation x (a misspelled word) and our job is to find the word w that generated this misspelled word. $P(w|x) = P(x|w)\\*P(w)/P(x)$ Noisy channel model of spelling using naive bayes\nThe noisy channel model is to maximize the product of likelihood(probability estimation) P(s|w) and the prior probability of correct words P(w). Intuitively it is modleing the noisy channel that turn a correct word \u0026lsquo;w\u0026rsquo; to the misspelling. The likelihood(probability estimation) P(s|w) is called the the channel/error model, telling if it was the word \u0026lsquo;w\u0026rsquo;, how likely it was to generate this exact error. The P(w) is called the language model Generative vs. Discriminative Models Generative(joint) models palce probabilities $p(c, d)$ over both observed data d and the hidden variables c (generate the obersved data from hidden stuff).\nDiscriminative(conditional) models take the data as given, and put a probability over hidden structure given the data, $p(c | d)$.\n在朴素贝叶斯与Logistic Regression, 以及HMM和CRF之间, 有生成式和判别式的区别. 生成式模型描述标签向量y如何有概率地生成特征向量x, 即尝试构建x和y的联合分布$p(y, x)$, 典型的模型有N-Gram语言模型, 朴素贝叶斯模型（Naive Bayes）， 隐马尔科夫模型（HMM）, MRF。\n而判别模型直接描述如何根据特征向量x判断其标签y, 即尝试构建$p(y | x)$的条件概率分布, 典型模型如如LR, SVM，CRF，MEMM等.\nExponential Models It is a family, includes Log-linear, MaxEnt, Logistic Regression models.\nMake probability model from the linear combination of weights λ and features f as votes, normalized by the total votes. It is a probabilistic distribution: it estimates a probability for each class/label, aka Softmax. It is a classifier, deciding how to weight features, given data. choose the highest probability label. Application: dependency parsing actions prediction, text classification, Word sense disambiguation Training Discriminative Model Features in NLP are more general, they specify indicator function(a yes/no[0,1] boolean matching function) of properties of the input and each class. Weights: low possibility features will associate with low/negative weight, vise versa. Define features: Pick sets of data points d which are distinctive enough to deserve model parameters: related words, words contians #, words end with ing, etc. Regularization in Discriminative Model The issue of scale:\nLots of features sparsity: easily overfitting: need smoothing Many features seen in training never occur again in test Optimization problem: feature weights can be infinite, and iterative solvers can take a long time to get to those infinities. See tutorial 4. Solution: Early stopping Smooth the parameter via L2 regularization. Smooth the data, like the add alpha smoothing, but hard to know what artificial data to create Morphology 构词学（英语言学分科学名：morphology，“组织与形态”)，又称形态学，是语言学的一个分支，研究单词（word）的内部结构和其形成方式。如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher。构词学正是研究这种单字间组成的关系，并试着整理出其组成的规则。\nMorphemes: The way words are built up from smaller meaning-bearing units.\nLemma:\nLexeme, refers to the set of all the forms that have the same meaning, lemma: refers to the particular form that is chosen by convention to represent the lexeme. E.g: run, runs, ran, running are forms of the same lexeme, with run as the lemma. Affixes: Adding additional meanings of various kinds. \u0026ldquo;+ed, un+\u0026rdquo;\nsuffix : follow the stem Plural of nouns \u0026lsquo;cat+s\u0026rsquo; Comparative and superlative of adjectives \u0026lsquo;small+er\u0026rsquo; Formation of adverbs \u0026lsquo;great+ly\u0026rsquo; Verb tenses \u0026lsquo;walk+ed\u0026rsquo; All inflectional morphology in English uses suffixes Prefix: precede the stem In English: these typically change the meaning Adjectives \u0026lsquo;un+friendly\u0026rsquo;, \u0026lsquo;dis+interested\u0026rsquo; Verbs \u0026rsquo;re+consider\u0026rsquo; Some language use prefixing much more widely Infix: inserted inside the stem Circumfix: do both(follow, precede) Root, stem and base are all terms used in the literature to designate that part of a word that remains when all affixes have been removed.\nThe root word is the primary lexical unit of a word, and of a word family (this root is then called the base word), which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents. E.g: In the form ‘untouchables’ the root is ‘touch’, to which first the suffix ‘-able’, then the prefix ‘un-‘ and finally the suffix ‘-s’ have been added. In a compound word like ‘wheelchair’ there are two roots, ‘wheel’ and ‘chair’. Stem is of concern only when dealing with inflectional morphology\nStemming: reduce terms to their stems in info retrieval, E.g: In the form ‘untouchables’ the stem is ‘untouchable’, ‘touched’ -\u0026gt; ‘touch’; ‘wheelchairs’ -\u0026gt; ‘wheelchair’. Morphological Parsing Use Finite-state transducers, FST, a transducer maps between one representation and another; It is a kind of FSA which maps between two sets of symbols.\nInflectional vs. Derivational Morphology Inflectional · nouns for count (plural: +s) and for possessive case (+’s) · verbs for tense (+ed, +ing) and a special 3rd person singular present form (+s) · adjectives in comparative (+er) and superlative (+est) forms.\nDerivational · Changing the part of speech, e.g. noun to verb: \u0026lsquo;word → wordify\u0026rsquo; · Changing the verb back to a noun · Nominalization: formation of new nouns, often verbs or adjectives\nInflectional Derivational does not change basic meaning or part of speech may change the part of speech or meaning of a word expresses grammatical features or relations between words not driven by syntactic relations outside the word applies to all words of the same part of speech, inflection occurs at word edges: govern+ment+s, centr+al+ize+d applies closer to the stem Challenge of Rich Morphology For a morphologically rich language, many issues would arise because of the morphological complexity.\nThese productive word-formation processes result in a large vocabulary for these languages Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages For POS, augmentations become necessary when dealing with highly inflected or agglutinative languages with rich morphology like Czech, Hungarian and Turkish., part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a single primitive tag. Dependency grammar is better than constituency in dealing with languages that are morphologically rich。 Linguistic and Representational Concepts Parsing Parsing is a combination of recognizing an input string and assigning a correct linguistic structure/tree to it based on a grammar. The Syntactic, Statistical parsing are constituent-based representations(context-free grammars). The Dependency Parsing are based on dependency structure(dependency grammars). Syntactic Parsing Syntactic parsing, is the task of recognizing a sentence and assigning a correct syntactic structure to it.\nSyntactic parsing can be viewed as a search search space: all possible trees generated by the grammar search guided by the structure of the space and the input. search direction top-down: start with root category (S), choose expansions, build down to words. bottom-up: build subtrees over words, build up to S. Search algorithm/strategy: DFS, BFS, Recursive descent parsing, CKY Parsing Challenge: Structual Ambiguity Statistical Parsing Or probabilistic parsing, Build probabilistic models of syntactic knowledge and use some of this probabilistic knowledge to build efficient probabilistic parsers.\nmotivation: to solve the problem of disambiguation algorithm: probability CKY parsing evaluation: Compare the output constituency parser with golden standard tree, a constituent(part of the output parser) marked as correct if it spans the same sentence positions with the corresponding constituent in golder standard tree. Then we get the precision, recall and F1 measure. constituency: S-(0:10), NP-(0:2), VP-(0:9)\u0026hellip; Precission = (# correct constituents)/(# in parser output), recall = (# correct constituents)/(# in gold standard) Not a good evaluation, because it higher order constituent is marked wrong simply it contains a lower level wrong constituent. Dependency Parsing Constituency Dependency Morphology Ambiguity Structural ambiguity: Occurs when the grammar can assign more than one parse to a sentence. Attachment ambiguity: A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place. Coordination ambiguity: different sets of phrases can be conjoined by a conjunction like and. E.g green egg and bread. Coordination: The major phrase types discussed here can be conjoined with conjunctions like and, or, and but to form larger constructions of the same type. Global and local ambiguity global ambiguity: multiple analyses for a full sentence, like I saw the man with the telescope local ambiguity: multiple analyses for parts of sentence. the dog bit the child: first three words could be NP (but aren’t). Building useless partial structures wastes time. Open-class Closed-class Closed classes are those with relatively fixed membership\nprepositions: on, under, over, near, by, at, from, to, with determiners: a, an, the pronouns: she, who, I, others conjunctions: and, but, or, as, if, when auxiliary verbs: can, may, should, are particles: up, down, on, off, in, out, at, by numerals: one, two, three, first, second, third Open-class\nNouns, verbs, adjectives, adverbs Word Sense A discrete representation of an aspect of a word\u0026rsquo;s meaning. How: Distributional semantic models\nCollocation: A sequence of words or terms that co-occur more often than would be expected by chance.\nSynonym: 代名词, When two senses of two different words (lemmas) are identical, or nearly identical, the two senses are synonyms. E.g. couch/sofa vomit/throw up filbert/hazelnut car/automobile\nSimilarity: Or distance, a looser metric than synonymy. Two ways to measure similarity:\nThesaurus词库-based: are words nearby in hypernym hierarchy? Do words have similar definitions? Distributional: do words have similar distributional contexts Hyponym: 下义词, One sense is a hyponym of another sense if the first sense is more specific, denoting a subclass of the other. E.g. car is a hyponym of vehicle; dog is a hyponym of animal, and mango is a hyponym of fruit.\nHypernym: Superordinate, 上位词, vehicle is a hypernym of car, and animal is a hypernym of dog.\nWord Sense Disambiguation WSD, The task of selecting the correct sense for a word, formulated as a classification task.\nChose features: Directly neighboring words, content words, syntactically related words, topic of the text, part-of-speech tag, surrounding part-of-speech tags, etc \u0026hellip; Distributional Semantic Models Vector semantics(embeddings): The meaning of a word is represented as a vector.\nTwo words are similar if they have similar word contexts vector. Term-context matrix(Co-occurrence\tMatrices): a word/term is defined by a vector over counts of context words. The row represent words, columns contexts. Problem: simple frequency isn\u0026rsquo;t the best measure of association between words. One problem is that raw frequency is very skewed and not very discriminative. “the” and “of” are very frequent, but maybe not the most discriminative. Sulution: use Pointwise mutual information. Then the Co-occurrence\tMatrices is filled with PPMI, instead of raw counts. Measuring vectors similarity based on PPMI: Dot product(inner product): More frequent words will have higher dot products, which cause similarity sensitive to word frequency. Cosine: normalized dot product , Raw frequency or PPMI is non-negative, so cosine range [0,1]. Evaluation of similarity Intrinsic: correlation between algorithm and human word similarity ratings. Check if there is correlation between similarity measures and word frequency. Application: sentiment analysis, see lab8 Pointwise Mutual Information PMI: do events x and y co-occur more than if they were independent?\nPMI between two words: $$PMI(w, c) = \\log_2 \\frac{P(w,c)}{P(W)P(c)}$$ Compute PMI on a term-context matrix(using counts): $$PMI(x, y) = log_2 \\frac{N \\times count(x, y)}{Count(x) Count(y)}$$ p(w=information, c=data) = 6/19 p(w=information) = 11/19 p(c=data) = 7/19 PMI(information,data) = log2(6*19/(11*7)) PMI is biased towards infrequent events, solution: Add-one smoothing PPMI: Positive PMI, could better handle low frequencies PPMI = max(PMI,0)\nT-test The t-test statistic, like PMI, can be used to measure how much more frequent the association is than chance.\nThe t-test statistic computes the difference between observed and expected means, normalized by the variance. The higher the value of t, the greater the likelihood that we can reject the null hypothesis. Null hypothesis: the two words are independent, and hence P(a,b) = P(a)P(b) correctly models the relationship between the two words.$$t\\textrm{-}test(a,b) = \\frac{P(a,b) - P(a)P(b)}{\\sqrt{P(a)P(b)}}$$ Minimum Edit Distance the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. Algorithm: searching the shortest path, use Dynamic programming to avoid repeating, (use BFS to search the shortest path?)\nWordNet A hierarchically organizesd lexical database, resource for English sense relations\nSynset: The set of near-synonyms for a WordNet sense (for synonym set) Word2Vec Sentence Meaning Representation 我们假设语言表达具有意义表征，这些表征由用于表示常识的类型相同的东西组成。而创建这种表征并将其分配给输入的语言的任务，称为语义分析（Semantic Analysis）。The symbols in our meaning representations language (MRL) correspond to objects, properties, and relations in the world. ![](/images/meaning_representation.png \u0026ldquo;A list of symbols, two directed graphs, and a record structure: a sampler of meaning representations for \u0026ldquo;I have a car\u0026rdquo;. image from: Speech and Language Processing\u0026rdquo;) 上图展示了使用四种常用的MRL表达“I have a car”，第一行是First order logic，有向图和其文字信息是 Abstract Meaning Representation (AMR)，其余两种是Frame-Based 和 Slot-Filler。\nQualifications of MRL:\nCanonical form: sentences with the same (literal) meaning should have the same MR. Compositional: The meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined. Verifiable: Can use the MR of a sentence to determine the truth of the sentence with respect to some given model(knowledge base) of the world. Unambiguous: an MR should have exactly one interpretation. Inference and Variables: we should be able to verify sentences not only directly, but also by drawing conclusions based on the input MR and facts in the knowledge base. Expressiveness: the MRL should allow us to handle a wide range of meanings and express appropriate relationships between the words in a sentence. Lexical semantics: the meaning of individual words.\nLexical semantic relationships: Relations between word senses\n模型论 从仅是正式的陈述到能够告诉我们世界某些事态的陈述，我们期望 meaning representations 弥合这种差距。而提供这种保证的依据就是模型。模型是一种正式的结构，可以代表真是世界的特定事态。\n意义表达的词汇表包含两部分：\n非逻辑词汇表，由构成我们试图表达的世界的对象，属性和关系的开放式名称组成。如 谓语predicates, nodes, labels on links, or labels in slots in frames。 逻辑词汇表，由一组封闭的符号，运算符，量词，链接等组成，它们提供了用给定意义表示语言编写表达式的形式化方法。 所有非逻辑词汇的元素都需要在模型中有一个表示（属于模型的固定的且定义明确的一部分）。 • 对象 Objects denote elements of the domain • 属性 Properties denote sets of elements of the domain • 关系 Relations denote sets of **tuples of elements of the domain\nFirst-order Logic FOL, Predicate logic, meets all of the MRL qualifications except compositionality.\nTerm: represent objects. Expressions are constructed from terms in three ways: Constants in FOL refer to specific objects in the world being described. FOL constants refer to exactly one object. Objects can, however, have multiple constants that refer to them. Functions in FOL correspond to concepts that are often expressed in English as genitives(所有格) 如 \u0026ldquo;Frasca’s location\u0026rdquo;, 一般表达为LocationOf(Frasca). Functions provide a convenient way to refer to specific objects without having to associate a named constant with them. Variables, 允许我们对对象做出断言和推理，而不必引用任何特定的命名对象。Make statements about anonymous objects: making statements about a particular unknown object and making statements about all the objects in some arbitrary world of objects. Predicate(谓语, 谓词, 宾词, 述语): symbols that represent properties of entities and relations between entities.\nTerms can be combined into predicate-argument structures. Restaurant(Maharani) 指明Maharani的属性是Restaurant. Predicates with multiple arguments represent relations between entities: member-of(UK, EU) /N to indicate that a predicate takes N arguments: member-of/2 Logical connectives: create larger representations by conjoining logical formulas using one of three operators. ∨(or), ∧(and), ¬(not), ⇒(implies). \u0026ldquo;I only have five dollars and I don’t have a lot of time.\u0026rdquo;, Have(Speaker,FiveDollars) ∧ ¬Have(Speaker,LotOfTime)\nVariables and Quantifiers:\nExistential Quantifiers: (“there exists”), \u0026ldquo;a restaurant that serves Mexican food near ICSI\u0026rdquo; - ∃xRestaurant(x) ∧ Serves(x, MexicanFood) ∧ Near((LocationOf(x), LocationOf(ICSI)), 头部的∃告诉我们如何解读句中的变量x: 要让句子为真, 那么对于变量x至少存在一个对象。 Universal Quantifier:∀(“for all”). \u0026ldquo;All vegetarian restaurants serve vegetarian food.\u0026rdquo; - ∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood). A predicate with a variable among its arguments only has a truth value if it is bound by a quantifier: ∀x.likes(x, Gim) has an interpretation as either true or false.\nLambda Notation Extend FOL, to work with ‘partially constructed’ formula, with this form λx.P(x).\nλ-reduction: 应用于逻辑 term 以产生新的FOL表达式, 其中形参变量绑定到指定的term, 形式为λx.P(x)(A) -\u0026gt; P(A). E.g.：λx.sleep(x)(Marie) -\u0026gt; sleep(Marie)\n嵌套使用, Verbal (event) MRs：λx.λy.Near(x,y)(Bacaro) -\u0026gt; λy.Near(Bacaro,y), λz. λy. λx. Giving1(x,y,z) (book)(Mary)(John) -\u0026gt; Giving1(John, Mary, book) -\u0026gt; John gave Mary a book Problem: fixed arguments Requires separate Giving predicate for each syntactic subcategorisation frame(number/type/position of arguments). Separate predicates have no logical relation: if Giving3(a, b, c, d, e) is true, what about Giving2(a, b, c, d) and Giving1(a, b, c). Solution: Reification of events 事件具象化 Inference 推断的两种思路, forward chaining 和 backward chaining.\nforward chaining systems: Modus ponens(if - then) states that if the left-hand side(antecedent) of an implication rule is true, then the right-hand side(consequent) of the rule can be inferred. VegetarianRestaurant(Leaf) ∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood) then Serves(Leaf ,VegetarianFood) 随着单个事实被添加到知识库中，modus ponens用于触发所有适用的implication rules。优点是事实可以在被在需要时才在知识库中呈现，因为在某种意义上来说所有推断都是预先执行的。这可以大大减少后续的queries所需的时间，因为都应该是简单的查找。但缺点是那些永远用不到的事实也可能被推断和存储。\nbackward chaining:\n第一步是通过查看query公式是否存在知识库中, 来确认query是否为真。比如查询Serves(Leaf ,VegetarianFood). 如果没有，则搜索知识库中存在的适用implication rules。对涉及到的antecedent递归运行backward chaining. 比如发动搜索适用规则，从而找到规则∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood), 对于term Leaf而言, 对应的antecedent是VegetarianRestaurant(Leaf), 存在于知识库中. Prolog 就是采用backward chaining 推断策略的编程语言.\n虽然forward和backward推理是合理的，但两者都不完备。这意味着单独使用这些方法的系统无法找到有效的推论。完备的推理是解析 resolution, 但计算成本很高. 在实践中，大多数系统使用某种形式的chaining并把负担压到知识库开发人员去解码知识，以支持必要inference可以推论.\nReification of Events John gave Mary a book -\u0026gt; ∃e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z)\nReify: to “make real” or concrete, i.e., give events the same status as entities. In practice, introduce variables for events, which we can quantify over Entailment relations: automatically gives us logical entailment relations between events [John gave Mary a book on Tuesday] -\u0026gt; [John gave Mary a book] ∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) ∧ Time(e, Tuesday) -\u0026gt; ∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) Semantic Parsing Aka semantic analysis. Systems for mapping from a text string to any logical form.\nMotivation: deriving a meaning representation from a sentence. Application: question answering Method: Syntax driven semantic analysis with semantic attachments Syntax Driven Semantic Analysis Principle of compositionality: the construction of constituent meaning is derived from/composed of the meaning of the constituents/words within that constituent, guided by word order and syntactic relations. Build up the MR by augmenting CFG rules with semantic composition rules. Add semantic attachments to CFG rules. Problem: encounter invalide FOL for some (base-form) MR, need type-raise. Training Semantic Attachments E.g\nVP → Verb NP : {Verb.sem(NP.sem)} Verb.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) NP.sem = Meat -\u0026gt; VP.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) (Meat) = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) The MR for VP, is computed by applying the MR function to VP\u0026rsquo;s children.\nComplete the rule:\nS → NP VP : {VP.sem(NP.sem)} VP.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) NP.sem = AyCaramba -\u0026gt; S.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) (AyCa.) = ∃e. Serving(e) ∧ Server(e, AyCaramba) ∧ Served(e, Meat) Abstract Meaning Representation AMR是Rooted，带标签的digraph，易于人们阅读，易于程序处理。扩展了PropBank的Frames集合。AMR 的特点是将句子中的词抽象为概念，因而使最终的语义表达式与原始的句子没有直接的对应关系，对相同意思的不同句子能够抽象出相同的表达。\nKnowledge-driven AMRL\nThe Alexa Meaning Representation Language World Knowledge for Abstract Meaning Representation Parsing：依赖于wordnet，而中文本身就是字组词， Knowledge-driven Abstract Meaning Representation AMR is bias towards English.\nSlot-fillers/Template-filling/Frame 在文档中找到此类情况并填写模板位置。这些空位填充符可以包含直接从文本中提取的文本段，也可以包括通过额外处理从文本元素中推断出的诸如时间，数量或本体实体之类的概念\n许多文本包含事件的报告，以及可能的事件序列，这些报告通常对应于世界上相当普遍的刻板印象。这些抽象的情况或story，和 script（Schank and Abelson，1975）有关，由子事件，参与者及其角色的原型序列组成。类似的定义还有 Frame ( Minsky（1974），Hymes（1974）和Goffman（1974）大约在同一时间提出的一系列相关概念) 和 schemata（Bobrow and Norman，1975）。\nTopic Modelling 假如知道有什么主题，或者对主题的数量和分布做出先验假设，此时可以使用监督学习, 如朴素贝叶斯分类, 把文章处理成词袋(bag of words). 但假如不知道这些先验呢?\n就要依靠无监督学习, 比如聚类：Instead of using supervised topic classification – rather not fix topics in advance nor do manual annotation, Use clustering to teases out the topics. Only the number of topics is specified in advance.\n这就是主题建模(Topic Modelling), 一种常用的文本挖掘方法，用于发现文本中的隐藏语义结构。此时主题数量就是一个超参数, 通过主题建模，构建了单词的clusters而不是文本的clusters。因此，文本被表达为多个主题的混合，每个主题都有一定的权重。\n因为主题建模不再是用词频来表达, 而是用主题权重{Topic_i: weight(Topic_i, T) for Topic_i in Topics}, 所以主题建模也是一种 Dimensionality Reduction.\n主题建模也可以理解为文本主题的tagging任务, 只是无监督罢了.\n主题建模的算法:\n(p)LSA: (Probabilistic) Latent Semantic Analysis – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra. SVD假设了Gaussian distributed. LDA: latent Dirichlet allocation, 假设了multimonial distribution。 LDA LDA是pLSA的generalization, LDA的hyperparameter设为特定值的时候，就specialize成 pLSA 了。从工程应用价值的角度看，这个数学方法的generalization，允许我们用一个训练好的模型解释任何一段文本中的语义。而pLSA只能理解训练文本中的语义。（虽然也有ad hoc的方法让pLSA理解新文本的语义，但是大都效率低，并且并不符合pLSA的数学定义。）这就让继续研究pLSA价值不明显了。\nLatent Dirichlet allocation(LDA): each document may be viewed as a mixture of various topics where each document is generated by LDA. A topic is a distribution over words generate document: Randomly choose a distribution over topics For each word in the document randomly choose a topic from the distribution over topics randomly choose a word from the corresponding topic (distribution over the vocabulary) training: repeat until converge assign each word in each document to one of T topics. For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t) Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d) Inference: LDA没法做精确inference，只有近似算法，比如variational inference。 LDA模型代码\nEvaluation Extrinsic Evaluation Use something external to measure the model. End-to-end evaluation, the best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves.\nPut each model in a task: spelling corrector, speech recognizer, MT system Run the task, get an accuracy for A and for B How many misspelled words corrected properly How many words translated correctly Compare accuracy for A and B Unfortunately, running big NLP systems end-to-end is often very expensive.\nIntrinsic Evaluation Measures independenly to any application. Train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. Which means whichever model assigns a higher probability to the test set\nHuman Evaluation E.g to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to gold labels match. We will refer to these human labels as the gold labels.\nPrecision, Recall, F-measure To deal with unbalanced lables Application: text classification, parsing. Evaluation in text classification: the 2 by 2 contingency table, golden lable is true or false, the classifier output is positive or negative. Precision: Percentage of positive items that are golden correct, from the view of classifier\nRecall: Percentage of golden correct items that are positive, from the view of test set.\nF-measure\nMotivation: there is tradeoff between precision and recall, so we need a combined meeasure that assesses the P/R tradeoff. The b parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of b \u0026gt; 1 favor recall, while values of b \u0026lt; 1 favor precision. Balanced F1 measure with beta =1, F = 2PR/(P+R) Confusion Matrix Recalled that confusion matrix\u0026rsquo;s row represent golden label, column represent the classifier\u0026rsquo;s output, to anwser the quesion：for any pair of classes(c1,c2), how many test sample from c1 were incorrectly assigned to c2\nRecall: Fraction of samples in $c_1$ classified correctly, $\\frac{CM(c_1, c_1)}{\\sum_jCM(c_1, j)}$ Precision: fraction of samples assigned $c_1$ that are actually $c_1$, $\\frac{CM(c_1, c_1)}{\\sum_iCM(i, c_1)}$ Accuracy: $\\frac{\\sum diagnal}{all}$ Correlation When two sets of data are strongly linked together we say they have a High Correlation. Correlation is Positive when the values increase together, and Correlation is Negative when one value decreases as the other increases.\nPearson correlation: covariance of the two variables divided by the product of their standard deviations.$$r = \\frac{\\sum_{i=1}^n(x_i - \\overrightarrow{x})(y_i - \\overrightarrow{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\overrightarrow{x})^2} \\sqrt{\\sum_{i=1}^n(y_i - \\overrightarrow{y})^2}}$$ Spearman correlation: the Pearson correlation between the rank values of the two variables Basic Text Processing Regular Expressions NLP工作必备技能(考试不需要).\n一些练习Regular Expressions的有趣网站: https://alf.nu/RegexGolf https://regexr.com/\nWord Tokenization NLP task needs to do text normalization:\nSegmenting/tokenizing words in running text Normalizing word formats Segmenting sentences in running text they lay back on the San Francisco grass and looked at the stars and their\nType: an element of the vocabulary. Token: an instance of that type in the actual text. 英文比较简单. 中文有一个难点, 需要分词.\n","permalink":"https://congchan.github.io/posts/inf-course-note-accelerated-natural-language-processing/","summary":"\u003cp\u003e爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh\u003c/p\u003e\n\u003cp\u003eReferences:\n\u003ca href=\"http://www.inf.ed.ac.uk/teaching/courses/anlp/\"\u003eAccelerated natural language processing\u003c/a\u003e\n\u003ca href=\"https://www.inf.ed.ac.uk/teaching/courses/anlp/review/review_ay17.html\"\u003eANLP revision guide\u003c/a\u003e\n\u003ca href=\"https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html\"\u003eLecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning\u003c/a\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"概率模型-probability-model\"\u003e概率模型 Probability Model\u003c/h2\u003e\n\u003cp\u003e概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率\u003c/p\u003e\n\u003cp\u003e估算概率（Probability Estimation）一般使用最大似然估计（MLE，相关频率）：\u003c/p\u003e\n$$p(x_i) = \\frac{Count(x_i)}{\\sum_{i=0}^nCount(x_i)}$$\u003ch3 id=\"平滑smoothing\"\u003e平滑Smoothing\u003c/h3\u003e\n\u003cp\u003e一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。\u003c/p\u003e\n\u003ch2 id=\"language-modeling\"\u003eLanguage modeling\u003c/h2\u003e\n\u003cp\u003eTo compute the probability of sentence /sequence of words $P(w_1, w_2, w_3...)$, or to predict upcomming words $P(w|w_1, w_2, w_3...)$\u0026hellip; a language model is also a probability model.\u003c/p\u003e","title":"Inf Course Note - Accelerated Natural Language Processing"},{"content":"爱丁堡大学信息学院课程笔记 Natural Language Understanding, Informatics, University of Edinburgh\nReferences: Natural language understanding CS224n: Natural Language Processing with Deep Learning Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning\nMeaning representations 意思的表达有很多方法。一种有效的表示单词的含义的方法是 distributional semantic.\nSemantics (from Ancient Greek: σημαντικός sēmantikos, \u0026ldquo;significant\u0026rdquo;) is the linguistic and philosophical study of meaning, in language, programming languages, formal logics, and semiotics.\n语义学 Semantics 在语言学中的研究目的在于找出语义表达的规律性、内在解释、不同语言在语义表达方面的个性以及共性；与计算机科学相关的语义学研究在于机器对自然语言的理解。\nTradition solution of usable meaning in a computer: Use e.g. WordNet, a resource containing lists of synonym sets and hypernyms.\nTo convert natural language into values that computer understands, represent words as discrete symbols: Words can be represented by one-hot vectors, Vector dimension is the vocabulary. But there is no natural notion of similarity for one-hot vectors!\nSo learn to encode similarity in the vectors themselves.\nThe core idea is representing words by their context, building a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\nDistributional models of meaning = vector-­space models of meaning = vector semantics. word vectors = word embeddings = word representations.\nFour kinds of vector models Sparse vector representations: 1, Mutual-­information weighted word co-­occurrence matrices\nDense vector representations: 2, Singular value decomposition (SVD): A special case of this is called LSA - Latent Semantic Analysis 3, Neural­‐network­‐inspired models (skip­‐grams, CBOW) 4, Brown clusters\nPrediction-­based models learn embeddings as part of the process of word prediction. Train a neural network to predict neighboring words. The advantages: · Fast, easy to train (much faster than SVD) · Available online in the word2vec package · Including sets of pretrained embeddings\nWord representation and Word2vec Word2vec is a framework for learning word vectors representation. Idea: 1, We have a large corpus of text 2, Every word in a fixed vocabulary is represented by a vector 3, Go through each position t in the text, which has a center word c and context (\u0026ldquo;outside\u0026rdquo;) words o 4, Use the similarity of the word vectors for c and o to calculate the probability of o given c (or vice versa) 5, Keep adjusting the word vectors to maximize this probability\n在上面第四点, 如果是给定中心词，计算上下文词, 那么就是 Skip-grams model, 比如 Given word $w_t$, in a context window of 2C words, predict 4 context words [wt-2, wt-1, wt+1, wt+2] Skip-grams 给予模型跳词能力，比如 \u0026ldquo;I hit the tennis ball\u0026rdquo; 有三个trigrams: \u0026ldquo;I hit the\u0026rdquo;, \u0026ldquo;hit the tennis\u0026rdquo;, \u0026ldquo;the tennis ball\u0026rdquo;. 但是，这个句子也同样包含一个同样重要但是N-Gram无法提取的trigram:\u0026ldquo;hit the ball\u0026rdquo;. 而使用 skip-grams 允许我们跳过 \u0026ldquo;tennis\u0026rdquo; 生成这个trigram.\n反之，给定 bag-of-words context, predict target word, 那就是 Continuous Bag of Words, CBOW model.\n缺点：因为output size 等于 vocabulary，而 softmax 分母中需要求和每一个词的 output size × hidden units 的内积， 计算会非常昂贵。解决办法是使用负采样 negative sampling。\nWord2vec的本质是遍历语料库的每一个词$w_i$，捕捉$w_i$与其上下文位置目标词的同时出现的概率。\n目标函数 Obejective funtion (cost or loss function) J(θ): For each position $t = 1, … , T$, predict context words within a window of fixed size m, given center word, use chain rule to multiply all the probability to get the likelihood $L(θ)$: The θ is the vectors representations, which is the only parameters we needs to optimize(其实还有其他hyperparameters，这里暂时忽略).\nThe loss function is the (average) negative log likelihood: Minimizing objective function ⟺ Maximizing predictive accuracy.\nThe problem is how to calculate $P(w_{t+j} \\mid w_t; θ)$:\n每个词由两个向量表示（Easier optimization. Average both at the end）： $v_w$ when w is a center word, $u_w$ when w is a context word.\nThen for a center word c and a \u0026ldquo;outside\u0026rdquo; word o: The numerator contains dot product, compares similarity of o and c, larger dot product = larger probability. The denominator works as a normalization over entire vocabulary.\n高频词二次采样 subsampling 二次采样是指当决定是否选取一个词作为样本时，它被选择的概率反比于它出现的概率，这样不仅可以降低无意义但高频的词(\u0026ldquo;the\u0026rdquo;, \u0026ldquo;a\u0026quot;等)的重要性，也可以加快采样速度。\n$$P(w_i) = (\\sqrt{\\frac{z(w_i)}{0.001}} + 1) \\cdot \\frac{0.001}{z(w_i)}$$ $z(w_i)$ 是词$w_i$在语料库中的占比，如果\u0026quot;peanut\u0026quot;在10亿语料库中出现了1,000次, 那么z(\u0026ldquo;peanut\u0026rdquo;) = 1e-6. Negative sampling 负采样是指每个训练样本仅更新模型权重的一小部分：only the output that represents the positive class(1) + other few randomly selected classes(0) are evaluated. 该论文指出\n负采样5-20个单词适用于较小的数据集，对于大型数据集只需要2-5个单词。\n修改目标函数，选择k个负样本（即除了概率最高的那个目标词之外的其他词）：\n这样可以最大化真正的外部词出现的概率，最小化随机负采样的词概率。\n负面样本的选择是基于 unigram 分布 $f(w_i)$: 一个词作为负面样本被选择的概率与其出现的频率有关，更频繁的词更可能被选作负面样本。 $$P(w_i) = \\frac{ {f(w_i)}^{3/4} }{\\sum_{j=0}^{n}\\left( {f(w_j)}^{3/4} \\right) }$$ 负采样的优点是： · Training speed is independent of the vocabulary size · Allowing parallelism. · 模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。\n与传统的NLP方法比较 在word2vec出现之前，NLP使用经典且直观的共生矩阵（co-occurrence matrix）来统计词语两两同时出现的频率，参考ANLP - Distributional semantic models。缺点也明显，词汇量的增加导致矩阵增大，需要大量内存，随之而来的分类模型出现稀疏性问题，模型不稳定。虽然可以使用SVD来降维，但是一个n×m矩阵的计算成本是O(mn2)浮点数（当n\u0026lt;m），还是非常大的。而且很难并入新词或新文档。 目前融合了两种方法的优点的Glove是最常用的。\nTODO(Glove) Morphological Recursive Neural Network (morphoRNN) Limitation of word2vec: • Closed vocabulary assumption • Cannot exploit functional relationships in learning:\n如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher\nTo walk closer to open vocabulary, use compositional representations based on morphemes. Instead of word embedding, embed morphemes - the smallest meaningful unit of language. Compute representation recursively from morphemes, word embedding 由 morphemes embedding 拼接而来.![](/images/Morphological_Recursive_Neural_Network.png \u0026ldquo;Morphological Recursive Neural Network. A vector representation for the word \u0026ldquo;unfortunately\u0026rdquo; is constructed from morphemic vectors: unpre, fortunatestm, lysuf. Dotted nodes are computed on-the-fly and not in the lexicon. image from: http://www.aclweb.org/anthology/W13-3512\")\n与基础版的morphoRNN结构相同，Context-insensitive Morphological RNN model (cimRNN) 考察 morphoRNN 在不参考任何上下文信息情况下， 仅仅用 morphemic representation 构造词向量的能力。训练时，给每个词xi定义损失函数s(xi)为新构造的词向量pc(xi)和参考词向量pr(xi)之间的欧几里得距离平方 该cimRNN模型没有机会改进可能被估计不足的罕见词的表达.\nContext-sensitive Morphological RNN (csmRNN) 在学习语素组成时同时参考语境信息，在训练过程中，神经网络顶层的更新将一直反向传播直至底层的语素层。 Compositional character representations 在自然语言处理中使用 word 作为基本单位的问题在于词汇量太大了，所以几乎所有主流模型都会省略很多词，比如Bengio的RNNs语言模型就把所有出现频率\u0026lt;3的单词统一标记为一个特殊词。但这样的操作也只是把词汇量降到了16,383。又比如word2vec模型只考虑出现频率最高的30,000个词。\n所以寻找其他有限集合的语言单位成为替代选择，比如字母 character（更确切地说是 unicode code points），比如前面提到的 Morphemes，还有其他比如 Character n-grams，Morphological analysis等，这些可以统称为 subwords units。\n然后再通过 subwords 来重构 word representation，进而构建整个文本的meaning representation.\n构建 word representation 最简单的方法就是把 subwords vectors 相加、平均或者拼接等，但更好的是使用非线性的方法，比如 Bidirectional LSTMs, Convolutional NNs 等。 哪种方式构建 subword representations 比较好？ 在 word representation 的重构中，涉及了几个变量: 1, Subword Unit 2, Composition Function • Linear Vector operation • Bi-LSTMs • Convolutional NNs\n3, Language Typology\nType example Morphology analysis Fusional (English) \u0026ldquo;reads\u0026rdquo; read-s read-3SG.SG Agglutinative (Turkish) \u0026ldquo;If I read …\u0026rdquo; oku-r-sa-m read-AOR.COND.1SG Root\u0026amp;Pattern (Arabic) \u0026ldquo;he wrote\u0026rdquo; k(a)t(a)b(a) write-PST.3SG.M Reduplication (Indonesian) \u0026ldquo;children\u0026rdquo; anak~anak child-PL 除了语言模型外, 其他NLP任务如SQuAd问答数据集上的很多优秀模型，也会加入character embedding.\n但目前 Character-level models 并不具有触及实际 morphology 的模型预测能力。\nMulti-word language representations Neural bag-of-words models: · Simply average (or just sum) word vectors, · Can improve effectiveness by putting output through 1+ fully connected layers (DANs) · Recurrent neural networks(LSTM/GRU): cannot capture phrases without prefix context, and empirically, representations capture too much of last words in final vector – focus is LM next word prediction · Convolutional Neural Network: compute vectors for every h-word phrase, often for several values of h. Example: \u0026ldquo;the country of my birth\u0026rdquo; computes vectors for: the country, country of, of my, my birth, the country of, country of my, of my birth, the country of my, country of my birth. Not very linguistic, but you get everything!\nData-dependent composition: Recursion is natural for describing language, Phrases correspond to semantic units of language.\nHow to map longer phrases into the same vector space? 利用复合性原理 principle of compositionality:\n在数学、语义学和语言哲学中，复合性原理是指，一个复杂表达式的意义是由其各组成部分的意义以及用以结合它们的规则来决定的。\nRecursive neural nets, a tree structure. For Structure Prediction: Inputs: two candidate children\u0026rsquo;s representations Outputs: 1, The semantic representation if the two nodes are merged. 2, Score of how plausible the new node would be.\n神经网络语言模型 如何构建一个神经网络语言模型? 语言模型的目的是输入一串字符, 输出下一个字符的概率分布, 可以使用 fixed-window neural Language Model, 类似于N-Gram, 仅考虑前(n-1)个窗口长度序列, \u0026ldquo;as the proctor started the clock the students opened their _\u0026rdquo; 得到定长的输入序列, 而 Feedforward neural networks 的输入就是要求固定长度的向量. 用前馈神经网络做语言模型的优点（相对于N-Gram）就是没有了稀疏性问题，而且模型的大小也控制在 O(n)（N-Gram是O(exp(n))）\n固定长度的前馈神经网络的固有缺陷就是它要求输入和输出都是固定长度的, 仅考虑前的(n-1)长度的序列, 很多时候会丢失NLP中的长距离依赖信息, 跟N-Gram的有一样的缺陷。而且实际的应用中语句的长度是不固定的，最好有一个神经网络可以接受任意长度的输入序列, 输出任意长度的序列。循环神经网络 (Recurrent neural networks, aka RNNs) 就可以解决这个问题.\n循环神经网络语言模型 不同于前馈神经网络使用输入序列的每一个词单独训练一行(或一列, 取决于矩阵的设计)参数矩阵, RNNs的设计核心是用输入序列的每一个词, 反复地训练同一个参数, 即\u0026quot;共享参数\u0026rdquo;. 因为参数共享: 1, 模型大小不会随着输入序列长度增加而增加。 2, 每一步的计算，理论上都使用到了之前的历史信息，所以理论上可以更好的捕捉长距离依赖（但实际上表现并不好，看后面的梯度消失与爆炸）. 3, 模型有更好的泛化能力\n使用基于Softmax的RNNs语言模型等同于解决矩阵分解问题, 参考Breaking the Softmax Bottleneck: A High-Rank RNN Language Model。\n循环神经网络语言模型使用损失函数评估模型表现: 损失函数 loss function on step t is usual 交叉熵 cross-entropy between predicted probability distribution and the true next word.\n传统的统计语言模型使用困惑度(perplexity)来评估模型表现，但其实降低困惑度等价于减小损失函数.\n神经网络语言模型的学习能力 Character models are good at reduplication (no oracle, though), works well on language with reduplication patterns like Indonesian, Malay. Character NLMs learn word boundaries, memorize POS tags.\nWhat do NLMs learn about morphology? 1, Character-level NLMs work across typologies, but especially well for agglutinative morphology. 2, predictive accuracy is not as good as model with explicit knowledge of morphology (or POS). 3, They actually learn orthographic similarity of affixes, and forget meaning of root morphemes accordong to qualitative analyses. 4, More generally, they appear to memorize frequent subpatterns\n总的来说，神经网络处理自然语言的能力并不特殊，表现的性能，跟神经网络本身的长处相匹配，如泛化、模式匹配、端到端应用的能力等。\nDependency parsing 语言学里有两种角度看待语法结构 - Constituency and Dependency：\nConstituency: phrase structure grammar, 从句子成分构造的角度看，capture the configurational patterns of sentences，即把句子的语法理解为词组成分的递归嵌套. 可以用 context-free grammars (CFGs) 来表达语法规则，就是语法树。 Dependency syntax: 主要是从语义的角度来看，显示哪些单词依赖于（一般指修改或作为参数其参数）哪些单词。特别用于区分动词的主格（subject position or with nominative inflection）宾格（object position or with accusative inflection）. Dependencies can be identified even in non-configurational languages. A sentence dependency structure explains the dependency relation between its words: represented as a graph with the words as its nodes, linked by directed, labeled edges, with the following properties: • connected: every node is related to at least one other node, and (through transitivity) to ROOT; • single headed: every node (except ROOT) has exactly one incoming edge (from its head); • acyclic: the graph cannot contain cycles of directed edges. Dependency trees 有两种，如果dependency graph中有edges交叉则是non-projective, 反之则是 projective。更确切的定义是：A dependency tree is projective wrt. a particular linear order of its nodes if, for all edges h → d and nodes w, w occurs between h and d in linear order only if w is dominated by h.\nA non-projective dependency grammar is not context-free. Motivation for Dependency parsing: • context-free parsing algorithms base their decisions on adjacency; • in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective); • we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present).\nEvaluation: accuracy (# correct dependencies with or ignore label)).\nGraph-based dependency parsing Based on maximum spanning trees (MST parser), views syntactic structure as a set of constraints\nIntuition as tagging problem: since each word has exactly one parent, the possible tags are the other words in the sentence (or a dummy node called root). If we edge factorize the score of a tree so that it is simply the product of its edge scores, then we can simply select the best incoming edge for each word.\nThe tartget function is to find the highest scoring dependency tree in the space of all possible trees for a sentence. The score of dependency tree y for sentence x is: $$s(x,y) = \\sum_{(i,j)\\in y} s(i,j)$$ $x = x_1...x_n, y$ is a set of dependency edges, with $(i, j) ∈ y$ if there is an edge from $x_i$ to $x_j$.\nScoring edges with a neural network The function g(aj, ai) computes an association score telling us how much word wi prefers word wj as its head. Association scores are a useful way to select from a dynamic group of candidates, 跟注意力机制的similarity score 异曲同工，方程的形式也很相似。\nParsing 算法：\nstart with a totally connected graph G, i.e., assume a directed edge between every pair of words; find the maximum spanning tree (MST) of G, i.e., the directed tree with the highest overall score that includes all nodes of G; this is possible in O(n2) time using the Chu-Liu-Edmonds algorithm; it finds a MST which is not guaranteed to be projective; 1, Each node j in the graph greedily selects the incoming edge with the highest score s(i,j) 2, If result were a tree, it would have to be the maximum spanning tree; If not, there must be a cycle. 3, Break the cycle by replacing a single incoming edge to one of the nodes in the cycle. To choose the node, decide recursively by identifying the cycle and contract it into a single node and recalculate scores of incoming and outgoing edges. Now call CLE recursively on the contracted graph. MST on the contracted graph is equivalent to MST on the original graph. 这里是指先识别出循环体saw ⇄ john②，然后在这个循环体范围内，使用CLE找出 root 进出这个循环体的最大概率路线(root → saw → john = 40) \u0026gt; (root → john → saw = 29)③； 4, Greedily collect incoming edges to all nodes, find out to be a tree and thus the MST of the graph. 把循环体以及其包含的nodes合并为一个node wjs，并且已经有了进出wjs的最大概率路径，这样就可以在整个图上继续运行CLE算法找出最大概率路线(root → wjs → mary = 70) \u0026gt; (root → mary → wjs = 40)④. Chu-Liu-Edmonds (CLE) Algorithm:\nIn graph theory, Edmonds\u0026rsquo; algorithm or Chu–Liu/Edmonds\u0026rsquo; algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching). It is the directed analog of the minimum spanning tree problem\nTransition-based dependency parsing An extension of shift-reduce parsing (MALT parser), views syntactic structure as the actions of an automaton: • for a given parse state, the transition system defines a set of actions T which the parser can take; • if more than one action is applicable, a machine learning classifier is used to decide which action to take; • just like in the MST model, this requires a mechanism to compute scores over a set of (possibly dynamic) candidates. if si is the ith top element on stack, and bi the ith element on buffer, then we have the following transitions: • LEFT-ARC(l): adds arc s1 → s2 with label l and removes s2 from stack (|s| ≥ 2); • RIGHT-ARC(l): adds arc s2 → s1 with label l and removes s1 from stack (|s| ≥ 2); • SHIFT: moves b1 from buffer to stack; recondition: |b| ≥ 1. 总的来说就是：父节点保留在stack中; 从始至终 root 一直都是父节点；从 buffer 中把候选词一个一个 push 到stack中，根据 classifier 预测的结果，分辨出哪个候选词是子节点，并把子节点 pop 出 stack；直到清空 buffer，stack 中只剩下 root。\nComparing MST and transition-based parsers: Both require dynamic classifiers, and these can be implemented using neural networks, conditioned on bidirectional RNN encodings of the sentence.\nThe MST parser selects the globally optimal tree, given a set of edges with scores; • it can naturally handle projective and non-projective trees;\nA transition-based parser makes a sequence of local decisions about the best parse action; • it can be extended to projective dependency trees by changing the transition set;\nAccuracies are similar, but transition-based is faster;\nRecurrent neural network grammars (RNNGs) Widespread phenomenon: Polarity items can only appear in certain contexts, e.g. \u0026ldquo;anybody\u0026rdquo;.\nIn linguistics, a polarity item is a lexical item that can appear only in environments associated with a particular grammatical polarity – affirmative or negative. A polarity item that appears in affirmative (positive) contexts is called a positive polarity item (PPI), and one that appears in negative contexts is a negative polarity item (NPI).\nThe environment in which a polarity item is permitted to appear is called a \u0026ldquo;licensing context\u0026rdquo;.\nThe lecture that I gave did not appeal to anybody; The lecture that I gave appealed to anybody.\n也许\u0026quot;anybody\u0026quot;出现的条件是前面出现过\u0026quot;not\u0026rdquo;，那么应该可以使用 RNNs 模型来解码这点信息。然而: The lecture that I did not give appealed to anybody.\n这说明 Language is hierarchical: The licensing context depends on recursive structure (syntax)。不能简单根据\u0026quot;not\u0026quot;是否出现来判断，而是需要看\u0026quot;not\u0026quot;修饰的成分，也就是说要考虑语法的合理。这就给文本生成任务（或者说构建语言模型）带来挑战。\nRecurrent neural network grammars (Dyer et al. 2016)提出了一种具有明确短语结构的语言模型 RNNGs。\nRNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions.\n就是在使用 RNNs 构建语言模型，除了考虑历史词信息, 还会生成历史的语法结构, 并以此为参考预测语法结构和词语,以保证生成的语言符合语法结构。这里的语法是针对 phrase structure (constituency) grammars，所以 RNNGs 也是一种 constituency parsing：\nGenerate symbols sequentially using an RNN Add some \u0026ldquo;control symbols\u0026rdquo; to rewrite the history periodically Periodically \u0026ldquo;compress\u0026rdquo; a sequence into a single \u0026ldquo;constituent\u0026rdquo; Augment RNN with an operation to compress recent history into a single vector (-\u0026gt; \u0026ldquo;reduce\u0026rdquo;) RNN predicts next symbol based on the history of compressed elements and non-compressed terminals (\u0026ldquo;shift\u0026rdquo; or \u0026ldquo;generate\u0026rdquo;) RNN must also predict \u0026ldquo;control symbols\u0026rdquo; that decide how big constituents are 首先注意到，如果有序地去遍历语法树，得出的就是一个序列： What information can we use to predict the next action, and how can we encode it with an RNN?\nUse an RNN for each of:\nPrevious terminal symbols Previous actions Current stack contents 最后得出的 stack 就是完整的语法树（以序列的形式）。 Syntactic Composition 人们通过较小元素的语义组合来解释较大文本单元的含义 - 实体，描述性词语，事实，论据，故事. When compressing \u0026ldquo;The hungry cat\u0026rdquo; into a single composite symbol, use Bi-LSTM to encode (NP The hungry cat). 基于此可以递归地解码更复杂的短语，比如(NP The (ADJP very hungry) cat), 只需要把原来的hungry替换为(ADJP very hungry)即可。\n这种递归地堆栈符号的构建行为映射了符号对应的树结构 除此了使用 Bi-LSTM 解码，还可以使用 Attention：Replace composition with one that computes attention over objects in the composed sequence, using embedding of NT for similarity.\nImplement RNNGs Stack RNNs\nAugment a sequential RNN with a stack pointer Two constant-time operations push - read input, add to top of stack, connect to current location of the stack pointer pop - move stack pointer to its parent A summary of stack contents is obtained by accessing the output of the RNN at location of the stack pointer Training RNNs:\nEach word is conditioned on history represented by a trio of RNNs backpropagate through these three RNNs, and recursively through the phrase structure S → NP VP. 完整的RNNGs模型，用 softmax 计算下一个 action 的概率分布： Parameter Estimation RNNGs jointly model sequences of words together with a \u0026ldquo;tree structure\u0026rdquo;.\nAny parse tree can be converted to a sequence of actions (depth first traversal) and vice versa (subject to wellformedness constraints).\nInference problems of RNNGs An RNNG is a joint distribution p(x,y) over strings (x) and parse trees (y), i.e. it jointly predicts the word, and the parse context together. So the model will still generate the syntactic information and the next word but we can discard the additional outputs if all we want is the language model.\nTwo inference questions: • What is $p(x)$ for a given x? - language modeling • What is $argmax_yp(y | x)$ for a given x? - parsing\nThe model predicts the next action (NT() GEN() or REDUCE in generative mode, NT() SHIFT or REDUCE in discriminative mode). The set of actions completely determines the string and tree structure, so we can get their joint probability by multiplying over the probabilities of all actions.\nIn discriminative mode, the input is a string of words, and the model cannot generate words, but instead \u0026ldquo;consumes\u0026rdquo; the words in the input buffer. The model can be used as a parser (find the maximum prob. tree, i.e., $argmax_yP(y \\mid x)$).\nIn generative mode, there is a respective GEN() action for every word, so the word is predicted with the action. To be a language model (find the maximum prob. sentence/assign probabilities to a sentence, i.e., $p(x)$), we must marginalize over trees to get the probability of the sentence. This is intractable so is approximated with importance sampling by sampling from a discriminatively trained model.\nimportance sampling Assume we\u0026quot;ve got a conditional distribution $q(y | x)$ s.t. (i) $p(x, y) \u003e 0 \\Rightarrow q(y | x) \u003e 0$ (ii) $y \\sim q(y | x)$ is tractable and (iii) $q(y | x)$ is tractable\nThe importance weights $w(x,y) = \\frac{p(x, y)}{q(y | x)}$\n从句子到语法树的seq2seq模型 其实从句子到语法的映射类似于一个seq2seq模型。而直接的把语法树以字符序列的形式表达，使用简单的 RNNs 直接构建句子到语法序列的 seq2seq 模型效果也不错，比如： input: The hungry cat meows . output: S( NP( _ _ _ ) VP( _ ) _ ) Vanilla RNNs 在模式匹配和计数方面非常出色，经验证明，训练有素的 seq2seq 模型通常会输出格式良好的字符串，见这篇文章 section 3.2\n但潜在的问题是，seq2seq 模型并不要求输出是有正确括号字符（数量对齐，位置正确）。另外，理论上单个RNN也只能记忆括号结构一定的有限深度，因为 RNNs 只有固定的有限数量的隐藏单元。例如，它将为这些输出分配非零概率： S( NP( _ _ ) VP ( _ ) _ ) S( NP( _ _ _ ) VP ( _ ) _ ) ) )\n理想情况下，模型应该给任何不完整的输出分配零概率。使用 RNNGs 是因为它本身能够履行这些限制， 保证生成完整正确的语法树。\n从中可以看出，seq2seq模型可以用于快速原型和 baseline 搭建，但如果遇到要求输出遵守某些约束条件的问题，则需要直接执行这些约束条件。\nParsing Parsing is a fundamental task in NLP. But what is parsing actually good for?\nParsing breaks up sentences into meaningful parts or finds meaningful relationships, which can then feed into downstream semantic tasks: • semantic role labeling (figure out who did what do whom); • semantic parsing (turn a sentence into a logical form); • word sense disambiguation (figure out what the words in a sentence mean); • compositional semantics (compute the meaning of a sentence based on the meaning of its parts).\nSemantic role labeling (SRL) 虽然可以使用 Distributional semantics 表达含义，只是 Distributional semantics 比较擅长处理相似度，且无法很明确地处理复合性 Compositionality。\n在数学、语义学和语言哲学中，复合性原理是指，一个复杂表达式的意义是由其各组成部分的意义以及用以结合它们的规则来决定的。\n为了能够处理复合性和推理，我们需要象征性和结构化的意义表示。\n虽然语言是无穷无尽的，句子是无限的集合，而人脑的能力却是有限的，但人们总能够理解一个句子的含义（假如人们熟知表达句子的语言）. 因此, 对于 semantics, 语义肯定是有限的集合, 这样才能确定句子的确切意义.\nIn generative grammar, a central principle of formal semantics is that the relation between syntax and semantics is compositional.\nThe principle of compositionality (Fregean Principle): The meaning of a complex expression is determined by the meanings of its parts and the way they are syntactically combined.\nSemantic role labeling means identifying the arguments (frame elements) that participate in a prototypical situation (frame) and labeling them with their roles;\nSRL task is typically broken down into a sequence of sub-tasks:\nparse the training corpus; match frame elements to constituents; extract features from the parse tree; train a probabilistic model on the features. 所谓 frame elements 是针对 Frame Semantics 而言的。\nSRL provides a shallow semantic analysis that can benefit various NLP applications; no parsing needed, no handcrafted features.\nFrame Semantics 表达词义，除了 Firth, J.R. (1957) 的 \u0026ldquo;a word is characterized by the company it keeps\u0026rdquo;（也即是 Distributional semantics）之外, 还有 Charles J. Fillmore 的 Frame Semantics.\nThe basic idea is that one cannot understand the meaning of a single word without access to all the essential knowledge that relates to that word.\nA semantic frame is a collection of facts that specify \u0026ldquo;characteristic features, attributes, and functions of a denotatum, and its characteristic interactions with things necessarily or typically associated with it.\u0026rdquo;\nA semantic frame can also be defined as a coherent structure of related concepts that are related such that without knowledge of all of them, one does not have complete knowledge of any one; they are in that sense types of gestalt.\nProposition Bank 完整的句子表达了命题 propositions, 也即一个主张. 比如\u0026quot;John smokes\u0026quot;这个句子的命题如果是真的,那么\u0026quot;John\u0026quot;在这里一定是某个\u0026quot;smokes\u0026quot;的人, 也就是必须是NP.\n在现代哲学、逻辑学、语言学中，命题是指一个判断（陈述）的语义（实际表达的概念），这个概念是可以被定义并观察的现象。命题不是指判断（陈述）本身。当相异判断（陈述）具有相同语义的时候，他们表达相同的命题。例如，雪是白的（汉语）和Snow is white（英语）是相异的判断（陈述），但它们表达的命题是相同的。在同一种语言中，两个相异判断（陈述）也可能表达相同命题。例如，刚才的命题也可以说成冰的小结晶是白的，不过，之所以是相同命题，取决于冰的小结晶可视为雪的有效定义。\nPropBank is a version of the Penn Treebank annotated with semantic roles. More coarse-grained than Frame Semantics: End-to-end SRL system 基本的结构单元是Bi-LSTM，用法是： · a standard LSTM layer processes the input in forward direction; · the output of this LSTM layer is the input to another LSTM layer, but in reverse direction; 这些Bi-LSTM单元可以叠加起来构造更深层的神经网络.\nThe input (processed word by word) features are: • argument and predicate: the argument is the word being processed, the predicate is the word it depends on; • predicate context (ctx-p): the words around the predicate; also used to distinguish multiple instances of the same predicate; • region mark (mr): indicates if the argument is in the predicate context region or not; • if a sequence has np predicates it is processed np times.\nOutput: semantic role label for the predicate/argument pair using IOB tags (inside, outside, beginning). Training: • Word embeddings are used as input, not raw words; • the embeddings for arguments, predicate, and ctx-p, as well as mr are concatenated and used as input for the Bi-LSTM; • the output is passed through a conditional random field (CRF); allows to model dependencies between output labels; • Viterbi decoding is used to compute the best output sequence\nModel learns \u0026ldquo;syntax\u0026rdquo;(Maybe): it associates argument and predicate words using the forget gate:\nSemantic Parsing Semantic Parsing 指语义分析，把文本解析为任意的逻辑形式(一种 meaning representation)，比如 first-order logic(FOL). Sam likes Casey - likes(Sam, Casey); Anna's dog Mr. PeanutButter misses her - misses(MrPB, Anna) ∧ dog(MrPB); Kim likes everyone - ∀x.likes(x, Kim). Predicate-argument structure is a good match for FOL, as well as structures with argument-like elements (e.g. NPs). Determiners, quantifiers (e.g. \u0026ldquo;everyone\u0026rdquo;, \u0026ldquo;anyone\u0026rdquo;), and negation can be expressed in FOL.\nHowever, much of natural language is unverifiable, ambiguous, non-canonical. That makes it hard to represent the wide-coverage meaning of arbitrary NL. Closed domains are easier, and can sometimes be harvested automatically, e.g. GEOQUERY dataset.\nThis leads to a proliferation of domain-specific MRs. · Pairs of NL sentences with structured MR can be collected, e.g. IFTTT dataset (Quirk et al. 2015). · WikiTableQuestions · Google\u0026rsquo;s knowledge graph\nViewing MR as a string, semantic parsing is just conditional language modeling. Trainable alternative to compositional approaches: encoder-decoder neural models. The encoder and decoder can be mixed and matched: RNN, top-down tree RNN. Works well on small, closed domains if we have training data, but there are many unsolved phenomena/ problems in semantics.\nAbstract meaning representation (AMR) • The edges (ARG0 and ARG1) are relations • Each node in the graph has a variable • They are labeled with concepts • d / dog means \u0026ldquo;d is an instance of dog\u0026rdquo; The dog is eating a bone (e / eat-01 :ARG0 (d / dog) :ARG1 (b / bone))\nThe dog wants to eat the bone (want-01 :ARG0 (d / dog) :ARG1 (e / eat-01 :ARG0 d :ARG1 (b / bone)))\nCoreference Charles just graduated, and now Bob wants Anna to give him a job. Q: who does him refer to?\nMetonymy Westminster decided to distribute funds throughout England, Wales, Northern Island, and Scotland decided(Parliament, …)\nImplicature That cake looks delicious - I would like a piece of that cake.\nEven more phenomena… • Abbreviations (e.g. National Health Service=NHS) • Nicknames (JLaw=Jennifer Lawrence) • Metaphor (crime is a virus infecting the city) • Time expressions and change of state • Many others\nTODO(指代消解 Coreference Resolution) Unsupervised Part-of-Speech Tagging Parts-of-speech(POS), word classes, or syntactic categories, 一般指八个词性：noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, 有时候是 numeral, article or determiner. 1, noun 名詞 ( n. ) 2, pronoun 代名詞 ( pron. ) 3, verb 動詞 ( v. ) 4, adjective 形容詞 ( adj. ) 5, adverb 副詞 ( adv. ) 6, preposition 介系詞 ( prep. ) 7, conjunction 連接詞 ( conj. ) 8, interjection 感歎詞 ( int. )\nTagging is a task that take a sentence, assign each word a label indicating its syntactic category (part of speech).\nOne common standard label is Penn Treebank PoS tagset.\nDT - Determiner 定语 IN - Preposition or subord. conjunction NN - Noun, singular or mass NNS - Noun, plural NNP - Proper noun, singular RB - Adverb TO - to VB - Verb, base form VBZ - Verb, 3rd person singular present\nIn supervised POS tagging, the input is the text and a set of allowed POS labels. The training data contains input and output examples. The output is a guess, for each word in the test data, which POS label it should have.\nA common approach is to use an HMM. To train it, choose parameters θ that maximize $P(x,y \\mid θ)$, the probability of the training data given the parameters. This is maximum likelihood estimation and it was covered in ANLP. You can use the model to predict y for each x in the test data by solving $P(y \\mid x,θ)$ using the Viterbi algorithm.\nA consequence of supervised training with MLE is that the model will only learn non-zero probability for tag-word pairs that actually appear in the data. Hence, if \u0026ldquo;the\u0026rdquo; is only ever tagged with DT in the training data, then the model will learn that the probability of producing \u0026ldquo;the\u0026rdquo; from any other tag is zero. This means that many word tokens will be (empirically) unambiguous, which is one of the things that makes supervised POS tagging easy.\nRNNs 虽然也可以处理序列模型, 但是神经网络需要目标函数, 没有目标无法计算损失, 就无法调整参数, 也就是\u0026quot;监督学习\u0026quot;.\nCurrent PoS taggers are highly accurate (97% accuracy on Penn Treebank). But they require manually labelled training data, which for many major language is not available. Hence motivated for unsupervised PoS tagging.\nIn unsupervised POS tagging, the input is the text and the number of clusters. The training data contains only input examples. The output is a guess, for each word in the text, which cluster the word belongs to. For example:\nNumber of clusters: 50 Input x: The hungry cat meows Output y: 23 45 7 18 What we hope is that the cluster labels will correlate with true POS labels; that is, that tokens labeled 23 will tend to be determiners, that clusters label 45 will tend to be adjectives, and so on.\n这个时候可以使用隐马尔科夫模型, 这个\u0026quot;隐\u0026quot;就是针对没有目标可以参考这种情况.\nHidden Markov Models The unsupervised tagging models here are based on Hidden Markov Models (HMMs). To train it, choose parameters θ that maximize $P(x \\mid θ)$, the probability of the training data given the parameters.\nThe parameters θ = (τ, ω) define: • τ : the probability distribution over tag-tag transitions; • ω: the probability distribution over word-tag outputs. The parameters are sets of multinomial distributions: • $ω = ω^{(1)} . . . ω^{(T)}$: the output distributions for each tag; • $τ = τ^{(1)} . . . τ^{(T)}$: the transition distributions for each tag; • $ω^{(t)} = ω_1^{(t)}. . . ω_W^{(t)}$: the output distribution from tag $t$; • $τ^{(t)} = τ_1^{(t)}. . . τ_T^{(t)}$: the transition distribution from tag $t$.\nAnother way to write the model, often used in statistics and machine learning:\n$w_i | t_i = t ∼ Multinomial(ω^{(t)})$\nSo as tag, given that $t_{i−1} = t$, the value of $t_i$ is drawn from a multinomial distribution with parameters $τ^{(t)}$.\nHow to estimate ω and τ without supervision. This is still maximum likelihood estimation, but notice that it\u0026rsquo;s more difficult because the tags y are unobserved, so you must marginalize them out.\nFor estimation (i.e., training the model, determining its parameters), we need a procedure to set θ based on data. Rely on Bayes Rule: \\begin{equation}\\begin{split} P(θ|w)\u0026amp;=\\frac{P(w|θ)P(θ)}{P(w)}\\\\ \u0026amp;∝P(w|θ)P(θ)\\\\ \\end{split}\\end{equation} Choose the θ that maximize the likelihood $P(w|θ)$. Basically, we ignore the prior. In most cases, this is equivalent to assuming a uniform prior.\nTo do this, you can use expectation maximization (EM), a variant of MLE that can cope with unobserved data, which was also covered in ANLP. For examples, forward-backward algorithm for HMMs, inside-outside algorithm for PCFGs, k-means clustering.\nFor inference (i.e., decoding, applying the model at test time), we need to know θ and then we can compute $P(t, w)$: E-step: use current estimate of θ to compute expected counts of hidden events ($n(t,t^{\\prime})$, $n(t,w)$). M-step: recompute θ using expected counts.\nYou can then use the trained model to predict y for each x in the test data by solving $P(y \\mid x,θ)$ using the Viterbi algorithm.\nBut EM often fails, even very small amounts of training data have been show to work better than EM. One consequence of unsupervised training with EM is that every word can be assigned to any cluster label. This makes things really difficult, because it means every word is ambiguous. The basic assumptions of EM (that any tag-word or tag-tag distribution is equally likely) make this even more difficult.\nInstead, use Bayesian HMM with Gibbs sampling.\nBayesian HMM When training HMM model, we are not actually interested in the value of θ, we could simply integrate it out. This approach is called Bayesian integration. Integrating over θ gives us an average over all possible parameters values.\nThe Bayesian HMM is simply an alternative way to solve the unsupervised POS tagging problem. The input and output is the same. But instead of learning θ, we directly solve $P(y \\mid x)$. Note that we don\u0026rsquo;t need to learn θ (though we could) - in this setting, we integrate it out, after first supplying some information about the tag-tag and word-tag distributions encoded in θ. Specifically, we tell the model that a sparse distribution is much more likely than a uniform distribution. We do this by defining a distribution $P(θ)$, and this gives us a new model, $P(y,x \\mid θ)×P(θ)$. By integrating out θ we can solve the unsupervised tagging problem directly.\nExample: we want to predict a spinner result will be \u0026ldquo;a\u0026rdquo; or not? • Parameter θ indicates spinner result: $P(θ = a) = .45$, $P(θ = b) = .35$, $P(θ = c) = .2$; • define t = 1: result is \u0026ldquo;a\u0026rdquo;, t = 0: result is not \u0026ldquo;a\u0026rdquo;; • make a prediction about one random variable (t) based on the value of another random variable (θ).\nMaximum likelihood approach: choose most probable θ, $\\hat{θ} = a$, and $P(t = 1|\\hat{θ}) = 1$, so we predict $t = 1$.\nBayesian approach: average over θ, $P(t = 1) = \\sum_θ P(t = 1|θ)P(θ) = 1(.45) + 0(.35) + 0(0.2) = .45$, predict t = 0.\nAdvantages of Bayesian integration: • accounts for uncertainty as to the exact value of θ; • models the shape of the distribution over θ; • increases robustness: there may be a range of good values of θ; • we can use priors favoring sparse solutions (more on this later).\nDirichlet distribution Choosing the right prior can make integration easier. A $K$-dimensional Dirichlet with parameters $α = α_1 . . . α_K$ is defined as:\n$$ P(θ) = \\frac{1}{Z} \\prod_{j=1}^K θ_j^{α_j−1} $$We usually only use symmetric Dirichlets, where $α_1 . . . α_K$ are all equal to β. We write Dirichlet(β) to mean $Dirichlet(β, . . . , β)$.\n注意到这是一个二维的概率密度图. $β\u003e1$意味着更喜欢均值分布, 此时$θ$大概率落在$0.5$附近,因为$θ_1+θ_2=1$, 所以此时$θ_1, θ_2$概率均等. 如果$β=1$, $θ_1$的任何取值是等概率的, 等于说任何$θ_1,θ_2$的组合概率都是均等的.\nTo Bayesianize the HMM, we augment with it with symmetric Dirichlet priors: To simplify things, use a bigram version of the Bayesian HMM; If we integrate out the parameters θ = (τ, ω), we get: Use these distributions to find $P(t|w)$ using an estimation method called Gibbs sampling.\nResults: Integrating over parameters is useful in itself, even with uninformative priors $(α = β = 1)$;\n总结： · Bayesian HMM improves performance by averaging out uncertainty; · allows us to use priors that favor sparse solutions as they occur in language data. · Using a tag dictionary is also really helpful. We still have no labeled training data, but if we only allow each word to be tagged with one of the labels that appears in the dictionary, then most word-tag pairs will have probability zero. So this is a very different way of supplying information to the unsupervised model that is very effective.\nBias in NLP The social impact of NLP Outcome of an NLP experiment can have a direct effect on people\u0026rsquo;s lives, e.g.\n频繁出现亚马逊 Alexa 突然发出诡异笑声，给多名用户造成困惑和恐慌, 因为人们谈话中偶然包含 trigger 词：\u0026ldquo;Alexa, laugh\u0026rdquo; 而发出 - 亚马逊的解决方案是把 trigger 改为更难触发的 \u0026ldquo;Alexa, can you laugh\u0026rdquo; Chatbot 对于人们敏感问题的不恰当回答, 比如 \u0026ldquo;Should I kill myself?\u0026rdquo; - \u0026ldquo;Yes.\u0026quot;，这些回答对患有心理障碍的人群或者青少年儿童带来非常大的危害。 Microsoft 的 AI chatbot 上线仅一天, 就通过 twitter 和人交谈并学会涉及种族, 性别歧视等的话语, 典型的 \u0026ldquo;garbage in, garbage out\u0026rdquo; 现象. 其他涉及数据隐私等问题 语言的特性，导致NLP涉及的社会伦理问题非常多, 而且影响非常大： · 语言传递着信息、偏见，是政治性的、权力的工具, 同时比其他技术带有更明显的拟人化、人格化倾向，这可能给个人生活带来不便或危害，给整个社会带来舆论影响。 · Any dataset carries demographic bias: latent information about the demographics of the people that produced it. That excludes people from other demographics.\n同时人类本身的认知容易加深偏见: The availability heuristic: the more knowledge people have about a specific topic, the more important they think it must be. Topic overexposure creates biases that can lead to discrimination and reinforcement of existing biases. E.g. NLP focused on English may be self-reinforcing.\nNLP 实验本身容易加深偏见： • Advanced grammar analysis can improve search and educational NLP, but also reinforce prescriptive linguistic norms. • Stylometric analysis can help discover provenance of historical documents, but also unmask anonymous political dissenters.\nNLP 技术可能被不恰当地使用： • Text classification and IR can help identify information of interest, but also aid censors. • NLP can be used to discriminate fake reviews and news, and also to generate them.\nWord embeddings contain human-like biases word2vec learns semantic/ syntactic relationships, also keep company with unsavoury stereotypes and biases? • Man:Woman - King:Queen • Man:Doctor - Woman:Nurse • Man:Computer Programmer - Woman:Homemaker\nMeasure bias using implicit association tests: 1, Compute similarity of group1 and stereotype1 word embeddings. Cosine similarity is use to measure association (in place of reaction time). 2, Compute similarity of group1 and stereotype 2 word embeddings. 3, Null hypothesis: if group1 is not more strongly associated to one of the stereotypes, there will be no difference in the means. 4, Effect size measured using Cohen\u0026rsquo;s d. 5, Repeat for group 2.\nExperiments • Uses GloVe trained on Common Crawl—a large-scale crawl of the web. • Removed low frequency names. • Removed names that were least \u0026ldquo;name-like\u0026rdquo; (e.g. Will) algorithmically. • Each concept is represented using a small set of words, designed for previous experiments in the psychology literature.\nResult: · flowers associate with pleasant, insects associate with unpleasant. $p \u003c 10^{−7}$ · Men\u0026rsquo;s names associate with career, women\u0026rsquo;s names associate with family. $p \u003c 10^{−3}$ · European American names associate with pleasant, African American names associate with unpleasant. $p \u003c 10^{−8}$\n这些结果的确真实地反映人类社会的现状。但大部分性别方面的偏见其实是反映了目前的社会分工，无所谓高低贵贱；人种的偏见倒是反映了历史问题对现在的影响，这种偏见是不符合道德的。人对于其他生物的偏见，虽然是没必要的，但人类的确倾向于喜爱行为\u0026quot;可爱\u0026rdquo;，外形\u0026quot;美好\u0026quot;的生物，比如大熊猫就是比鳄鱼受欢迎。\n偏见的存在不一定合理。哪些偏见是不合理的，才是人们更应该去思考和讨论的地方。\nDebiasing word embeddings Bolukbasi. et. al., 2016. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings提供了一个思路:\n确认偏见的方向 中和抵消偏见: 对于非定性的词（如\u0026quot;医生\u0026quot;），通过投射来消除偏见 等价：让father - mother和boy - girl等距，让定性词间的距离只有性别的距离；或者让doctor - woman和doctor - man等距，消除非定性词的性别偏见。 什么词需要抵消偏见: 训练一个线性分类器来确定词是非定性还是非定性的, 结果当然是大部分英语词都是非定性的.\nIf analogies reveal a gender dimension, use analogies on specific seed pairs to find it. y 轴下面的词属于定性词, 不需要中性化, 而y轴之上的词则需要进行中性化处理.\n不同的偏见, 需要不同的 seed words; 一种偏见, 可以有多种 seed words 选择: 除了用\u0026quot;She-He\u0026quot;作为性别偏见的基准, 还有其他选择.\n编码器—解码器 Sequence-to-sequence 和注意力机制 当输入输出都是不定长序列时, 比如机器翻译这种任务，需要使用 Sequence-to-sequence（seq2seq）或者 encoder-decoder 神经网络结构。这种结构可以通过一种方法叫注意力机制来显著提高性能。\n编码器—解码器 Sequence-to-sequence（seq2seq） 编码器：所谓编码，就是把不定长的输入序列输入RNN，以得出某种定长的编码信息。 解码器：所谓解码，就是把编码器编码后的信息（一般取编码器的RNN最终时刻的隐含层变量）输入到解码器的RNN中，每个t时刻的输出既取决于之前时刻（t-1）的输出又取决于编码信息。等同于一个以解码信息作为条件概率生成目标语言句子的语言模型。\n所以 seq2seq 本质是一个条件概率语言模型：语言模型是指解码器每次会预测下一个出现的单词，条件概率是指预测是基于编码后的源句子。\n注意力 在传统的seq2seq模型中，解码器各个时刻都使用相同的编码信息，这就要求解码器把源输入序列的所有信息都解码并整合到最后时刻的隐含状态中，这个是很大的信息瓶颈。而人们知道，在实际任务中，比如机器翻译，目标句子的不同单词，一般只对应源句子的某一部分而已。如果能够让解码器在解码时，在不同时刻专注于源输入序列的不同部分，那么就可以突破这个瓶颈。\n对于解码器的每一时间步的隐含状态st，可以衡量其与编码器的所有时间步隐含状态h0……et的相似性(或score评分) e = α(s, h)，简单的评分方式是元素间相乘, e = s*h（Bahanau的论文提供了更复杂的形式), 也可以参考论文Effective Approaches to Attention-based Neural Machine Translation探讨的集中评分方式, 这篇论文提供了一种 Bilinear 形式的相似性评分法, 就是在s和h之间以点乘的形式插入一个交互矩阵 interaction matrix. 对得出的评分求加权平均a = softmax(e), 得出的权值分布也称注意力权重 通过注意力权重把编码器隐含状态加权求和，得到注意力输出 A = Σah 最后把注意力输出和对应时间步的解码器隐含状态st拼接在一起 [A;st]，作为解码器rnn的隐含层. ","permalink":"https://congchan.github.io/posts/inf-course-note-natural-language-understanding/","summary":"\u003cp\u003e爱丁堡大学信息学院课程笔记 Natural Language Understanding, Informatics, University of Edinburgh\u003c/p\u003e\n\u003cp\u003eReferences:\n\u003ca href=\"http://www.inf.ed.ac.uk/teaching/courses/nlu/\"\u003eNatural language understanding\u003c/a\u003e\n\u003ca href=\"http://web.stanford.edu/class/cs224n/syllabus.html\"\u003eCS224n: Natural Language Processing with Deep Learning\u003c/a\u003e\n\u003ca href=\"https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html\"\u003eLecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning\u003c/a\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"meaning-representations\"\u003eMeaning representations\u003c/h2\u003e\n\u003cp\u003e意思的表达有很多方法。一种有效的表示单词的含义的方法是 distributional semantic.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSemantics (from Ancient Greek: σημαντικός sēmantikos, \u0026ldquo;significant\u0026rdquo;) is the linguistic and philosophical study of meaning, in language, programming languages, formal logics, and semiotics.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e语义学 Semantics 在语言学中的研究目的在于找出语义表达的规律性、内在解释、不同语言在语义表达方面的个性以及共性；与计算机科学相关的语义学研究在于机器对自然语言的理解。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eTradition solution of usable meaning in a computer: Use e.g. WordNet, a resource containing lists of synonym sets and hypernyms.\u003c/p\u003e","title":"Inf Course Note - Natural Language Understanding"},{"content":"爱丁堡大学信息学院课程笔记 Parallel Programming Language and Systems, Informatics, University of Edinburgh\nReference: http://www.inf.ed.ac.uk/teaching/courses/ppls/ CMU 15213: Introduction to Computer Systems (ICS) Computer Systems: A Programmer\u0026rsquo;s Perspective A Comprehensive MPI Tutorial Resource A chapter on MPI from Ian Foster\u0026rsquo;s online Book Designing and Building Parallel Programs\nIntroduction to parallel computer architecture Covering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model.\nBridging the gap between the parallel applications and algorithms which we can design and describe in abstract terms and the parallel computer architectures (and their lowest level programming interfaces) which it is practical to construct.\nThe ability to express parallelism (a.k.a concurrency) concisely, correctly and efficiently is important in several contexts: • Performance Computing: parallelism is the means by which the execution time of computationally demanding applications can be reduced. In the era of static (or even falling) clock speeds and increasing core count, this class is entering the computing mainstream. • Distributed Computing: when concurrency is inherent in the nature of the system and we have no choice but to express and control it. • Systems Programming: when it is conceptually simpler to think of a system as being composed of concurrent components, even though these will actually be executed by time-sharing a single processor.\nParallel Architecture Two types (mainstream):\nShared Memory architectures: in which all processors can physically address the whole memory, usually with support for cache coherency (for example, a quad or oct core chip, or more expensive machines with tens or hundreds of cores) Multicomputer architectures: in which processors can only physically address their \u0026ldquo;own\u0026rdquo; memory (for example, a networked cluster of PCs), which interact with messages across the network. Increasingly, systems will span both classes (e.g. cluster of manycore, or network-onchip manycores like the Intel SCC), and incorporate other specialized, constrained parallel hardware such as GPUs.\nReal parallel machines are complex, with unforseen semantic and performance traps. We need to provide programming tools which simplify things, but without sacrificing too much performance.\nShared Memory Architectures Uniform Memory Access (UMA) architectures have all memory \u0026ldquo;equidistant\u0026rdquo; from all CPUs. For NUMA performance varies with data location. NUMA is also confusingly called Distributed Shared Memory as memory is physically distributed but logically shared. Memory consistency challenge: when, and in what order should one processor public updates to the shared memory? Exactly what and when it is permissible for each processor to see is defined by the Consistency Model, which is effectively a contract between hardware and software, must be respected by application programmers (and compiler/library writers) to ensure program correctness.\nDifferent consistency models trade off conceptual simplicity against cost (time/hardware complexity):\nSequential consistency: every processor \u0026ldquo;sees\u0026rdquo; the same sequential interleaving of the basic reads and writes. This is very intuitive, but expensive to implement. Release consistency: writes are only guaranteed to be visible after program specified synchronization points (triggered by special machine instructions). This is less intuitive, but allows faster implementations. Shared memory architectures also raise tricky performance issues: The unit of transfer between memory and cache is a cache-line or block, containing several words. False sharing occurs when two logically unconnected variables share the same cache-line. Updates to one cause remote copies of the line, including the other variable, to be invalidated.\nMulticomputer architectures Lack of any hardware integration between cache/memory system and the interconnect. Each processor only accesses its own physical address space, so no consistency issues. Information is shared by explicit, co-operative message passing Performance/correctness issues include the semantics of synchronization and constraints on message ordering.\nParallel Applications and Algorithms Three well-known parallel patterns: Bag of Tasks, Pipeline and Interacting Peers.\nHere using the co, \u0026lt; \u0026gt;, await notation.\n在co oc内的代码, 顺序是任意的.\n# 这里暂时用 // 表示并行的代码 co a=1; // a=2; // a=3; ## all happen at the same time, What is a in the end? oc To answer the above question, we need to define Atomic Actions: Reads and writes of single variables as being atomic. For more than one statements, if they appear to execute as a single indivisible step with no visible intermediate states, they are atomic, must be enclosed in \u0026lt; \u0026gt;.\na=0; co a=1; // a=2; // b=a+a; ## what is b? oc The above code has no \u0026lt; \u0026gt;, each value accessed in an expression is a read. Each assignment is a write. Thus, b could be 0, 1, 2, 3, or 4.\na=0; co a=1; // a=2; // \u0026lt;b=a+a;\u0026gt; oc Now the only outcomes for b are 0, 2 or 4.\nSequential memory consistency (SC) To make agreement on such inconsistency, we define the sequential memory consistency (SC), to be consistent with the following rules:\nordering of atomic actions (particularly reads and writes to memory) from any one thread have to occur in normal program order atomic actions from different threads are interleaved arbitrarily (ie in an unpredictable sequential order, subject only to rule 1) It doesn\u0026rsquo;t mean that SC programs have to be executed sequentially! It only means that the results we get must be the same as if the program had been executed in this way.\nAwait The await notation \u0026lt; await (B) S \u0026gt; allows us to indicate that S must appear to be delayed until B is true, and must be executed within the same atomic action as a successful check of B.\na=0; flag=0; co {a=25; flag=1;} // \u0026lt;await (flag==1) x=a;\u0026gt; ## x = 25 oc However, it is not guaranteed that, an await statement is executed right after its condition becomes true. If other atomic actions make the condition false again, before the await executes, it will have to wait for another chance.\nThe Bag-of-Tasks Example: Adaptive Quadrature, compute an approximation to the shaded integral by partitioning until the 梯形 trapezoidal approximation is \u0026ldquo;good enough\u0026rdquo;, compared with the sum of its two sub-divided trapezoidals\u0026rsquo;s area. area = quad (a, b, f(a), f(b), (f(a)+f(b))*(b-a)/2); The recursive calls to quad do not interfere with each other. So we can parallelize the program by changing the calls to\n# 简单地并行 co larea = quad(left, mid, f(left), f(mid), larea); // rarea = quad(mid, right, f(mid), f(right), rarea); oc In practice, there is very little work directly involved in each call to quad. The work involved in creating and scheduling a process or thread is substantial (much worse than a simple function call), program may be swamped by this overhead.\nUsing the Bag of Tasks pattern: a fixed number of worker processes/threads maintain and process a dynamic collection of homogeneous \u0026ldquo;tasks\u0026rdquo;. Execution of a particular task may lead to the creation of more task instances.\n# Bag of Tasks pattern co [w = 1 to P] { while (all tasks not done) { get a task; execute the task; possibly add new tasks to the bag; } } 1, Shared bag: contains task(a, b, f(a), f(b), area) 2, Get a task: remove a record from the bag, either: • adds its local area approximation to the total • or creates two more tasks for a better approximation (by adding them to the bag).\nAdvantage: 1, It constraints the number of processes/threads to avoid overhead. 2, Useful for independent tasks and to implement recursive parallelism 3, Naturally load-balanced: each worker will probably complete a different number of tasks, but will do roughly the same amount of work overall.\nBag of Tasks Implementation: The challenge is to make accessing the bag much cheaper than creating a new thread. With a shared address space, a simple implementation would make the bag an atomically accessed shared data structure.\nshared int size = 1, idle = 0; shared double total = 0.0; bag.insert (a, b, f(a), f(b), approxarea); co [w = 1 to P] { while (true) { \u0026lt; idle++; \u0026gt; \u0026lt; await ( size \u0026gt; 0 || idle == P ) ## 检测 termination if (size \u0026gt; 0) { ## get a task bag.remove (left, right ...); size--; idle--; } else break; \u0026gt; ## the work is done mid = (left + right)/2; ..etc.. ## compute larea, etc if (fabs(larea + rarea - lrarea) \u0026gt; EPSILON) { ## create new tasks \u0026lt; bag.insert (left, mid, fleft, fmid, larea); bag.insert (mid, right, fmid, fright, rarea); size = size + 2; \u0026gt; } else \u0026lt; total = total + larea + rarea; \u0026gt; } } Detecting termination: 不能仅仅因为 bag 空了就认为可以结束了, 因为还可能有还在工作的 workers 未来会产生新的任务. 所以需要让 workers 有能力把自己的工作完成状况告知 bag. When bag is empty AND all tasks are done; All tasks are done when all workers are waiting to get a new task.\nIf a bag of tasks algorithm has terminated, there are no tasks left. However, the inverse is not true. I.e. no tasks in a bag could mean that one of the workers is still processing a task which can lead to creation of multiple new tasks. To solve this problem, workers could have the ability to notify the master/bag once they finish the current task. As a result, an implementation of bag of tasks can then contain a count of idle and active works to prevent early termination\nA more sophisticated implementation (with less contention) might internally have a collection of bags, perhaps one per worker, with task-stealing to distribute the load as necessary.\nWith message passing, a simple scheme might allocate an explicit \u0026ldquo;farmer\u0026rdquo; node to maintain the bag. Again, a more sophisticated implementation could distribute the bag and the farmer, with task-stealing and termination checking via messages.\nPipeline Patterns. Example: The Sieve of Eratosthenes algorithms for finding all prime numbers.\nTo find all prime numbers in the range 2 to N. The algorithm write down all integers in the range, then repeatedly remove all multiples of the smallest remaining number. Before each removal phase, the new smallest remaining number is guaranteed to be prime.\nNotice that, it is not necessarily to wait one Sieve completed then start another. As long as one Sieve stage finds out one candidate number could not be divided exactly by the sieve number, it could generate a new stage with this candidate number as Sieve. And different sieve just remove the multiples of its own Sieve number.\n# a message-passing style pipeline pseudocode main() { # the generator spawn the first sieve process; for (i=2; i\u0026lt;=N; i++) { send i to first sieve; } send -1 to first sieve; # a \u0026#34;stop\u0026#34; signal } sieve() { int myprime, candidate; receive myprime from predecessor and record it; do { receive candidate from predecessor; if (candidate == -1) {send -1 to successor if it exists} else if (myprime doesn\u0026#39;t divide candidate exactly) { if (no successor yet) spawn successor sieve process; send candidate to successor sieve process; } } while (candidate != -1) } 每一个数(2-N)都可能作为筛子, 筛掉能整除这个筛子的其他数，而筛子之间是互相独立的，所以可以以流水线模式 pipeline patterns来并行操作，动态生成筛子。最开始最小的数字2会成为筛子。筛子可以理解为不同的工序，其余数字从小到大逐一通过这些工序加工（在 Sieve of Eratosthenes 问题中变为筛选排查），无法被筛子整除的数字会被传递到下个筛子（如果没有下一个筛子，则以这个数字创建新的筛子），这样保证生成的筛子就都是素数了。虽然工序是按顺序过的，但是所有工序可以同时对不同的产品（数字）开工，从而达到并行目的。\nFor pipeline patterns, the potential concurrency can be exploited by assigning each operation (stage of the pipeline) to a different worker and having them work simultaneously, with the data elements passing from one worker to the next as operations are completed. Despite the dependencies (order constraints) of the processing steps, the pipeline threads can work in parallel by applying their processing step to different data (products).\nThink of pipeline patterns as the factory assembly line. We need to pick out prime number from a range of numbers N, each number is passed into a sequence of stages, each stages checks a pass in number based on the stages\u0026rsquo;s Sieve. The numbers that finally pass all stages without being removed is a prime number.\nPipelines are composed of a sequence of threads, in which each thread\u0026rsquo;s input is the previous thread\u0026rsquo;s output, (Producer-Consumer relationships).\nThe advantages of pipeline patterns is that construction of pipeline stages is dynamic and data-dependent.\nTo allow production and consumption to be loosely synchronized, we will need some buffering in the system.\nThe programming challenges are to ensure that no producer overwrites a buffer entry before a consumer has used it, and that no consumer tries to consume an entry which doesn\u0026rsquo;t really exist (or re-use an already consumed entry)\nInteracting Peers Pattern Models of physical phenomena are often expressed as a system of partial differential equations. These can be approximately solved by \u0026ldquo;finite difference methods\u0026rdquo; which involve iteration on a matrix of points, in an interacting peers pattern. The \u0026ldquo;compute\u0026rdquo; step usually involves only a small number of neighbouring points. The termination test looks for convergence.\nWe could use a duplicate grid and barriers to enforce correct synchronization between iterations:\nshared real grid[n+2, n+2], newgrid[n+2, n+2]; shared bool converged; local real diff; co [i = 1 to n, j = 1 to n] { initialise grid; do { barrier(); ## before resetting test converged = true; ## provisionally newgrid[i,j] = (grid[i-1,j] + grid[i+1,j] + grid[i,j-1] + grid[i,j+1])/4; ## compute new value diff = abs (newgrid[i,j] - grid[i,j]); ## compute local change barrier(); ## before converged update if (diff \u0026gt; EPSILON) converged = false; ## any one will do grid[i,j] = newgrid[i,j]; ## copy back to real grid barrier(); ## before global check } while (not converged); } A barrier() in ppls makes any thread that arrive here has to wait all the other threads arriving here.\n以方腔热对流的模拟计算模型为例，每个网格节点$(i,j)_{t+1}$ 的更新依赖于上一个迭代时间点的$(i,j)_t$以及其临近几个点的值，创建最多跟网格点数量一样的threads，然后并行地计算网格点的新值，更新的值用一个buffer层来缓存，用barrier()来保证所有网格点的更新值都计算完毕，再检查收敛情况，再用一个barrier()保证所有buffer层的值都更新到原网格上，再决定是否进行下一次计算。\nSingle Program Multiple Data (SPMD): A programming style, all processes execute more or less the same code, but on distinct partitions of the data.\nOther Patterns Other candidate patterns include MapReduce (championed by Google), Scan, Divide \u0026amp; Conquer, Farm as well as application domain specific operations.\nShared Variable Programming In the shared-memory programming model, tasks share a common address space, which they read and write asynchronously. An advantage of this model from the programmer\u0026rsquo;s point is that the notion of data \u0026ldquo;ownership\u0026rdquo; is lacking, so there is no need to specify explicitly the communication of data between tasks. Program development can often be simplified.\nThere are two fundamentally different synchronization in shared variable programming. Mutual Exclusion and Condition Synchronization.\nMutual Exclusion Atomic actions, at most one thread is executing the critical section at a time. Prevent two or more threads from being active concurrently for some period, because their actions may interfere incorrectly. For example, we might require updates to a shared counter (e.g., count++) to execute with mutual exclusion.\nCritical Sections problem A simple pattern of mutual exclusion occurs in the critical section problem - when n threads execute code of the following form, in which it is essential that at most one thread is executing statements in the critical section at a time (because of potentially unsafe access to shared variables)\nco [i = 1 to n] { while (something) { lock(l); #entry section critical section; unlock(l); #exit section non-critical section; } } Design code to execute before (entry protocol) and after (exit protocol) the critical section to make the critical section atomic. If one thread lock the critical section, no one(thread) else could lock it or unlock it anymore, until the thread unlock it.\nImportant properties:\nMutual exclusion: When a thread is executing in its critical section, no other threads can be executing in their critical sections. Absence of Deadlock (or Livelock): If two or more threads are trying to enter the critical section, at least one succeeds. A deadlock is a state in which each member of a group is waiting for some other member to take action, such as sending a message or more commonly releasing a lock, so that neither of them take action. 类似两个人相遇互不相让, 没人肯挪动. Livelock is a condition that takes place when two or more programs change their state continuously, with neither program making progress. 类似两个人相遇同时往相同方向避让.\nAbsence of Unnecessary Delay: If a thread is trying to enter its critical section and the other threads are executing their non-critical sections, or have terminated, the first thread is not prevented from entering its critical section. Eventual Entry (No Starvation): A thread that is attempting to enter its critical section will eventually succeed. May not matter in some \u0026ldquo;performance parallel\u0026rdquo; programs - as long as we are making progress elsewhere. Simple implementation of each lock with a shared boolean variable: if false, then one locking thread can set it to true and be allowed to proceed. Other attempted locks must be forced to wait.\n# model assumes that the l = false; # write is already atomic # This might fail if the model is more relaxed than SC. lock_t l = false; co [i = 1 to n] { while (something) { \u0026lt; await (!l) l = true; \u0026gt; # guarantee the others waiting critical section; l = false; # unlock the lock, open the critical section non-critical section; } } To implement the \u0026lt; await (!l) l = true; \u0026gt;, we rely on some simpler atomic primitive, implemented with hardware support. There are many possibilities, including \u0026ldquo;Fetch-and-Add\u0026rdquo;, \u0026ldquo;Test-and-Set\u0026rdquo; and the \u0026ldquo;Load-Linked, Store-Conditional\u0026rdquo; pairing.\nTest-and-Set (TS) instruction Behaving like a call-by-reference function, so that the variable passed in is read from and written to, but in reality it is a single machine instruction. The key feature is that this happens (or at least, appears to happen) atomically.\n# A Test-and-Set (TS) instructionW bool TS (bool v) { \u0026lt; bool initial = v; v = true; return initial; \u0026gt; } lock_t l = false; co [i = 1 to n] { while (something) { while (TS(l)) ; ## spin lock critical section; l = false; non-critical section; } } This is called spin lock,\nSimple spin locks don\u0026rsquo;t make good use of the cache (those spinning Test-And-Sets play havoc with contention and coherence performance). A pragmatically better spin locks is known as Test-and-Test-and-Set - mainly spinning on a read rather than a read-write function.\n... while (something) { while (l || TS(l)); /* only TS() if l was false*/ critical section; ... } ... Simply read l until there is a chance that a Test-and-Set might succeed.\nSpin lock guarantees mutual exclusion, absence of deadlock and absence of delay, but does not guarantee eventual entry.\nLamport\u0026rsquo;s Bakery Algorithm Implement critical sections using only simple atomic read and simple atomic write instructions (i.e. no need for atomic read-modify-write).\n采用商店结账排队机制，顾客就是一个个threads，根据排队码，越小的优先级越高（0 除外，0 代表没有结账需求），最小的可以进入critical section。\nThe challenge is entry protocal, if a thread intends to access the critical section:\n排队取号：It sets its turn turn[i] = max(turn[:])+1 (Threads not at or intend to access the critical section have a turn of 0) 等待叫号：This thread waits until its turn comes up (until it has the smallest turn). int turn[n] = [0, 0, ... 0]; co [i=1 to n] { while (true) { turn[i] = max (turn[1..n]) + 1; for (j = 1 to n except i) { while ((turn[j]!=0 and (turn[i] \u0026gt; (turn[j])) skip; } critical section; turn[i] = 0; noncritical section; } } 因为max (turn[1..n]) + 1不是atomic的, 所以会出现问题.\n问题一: if turn setting is not atomic then two (or more) threads may claim the same turn.\n两个threads在取号阶段turn[i] = max(turn[:])+1出现并发，两个都先max, 之后再+1.\n问题二: there is possibility that a thread can claim a lower turn than another thread which enters the critical section before it!\n两个threads在取号阶段turn[i] = max(turn[:])+1出现并发, 并且在两个threads分别进行max的间隙, 刚好在CS中的thread完成并退出CS，导致两个thread看到的max值不一样了. 前者比后者看到的大, 但前者却因为更早进行+1操作而提前进入了CS.\n举例：假如同时有三个thread A B C, A 已经在CS中(turn(A)\u0026gt;0)：\nB 先运行max比较(max = turn(A)), C 在 A 退出后(turn(A) = 0)才进行比较(max = 0), B 先进行+1操作(turn(B) = turn(A)+1 \u0026gt; 1), B 进行比较后允许进入CS (此时turn(C)还是0, 0是被忽略的); 之后C才 +1(turn(C) = 0 + 1 = 1); 这样导致B的值虽然比C大, 但B还是比C先进入CS; 之后因为 C 的 turn 比较小， 所以 C 也跟着进入 CS。 问题一解决方案 - 使用线程ID（绝不相同）做二次区分, 在相同 turn 的情况下，具有较低ID的 thread 有限。\n问题二解决方案 - 在max (turn[1..n]) + 1之前先turn[i] = 1;. • 这样，任何 threads 想取号都要先标记为 1 • 标记后，才有资格跟其他 thread 比较 • 以max+1作为号码进入队列，这样任何的可能的 turn 值都必定大于 1 • B 无法提前进入CS (此时turn(C)不再是被忽略的0, 而是最小正整数1).\n# (x, a) \u0026gt; (y,b) means (x\u0026gt;y) || (x==y \u0026amp;\u0026amp; a\u0026gt;b). while (true) { turn[i] = 1; turn[i] = max (turn[1..n]) + 1; for (j = 1 to n except i) { while ((turn[j]!=0 and (turn[i], i) \u0026gt; (turn[j], j)) skip; } ... } Lamport\u0026rsquo;s algorithm has the strong property of guaranteeing eventual entry (unlike our spin lock versions). The algorithm is too inefficient to be practical if spin-locks are available.\nCondition Synchronization Delay an action until some condition (on the shared variables such as in producer-consumer, or with respect to the progress of other threads such as in a Barrier) becomes true.\nBarrier synchronization Barrier synchronization is a particular pattern of condition synchronization, a kind of computation-wide waiting:\nco [i = 1 to n] { while (something) { do some work; wait for all n workers to get here; } } A Counter Barriers\n\u0026lt;count = count + 1;\u0026gt; \u0026lt;await (count == n);\u0026gt; is fine as a single-use barrier, but things get more complex if (as is more likely) we need the barrier to be reusable.\n改良为\u0026lt;await (count == n); count = 0;\u0026gt;也不行: an inter-iteration race, 假如count == n, 那么n个threads都完成了前面的statements并准备执行await, 但其中任何一个 thread 先执行完整个代码都使count = 0,这样剩余的threads就无法通过await条件了.\nSense Reversing Barrier A shared variable sense is flipped after each use of the barrier to indicate that all threads may proceed. 关键每个 thread 都有自己的 private variable mySense 和 while spin lock。解决了前面的死锁问题。\nshared int count = 0; shared boolean sense = false; co [i = 1 to n] { private boolean mySense = !sense; ## one per thread while (something) { do some work; \u0026lt; count = count + 1; if (count == n) { count = 0; sense = mySense; } ## flip sense \u0026gt; while (sense != mySense); ## wait or pass mySense = !mySense; ## flip mySense // 或者使用 \u0026lt; await (sense==mySense) mySense = !sense;\u0026gt; } } 所有thread的local variable mySense开始都被赋值为!sense(true)，前面n-1个thread都得在内循环while那里等着；直到最后一个thread完成工作后, if条件才满足, count重置为0, 反转sense(被赋值为mySense也即是true), 之后所有threads才能结束内部while循环，接着再次反转sense(被赋值为!mySense也即是false), 然后进行下一轮大循环，借此达到重复利用barrier的目的. sense初始值是什么无所谓, 反转才是关键.\n缺点：$O(n)$效率，count次数（同步次数）正比于thread数量。\nSymmetric Barriers Symmetric barriers are designed to avoid the bottleneck at the counter. 通过 pair-threads barriers 多轮同步来构建一个完整的 n-threads barriers，让所有threads都知道大家已经完成任务。总共是$\\log_2n$ 轮同步。每个thread在完成必要工作后, 开始进入下面的pairwise同步环节，自己(myid)的初始arrive状态为0:\n# arrive[i] == 1 means arrive barrier # there will be log_2 #threads stages, # 每个stage代表一次pairwise同步 for [s = 0 to stages-1] { \u0026lt;await (arrive[myid] == 0);\u0026gt; # 1 arrive[myid] = 1; # 2 work out who my friend is at stage s; \u0026lt;await (arrive[friend] == 1);\u0026gt; # 3 arrive[friend] = 0; # 4 } 这样保证了，每个thread需要先把自己的arrive状态标记为1(#1，#2)，才可以去看同伴的状态（#3），假如同伴也是1，那么表明自己这一组是都到达了barrier状态（大家都是1），那么就会把对方的状态初始化为0 （#4），进入下一阶段，更换同伴，继续同步比较。 When used as a step within a multistage symmetric barrier, 会出现问题：假如有四个thread，那么就会有两个stages：第一次是1和2同步，3和4同步。2一直没到barrier，1一直卡在#3。而3和4 同步完后开始检查1的状况，发现arrive[1] = 1，就运行Lines (3) and (4), 结果1就被初始化了，而2还没是没到barrier。\n解决办法是给每个stage分配新的arrive变量。\nfor [s = 0 to stages-1] { \u0026lt;await (arrive[myid][s] == 0);\u0026gt; arrive[myid][s] = 1; work out who my friend is at this stage; \u0026lt;await (arrive[friend][s] == 1);\u0026gt; arrive[friend][s] = 0; } 这样假如出现2一直没到barrier的情况, 那么1会卡在当前stage, 1的stage+1的arrive状态就无法更新为1.\nDissemination Barriers If n isn\u0026rsquo;t a power of 2, instead of pairwise synchs, we have two partners at each stage for each thread, one incoming and one outgoing. Structured Primitives Instead of implementing directly in the user-address space, a number of more structured primitives have been devised for implementation with the assistance of the operating system, so that threads can be directly suspended and resumed by the OS\u0026rsquo;s scheduler.\n• Machine code, instructions and data directly understandable by a CPU; • Language primitive, the simplest element provided by a programming language; • Primitive data type, a datatype provided by a programming language.\nSemaphores 信号灯 A semaphore is a special shared variable, accessible only through two atomic operations, P(try to decrease) and V(increase), defined by: P(s): \u0026lt;await (s\u0026gt;0) s=s-1;\u0026gt; V(s): \u0026lt;s=s+1;\u0026gt;\nProperty: A thread executing P() on a 0 valued semaphore will be suspended on a queue until after some other thread has executed a V().\nApplication: A semaphore appears to be a simple integer. A thread waits for permission to proceed a critical section, and then signals that it has proceeded by performing a P() operation on the semaphore.\nBinary semaphore: A semaphore whose usage is organised to only ever take the value (0, 1) as a mutex 互斥. Counting(split) semaphore: can take on arbitrary nonnegative values.\nSemaphores still require careful programming: there is no explicit connection in the program source between \u0026ldquo;matching\u0026rdquo; semaphore operations. It is easy to get things wrong.\nSimilarly, there is no obvious indication of how semaphores are being used - some may be for mutual exclusion, others for condition synchronization. Again confusion is possible.\nSemaphores for Critical Section (mutual exclusion) sem mutex = 1; co [i = 1 to n] { while (whatever) { P(mutex); critical section; V(mutex); noncritical section; } } Semaphores for Barrier Synchronisation 实现 symmetric barrier: an array of arrive semaphores for each stage\nfor [s = 1 to stages] { V(arrive[myid][s]); work out who my friend is at stage s; P(arrive[friend][s]); } Semaphores for Producer-Consumer Buffering 针对单个producer和consumer，控制其接触单个容量的buffer权限：一个semaphores标识buffer已满full，一个标识空empty。这种情况下，只能有一个semaphore是1，故称之为split binary semaphore。 P(full) 执行 wait full \u0026gt; 0 : full -= 1, V(empty)执行empty += 1\nT buf; sem empty = 1, full = 0; co co [i = 1 to M] { while (whatever) { ...produce new data locally P(empty); buf = data; # producer V(full); } } // co [j = 1 to N] { while (whatever) { P(full); result = buf; # consumer V(empty); ... handle result locally } } oc Bounded Buffer: Control access to a multi-space buffer (the producer can run ahead of the consumer up to some limit)\nImplement the buffer itself with an array (circular), and two integer indices, indicating the current front and rear of the buffer and use arithmetic modulo n (the buffer size), so that the buffer conceptually becomes circular For a single producer and consumer, we protect the buffer with a split \u0026ldquo;counting\u0026rdquo; semaphore, initialised according to the buffer size. Think of full as counting how many space in the buffer are full, and empty as counting how many are empty T buf[n]; int front = 0, rear = 0; sem empty = n, full = 0; co ## Producer while (whatever) { ...produce new data locally P(empty); # empty\u0026gt;0, 才能生产, empty-=1 buf[rear] = data; rear = (rear + 1) % n; V(full); } // ## Consumer while (whatever) { P(full); # full\u0026gt;0, 才能消耗, full-=1 result = buf[front]; front = (front + 1) % n; V(empty); ... handle result locally } oc Multiple Producers/Consumers: Because each producer may access the same pointer to overide each other, so as consumer. Thus we need two levels of protection.\nUse a split counting semaphore to avoid buffer overflow (or underflow), as previously. Use a mutual exclusion semaphores to prevent interference between producers (and another to prevent interference between consumers). This allows up to one consumer and one producer to be actively simultaneously within a non-empty, non-full buffer. T buf[n]; int front = 0, rear = 0; 86 sem empty = n, full = 0, mutexP = 1, mutexC = 1; co co [i = 1 to M] { while (whatever) { ...produce new data locally P(empty); P(mutexP); # stop the other producers from accessing the buffer buf[rear] = data; rear = (rear + 1) % n; V(mutexP); V(full); } } // co [j = 1 to N] { while (whatever) { P(full); P(mutexC); result = buf[front]; front = (front + 1) % n; V(mutexC); V(empty); ... handle result locally } } oc Extending Multiple Producers/Consumers: If the buffered items are large and take a long time to read/write, we would like to relax this solution to allow several producers and/or consumers to be active within the buffer simultaneously.\nWe need to ensure that these workers accesse distinct buffer locations, which require the index arithmetic to be kept atomic. Make sure that the producer/consumers wait for that element to be empty/full before actually proceeding. ![](/images/Multiple_Producers_Consumers.png \u0026ldquo;The producers are filling distinct slots, but not necessarily completing these fills in strict order - slot i+1 might finish filling before slot i. However, consumers only know that a slot has been filled and assume, possibly incorrectly, that it is the \u0026ldquo;next\u0026rdquo; one. Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf\") The solution is to have extra semaphores pair for each buffer location.\nMonitors The monitor is a more structured mechanism which allows threads to have both mutual exclusion and the ability to wait (block) for a certain condition to become true. It has a mechanism for signaling other threads that their condition has been met. A monitor consists of a mutex (lock) object and condition variables (cv). A condition variable is basically a container of threads that are waiting for a certain condition.\nFor Mutual Exclusion: i.e. a mutex (lock) object, ensures that at most one thread is active within the monitor at each point in time. 不同线程的下一条即将执行的指令 (suspended) 可能是来自同一个 monitor (由os自行分配), 但同一时间内，至多只能有一个线程执行下一条指令，但可能不同线程各自收到了来自这个 monitor 代码的不同指令. It is as if the body of each monitor method is implicitly surrounded with P() and V() operations on a single hidden binary semaphore, shared by all methods.\nFor Condition Synchronization, using a cv with a monitor to control a queue of delayed threads by a kind of Signal and Continue (SC) scheme.![](/images/signal_and_continue.png \u0026ldquo;State transition diagram for \u0026ldquo;signal-and-continue\u0026rdquo; monitors. Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf\") For a condition_variables x;\nwait(x): Release lock; wait for the condition to become true; reacquire lock upon return (Java wait()) Signal(x): Wake up a waiter, if any (Java notify()) signal-all(x)orBroadcast(x): Wake up all the waiters (Java notifyAll()) For the thread active inside a monitor method - executing in monitor state\nIf the thread could not proceed, it may call the wait(cv) operation to give up the (implicit) lock it holds on the monitor, and being suspended (push to the end of CV queue). Each CV has its unique block queue. Or the thread could calls the operation signal(cv) to release the lock. This allow one previously blocked thread (normally chosen by a FIFO discipline) to become ready for scheduling again (only one will be allowed to enter the monitor entry queue at a time). The signalling thread continues uninterrupted. Or return(). If no threads are waiting, then a signal() is \u0026ldquo;lost\u0026rdquo; or \u0026ldquo;forgotten\u0026rdquo;, whereas a V() in Semaphores allows a subsequent P() to proceed.\nMonitor semantics mean that when a thread which was previously blocked on a condition is actually awakened again in the monitor.\nThe point to remember is that when the signal happened, the signalled thread was allowed to try to acquire the monitor lock again). It could be that some other thread acquires the lock first, and does something which negates the condition again (for example, it consumes the “new item” from a monitor protected buffer).\nThus it is often necessary, in all but the most tightly constrained situations, to wrap each conditional variable wait() call in a loop which rechecks the condition it was waiting for is still true.\nSingle producer, single consumer bounder buffer\nmonitor Bounded_Buffer { typeT buf[n]; # an array of some type T int front = 0, # index of first full slot rear = 0; # index of first empty slot count = 0; # number of full slots ## rear == (front + count) % n condition_variables not_full, # signaled when count \u0026lt; n not_empty; # signaled when count \u0026gt; 0 procedure deposit(typeT data) { # 存 while (count == n) wait(not_full); buf[rear] = data; rear = (rear+1) % n; count++; signal(not_empty); } procedure fetch(typeT \u0026amp;result) { # 取 while (count == 0) wait(not_empty); result = buf[front]; front = (front+1) % n; count--; signal(not_full); } } Why the while loop is necessary as a safety check on the wait calls (why not use if)? - 因为notify()只会让正在 wait queue 的 thread 进入准备状态, 但不会直接控制其恢复工作（是否马上开始，谁先开始，都是由os内部控制的）, 所以导致不同 thread 进度不同; 而while可以保证当即使 thread 因为受到notify()而结束wait()开始进入准备状态(entry queue)后, 继续检查 buffer 状态, 这样假如发现自己是最优先安排的那个, 就可以跳出while循环进入工作状态; 假如发现自己优先度不是最高的(while循环条件继续满足), 则继续wait().\nThe key difference to semaphores: signal() on a condition variable is not \u0026ldquo;remembered\u0026rdquo; in the way that V() on a semaphore is. If no threads are waiting, then a signal() is \u0026ldquo;lost\u0026rdquo; or \u0026ldquo;forgotten\u0026rdquo;, whereas a V() will allow a subsequent P() to proceed.\nReal Shared Variable Programming Systems Various concepts for shared variable programming have been embedded in real programming systems. In particular C\u0026rsquo;s Posix threads (Pthreads) library and Java\u0026rsquo;s threads and monitors.\nPOSIX Threads (Pthread) Create a new thread: Threads (type pthread_t) begin by executing a given function, and terminate when that function exits (or when killed off by another thread).\nint pthread_create (pthread_t *thread, p_thread_attr_t *attr, void *(*function) (void *), void *arguments); Wait for thread termination: int pthread_join (pthread_t t, void ** result);\n//一个简单但是有错误的例子， int target; void *adderthread (void *arg) { int i; for (i=0; i\u0026lt;N; i++) { target = target+1; } } int main (int argc, char *argv[]) { int i; pthread_t thread[P]; target = 0; for (i=0; i\u0026lt;P; i++) { pthread_create(\u0026amp;thread[i], NULL, adderthread, NULL); } ..... } Variable target is accessible to all threads (shared memory). Its increment is not atomic, so we may get unpredictable results.\nPOSIX provides mechanisms to coordinate accesses including semaphores and building blocks for monitors.\nPthreads semaphores //用 pthread semaphores 改写前面的代码 sem_t lock; void *adderthread (void *arg) { int i; for (i=0; i\u0026lt;N; i++) { sem_wait(\u0026amp;lock); target = target+1; sem_post(\u0026amp;lock); } } int main (int argc, char *argv[]) { target = 0; sem_init(\u0026amp;lock, 0, 1); ..... } sem_init(\u0026amp;sem, share, init), where init is the initial value and share is a \u0026ldquo;boolean\u0026rdquo; (in the C sense) indicating whether the semaphore will be shared between processes (true) or just threads within a process (false). sem_wait(s), which is the Posix name for P(s) sem_post(s), which is the Posix name for V(s) A Producers \u0026amp; Consumers:\nsem_t empty, full; // the global semaphores int data; // shared buffer int main (int argc, char *argv[]) { pthread_t pid, cid; .... sem_init(\u0026amp;empty, 0, 1); // sem empty = 1 sem_init(\u0026amp;full, 0, 0); // sem full = 0 pthread_create(\u0026amp;pid, \u0026amp;attr, Producer, NULL); pthread_create(\u0026amp;cid, \u0026amp;attr, Consumer, NULL); pthread_join(pid, NULL); pthread_join(cid, NULL); } void *Producer (void *arg) { int produced; for (produced = 0; produced \u0026lt; numIters; produced++) { sem_wait(\u0026amp;empty); data = produced; sem_post(\u0026amp;full); } } void *Consumer (void *arg) { int total = 0, consumed; for (consumed = 0; consumed \u0026lt; numIters; consumed++) { sem_wait(\u0026amp;full); total = total+data; sem_post(\u0026amp;empty); } printf(\u0026#34;after %d iterations, the total is %d (should be %d)\\n\u0026#34;, numIters, total, numIters*(numIters+1)/2); } Pthreads Monitors Pthreads provides locks, of type pthread_mutex_t m;. These can be\nInitialized with pthread_mutex_init(\u0026amp;m, attr), where attr are attributes concerning scope (as with semaphore creation). If attr is NULL, the default mutex attributes (NONRECURSIVE) are used; Locked with pthread_mutex_lock(\u0026amp;m), which blocks the locking thread if m is already locked. There is also a non-blocking version pthread_mutex_trylock(\u0026amp;m). Unlocked with pthread_mutex_unlock(\u0026amp;m). Only a thread which holds a given lock, should unlock it! Pthreads provides condition variables pthread_cond_t. As well as the usual initialization, these can be:\nWaited on with pthread_cond_wait(\u0026amp;cv, \u0026amp;mut) where cv is a condition variable, and mut must be a lock already held by this thread, and which is implictly released. Signalled with pthread_cond_signal(\u0026amp;cv) by a thread which should (but doesn\u0026rsquo;t strictly have to) hold the associated mutex. The semantics are \u0026ldquo;Signal-and-Continue\u0026rdquo; as previously discussed. Signalled all with pthread_cond_broadcast(\u0026amp;cv). This is \u0026ldquo;signal-all\u0026rdquo; A simple Jacobi grid-iteration program with a re-usable Counter Barrier. To avoid copying between \u0026ldquo;new\u0026rdquo; and \u0026ldquo;old\u0026rdquo; grids, each iteration performs two Jacobi steps. Convergence testing could be added as before.\npthread_mutex_t barrier; // mutex semaphore for the barrier pthread_cond_t go; // condition variable for leaving int numArrived = 0; void Barrier() { pthread_mutex_lock(\u0026amp;barrier); numArrived++; if (numArrived == numWorkers) { numArrived = 0; pthread_cond_broadcast(\u0026amp;go); } else { pthread_cond_wait(\u0026amp;go, \u0026amp;barrier); } pthread_mutex_unlock(\u0026amp;barrier); } int main(int argc, char *argv[]) { pthread_t workerid[MAXWORKERS]; pthread_mutex_init(\u0026amp;barrier, NULL); pthread_cond_init(\u0026amp;go, NULL); InitializeGrids(); for (i = 0; i \u0026lt; numWorkers; i++) pthread_create(\u0026amp;workerid[i], \u0026amp;attr, Worker, (void *) i); for (i = 0; i \u0026lt; numWorkers; i++) pthread_join(workerid[i], NULL); } void *Worker(void *arg) { int myid = (int) arg, rowA = myid*rowshare+1, rowB = rowA+rowshare-1; for (iters = 1; iters \u0026lt;= numIters; iters++) { for (i = rowA; i \u0026lt;= rowB; i++) { for (j = 1; j \u0026lt;= gridSize; j++) { grid2[i][j] = (grid1[i-1][j] + grid1[i+1][j] + grid1[i][j-1] + grid1[i][j+1]) * 0.25; } } Barrier(); for (i = rowA; i \u0026lt;= rowB; i++) { for (j = 1; j \u0026lt;= gridSize; j++) { grid1[i][j] = (grid2[i-1][j] + grid2[i+1][j] + grid2[i][j-1] + grid2[i][j+1]) * 0.25; } } Barrier(); } } Memory Consistency in Pthreads Weak consistency models can wreck naive DIY synchronization attempts!\nTo enable portability, Pthreads mutex, semaphore and condition variable operations implicitly act as memory fences, executing architecture specific instructions.\nIn effect, the C + Pthreads combination guarantees a weak consistency memory model, with the only certainties provided at uses of Pthreads primitives.\nFor example, all writes by a thread which has released some mutex, are guaranteed to be seen by any thread which then acquires it. Nothing can be assumed about the visibility of writes which cannot be seen to be ordered by their relationship to uses of Pthread primitives.\nThe programmer must also be careful to use only thread-safe code, which works irrespective of how many threads are active.\nThread-safe code only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specification without unintended interaction. Implementation is guaranteed to be free of race conditions when accessed by multiple threads simultaneously.\nTypical problems involve the use of non-local data. For example, imagine a non-thread safe malloc. Unluckily interleaved calls might break the underlying free space data structure. Some libraries will provide thread-safe versions (but of course, which pay an unnecessary performance penalty when used in a single threaded program).\nJava Concurrency Java是一种多线程 multi-threaded 编程语言，其同步模型是基于 monitor 概念，可用于开发多线程程序。多任务 multtasking 就是多个进程共享公共处理资源（如CPU）的时候。多线程将多任务的思想扩展到可以将单个应用程序中的特定操作细分为单独线程的应用程序。每个线程都可以并行运行。操作系统不仅在不同的应用程序之间分配处理时间，而且在应用程序内的每个线程之间分配处理时间。\nJava Threads Threads can be created from classes which extend java.lang.Thread\nclass Simple extends Thread { public void run() { System.out.println(\u0026#34;this is a thread\u0026#34;); } } new Simple().start(); // implicitly calls the run() method Or implement java.lang.Runnable (so we can extend some other class too).\nclass Bigger extends Whatever implements Runnable { public void run() { .... } } new Thread( new Bigger (...) ).start(); Wait to join with another thread\nclass Friend extends Thread { private int me; public Friend (int i) { me = i; } public void run() { System.out.println(\u0026#34;Hello from thread \u0026#34; + me); } } class Hello throws java.lang.InterruptedException { private static final int n = 5; public static void main(String[] args) { int i; Friend t[] = new Friend[n]; System.out.println (\u0026#34;Hello from the main thread\u0026#34;); for (i=0; i\u0026lt;n; i++) { t[i] = new Friend(i); t[i].start(); } for (i=0; i\u0026lt;n; i++) { t[i].join(); // might throw java.lang.InterruptedException } System.out.println (\u0026#34;Goodbye from the main thread\u0026#34;); } } Java \u0026ldquo;Monitors\u0026rdquo; Java provides an implementation of the monitor concept (but doesn’t actually have monitor as a keyword).\nAny object in a Java program can, in effect, become a monitor, simply by declaring one or more of its methods to be synchronized, or by including a synchronized block of code.\nEach such object is associated with one, implicit lock. A thread executing any synchronized code must first acquire this lock. This happens implicitly (i.e. there is no source syntax). Similarly, upon leaving the synchronized block the lock is implicitly released.\nJava\u0026rsquo;s condition variable mechanism uses Signal-and-Continue semantics (The signalling thread continues uninterrupted). Each synchronizable object is associated with a single implicit condition variable. Manipulated with methods wait(), notify() and notifyAll(). We can only have one conditional variable queue per monitor (hence the absence of any explicit syntax for the condition variable itself).\nwait(): has three variance, one which waits indefinitely for any other thread to call notify or notifyAll method on the object to wake up the current thread. Other two variances puts the current thread in wait for specific amount of time before they wake up.\nnotify(): wakes up only one thread waiting on the object and that thread starts execution.\nnotifyAll(): wakes up all the threads waiting on the object, although which one will process first depends on the OS implementation.\nThese methods can be used to implement producer consumer problem where consumer threads are waiting for the objects in Queue and producer threads put object in queue and notify the waiting threads.\nReaders \u0026amp; Writers problem requires control access to some shared resource, such that there may be many concurrent readers, but only one writer (with exclusive access) at a time.\n/* 2 readers and 2 writers making 5 accesses each with concurrent read or exclusive write. */ class ReadWrite { // driver program -- two readers and two writers static Database RW = new Database(); // the monitor public static void main(String[] arg) { int rounds = Integer.parseInt(arg[0],10); new Reader(rounds, RW).start(); new Reader(rounds, RW).start(); new Writer(rounds, RW).start(); new Writer(rounds, RW).start(); } } class Reader extends Thread { int rounds; Database RW; private Random generator = new Random(); public Reader(int rounds, Database RW) { this.rounds = rounds; this.RW = RW; } public void run() { for (int i = 0; i\u0026lt;rounds; i++) { try { Thread.sleep(generator.nextInt(500)); } catch (java.lang.InterruptedException e) {} System.out.println(\u0026#34;read: \u0026#34; + RW.read()); } } } class Writer extends Thread { int rounds; Database RW; private Random generator = new Random(); public Writer(int rounds, Database RW) { this.rounds = rounds; this.RW = RW; } public void run() { for (int i = 0; i\u0026lt;rounds; i++) { try { Thread.sleep(generator.nextInt(500)); } catch (java.lang.InterruptedException e) {} RW.write(); } } } Implement the \u0026ldquo;database\u0026rdquo;. Allowing several readers to be actively concurrently. The last reader to leave will signal a waiting writer.\nThus we need to count readers, which implies atomic update of the count. A reader needs two protected sections to achieve this.\nNotice that while readers are actually reading the data they do not hold the lock.\nclass Database { private int data = 0; // the data int nr = 0; // synchronized means no more than one thread could do that private synchronized void startRead() { nr++; } private synchronized void endRead() { nr--; if (nr==0) notify(); }// awaken a waiting writer public int read() { int snapshot; startRead(); snapshot = data; // read data endRead(); return snapshot; } public synchronized void write() { int temp; while (nr\u0026gt;0) try { wait(); } catch (InterruptedException ex) {return;} temp = data; // next six lines are the ‘‘database’’ update! data = 99999; // to simulate an inconsistent temporary state try { Thread.sleep(generator.nextInt(500)); // wait a bit, for demo purposes only } catch (java.lang.InterruptedException e) {} data = temp+1; // back to a safe state System.out.println(\u0026#34;wrote: \u0026#34; + data); notify(); // awaken another waiting writer } } We could express the same effect with synchronized blocks\npublic int read() { int snapshot; synchronized(this) { nr++; } // this - the database object snapshot = data; synchronized(this) { nr--; if (nr==0) notify(); // awaken a waiting writer } return snapshot; } Would it be OK to use notifyAll() in read()? - Yes, but with extra transmission cost.\nBuffer for One Producer - One Consumer\n/** (borrowed from Skansholm, Java from the Beginning) */ public class Buffer extends Vector { public synchronized void putLast (Object obj) { addElement(obj); // Vectors grow implicitly notify(); } public synchronized Object getFirst () { while (isEmpty()) try {wait();} catch (InterruptedException e) {return null;} Object obj = elementAt(0); removeElementAt(0); return obj; } } The java.util.concurrent package Including a re-usable barrier and semaphores (with P() and V() called acquire() and release()). It also has some thread-safe concurrent data structures (queues, hash tables).\nThe java.util.concurrent.atomic package provides implementations of atomically accessible integers, booleans and so on, with atomic operations like addAndGet, compareAndSet.\nThe java.util.concurrent.locks package provides implementations of locks and condition variables, to allow a finer grained, more explicit control than that provided by the built-in synchronized monitors.\nMessage Passing Programming When the underyling archictecture doesn\u0026rsquo;t support physically shared memory (for example, by distributing the OS and virtual memory system, i.e. Multicomputer architectures), we can make the disjoint nature of the address spaces apparent to the programmer, who must make decisions about data distribution and invoke explicit operations to allow interaction across these.\nMessage passing, which is a approache to abstract and implement such a model, dominates the performance-oriented parallel computing world.\nMessage passing is characterized as requiring the explicit participation of both interacting processes, since each address space can only be directly manipulated by its owner. The basic requirement is thus for send and receive primitives for transferring data out of and into local address spaces.\nThe resulting programs can seem quite fragmented: we express algorithms as a collection of local perspectives. These are often captured in a single program source using Single Program Multiple Data (SPMD) style, with different processes following different paths through the same code, branching with respect to local data values and/or to some process identifier.\n// SPMD Compare-Exchange co [me = 0 to P-1] { // assumes P is even int a, temp; // these are private to each process now ...... // typical one step within a parallel sorting algorithm if (me%2 == 0) { send (me+1, a); // send from a to process me+1 recv (me+1, temp); // receive into temp from process me+1 a = (a\u0026lt;=temp) ? a : temp; // 取较小值 } else { send (me-1, a); recv (me-1, temp); a = (a\u0026gt;temp) ? a : temp; // 取较大值 } ...... } 1, Synchronization: Must a sending process pause until a matching receive has been executed (synchronous), or not (asynchronous)? Asynchronous semantics require the implementation to buffer messages which haven\u0026rsquo;t yet been, and indeed may never be, received. If we use synchronous semantics, the compare-exchange code above will deadlock. Can you fix it?\nOne way s to make the send be a non-blocking one (MPI_Isend) Another way is to reverse the order of one of the send/receive pairs:\n} else { recv (me-1, temp); send (me-1, a); a = (a\u0026gt;temp) ? a : temp; // 取较大值 } ...... 2, Addressing: When we invoke a send (or receive) do we have to specify a unique destination (or source) process or can we use wild-cards? Do we require program-wide process naming, or can we create process groups and aliases? 3, Collective Operations: Do we restrict the programmer to single-source, single-destination, point-to-point messages, or do we provide abstractions of more complex data exchanges involving several partners?• Broadcast: Everyone gets a copy of the same value. • Scatter: Data is partitioned and spread across the group. • Gather: Data is gathered from across the group. • Reduction: Combine the gathered values with an associative operation. • Scan (Prefix): Reduce and also compute all the ordered partial reductions.\nMessage Passing Interface (MPI) Message Passing Interface (MPI) is a standardized and portable message-passing standard. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran.\nProcesses can be created statically when the program is invoked (e.g. using the mpirun command) or spawned dynamically.\nAll communications take place within the context of \u0026ldquo;communication spaces\u0026rdquo; called communicators, which denote sets of processes, allows the MPI programmer to define modules that encapsulate internal communication structures. A process can belong to many communicators simultaneously. New communicators can be defined dynamically.\nSimple send/receives operate with respect to other processes in a communicator. Send must specify a target but receive can wild card on matching sender.\nMessages can be tagged with an extra value to aid disambiguation.\nMessage-passing programming models are by default nondeterministic: the arrival order of messages sent from two processes A and B, to a third process C, is not defined. (However, MPI does guarantee that two messages sent from one process A, to another process B, will arrive in the order sent.)\nThere are many synchronization modes and a range of collective operations.\nMPI Primitives (6 basics functions) 1, int MPI_Init(int *argc, char ***argv): Initiate an MPI computation. 2, int MPI_Finalize(): Terminate a computation. These must be called once by every participating process, before/after any other MPI calls. They return MPI_SUCCESS if successful, or an error code.\nEach process has a unique identifier in each communicator of which it is a member (range 0\u0026hellip;members-1). MPI_COMM_WORLD is the built-in global communicator, to which all processes belong by default.\nA process can find the size of a communicator, and its own rank within it: 3, int MPI_Comm_Size (MPI_Comm comm, int *np): Determine number of processes (comm - communicator). The processes in a process group are identified with unique, contiguous integers numbered from 0 to np-1. 4, int MPI_Comm_rank (MPI_Comm comm, int *me): Determine my process identifier.\n5, MPI_SEND: Send a message. 6, MPI_RECV: Receive a message.\nMPI Task Farm A task farm is bag-of-tasks in which all the tasks are known from the beginning. The challenge is to assign them dynamically to worker processes, to allow for the possibility that some tasks may take much longer to compute than others.\nTo simplify the code, we assume that there are at least as many tasks as processors and that tasks and results are just integers. In a real application these would be more complex data structures.\nNotice the handling of the characteristic non-determinism in the order of task completion, with tags used to identify tasks and results. We also use a special tag to indicate an \u0026ldquo;end of tasks\u0026rdquo; message.\n/** SPMD style 农场主分配任务给工人 */ #define MAX_TASKS 100 #define NO_MORE_TASKS MAX_TASKS+1 #define FARMER 0 // 第一个 process 是farmer,其余是worker int main(int argc, char *argv[]) { int np, rank; MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;np); if (rank == FARMER) { farmer(np-1); } else { worker(); } MPI_Finalize(); } void farmer (int workers) { int i, task[MAX_TASKS], result[MAX_TASKS], temp, tag, who; MPI_Status status; // 1, 给每个人发送任务 for (i=0; i\u0026lt;workers; i++) { MPI_Send(\u0026amp;task[i], 1, MPI_INT, i+1, i, MPI_COMM_WORLD); } // 2, 收取任务结果, 继续发放剩余任务 while (i\u0026lt;MAX_TASKS) { MPI_Recv(\u0026amp;temp, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, \u0026amp;status); who = status.MPI_SOURCE; tag = status.MPI_TAG; result[tag] = temp; MPI_Send(\u0026amp;task[i], 1, MPI_INT, who, i, MPI_COMM_WORLD); i++; } // 3, 所有任务已经完成, 收集最后一个任务结果, 并且发出结束任务信号 for (i=0; i\u0026lt;workers; i++) { MPI_Recv(\u0026amp;temp, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, \u0026amp;status); who = status.MPI_SOURCE; tag = status.MPI_TAG; result[tag] = temp; MPI_Send(\u0026amp;task[i], 1, MPI_INT, who, NO_MORE_TASKS, MPI_COMM_WORLD); } } Notice that the final loop, which gathers the last computed tasks, has a predetermined bound. We know that this loop begins after dispatch of the last uncomputed task, so there must be exactly as many results left to gather as there are workers.\nvoid worker() { int task, result, tag; MPI_Status status; MPI_Recv(\u0026amp;task, 1, MPI_INT, FARMER, MPI_ANY_TAG, MPI_COMM_WORLD, \u0026amp;status); tag = status.MPI_TAG; while (tag != NO_MORE_TASKS) { result = somefunction(task); MPI_Send(\u0026amp;result, 1, MPI_INT, FARMER, tag, MPI_COMM_WORLD); MPI_Recv(\u0026amp;task, 1, MPI_INT, FARMER, MPI_ANY_TAG, MPI_COMM_WORLD, \u0026amp;status); tag = status.MPI_TAG; } } A worker is only concerned with its interaction with the farmer. 这样速度较快的worker可以自动接更多的任务，最终整体上达成 load balance。\nSend in standard mode int MPI_Send(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm) Send count items of given type starting in position buf to process dest in communicator comm, tagging the message with tag (which must be non-negative).\nThere are corresponding datatypes for each basic C type, MPI_INT, MPI_FLOAT etc, and also facilities for constructing derived types which group these together.\nAre MPI_Send and MPI_Recv synchronous or asynchronous?\nReceive in standard mode int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status) Receive count items of given type starting in position buf, from process source in communicator comm, tagged by tag. It attempts to receive a message that has an envelope corresponding to the specified tag, source, and comm, blocking until such a message is available. When the message arrives, elements of the specified datatype are placed into the buffer at address buf. This buffer is guaranteed to be large enough to contain at least count elements.\nNon-determinism (within a communicator) is achieved with \u0026ldquo;wild cards\u0026rdquo;, by naming MPI_ANY_SOURCE and/or MPI_ANY_TAG as the source or tag respectively.\nA receive can match any available message sent to the receiver which has the specified communicator, tag and source, subject to the constraint that messages sent between any particular pair of processes are guaranteed to appear to be non-overtaking. In other words, a receive cannot match message B in preference to message A if A was sent before B by the same process, the receive will receive the first one which was sent, not the first one to arrive.\nThe status variable can be used subsequently to inquire about the size, tag, and source of the received message. Status information is returned in a structure with status.MPI_SOURCE and status.MPI_TAG fields. This is useful in conjunction with wild card receives, allowing the receiver to determine the actual source and tag associated with the received message.\nPrime Sieve Generator int main(int argc, char *argv[]) { MPI_Comm nextComm; int candidate = 2, N = atoi(argv[1]); MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_spawn(\u0026#34;sieve\u0026#34;, argv, 1, MPI_INFO_NULL, 0, MPI_COMM_WORLD, \u0026amp;nextComm, MPI_ERRCODES_IGNORE); while (candidate\u0026lt;N) { MPI_Send(\u0026amp;candidate, 1, MPI_INT, 0, 0, nextComm); candidate++; } candidate = -1; MPI_Send(\u0026amp;candidate, 1, MPI_INT, 0, 0, nextComm); MPI_Finalize(); } We use MPI_Comm_spawn to dynamically create new sieve processes as we need them, and MPI_Comm_get_parent to find an inter-communicator to the process group which created us.\nint main(int argc, char *argv[]) { MPI_Comm predComm, succComm; MPI_Status status; int myprime, candidate; int firstoutput = 1; // a C style boolean MPI_Init (\u0026amp;argc, \u0026amp;argv); MPI_Comm_get_parent (\u0026amp;predComm); MPI_Recv(\u0026amp;myprime, 1, MPI_INT, 0, 0, predComm, \u0026amp;status); printf (\u0026#34;%d is a prime\\n\u0026#34;, myprime); MPI_Recv(\u0026amp;candidate, 1, MPI_INT, 0, 0, predComm, \u0026amp;status); while (candidate!=-1) { if (candidate%myprime != 0) { // not sieved out if (firstoutput) { // create my successor if necessary MPI_Comm_spawn(\u0026#34;sieve\u0026#34;, argv, 1, MPI_INFO_NULL, 0, MPI_COMM_WORLD, \u0026amp;succComm, MPI_ERRCODES_IGNORE); firstoutput = 0; } MPI_Send(\u0026amp;candidate, 1, MPI_INT, 0, 0, succComm) // pass on the candidate } MPI_Recv(\u0026amp;candidate, 1, MPI_INT, 0, 0, predComm, \u0026amp;status); // next candidate } if (!firstoutput) MPI_Send(\u0026amp;candidate, 1, MPI_INT, 0, 0, succComm); // candidate=-1, shut down MPI_Finalize(); } The message flow is insured by the method in which new processes are spawned/created. Every time a new “sieve” process is spawned, MPI creates it in a new group/communicator. succComm is a handle to this new group which always contains only one process. Therefore, when a candidate is sent to the process, there is only one process in the succComm group and it has id 0.\nThe Recv function works in the same way predComm is a handle of the parent group (i.e. group of the process that created this sieve). And because the parent was the only process in this group/communicator, it can be identified by id 0.\nIn conclusion, a process creates at most one successor. This successor is the only process in its group/communicator. The succCom and predComm are handles to the children and parent groups respectively, both of which contain a single process with id 0 which is unique in its own group/communicator.\nSpawning New MPI Processes\nint MPI_Comm_spawn (char *command, char *argv[], int p, MPI_Info info, int root, MPI_Comm comm, MPI_Comm *intercomm, int errcodes[]) This spawns p new processes, each executing a copy of program command, in a new communicator returned as intercomm. To the new processes, intercomm appears as MPI_COMM_WORLD. It must be called by all processes in comm (it is \u0026ldquo;collective\u0026rdquo;), with process root computing the parameters. info and errcodes are used in system dependent ways to control/monitor process placement, errors etc.\nMPI_Comm_get_parent gives the new processes a reference to the communicator which created them.\nSynchronization in MPI MPI uses the term blocking in a slightly unconventional way, to refer to the relationship between the caller of a communication operation and the implementation of that operation (i.e. nothing to do with any matching operation). Thus, a blocking send complete only when it is safe to reuse the specified output buffer (because the data has been copied somewhere safe by the system). 注意这里跟前面提到的synchronous概念不一样，synchronous 强调接收成功才是判断发送成功与否的标识，而 blocking 只需要保证缓存可以被安全改写即可。\nIn contrast, a process calling a non-blocking send continues immediately with unpredictable effects on the value actually sent. Similarly, there is a non-blocking receive operation which allows the calling process to continue immediately, with similar issues concerning the values which appear in the buffer. 意义在于，当需要发送的信息字节非常巨大时，发送和接收耗时都非常久，这时候如果可以不需要等待这些巨量信息的传输而直接继续下一个任务，则能提高效率。\nTo manage these effects, there are MPI operations for monitoring the progress of non-blocking communications (effectively, to ask, \u0026ldquo;is it OK to use this variable now?\u0026rdquo;). - The idea is that with careful use these can allow the process to get on with other useful work even before the user-space buffer has been safely stored.\nBlocking Communication Semantics in MPI MPI provides different blocking send operations, vary in the level of synchronization they provide. Each makes different demands on the underlying communication protocol (i.e. the implementation).\n1, Synchronous mode send (MPI_Ssend) is blocking and synchronous, only complete when a matching receive has been found.\n2, Standard mode send (MPI_Send) is blocking. Its synchronicity depends upon the state of the implementation buffers, in that it will be asynchronous unless the relevant buffers are full, in which case it will wait for buffer space (and so may appear to behave in a \u0026ldquo;semi\u0026rdquo; synchronous fashion).\n3, Buffered mode send (MPI_Bsend) is blocking and asynchronous, but the programmer must previously have made enough buffer space available (otherwise an error is reported). There are associated operations for allocating the buffer space.\nReceiving with MPI_Recv blocks until a matching message has been completely received into the buffer (so it is blocking and synchronous).\nMPI also provides non-blocking sends and receives which return immediately (i.e. possibly before it is safe to use/reuse the buffer). There are immediate versions of all the blocking operations (with an extra \u0026ldquo;I\u0026rdquo; in the name). For example, MPI_Isend is the standard mode immediate send, and MPI_Irecv is the immediate receive.\nNon-blocking operations have an extra parameter, called a \u0026lsquo;request\u0026rsquo; which is a handle on the communication, used with MPI_Wait and MPI_Test to wait or check for completion of the communication (in the sense of the corresponding blocking version of the operation).\nProbing for Messages A receiving process may want to check for a potential receive without actually receiving it. For example, we may not know the incoming message size, and want to create a suitable receiving buffer.\nint MPI_Probe(int src, int tag, MPI_Comm comm, MPI_Status *status) behaves like MPI_Recv , filling in *status, without actually receiving the message.\nThere is also a version which tests whether a message is available immediately int MPI_Iprobe(int src, int tag, MPI_Comm comm, int *flag, MPI_Status *status) leaving a (C-style) boolean result in *flag (i.e. message/no message).\nWe can then determine the size of the incoming message by inspecting its status information. int MPI_Get_count(MPI_Status *status, MPI_Datatype t, int *count) sets *count to the number of items of type t in message with status *status.\nWe could use these functions to receive (for example) a message containing an unknown number of integers from an unknown source, but with tag 75, in a given communicator comm.\nMPI_Probe(MPI_ANY_SOURCE, 75, comm, \u0026amp;status); MPI_Get_count(\u0026amp;status, MPI_INT, \u0026amp;count); buf = (int *) malloc(count*sizeof(int)); source = status.MPI_SOURCE; MPI_Recv(buf, count, MPI_INT, source, 75, comm, \u0026amp;status); Collective Operations MPI offers a range of more complex operations which would otherwise require complex sequences of sends, receives and computations.\nThese are called collective operations, because they must be called by all processes in a communicator.\n1, MPI_Bcast broadcasts count items of type t from buf in root to buf in all other processes in comm:\nint MPI_Bcast (void *buf, int count, MPI_Datatype t, int root, MPI_Comm comm) 2, MPI_Scatter is used to divide the contents of a buffer across all processes.\nint MPI_Scatter (void *sendbuf, int sendcount, MPI_Datatype sendt, void *recvbuf, int recvcount, MPI_Datatype recvt, int root, MPI_Comm comm) $i^{th}$ chunk (size sendcount) of root\u0026rsquo;s sendbuf is sent to recvbuf on process $i$ (including the root process itself). The first three parameters are only significant at the root. Counts, types, root and communicator parameters must match between root and all receivers.\n3, MPI_Gather is the inverse of MPI_Scatter. Instead of spreading elements from one process to many processes, MPI_Gather takes elements from many processes and gathers them to one single process.\nMPI_Gather takes elements from each process and gathers them to the root process. The elements are ordered by the rank of the process from which they were received. Only the root process needs to have a valid receive buffer. The recv_count parameter is the count of elements received per process, not the total summation of counts from all processes.\nMPI_Gather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator) 4, MPI_Allreduce computes a reduction, such as adding a collection of values together. No root, all Processes receive the reduced result.\nint MPI_Allreduce (void *sendbuf, void *recvbuf, int count, MPI_Datatype sendt, MPI_Op op, MPI_Comm comm) Reduces elements from all send buffers, point-wise, to count single values, using op, storing result(s) in all receive buffers. The op is chosen from a predefined set (MPI_SUM, MPI_MAX etc) or constructed with user code and MPI_Op_create. MPI_Allreduce is the equivalent of doing MPI_Reduce followed by an MPI_Bcast.\nJacobi (1-dimensional wrapped), each neighour is owned by distinct process, thus could not read each other\u0026rsquo;s data - introduce a layer of message passing, introduce halo as buffer.\n// here for convenience MPI_Sendrecv combines a send and a receive. int main(int argc, char *argv[]) { MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;p); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); if (rank == 0) read_problem(\u0026amp;n, work); // 数据存在 root - 0号进程 MPI_Bcast(\u0026amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD); // 广播数据 mysize = n/p; // assume p divides n, for simplicity local = (float *) malloc(sizeof(float) * (mysize+2)); //include fringe/halo MPI_Scatter(work, mysize, MPI_FLOAT, \u0026amp;local[1], mysize, MPI_FLOAT, 0, MPI_COMM_WORLD); // scatter 分发数据到各process主位置 left = (rank+p-1)%p; // who is my left neighour? right = (rank+1)%p; // who is my right neighour? do { //[0]和[mysize+1]halo MPI_Sendrecv(\u0026amp;local[1], 1, MPI_FLOAT, left, 0, // send this \u0026amp;local[mysize+1], 1, MPI_FLOAT, right, 0, // receive this MPI_COMM_WORLD, \u0026amp;status); // anti-clockwise MPI_Sendrecv(\u0026amp;local[mysize], 1, MPI_FLOAT, right, 0, \u0026amp;local[0], 1, MPI_FLOAT, left, 0, MPI_COMM_WORLD, \u0026amp;status); // clockwise do_one_step(local, \u0026amp;local_error); MPI_Allreduce(\u0026amp;local_error, \u0026amp;global_error, 1, MPI_FLOAT, MPI_MAX, MPI_COMM_WORLD); } while (global_error \u0026gt; acceptable_error); MPI_Gather (\u0026amp;local[1], mysize, MPI_FLOAT, work, mysize, MPI_FLOAT, 0, MPI_COMM_WORLD); if (rank == 0) print_results(n, work); } int MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag, void *recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag, MPI_Comm comm, MPI_Status *status) Communicators Communicators define contexts within which groups of processes interact. All processes belong to MPI_COMM_WORLD from the MPI initialisation call onwards.\nCreate new communicators from old ones by collectively calling MPI_Comm_split(MPI_Comm old, int colour, int key, MPI_Comm *newcomm) to create new communicators based on colors and keys: color - control of subset assignment (nonnegative integer). Processes with the same color are in the same new communicator. key - control of rank assignment (integer).\nWithin each new communicator, processes are assigned a new rank in the range $0...p^{\\prime} − 1$, where $p^{\\prime}$ is the size of the new communicator. Ranks are ordered by (but not necessarily equal to) the value passed in as the key parameter, with ties broken by considering process rank in the parent communicator.\nThis can be helpful in expressing algorithms which contain nested structure. For example, many divide-and-conquer algorithms split the data and machine in half, process recursively within the halves, then unwind to process the recursive results back at the upper level.\n//Divide \u0026amp; Conquer Communicators void some_DC_algorithm ( ..., MPI_Comm comm) { MPI_Comm_size(comm, \u0026amp;p); MPI_Comm_rank(comm, \u0026amp;myrank); ... pre-recursion work ... if (p\u0026gt;1) { MPI_Comm_split (comm, myrank\u0026lt;(p/2), 0, \u0026amp;subcomm); // two sub-machines some_DC_algorithm (..., subcomm); // recursive step // in both sub-machines } else do_base_case_solution_locally(); ... post-recursion work ... } Task and Pattern Based Models Programming explicitly with threads (or processes) has some drawbacks: • Natural expression of many highly parallel algorithms involves creation of far more threads than there are cores. Thread creation and scheduling have higher overheads than simpler activities like function calls (by a factor of 50-100). • The OS has control over the scheduling of threads to processor cores, but it does not have the application specific knowledge required to make intelligent assignments (for example to optimize cache re-use). Traditional OS concerns for fairness may be irrelevant or even counter-productive.\nTo avoid this, programmers resort to complex scheduling and synchronization of a smaller number of coarser grained threads. How to avoid this?\nA number of languages and libraries have emerged which • separate the responsibility for identifying potential parallelism, which remains the application programmer\u0026rsquo;s job, from detailed scheduling of this work to threads and cores, which becomes the language/library run-time\u0026rsquo;s job. • provide abstractions of common patterns of parallelism, which can be specialized with application specific operations, leaving implementation of the pattern and its inherent synchronization to the system.\nThese are sometimes called task based approaches, in contrast to traditional threaded models. Examples include OpenMP, which is a compiler/language based model, and Intel\u0026rsquo;s Threading Building Blocks library.\nThreading Building Blocks Threading Building Blocks (TBB) is a shared variable model, C++ template-based library. It uses generic programming techniques to provide a collection of parallel algorithms, each of which is an abstraction of a parallel pattern. It also provides a direct mechanism for specifying task graphs and a collection of concurrent data structures and synchronization primitives.\n泛型程序设计（generic programming）是程序设计语言的一种风格或范式，允许程序员在强类型程序设计语言中编写代码时使用一些以后才指定的类型，在实例化时作为参数指明这些类型。\nIt handles scheduling of tasks, whether explicit programmed or inferred from pattern calls, to a fixed number of threads internally. In effect, this is a hidden Bag-of-Tasks, leaving the OS with almost nothing to do.\nGame of Life (cs106b 作业1) Original Code for a Step\nenum State {DEAD,ALIVE} ; // cell status typedef State **Grid; void NextGen(Grid oldMap, Grid newMap) { int row, col, ncount; State current; for (row = 1; row \u0026lt;= MAXROW; row++) { for (col = 1; col \u0026lt;= MAXCOL; col++) { current = oldMap[row][col]; ncount = NeighborCount(oldMap, row, col); newMap[row][col] = CellStatus(current, ncount); } } } TBB parallel_for 假设我们想将上面的函数NextGen应用到数组(网格)的每个元素，这个例子是可以放心使用并行处理模式的。函数模板tbb::parallel_for 将此迭代空间(Range)分解为一个个块，并把每个块运行在不同的线程上。要并行化这个循环，第一步是将循环体转换为可以在一个块上运行的形式 - 一个STL风格的函数对象，称为body对象，其中由operator()中处理。 Game of Life Step Using parallel_for\nvoid NextGen(Grid oldMap, Grid newMap) { parallel_for (blocked_range\u0026lt;int\u0026gt;(1, maxrow+1), // Range CompNextGen(oldMap, newMap), // Body affinity_partitioner()); // Partitioner } Range defines a task(iteration) space, and its sub-division (partition) technique; Body defines the code which processes a range; Partitioner (optional parameter) influencing partitioning and scheduling strategy.\nThe parallel_for Template:\ntemplate \u0026lt;typename Range, typename Body\u0026gt; void parallel_for(const Range\u0026amp; range, const Body \u0026amp;body); Requires definition of:\nA range space to iterate over Must define a copy constructor and a destructor a destructor to destroy these copies Defines is_empty() Defines is_divisible() Defines a splitting constructor, R(R \u0026amp;r, split) A body type that operates on the range (or a subrange) Must define a copy constructor, which is invoked to create a separate copy (or copies) for each worker thread. Defines operator() In the C++ programming language, a copy constructor is a special constructor for creating a new object as a copy of an existing object.\n//通用形式 classname (const classname \u0026amp;obj) { // body of constructor } //实例 #include \u0026lt;iostream\u0026gt; using namespace std; class Line { public: int getLength( void ); Line( int len ); // simple constructor Line( const Line \u0026amp;obj); // copy constructor ~Line(); // destructor private: int *ptr; }; Range Class A blocked_range\u0026lt;T\u0026gt; is a template class provided by the library. It describes a one-dimensional iteration space over type T. and be queried for the beginning (r.begin()) and end (r.end()) of the range.\nThe TBB runtime can break a blocked_range into two smaller ranges, each (roughly) half the size.\nNote that a blocked_range carries no problem data. The values in the range can be used as we choose, for example to index into arrays. Range is Generic:\nR::R (const R\u0026amp;) // Copy constructor R::~R() // Destructor bool R::is_empty() const // True if range is empty bool R::is_divisible() const // True if range can be partitioned R::R (R\u0026amp; r, split) // Splitting constructor; splits r into two subranges Besides the provided blocked_range and blocked_range2d, users can define their own ranges. TBB DIY Range Example: Compute Fibonacci numbers.\nclass FibRange { public: int n_ ; // represents the range corresponding to fib(n) FibRange(int n) : n_(n) { } FibRange(FibRange\u0026amp; other, split) // split constructor : n_(other.n_ - 2) // initialize the new object { other.n_ = other.n_ - 1;} // reuse the other range object bool is_divisible() const { return (n_ \u0026gt; 10); } // sequential threshold bool is_empty() const { return n_ \u0026lt; 0; }; }; Body Class class CompNextGen { Grid oldMap, newMap; public: CompNextGen (Grid omap, Grid nmap) : oldMap(omap), newMap(nmap) {} // 分割迭代空间的方式多种多样 void operator()( const blocked_range\u0026lt;int\u0026gt;\u0026amp; r ) const { for (int row = r.begin(); row \u0026lt; r.end(); row++){ // 这里按行分割 for (int col = 1; col \u0026lt;= maxcol; col++) { nState current = oldMap[row][col]; int ncount = NeighborCount(oldMap, row, col); newMap[row][col] = CellStatus(current, ncount); } } } } Body is Generic\nBody::Body(const Body\u0026amp;) \\\\ Copy constructor Body::~Body() \\\\ Destructor void Body::operator() (Range\u0026amp; subrange) const \\\\ Apply the body to subrange. Because the body object might be copied, its operator() should not modify the body hence should be declared const. Otherwise the modification might or might not become visible to the thread that invoked parallel_for, depending upon whether operator() is acting on the original or a copy. Credit from www.threadingbuildingblocks.org\nparallel_for partitions original range into subranges, and deals out subranges to worker threads in a way that: Balances load, Uses cache efficiently, and Scales.\nGame of Life 1D with C++11 Lambda Function, an alternative interface to parallel_for allows us to use a C++ lambda expression to avoid writing a body class.\nvoid NextGen(Grid oldMap, Grid newMap) { parallel_for (blocked_range\u0026lt;int\u0026gt;(1, maxrow+1), [\u0026amp;](const blocked_range\u0026lt;int\u0026gt;\u0026amp; r){ for (int row = r.begin(); row \u0026lt; r.end(); row++){ for (int col = 1; col \u0026lt;= MAXCOL; col++) { State current = oldMap[row][col]; int ncount = NeighborCount(oldMap, row, col); newMap[row][col] = CellStatus(current, ncount); } } } );} [\u0026amp;]引入 lambda 表达式. 该表达式创建一个类似于CompNextGen的函数对象. 当局部变量在 lambda expression 之外声明，但又在lambda表达式内使用时, 它们被\u0026quot;捕获\u0026quot;为函数对象内的字段. [\u0026amp;]指定引用，[=]指定按值捕获.\nTBB Partitioners TBB supports different partitioning strategy: 1, tbb::parallel_for( range, body, tbb::simple_partitioner() ); forces all ranges to be fully partitioned (i.e. until is_divisible() fails). 2, tbb::parallel_for( range, body, tbb::auto_partitioner() ); allows the TBB runtime to decide whether to partition the range (to improve granularity). 3, tbb::parallel_for( range, body, tbb::affinity_partitioner ); is like auto_partitioner() but also, when the parallel_for is inside a loop, tries to allocate the same range to the same processor core across iterations to improve cache behaviour.\nGame of Life Using a 2D decomposition\nvoid NextGen(Grid oldMap, Grid newMap) { parallel_for (blocked_range2d\u0026lt;int, int\u0026gt; (1, maxrow+1, 1, maxcol+1), // Range CompNextGen(oldMap, newMap)); // Body auto_partitioner()); // Partitioner } class CompNextGen { Grid oldMap, Grid newMap; public: CompNextGen (Grid omap, Grid nmap) : oldMap(omap), newMap(nmap) {} // 二维分割 void operator()( const blocked_range2d\u0026lt;int, int\u0026gt;\u0026amp; r ) const { for (int row = r.rows().begin(); row \u0026lt; r.rows().end(); row++){ for (int col = r.cols().begin(); col \u0026lt; r.cols().end(); col++) { State current = oldMap[row][col]; int ncount = NeighborCount(oldMap, row, col); newMap[row][col] = CellStatus(current, ncount); } } };} blocked_range2d is partitioned in alternating dimensions, level by level.\nTBB parallel_reduce Template TBB parallel_reduce has similar structure to parallel_for but additionally allows bodies to gather results internally as they go along.\nWe could parallelize a loop reduction (iterations are independent), as in a Numerical Integration example, with a parallel_for, but we would need a critical section of some kind to accumulate the partial results. parallel_reduce structures and hides this, with one further generic operation, called join.\ntemplate \u0026lt;typename Range, typename Body\u0026gt; void parallel_reduce (const Range\u0026amp; range, Body \u0026amp;body); Body::Body( const Body\u0026amp;, split ) //Splitting constructor Body::~Body() // Destructor void Body::operator() (Range\u0026amp; subrange) const // Accumulate results from subrange void Body::join( Body\u0026amp; rhs ); // Merge result of rhs into the result of this. When a worker thread is available, as decided by the task scheduler, parallel_reduce invokes the splitting constructor to create a subtask for the worker. When the subtask completes, parallel_reduce uses method join to accumulate the result of the subtask. It reuses Range concept from parallel_for. The Fib Body Class (with operator()), using the DIY range example - FibRange from above\nclass Fib { public: int fsum_ ; Fib() : fsum_(0) { } Fib(Fib\u0026amp; other, split) : fsum_(0) { } // use += since each body may accumulate more than one range void operator() (FibRange\u0026amp; range) { fsum_ += fib(range.n_ ); } int fib(int n) {if (n \u0026lt; 2) return 1; else return fib(n-1)+fib(n-2);} void join(Fib\u0026amp; rhs) { fsum_ += rhs.fsum_; }; }; int main( int argc, char* argv[] ) { task_scheduler_init init(2); Fib f; parallel_reduce(FibRange(FIBSEED), f, simple_partitioner()); cout \u0026lt;\u0026lt; \u0026#34;Fib \u0026#34; \u0026lt;\u0026lt; FIBSEED \u0026lt;\u0026lt; \u0026#34; is \u0026#34; \u0026lt;\u0026lt; f.fsum_ \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; return 0; } Using a simple_partitioner forces full splitting of the ranges. We could use auto_partitioner to let the TBB run-time system control this.\nThe Task Scheduler 如果一个算法不能自然地映射到前面提到的任何其中一种 high-level loop templates，可以使用 task scheduler 直接操作任务, 可以构建新的高级模板。\nAll of TBB\u0026rsquo;s parallel pattern constructs are implemented via the same underlying task scheduler, which executes a task graph representing the pattern.\nTBB also allows the programmer to (carefully!) create task graphs directly. This allows expression of unstructured task graphs, or the implementation and abstraction of further patterns.\nThere are functions to create new tasks as children of existing tasks and to specify the control dependencies between them.\nHow to code Fibonacci using tasks directly? The key method is task::execute, which we override with our application specific behaviour.\nRecursion is typically used to calculate Fibonacci number but leads to unbalanced task graph.\nFibonacci - Task Spawning Solution - Use TBB tasks to thread creation and execution of task graph:\nAllocate space for the task by a special \u0026ldquo;overloaded new\u0026rdquo; and method task::allocate_root - Create root of a task tree. Tasks must be allocated by special methods so that the space can be efficiently recycled when the task completes. Construct task with the constructor FibTask(n, \u0026amp;sum) invoked by new. When the task is run in step 3, it computes the nth Fibonacci number and stores it into *sum. Run the task and wait for completion with task::spawn_root_and_wait. #include \u0026#34;tbb/task.h\u0026#34; ... long ParallelFib( long n ) { long sum; FibTask\u0026amp; a = *new(task::allocate_root()) FibTask(n, \u0026amp;sum); task::spawn_root_and_wait(a); return sum; } class FibTask: public task { public: const long n; long* const sum; FibTask( long n_, long* sum_ ) : n(n_), sum(sum_) {} task* execute() { // Overrides virtual function task::execute if( n \u0026lt; CutOff ) { *sum = SerialFib(n); } else { long x, y; FibTask\u0026amp; a = *new( allocate_child() ) FibTask(n-1,\u0026amp;x); FibTask\u0026amp; b = *new( allocate_child() ) FibTask(n-2,\u0026amp;y); set_ref_count(3); // Set to 3 = 2 children + 1 for wait spawn( b ); // Start b running. // Start a running and wait for all children (a and b). spawn_and_wait_for_all(a); *sum = x+y; // Do the sum } return NULL; } }; The TBB scheduler runs tasks in a way that tends to minimize both memory demands and cross-thread communication. The intuition is that a balance must be reached between depth-first and breadth-first execution.\nAt any point in execution, the collection of known tasks is maintained as a shared graph. Each thread maintains its own double-ended queue of tasks (roughly, as pointers into the shared graph).\nNewly spawned tasks are added to the front of the local queue.\n当一条线程参与 task graph 时，它会不断按照优先原则执行下面的规则来获取任务:\nlooks at the front of its local queue, which encourages locality within one thread\u0026rsquo;s work; 如果 deque 为空，则此规则不适用； 假如失败, steal a task from the back of one other (randomly chosen) thread\u0026rsquo;s queue, which encourages stealing of big tasks, and discourages locality across threads. Linda Linda presents an alternative conceptual model for parallelism, based around a small library of operations. The Linda model saw something of a revival in distributed java systems programming, under the name JavaSpaces.\nThe key concept is that processes interact through tuple space, a global, content addressable memory, which is thread safe, with no race conditions, therefore does not require explicit locks. Each tuple is an ordered collection of typed data fields. Duplicate tuples are allowed.\nThe tuple space itself acts like a monitor. If a process tries to access a tuplen, it is blocked until a matching tuple becomes available.\nSemaphore - Linda have tuple (or a set of tuples for a counting semaphore) that represent the locks. If someone needs to enter the lock, it waits until a tuple is available in the bag, pull it out of the bag and inserts it back into the tuple space.\nProcesses run asynchronously and can operate on tuple space with six operations.\n1, Add new tuple to tuple space: out(exp1, exp2, ...., expN); - evaluates the expressions in the parameter list before atomically placing a copy of the results as a new tuple in tuple space. It could be considered as an asynchronous send with a wild-card destination in message-passing. out(\u0026quot;sum\u0026quot;, 2, 3), out(\u0026quot;Green\u0026quot;, x*y, square(2));\n2, To take a tuple from tuple space: in(tuple-template); - atomically removes from tuple space a tuple which matches the template. template contains actual values and formal parameters (indicated by ?) to be assigned during the match. 匹配包含与实际值的匹配，以及与形式参数类型 types 相匹配. in is blocking, in the sense that the caller is suspended until a matching tuple becomes available. For example: in(\u0026quot;sum\u0026quot;,?i,?j) matches \u0026quot;sum\u0026quot;, assigns 2 to i and 3 to j and the tuple is removed from the tuple space. in(\u0026quot;Green\u0026quot;, ?y, ?r, FALSE);. We could think of in as a blocking, asynchronous receive, with wild-card source, but with additional constraints implied by the pattern matching.\n3, Atomically read a tuple from tuple space with rd(tuple-template);\n4, Tuples may also be created with eval(expr, expr, ...) which is like out, but dynamically creates new processes to evaluate each field of the tuple which has been expressed as a function call. The calling process continues immediately, and the resulting tuple enters tuple space atomically when all the newly sparked processes have terminated\n5, Finally, there are non-blocking forms inp, rdp (p for \u0026ldquo;predicate\u0026rdquo;) which complete \u0026ldquo;immediately\u0026rdquo;, returning a boolean indicating whether or not a match occurred. This allow a process to carry on with a different task and then try again later.\nBag of Tasks Implementation: 同样以前面的 Adaptive Quadrature 为例. Use a (\u0026quot;counts\u0026quot;, x, y) tuple, in effect as a shared variable, to count the number of tasks and number of idle workers. The final field in a task tuple indicates whether this is a real task or a \u0026ldquo;no more tasks\u0026rdquo; signal.\nint main () { out(\u0026#34;total\u0026#34;, 0.0); out(\u0026#34;counts\u0026#34;, 1, P); // set initial #task and #idle out(\u0026#34;task\u0026#34;, a, b, f(a), f(b), approxarea, FALSE); // make initial task for (i = 0; i\u0026lt;P; i++) eval(worker()); // create P workers in (\u0026#34;counts\u0026#34;, 0, P); // no tasks left, and P workers idle in (\u0026#34;total\u0026#34;, ?total); // get the result out (\u0026#34;task\u0026#34;, 0.0, 0.0, 0.0, 0.0, 0.0, TRUE); // indicate no more tasks ... //use total } int worker() { while (true) { in(\u0026#34;task\u0026#34;, ?left, ?right, ?fleft, ?fright, ?lrarea, ?gameOver); if (gameOver) { // if gameOver == TRUE out (\u0026#34;task\u0026#34;, 0.0, 0.0, 0.0, 0.0, 0.0, TRUE); // for others to see break; } in(\u0026#34;counts\u0026#34;, ?size, ?idle); out(\u0026#34;counts\u0026#34;, size-1, idle-1); ... usual task calculations ... if (abs (larea + rarea - lrarea) \u0026gt; EPSILON) { // create new tasks out(\u0026#34;task\u0026#34;, left, mid, fleft, fmid, larea, FALSE); out(\u0026#34;task\u0026#34;, mid, right, fmid, fright, rarea, FALSE); in(\u0026#34;counts\u0026#34;, ?size, ?idle); out(\u0026#34;counts\u0026#34;, size+2, idle+1); } else { in (\u0026#34;total\u0026#34;, ?total); out (\u0026#34;total\u0026#34;, total+larea+rarea); in(\u0026#34;counts\u0026#34;, ?size, ?idle); out(\u0026#34;counts\u0026#34;, size, idle+1); } } } Pipeline Implementation: Use eval() to create the sieve processes dynamically as we need them. The sieve processes eventually turn into part of an \u0026ldquo;array\u0026rdquo; of primes in tuple space. We ensure pipelined message flow by tagging tuples with their destination and position in the sequence.\nvoid main (int argc, char *argv[]) { int i; eval(\u0026#34;prime\u0026#34;, 1, sieve(1)); // the 1st prime number, the 1st worker for (i=2; i\u0026lt;LIMIT; i++) { out(\u0026#34;number\u0026#34;, 1, i-1, i); // send number to sieve } } int sieve (int me) { int n, p, in_seq=1, out_seq=1, stop=FALSE; in(\u0026#34;number\u0026#34;, me, in_seq, ?p); // in_seq = 1, first arrival is prime while (!stop) { in_seq++; in(\u0026#34;number\u0026#34;, me, in_seq, ?n); // get the next candidate if (n==LIMIT) { stop = TRUE; out(\u0026#34;number\u0026#34;, me+1, out_seq, n); // pass on the signal } else if (n%p !=0) { // if never created a successor before if (out_seq == 1) eval(\u0026#34;prime\u0026#34;, me+1, sieve(me+1)); // new sieve out(\u0026#34;number\u0026#34;, me+1, out_seq, n); // and its first input out_seq++; } } return p; } Tuple Space Linda\u0026rsquo;s powerful matching model sets a demanding implementation challenge, way beyond the associative memory hardware used in on-chip caches.\nIndexing and hashing techniques adapted from relational database technology can help (e.g. Linda rd and SQL select).\nAdvanced Linda implementations perform considerable compile-time analysis of program specific tuple usage. For example, possible tuples (in a given program) can be categorised into a set of classes by type signature, and stored separately.\n","permalink":"https://congchan.github.io/posts/inf-course-note-parallel-programming-language-and-systems/","summary":"\u003cp\u003e爱丁堡大学信息学院课程笔记 Parallel Programming Language and Systems, Informatics, University of Edinburgh\u003c/p\u003e\n\u003cp\u003eReference:\n\u003ca href=\"http://www.inf.ed.ac.uk/teaching/courses/ppls/\"\u003ehttp://www.inf.ed.ac.uk/teaching/courses/ppls/\u003c/a\u003e\n\u003ca href=\"http://www.cs.cmu.edu/~213/\"\u003eCMU 15213: Introduction to Computer Systems (ICS)\u003c/a\u003e\n\u003ca href=\"http://csapp.cs.cmu.edu/\"\u003eComputer Systems: A Programmer\u0026rsquo;s Perspective\u003c/a\u003e\n\u003ca href=\"http://mpitutorial.com/\"\u003eA Comprehensive MPI Tutorial Resource\u003c/a\u003e\n\u003ca href=\"http://www.mcs.anl.gov/~itf/dbpp/text/node94.html#SECTION03500000000000000000\"\u003eA chapter on MPI from Ian Foster\u0026rsquo;s online Book Designing and Building Parallel Programs\u003c/a\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"introduction-to-parallel-computer-architecture\"\u003eIntroduction to parallel computer architecture\u003c/h2\u003e\n\u003cp\u003eCovering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model.\u003c/p\u003e","title":"Inf Course Note - Parallel Programming Language and Systems"},{"content":"爱丁堡大学信息学院课程笔记 Software Architecture, Process, and Management, Informatics, University of Edinburgh\nReference: microsoft IBM Software Architecture in Practice (3rd edition), Bass, Clements, and Kazman\nWhat is Software Architecture? Software architecture is often described as the organization or structure of a system, where the system represents a collection of components that accomplish a specific function or set of functions.\ngrouping components into areas of concern (layers): For example, the UI, business processing, and data access. focus on interaction between the components and how different components work together. 在书中的定义：\nThe software architecture of a system is the set of structures needed to reason about the system, which comprise software elements, relations among them, and properties of both. \u0026ndash; Software Architecture in Practice (3rd edition), Bass, Clements, and Kazman\n架构的关注点在于系统内各个应用和模块的交互和调用。软件架构的设计，需要考虑满足什么样的需求（用户或甲方），如何解决和优化问题（不同的方向各有偏重），操作中如何做选择（在不同的方面平衡，妥协）。\nArchitecture and design concerns very often overlap - The selection of data structures and algorithms or the implementation details of individual components are design concerns.\n没必要强硬区分二者，而应该综合起来看待。某些情况下，决策是自然而然的结构层面的；在某些情况下，决策更多是关于于设计层面，以及设计如何帮助实现架构。\n软件架构的定义有如下隐含意思： 1, Architecture Is a Set of Software Structures Three frequently occurring types of structure: – Modular structure: static structure that focus on how functionality is divided up, structured, and assigned to development and implementation teams. – Component and Connector structure: runtime structures that focus on how components interact (e.g. information passing, synchronisation, interference,…) – Allocation structures: mapping to organizational, development, installation, execution environments. (e.g. Components are deployed onto hardware to execute)\n2, Architecture Is an abstraction Architecture specifcally omits certain information about elements that is not useful for reasoning about the system - in particular, it omits information that has no ramifcations outside of a single element.\n3, Every Software System Has a Software architecture\n4, Architecture Includes behavior Behavior embodies how elements interact with each other.\n在本课程, 一个软件项目成功与否, 基于如下三点考量: – The software is delivered on schedule – Development costs were within budget – The software meets the needs of users\nContexts for Software Architecture Technical: where architecture supports technical activity like measurement, V\u0026amp;V, compliance,… Controlling Quality Attributes Availability - ensuring there is a system to take over if a system fails. Safety - ensuring that the system only behaves as intended and has no additonal behaviour. Testability - ensuring: elements are clearly able to be isolated we know what behaviour to expect of components of the system we know how components relate to modules so we can track down faulty code We know how components are intended to integrate to give the overall behaviour Other qa: performance, usability, interoperability,.. Design - Patterns, Styles, Domain Specific Architecture (DSSA) A DSSA is collection of (pre-decided) design decisions that: Capture important aspects of particular tasks (domain), Common across a range of systems in the domain Typically they will have some predefined structures These are not general purpose because they incorporate many specific characteristics of the domain. Architectural pattern is a set of architectural design decisions that are applicable to a recurring design problem, and parameterized to account for different software development contexts in which that problem appears. Similar to DSSAs but capture less of the behaviour and attributes of the system More general because they are intended to abstract a common pattern over several domains. Three-Tiered Pattern: State(database)-Logic(Business)-Display(UI) Model-View-Controller (MVC): to separate between information, presentation and user interaction. Sense-Compute-Control: Structuring embedded control applications Project lifecycle: where architecture interacts with and supports development process Lifecycle Models: V-model, iterative models (Boehm’s spiral model), Agile Business: where architecture supports organisations, e.g. customer organisations and development organisations. Professional: where the role of architect defines requirements and constraints on architects. Quality Attributes (QA) Architecture is the right level of abstraction to resolve conflicts between Stakeholders.\nQuality Attributes specify, usually quantitative, requirements on particular bits of functionality or on the whole systems (e.g. that the system should be available 99% of the time).\nProblems With QA 1, Often QA requirements are not “testable”, for example modifiable, usable, dependable or resilient. 2, It can be difficult to map from a concern about the system to a QA. For example, a high failure rate in some transaction could be a performance issue or it could be an availability issue. 3, Communities around a particular quality attribute have developed their own terminology (e.g. security has attacks, performance has events, etc).\nThe solution for 1 and 2 is to use quality attribute scenarios to provide sufficient specificity to avoid some of these issues.\nQuality Attributes Scenarios 场景 A quality attribute requirement should be unambiguous and testable. To specify quality attribute requirements, we capture them formally as six-part scenarios:\nSource of stimulus. This is some entity (a human, a computer system, or a system administrator) that generated the stimulus. Stimulus. A condition (event) that requires a response when it arrives at a system. e.g. a user operation to the usability community, or an attack to the security community. Environment. The stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation. For many systems, “normal” operation can refer to one of a number of modes. For these kinds of systems, the environment should specify in which mode the system is executing. Artifact. A collection of systems, the whole system, or part of the system that is stimulated e.g. the configuration checker in the system. Response. The response is the activity undertaken as the result of the arrival of the stimulus. e.g. the configuration issue is identified and then repaired. Response measure. how to measure the response so the scenario is testable. e.g. time to detect the wrong configuration and the time to repair. Each QA has a General Scenario associated with it that tries to capture the possible components involved in that particular QA. This acts as a template or guide for the architect specifying a specific QA Scenario.\nSpecific QA Scenarios take account of specific stimuli and measures on response, they capture the specification of the QA for a particular system.\nAchieving QA through tactics Architectural tactics are design decisions to achieve the required quality attributes, more specifcally, to control responses to stimuli.\nThe focus of a tactic is on a single quality attribute response. Within a tactic, there is no consideration of tradeoffs (differ from architectural patterns, where tradeoffs are built into the pattern).\nBy cataloging tactics, we provide a way of making design more systematic within some limitations.\nAn architecture can be viewed as the result of applying a collection of design decisions. A systematic categorization of these decisions:\nAllocation of responsibilities 责任分配: Identifying the important responsibilities, and determining how these responsibilities are allocated to static and runtime elements (namely, modules, components, and connectors). Coordination model 模型协调 - Components in the architecture interact with one another via a collection of mechanisms. What elements in the system need to coordinate with one another. What properties the coordination needs to have (e.g. timing properties, security of coordination, …) Choosing the mechanisms (ideally a small number) that realize properties like statefulness, synchrony, delivery guarantees, performance. Data model: Every system must represent artifacts of system-wide interest—data—in some internal fashion Choosing abstractions, operations, and properties. How data is created and destroyed, access methods, \u0026hellip; Maintaining metadata that controls the interpretation of the data. Organising the data, what kind of system will be used to store it, how will it be backed up, how do we recover from data loss Management of resources: hard (CPU, memory, battery, I/O ports…) or soft resources(system locks, software buffers, thread pools…): Identifying resources to be managed What system element should manage a resource 资源共享策略和争端仲裁 Work out sharing strategies and how to arbitrate in contention situations Consider the consequences of resource starvation(e.g. Memory). Mapping among architectural elements two important types of mapping: Mapping between different types of elements in the architecture, e.g. from static development structures (modules) to execution elements e.g. threads or processes. Mappings between software elements and environment elements, e.g. from processes to specific processors and other hardware. Useful mappings include: code to runtime structure; runtime elements to environment; data model elements to data stores. Binding time decisions: introduce allowable ranges of variation. This variation can range from design time by a designer to runtime by an end user might allocate a responsibility. The decisions in the other six categories have an associated binding time decision: we might want some variability in the resources to be managed determined at run time or we might make the coordination model negotiable at runtime if we want to inter-operate with a range of systems. Choice of technology: critical to being able to realize all the other decisions in a concrete system. What technologies are available What tools are available to support technologies How much training will it take to be able to use a technology? What are the full range of consequences of the choice of a technology (e.g. it may restrict markets because it is incompatible with some other technologies). If the technology is new, how does it fit into the existing preferred technologies for the organisation. Availability Availability refers to a property of software that it is there and ready to carry out its task when you need it to be. The availability of a system is usually defined to be the probability it will be there when you ask it to work: $\\frac{mtbf}{mtbf+mttr}$\n$mtbf$ – mean time between failures: MTBF of a component is the sum of the lengths of the operational periods divided by the number of observed failures: $mtbf = \\frac{t}{N(t)}$, $t$ is the cumulative operating time, $N(t)$ is the observed number of failures by time $t$. 假设恒定的故障率 $\\lambda$，则 $mtbf = \\frac{1}{\\lambda}$\n$mttr$ – mean time to repair\nAvailability measures the quality of service in terms of running versus down time\nReliability indicates the fraction of all attempted operations that complete successfully. The reliability of the system is: $R(t) = e^{-\\lambda t}$ where the parameter $\\lambda$ is called the failure rate. 由于MTBF主要针对可以修复的系统，因此建议针对不可修复的系统（在故障后选择更换而不是修复系统的情况）使用平均故障时间（MTTF），在数学上二者是等价的。 MTTF: Mean Time To (first) Failure, or Expected Life. $ MTTF = E(t_f) = \\int_0^\\infty R(t)dt = \\frac{1}{\\lambda}$\nFaults, Errors, Failures: A fault is something in the system (e.g. failed component, wrong bit of code,…) that can cause the system to move into an error state when the fault is activated, an error may then eventually cause an externally observable deviation from the intended operation - failure.\nGeneric Scenario Design Checklist for Availability Allocation of Responsibilities ■ Determine the system responsibilities that need to be highly available. ■ Within those responsibilities, ensure that additional responsibilities have been allocated to detect an omission, crash, incorrect timing, or incorrect response. ■ Additionally, ensure that there are responsibilities to do the following: • Log the fault • Notify appropriate entities (people or systems) • Disable the source of events causing the fault • Be temporarily unavailable • Fix or mask the fault/failure • Operate in a degraded mode\nCoordination Model Determine the system responsibilities that need to be highly available. With respect to those responsibilities, do the following: ■ Ensure that coordination mechanisms can detect an omission, crash, incorrect timing, or incorrect response. For example, whether guaranteed delivery is necessary. Will the coordination work under conditions of degraded communication? ■ Ensure that coordination mechanisms enable the logging of the fault, notification of appropriate entities, disabling of the source of the events causing the fault, fxing or masking the fault, or operating in a degraded mode. ■ Ensure that the coordination model supports the replacement of the artifacts used (processors, communications channels, persistent storage, and processes). For example, does replacement of a server allow the system to continue to operate? ■ Determine if the coordination will work under conditions of degraded communication, at startup/shutdown, in repair mode, or under overloaded operation. For example, how much lost information can the coordination model withstand and with what consequences?\nData Model ■ Determine which portions of the system need to be highly available. ■ Within those portions, determine which data abstractions, along with their operations or their properties, could cause a fault of omission, a crash, incorrect timing behavior, or an incorrect response. ■ For those data abstractions, operations, and properties, ensure that they can be disabled, be temporarily unavailable, or be fxed or masked in the event of a fault. ■ For example, ensure that write requests are cached if a server is temporarily unavailable and performed when the server is returned to service.\nMapping among Architectural Elements ■ Determine which artifacts (processors, communication channels, persistent storage, or processes) may produce a fault. ■ Ensure that the mapping (or remapping) of architectural elements is ﬂexible enough to permit the recovery from the fault. This may involve a consideration of the following: • Which processes on failed processors need to be reassigned at runtime • Which processors, data stores, or communication channels can be activated or reassigned at runtime • How data on failed processors or storage can be served by replacement units • How quickly the system can be reinstalled based on the units of delivery provided • How to (re)assign runtime elements to processors, communication channels, and data stores • When employing tactics that depend on redundancy of functionality, the mapping from modules to redundant components is important. For example, it is possible to write one module that contains code appropriate for both the active component and backup components in a protection group.\nResource Management ■ Determine what critical resources are necessary to continue operating in the presence of a fault. ■ Ensure there are suffcient remaining resources in the event of a fault to log the fault; notify appropriate entities (people or systems); disable the source of events causing the fault; be temporarily unavailable; fx or mask the fault/failure; operate normally, in startup, shutdown, repair mode, degraded operation, and overloaded operation. ■ Determine the availability time for critical resources, what critical resources must be available during specifed time intervals, time intervals during which the critical resources may be in a degraded mode, and repair time for critical resources. Ensure that the critical resources are available during these time intervals. ■ For example, ensure that input queues are large enough to buffer anticipated messages if a server fails so that the messages are not permanently lost.\nBinding Time ■ Determine how and when architectural elements are bound. ■ If late binding is used to alternate between components that can themselves be sources of faults (e.g., processes, processors, communication channels), ensure the chosen availability strategy is suffcient to cover faults introduced by all sources. ■ For example: • If late binding is used to switch between artifacts such as processors that will receive or be the subject of faults, will the chosen fault detection and recovery mechanisms work for all possible bindings? • If late binding is used to change the defnition or tolerance of what constitutes a fault (e.g., how long a process can go without responding before a fault is assumed), is the recovery strategy chosen suffcient to handle all cases? For example, if a fault is ﬂagged after 0.1 milliseconds, but the recovery mechanism takes 1.5 seconds to work, that might be an unacceptable mismatch. • What are the availability characteristics of the late binding mechanism itself? Can it fail?\nChoice of Technology ■ Determine the available technologies that can (help) detect faults, recover from faults, or reintroduce failed components. ■ Determine what technologies are available that help the response to a fault (e.g., event loggers). ■ Determine the availability characteristics of chosen technologies themselves: What faults can they recover from? What faults might they introduce into the system?\nPerformance To ensure resource is effectively monitored and managed.\nDesign Checklist for Performance Allocation of Responsibilities ■ Work out areas responsibility of that require heavy resource use to ensure time-critical events take place. ■ Work out processing requirements. ■ Take account of: • Responsibilites arising from threads crossing boundaries of responsibility • Responsibilities for thread management • Responsibilities for scheduling shared resources\nCoordination Model ■ What needs to coordinate. ■ Is there concurrency? Ensure it is safe. ■ Ensure coordination is appropriate for the style of stimulus. ■ Ensure the properties of the coordination model are good for the stimuli and concurrency control?\nData Model ■ Determine what parts of the data model will be heavily loaded or behaves tight time constraints. ■ For those data abstractions, determine: • Would keeping multiple copies help? • Would partitioning the data help? • Is it possible to reduce processing requirements for the data? • Does adding resource help deal with data bottlenecks?\nMapping Among Architecture Elements ■ Does colocation of some components reduce latencies? ■ Ensure components with high processing needs are allocated to big processors ■ Consider introducing concurrency when you map. ■ Consider whether some mappings introduce bottlenecks (e.g. allocating non-interfering tasks to the same thread)\nResource Management ■ Work out what needs high levels of resource ■ Ensure these are monitoredand managed under all operating modes. ■ For example: • Time critical components • Thread management • Prioritization • Locking and scheduling strategies • Deploying additional resource to meet elevated load.\nBinding time ■ Look at when you bind. ■ Consider the cost of binding at different times ■ Try to avoid performance penalties caused by late binding.\nChoice of Technology ■ Is the technology right to let you meet hard deadlines and resource use (e.g. use a real-time OS with proper scheduling). ■ Do you know its characteristics under load and its limits? ■ Does your choice of technology give you the ability to set the following: • Good scheduling • Priorities • Policies for demand reduction • Allocating processing to tasks • Other performance-related parameters. ■ Does your choice of technology introduce excessive overhead for heavily used operations?\nSecurity 最简单的表征安全的三个特征 - confdentiality, integrity, and availability (CIA):\n机密性 Confidentiality: Only those who should have access are given access. 完整性 Integrity: Data or services are not subject to unauthorised manipulation. 可用性 Availability: the system is available for legitimate use. 其他用于支撑 CIA 的特征: 4. 认证识别 Authentication verifes the identities of the parties to a transaction and checks if they are truly who they claim to be. 5. 不可否认性 Nonrepudiation guarantees that the sender of a message cannot later deny having sent the message, and that the recipient cannot deny having received the message. 6. 授权 Authorization grants a user the privileges to perform a task.\nSecurity General Scenario A Design Checklist for Security Allocation of Responsibilities ■ Ensure all actors have identities ■ Authenticate identities ■ Check authorizations ■ Ensure authorization is required for all such actors ■ Log attempts, successes and failures on all sensitive operations ■ Ensure data is encrypted ■ Ensure responsibilities are allocated to appropriate actors.\nCoordination Model ■ Ensure coordination mechanisms use authentication and authorisation. ■ Ensure coordination mechanisms are not vulnerable to impersonation, tampering, interception, … ■ Ensure data involved in coordination is protected using encryption. ■ Monitor level of demand for communication to identify excessive demands\nData Model ■ Ensure there is a valid data model that disallows invalid data flows. ■ Ensure logging of access, modification and attempted access or modification. ■ Data is protected in flight and at rest using appropriate encryption. ■ Ensure appropriate backup/recovery mechanisms are in place.\nMapping among Architectural Elements ■ Explore how different mappings change the way users can access resources. ■ Ensure for all of these mappings the models of access and authorisation are preserved. • Actors should be identified and authenticated • Use appropriate authorisation mechanisms • Ensure logging is enabled • Ensure data is protected by encryption • Recognise impact of attack on resources ■ Ensure recovery from attack is possible\nResource Management ■ Explore the overheads resulting from monitoring, detecting, preventing and recovering from attacks. ■ Analyse how a user can access and make demands on critical resources. ■ Manage resource access to ensure malicious use of resource is detected and managed. ■ Identify the potential for corruption/contamination and how to manage this. ■ Explore the potential for resource use to be used as a covert channel to transmit data. ■ Limit resources used to manage attempts at unauthorised use\nBinding Time ■ Explore the consequences of varying binding times on the ability to trust an actor or component. ■ Put in place appropriate mechanisms to ensure trust given binding time. ■ Explore potential impact on resource use, capacity/throughput, response time ■ Ensure appropriate encryption of all data around binding. ■ Explore the potential of variation in binding time as a covert channel.\nChoice of Technologies ■ Ensure limitations of technologies are understood and the potential for future compromise is well identified. ■ Ensure your chosen technologies support the tactics you want to deploy to protect the system.\nConnectors Key part of Architectures ■ Connect components and define the rules of interaction between components • Simple: shared variable access; method calls; … • Complex: database access; client-server; scheduler; load balancer ■ Connectors provide: Interaction ducts;\nIn coding often connectors are implicit, but in software architecture: ■ They are identified and have an identity ■ Capture system interaction (at the level of components) ■ They have a specification that can be complex\nRelationship between Connectors and components: ■ Components have application-specific functionality. ■ Connectors provide interaction mechanisms that are generic across different applications. ■ Interaction may involve multiple components ■ Interaction may have a protocol associated to it. The specification of the connector protocols determine: the types of interface that it works with; properties of interaction; rules about ordering of interaction; measurable features of interaction.\nBenefits of Explicit Connectors ■ Interaction is defined by the arrangement of the connectors (as far as possible) ■ Component interaction is defined by the pattern of connectors in the architecture ■ Interaction is “independent” of the components\nThe main roles(services) of Connectors are:\nCommunication Information is transmitted between components (e.g. message passing; method call; remote procedure call,…). Connectors constrain things: Direction of flow (e.g. pipes), Capacity, rates of flow, etc. May have other effects e.g. coordination (e.g. blocking I/O) Influences measurable Quality Attributes of the system Separates communication from functional aspects (components do the functional part). Coordination: Controls the timing relationship of functional aspects of the system, e.g. coordinating the arrival of data at a collection of components Conversion How to get components to interact that don\u0026rsquo;t have the right means of interaction. 如何让兼容性差的组件进行交互？ Incompatibilities might be related to: datatypes, ordering, frequency, structure of parameters etc. Examples of types of converters: Wrappers (deal with structural issues), Adaptors (deal with datatype incompatibilities) Facilitation Enable interaction among a group of components that are intended to interact. Help manage the interaction Examples: load balancer; replication management; redundancy management; scheduler Can also relate to coordination, e.g. synchronization (critical sections; monitors) Select Connectors Types of Connector: • Method/Procedure call • Data access • Events • Stream • Distributor • Arbitrator • Adaptor\nSelection\nDetermine a system’s interconnection and interaction needs Determine roles to be fulfilled by the system’s connectors: Communication, coordination, conversion, facilitation For each connector Determine its appropriate type(s) Determine its dimensions of interest Select appropriate values for each dimension For multi-type, i.e., composite connectors, determine the atomic connector compatibilities Architectural Patterns An architectural patterns is a package of design decisions that is found repeatedly in practice, has known properties that permit reuse, and describes a class of architectures.\nAn architectural pattern comprises:\nA context that provides the frame for a problem. A problem that is a generalised description of a class of problems often with QA requirements that should be met. A solution that is suitably generalised in the same way as the problem. A solution: Describes the architectural structures that solve the problem, including how to balance the many forces at work. The solution might be static, runtime or deployment oriented. The solution for a pattern is determined and described by: A set of element types (for example, data repositories, processes, and objects) A set of interaction mechanisms or connectors (for example, method calls, events, or message bus) A topological layout of the components A set of semantic constraints covering topology, element behavior, and interaction mechanisms Module Patterns Static Pattern: Layered Pattern\nOverview: The layered pattern defines layers (groupings of modules that offer a cohesive set of services) and a unidirectional allowed-to-use relation among the layers. The pattern is usually shown graphically by stacking boxes representing layers on top of each other. Suitable for controlling static aspects of architecture.\nElements: Layer, a kind of module. The description of a layer should define what modules the layer contains and a characterization of the cohesive set of services that the layer provides.\nRelations: Allowed to use, which is a specialization of a more generic depends-on relation. The design should define what the layer usage rules are (e.g., “a layer is allowed to use any lower layer” or “a layer is allowed to use only the layer immediately below it”) and any allowable exceptions. Constraints: ■ Every piece of software is allocated to exactly one layer. ■ There are at least two layers (but usually there are three or more). ■ The allowed-to-use relations should not be circular (i.e., a lower layer cannot use a layer above).\nWeaknesses: ■ The addition of layers adds up-front cost and complexity to a system. ■ Layers contribute a performance penalty.\nComponent-and-Connector Patterns Model-View-Controller Pattern\nContext: User interface software is typically the most frequently modifed portion of an interactive application. For this reason it is important to keep modifcations to the user interface software separate from the rest of the system.\nProblem: • Isolating the UI functionality from the Application functionality. • Maintaining multiple views in the presence of change in the underlying data.\nSolution: Other Component-Connector Patterns • Pipe and Filter Pattern • Broker Pattern • Client-Server Pattern • Peer-to-Peer Pattern • Service-Oriented Architecture Pattern • Publish-Subscribe Pattern • Shared Data Pattern\nDeployment/Allocation Patterns Context: – we are concerned with resource use – We might consider flexible deployment of resource – The QAs we care about are sensitive to the pattern of deployment and the use of resources.\nAllocation: Map-Reduce Pattern Context: – We have large quantities of data we wish to treat as “population” data. – This encourages an approach that involves significant amounts of independent processing.\nProblem: Where for ultra-large data sets doing some individual processing to a portion of the data set and then sorting and analyzing grouped data, map-reduce provides a simple way of doing this processing.\nSolution: Other Allocation Patterns • Multi-tier architecture pattern • Cloud architectures\nRelationships between Tactics and Patterns Architectural patterns and tactics are ways of capturing proven good design structures and making them reusable.\nTactics are simpler and more atomic than patterns • Tactics capture one step to take for a particular Quality Attribute to change behaviour with respect to that QA. • use just a single structure or computational mechanism, and they are meant to address a single architectural force. • Tactics can be seen as the building blocks of patterns; Most patterns consist of (are constructed from) several different tactics. Testability Testability illustrate QAs from a static perspective.\nA system or element of a system is testable if it is possible to test it in the way required by a particular development or maintenance process.\nTestability Concerns • Unlike the other QA (availability, performance and security), testability is concerned with the code structure rather than the connector/component view or deployment view. • The system elements we consider are code modules and the relationships are dependencies involved in building the code for components.\nTestability General Scenario 举例 Coverage Concrete Scenario • Source: Regression Tester • Stimulus: Completion of maintenance development to repair a critical bug • Artifact: Modules for the full system • Environment: Maintenance Development • Response: Results from path coverage tool • Response Measure: Path coverage is better than 95% of non-looping paths inside modules\nTestability Tactics\nadding controllability and observability to the system. Specialized Interfaces Record/Playback Localize State Storage Abstract Data Sources Sandbox Executable Assertions limiting complexity in the system’s designs: If it could be broken into smaller modules with lower complexity that could allow the regression test to achieve higher path coverage. Limit Structural Complexity Limit behavioral complexity - Nondeterminism A Design Checklist for Testability Allocation of Responsibilities ■ Determine which system responsibilities are most critical and hence need to be most thoroughly tested. ■ Ensure that additional system responsibilities have been allocated to do the following: Execute test suite and capture results (external test or self-test) • Capture (log) the activity that resulted in a fault or that resulted in unexpected (perhaps emergent) behavior that was not necessarily a fault • Control and observe relevant system state for testing Make sure the allocation of functionality provides high cohesion, low coupling, strong separation of concerns, and low structural complexity.\nCoordination Model Ensure the system\u0026rsquo;s coordination and communication mechanisms ■ Support the execution of a test suite and capture the results within a system or between systems ■ Support capturing activity that resulted in a fault within a system or between systems ■ Support injection and monitoring of state into the communication channels for use in testing, within a system or between systems ■ Do not introduce needless nondeterminism\nData Model Determine the major data abstractions that must be tested to ensure the correct operation of the system. ■ Ensure that it is possible to capture the values of instances of these data abstractions ■ Ensure that the values of instances of these data abstractions can be set when state is injected into the system, so that system state leading to a fault may be re-created ■ Ensure that the creation, initialization, persistence, manipulation, translation, and destruction of instances of these data abstractions can be exercised and captured\nMapping among Architectural Elements ■ Determine how to test the possible mappings of architectural elements (especially mappings of processes to processors, threads to processes, and modules to components) so that the desired test response is achieved and potential race conditions identifed. ■ In addition, determine whether it is possible to test for illegal mappings of architectural elements.\nResource Management ■ Ensure there are suffcient resources available to execute a test suite and capture the results. ■ Ensure that your test environment is representative of (or better yet, identical to) the environment in which the system will run. ■ Ensure that the system provides the means to do the following: • Test resource limits • Capture detailed resource usage for analysis in the event of a failure • Inject new resource limits into the system for the purposes of testing • Provide virtualized resources for testing\nBinding Time ■ Ensure that components that are bound later than compile time can be tested in the late-bound context. ■ Ensure that late bindings can be captured in the event of a failure, so that you can re-create the system’s state leading to the failure. ■ Ensure that the full range of binding possibilities can be tested\nChoice of Technology ■ Determine what technologies are available to help achieve the testability scenarios that apply to your architecture. Are technologies available to help with regression testing, fault injection, recording and playback, and so on? ■ Determine how testable the technologies are that you have chosen (or are considering choosing in the future) and ensure that your chosen technologies support the level of testing appropriate for your system. For example, if your chosen technologies do not make it possible to inject state, it may be diffcult to re-create fault scenarios.\nModifiability Modifiability illustrate QAs from a static perspective. Measure how easy it might be to modify. This is a key area because change incurs cost.\nFour key questions: – What can change? – How likely is something to change? – When, where, how and by whom will changes be made? – What is the cost of making the change?\nGeneral Scenario Tactics to control modifiability GPES Example Version 1: General purpose query facility in each GP system. Version 2: Building a specific piece of business logic for each different query. Think about: – What changes can happen? – How likely is a change? – When, where, how and by whom? – How mush will it cost?\nGPES-relevant Scenario • Source: One of the stakeholders e.g. Medicines and Healthcare Products Regulatory Agency • Stimulus: Wants prescribing data on NSAIDs • Artifacts: Code (but depending on the architecture this could be configuration data) • Environment: Operation • Response: Develop the code • Response Measure: Data available 5 weeks after request\nGPES Version 1 • Design and validate the query with the Medicines agency. • Code the query. • Test on some systems to ensure it does not have bad effects. • Rollout to all systems. • Make the query available to Medicines agency.\nGPES Version 2 • Design and validate the query with Medicines agency. • Negotiate with the GP system providers on the design of the business logic (different in all systems?) • Are the providers the only vendor of such services? Should it go to a procurement? • Validate the queries on each system • Integrate the results • Roll out to all systems • Make the query available to the Medicines Agency\nIt seems likely that the GPES V2 architecture will not pass the modifiability scenario we describe. Are any of the modifiability tactics appropriate to change the architecture to enable it to pass the scenario? ■ Reduce Coupling is the category of tactics we need to consider. ■ Each of the following offer potential routes with slightly different emphases: • Use an intermediary • Restrict dependencies • Refactor • Abstract common services ■ Defer Binding: can we do this later in the process so it is more likely to be done by a computer than a human? Here this is unlikely. ■ More on Binding Time • Compile time/Build Time: component replacement, compile time parameters,… • Deployment time: configuration scripts that bind at deployment, … • Initialization time: resource files • Runtime: dynamic lookup, service lookup, name servers, plugins, publish-subscribe, shared repositories, (Maybe just in time compilation fits here too)\nDesign checklist for Modifiability Allocation of responsibilities Work out how things are likely to change e.g. technical, legal, organisational, social, markets, customers.. ■ Work out what responsibilities change. ■ Try to modularise so a change does not affect responsibilities that span many modules.\nCoordination model Look at how changes are likely to affect coordination and try to ensure that the most likely changes impact coordination across a small number of modules\nData model Similar to coordination model – see how a change impacts on data models and try to esnure data model changes span as few modules as possible.\nMapping among architectural elements ■ Looking at potential changes to the system, assess whether some may best be responded to by changing the mapping to elements. ■ Explore issues such as dependencies between elements, data holdings in elements, assignment of elements to processes, threads or processors.\nResource Management ■ Determine how a change in responsibility or quality attribute will change resource. ■ Attempt to localise resourcing change resulting from a likely change to a small number of modules. ■ Look at ways of using policies or configuration to manage resource change more effectively\nBinding Time ■ Control choice of binding times so there are not too many combinations to consider. ■ Consider attempting to defer binding to later, balance this against the cost of providing a later binding mechanism.\nChoice of Technology Choose technologies that make the most likely changes easier (e.g. choose a technology that allows runtime alteration of critical parameters rather than one where parameters are chosen at compile time) but balance this agains the cost of the different technologies.\nArchitectural Modelling Software Architecture is intended to give us control over Quality Attributes. Ideally we’d like to be able to use Software Architecture to predict Quality Attributes. We should be able to build a predictive model of the Software Architecture and use the model to predict QAs. The current situation is patchy…\nSome quality attributes, most notably performance and availability, have well-understood, time-tested analytic models that can be used to assist in an analysis. Analytic model means one that supports quantitative analysis.\nTypes of Analysis • Thought experiment: just a sort of discussion using informed people. • Back of the envelope: using very approximate techniques with unreliable assumptions. • Checklist: collated experience. • Analytic Model: based on sound abstractions – heavily dependent on estimates being correct • Simulation: higher level of detail – less analytic, more concrete. • Prototype: approximate system in an experimental setup. • Experiment: fielded system, simulated load • Instrumentation: measuring the variable of interest\nAnalyzing Performance Models have parameters, which are values you can set to predict values about the entity being modeled. Model can be used to understand the latency characteristics of an architectural design. Data Needed for the Queuing Model ■ We need the following information in order to model effectively: • The distribution for the arrival of service requests • The queuing discipline • The scheduling algorithm • The distribution of service times for service requests • Network characteristics ■ The theory places restrictions on the distributions • Arrivals are usually expected to be Poisson Distributions specified by arrival rate • Service times are usually exponentially distributed on the service rate. • Some queuing behaviors are excluded such as reneging or jockying\nExample: MVC, says nothing about its deployment. That is, there is no specifcation of how the model, the view, and the controller are assigned to processes and processors; that’s not part of the pattern’s concern. These and other design decisions have to be made to transform a pattern into an architecture. Until that happens, one cannot say anything with authority about how an MVC-based implementation will perform. Data for MVC • Rate of service requests: the View component will service them at some rate. • Service requests to the Controller are generated by the View component. • Service requests from the Controller to the View component • Service requests from the Controller to the model • Service requests from the Model to the View Component\nModelling MVC We need estimates of: ■ Distribution of external service demands ■ Queuing Disciplines within the queues in front of each component. ■ Network latencies ■ Transfer characteristics: • View – Controller • Controller – View • Controller – Model • Model – View ■ Scaling to large numbers of components is an issue\nAnalyzing Availability One key issue is how long it takes to detect that a failure has taken place. Example is a Broker system. Hot Spare 热备用 (Active Redundancy) • Active and redundant both receive identical request stream. • Synchronous maintenance of broker state. • Fast failover in the event of failure of the active system.\nWarm Spare (Passive Redundancy) • Warm broker is maintained at the most recent checkpoint state. • In the event of failure the system rolls back to the most recent checkpoint. • This is slower than the hot spare approach\nCold Spare • No attempt to synchronise. • In the event of failure the cold spare is started. • The system state is recovered via interaction with other systems (so they have to be resilient to failure in the broker)\nAnalysis at Different Stages of the Life Cycle Architecture in the Life Cycles 前面部分关注软件架构的 technical context。这里开始关注 life cycles。 The role of software architecture is different for different lifecycles.\nBalancing Agility and Discipline • Lifecycles generally impose some discipline on the development process. • Software Architectures often feature in Lifecycles as a stage or support for analysis or design • Lifecycles exist because they codify useful patterns of activity and save us time and effort • Agility focusses on getting adequate solutions to stakeholders with less time and effort • We need to balance the discipline of lifecycles against the delivery focus of agility\nLifecycles • Lifecycles underpin development processes by ordering stages and activities. • Any good organisation is always looking to improve its processes so there is usually an ongoing process improvement cycle focussed on making the process better.\nV-Model approach works well when you understand the concept and requirements. Agile Practice • Test-first programming • Refactoring • Continuous integration • Simple Design • Pair Programming • Common Codebase • Coding Standards • Open Work Area\nAgile vs. Plan Driven Early software development methods that emerged in the 1970s - such as the Waterfall method - is plan-driven and inﬂexible. But having a strong 先期 up-front plan provides for considerable predictability (as long as the requirements don’t change too much) and makes it easier to coordinate large numbers of teams.\nAgile methods and practitioners, on the other hand, often 轻视 scorn planning, preferring teamwork, frequent face-to-face communication, ﬂexibility, and adaptation. This enhances invention and creativity.\n• Work top-down and bottom-up simultaneously - balance will depend on the size and complexity of the project. • Top-down does architectural work based on things like patterns, product-line. • Bottom-up develops implementation and environment-specific constraints and solutions. • Focus on QAs, scenarios, tactics and processes to 调和 reconcile competing aspects provides a bottomup/top-down link • Balancing commitment and flexibility\nAnalysis Techniques Product Line Architecture One of the early success areas for Software Architecture was the development of Product Line Architectures. Product Line Architecture is an approach to adopt systematic reuse of architectural elements that involves changes in development process supported by specific practices that encourage reuse.\nA collection of software-intensive systems sharing a common, managed, set of features that satisfy the specific needs of a market segment or mission that are developed from a set of core assets in a prescribed way.\nSoftware Product Lines are directed by business goals in a particular application domain. • The products in a product line share a software product line architecture • Products are structured by the product line architecture and are built from services and components. • Architercture and components are the core assets used to satisfy the business goals. • Product line leverage commonality and limit variability of the product.\nBenefits to the organisation • Large-scale productivity gains • Improve time to market • Maintian market presence (rapidly evolving variants) • Sustain growth • Improved market agility • Better use of skills • Enable mass customisation • Gain control of configuration • Improve product quality • Better predictability of cost, schedule and quality\nCosts of a product line • Architecture: flexible enough to support variation in the products • Software components: general enough to support variability • Test plans, cases, data: take account of variation in components • Business cases: must operate at the level of a product family • Project plans: generic and extensible to deal with variation • Tools and processes: must support architecture, variation, configuration, .. • People, skills, training: need to be skilled in architecture and product lines. Product lines spread costs over several products:• Requirements and requirements analysis • Domain model • Architecture and design • Performance engineering • Documentation • Test cases, data, and plans • Skills • Processes, methods and tools • Defect fixing • Components and services\nCore Process Activities • Core asset development: improving the base components in terms of qualities, products they support, and architecture. • Product development: identifying and building products to meet market need inside the product line. • Management: monitoring and improving the processes, tools and practices.\nIntroducing Product Lines • Proactive: Up-front investment to develop the core assets - need to know the market well (maybe have an already established set of products) • Reactive: Start with one or two products and use them to generate core assets. • Incremental: Develop core assets as the business need evolves.\nExample: Bosch Gasoline Systems Goals ■ Competitiveness: • Reduced hardware resource consumption • Reduced time to market for new features ■ Development efficiency • Reuse: Applications can be used across different generations of system; “core” software is highly configurable and is reused via reconfiguration; “Vehicle functions” can be used across gasoline and diesel engines • Easy configuration of software products • Increased planning accuracy ■ Quality • Interface integrity • Reuse of core assets ■ Customer needs • Differentiation by individual software solutions • Clear feature-cost mapping\nComponent Redesign ■ Focussed on: reuse; simplification of calibration; resource consumption; stabilisation of interfaces (within the architecture) ■ Redesign progressed by: • Analysing existing software inventory: features, sources of variability; relation to product line; document interdependency. • Concept development and design of components: simplification; configurability; architecture driven structure; document relations between features and components; • Baselines for variants of software components: document baselines; implement; maintain up-to-date document and implementation.\nPhased Introduction ■ Investigate and customise product line engineering. ■ Design and pilot adequate processes and methods. ■ Roll out and institutionalise in the standard development process. DevOps The line between development and operation becomes more blurred and the use of the live environment to test innovations becomes more common. DevOps is a set of practices that span development and operation.\nOperations have the direct experience of use of the system – monitoring that use is a way of empirically verifying quality – operations have the data that is used to regulate operations and is essential information for development.\nDevelopment is responsible for building in the right monitoring to ensure operations can operate effectively.\nDevOps is a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal operation while ensuring necessary quality.\nOpen Services for Lifecycle Collaboration (OSLC): OSLC is an open and scalable approach to lifecycle integration. It simplifies key integration scenarios across heterogeneous tools.\nTraditionally we use test as the way of delivering quality change but we can “shepherd” committed change into use by controlling quantities of change, users experiencing change, results of monitoring than this may offer a better way. Delivery mechanism needs to be high quality: reliable, repeatable, available.\nCritical points • Making the decision to commit the code to be introduced into the system. • Transitioning from being under consideration into part of the production deployment that will be used by all users. • Issues is how to have enough confidence to make each of these transitions. Monitoring is critical. • The question is how to ensure the transitions are as reliable as possible.\nThe extent of the lifecycle • Involves all people involved in the delivery of the service/application • Operations and development people are in continuous interaction. • We need architecture to achieve this. • Microservices architectural pattern is often used.\nMicroservices The term \u0026ldquo;Microservice Architecture\u0026rdquo; has sprung up over the last few years to describe a particular way of designing software applications as suites of independently deployable services.\nThe microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. \u0026mdash; https://martinfowler.com/articles/microservices.html\nAttributes of Microservice Architecture • Separately deployed units • Very small service components • Single purpose function or an independent portion of functionality • Distributed • Loosely coupled • Multiple versions are acceptable • Asynchronous • No Orchestration\nArchitecture Evaluation Evaluation by Designer • The consequences of the decision making regulate how much effort to put into the process – more importance means more effort in evaluation. • Try to use iterative approaches that get deeper in order to eliminate unpromising alternatives early. • Don’t strive for perfection, good enough for the context is usually enough.\nPeer Evaluation • Fix on the QAs to consider as part of the review – may be determined by the process or the business case. • The architect presents the architecture to the reviewers – questions are for information. • The review is driven by the relevant scenarios – the architect talks the review team through a scenario demonstrating the architecture meets the requirements captured in the scenario. • The outcome is a list of potential issues with actions: fix, mitigate, tolerate, …\nExternal Evaluation • Means to bring in additional expertise. • May represent some stakeholder interests. • More expensive and difficult to organise so this will often correspond to some major hurdle in the process.\nThe Architecture Tradeoff Analysis Method (ATAM) ATAM is a risk-mitigation process. Its purpose is to help choose a suitable architecture for a software system by discovering trade-offs and sensitivity points, to capture project risks. ATAM is most beneficial when done early in the software development life-cycle, when the cost of changing architectures is minimal.\nDesigned to be usable where: – Evaluators are not expert in the architecture – Evaluators need not be familiar with the business goals. – The system need not be fully developed – There may be large numbers of stakeholders\nParticipants in ATM • The evaluation team: 3-5 people with designated roles (people may have multiple roles). Team members should be seen to be neutral with respect to the project. • Project decision takers: manager of the project, funder of the project, main architect • Architecture stakeholders: developers, testers, integrators, maintainers, performance engineers, …\nATAM evaluation team roles and responsibilities Team Leader Sets up the evaluation; coordinates with client, making sure client\u0026rsquo;s needs are met; establishes evaluation contract; forms evaluation team; sees that final report is produced and delivered (although the writing may be delegated)\nEvaluation Leader Runs evaluation; facilitates elicitation of scenarios; administers scenario selection/prioritization process; facilitates evaluation of scenarios against architecture; facilitates onsite analysis\nScenario Scribe Writes scenarios on flipchart or whiteboard during scenario elicitation; captures agreed-on wording of each scenario, halting discussion until exact wording is captured\nProceedings Scribe Captures proceedings in electronic form on laptop or workstation, raw scenarios, issue(s) that motivate each scenario (often lost in the wording of the scenario itself), and resolution of each scenario when applied to architecture(s); also generates a printed list of adopted scenarios for handout to all participants\nTimekeeper Helps evaluation leader stay on schedule; helps control amount of time devoted to each scenario during the evaluation phase\nProcess Observer Keeps notes on how evaluation process could be improved or deviated from; usually keeps silent but may make discreet process-based suggestions to the evaluation leader during the evaluation; after evaluation, reports on how the process went and lessons learned for future improvement; also responsible for reporting experience to architecture evaluation team at large\nProcess Enforcer Helps evaluation leader remember and carry out the steps of the evaluation method\nQuestioner Raise issues of architectural interest that stakeholders may not have thought of\nATAM Outputs • Concise presentation of the architecture – needs to be presentable in around one hour. • Articulation of the business goals – clearly communicated to all participants • Prioritized QA requirements expressed as scenarios – testable QA requirements. • Risks and non-risks – architecture decision that carries risks (or not). • Risk themes – attempt to identify systemic risk by grouping risks into themes. • Mapping of Architecture Decisions to QA requirements – motivating architecture decisions by QA requirements • Identified sensitivity and tradeoff decisions – critical decisions that have significant impact on QA requirements.\nPartnership and preparation: Getting the schedule, agendas and list of stakeholders prepared, preparing necessary documents and presentations, and gettting documents to the evaluation team\nSteps of Evaluation Phase The ATAM analysis phases (phase 1 and phase 2) consist of nine steps.\nSteps 1 through 6 are carried out in phase 1\nPresentation of the ATAM approach – remind participants of the approach Business drivers presentation – functions; constraints; business goals; major stakeholders; architectural drivers Architecture presentation: Context for the system Static modular view Component and connector view Deployment view Main QA requirements and how the architecture addresses them: What has been reused Trace of key use cases Trace of key change scenarios Main issues/risks driving architectural change Identify architectural approaches – create a catalogue of patterns and tactics used in the architecture. Generate Quality Attribute Utility Tree this is an approach to identifying architecturally significant requirements (ASR) by looking through the QAs - identifying particular aspects of the QA that are relevant and any requirements related to that aspect of the QA. Each ASR is ranked High, Medium or Low in importance. Analyze architectural approaches – look at the most important QA requirement scenarios as identified at stage 5 and probe how the architecture meets the QA scenario. In phase 2, with all stakeholders present, those steps are summarized 7. Brainstorm prioritization of scenarios – revisit the prioritization for additional scenarios, e.g. a particular stakeholder (performance engineer) might propose a scenario on the response time of the system. 8. Analyze Architectural Approaches – revisit stage 6 but with an expanded and reprioritized set of scenarios 9. Present results – the evaluation group tries to group risks into risk themes to identify systemic issues and results are presented.\nATAM Results • Documentation of architectural approaches taken by the project. • Prioritized list of scenarios • Utility tree • Risks discovered • Non-risks identified • Sensitivity and Tradeoff points identified\nGeneral Practice Extraction Service (GPES) An IT system designed to allow NHS organizations to extract data from GP practice computer systems in England. This data would be used to monitor quality, plan and pay for health services and help medical research. 数据的请求和返回不需要实时，更多的是定期的请求，一定时间内返回数据。\nGeneral practitioner (GP), 全科医生。在英国，每个人都需要注册一个全科医生的诊所，当人们感到身体不适后首先会去联系的自己的全科医生。全科医生只进行有限的治疗，并建议是否有必要去医院看专科医生。每个 GP 都像小公司一样运作，有自己的 GP 系统，为患者保留病患记录。在英国，各种不同的机构组织可能需要了解GP正在做什么，因此需要从所有这些GP系统中提取数据。GPES 系统允许那些已经得到授权的机构组织，通过 NHSCIC（国家卫生和社会保健信息中心）提取各种GP数据。因为不同机构需要的信息不同，NHSCIC 需要研究制定如何提取指定的数据，并运行 GPES 系统从英国的所有GP系统提取数据。GP 可以从四种不同的 GP 系统中四选一。而 GPES 的挑战在于整合来自各个不同系统的GP的数据。\n问题\nThe project has been significantly delayed and many customers have yet to receive data. Mistakes in the original 采购 procurement and contract management contributed to losses of public funds, through asset write-offs and settlements with suppliers. Only one customer, NHS England has so far received data from GPES. The time needed to design a new type of extract and restrictions in the contracts severely limits HSCIC’s ability to provide data to those who request it. It is unlikely that GPES in its current form can provide the NHS-wide service planned. Data Extract Issue\nNHS did a technical review of GPES in early 2011, which recommended several significant changes to its design. In the original design, each GP system supplier would use a common query language as part of their extraction system. This would allow the NHSIC to design a single extract centrally using the query tool, which all GP clinical systems could understand. The technical review recommended an alternative where each supplier would be free to develop their own query methods. New queries would no longer be designed in the query tool using a common language, but would instead need to be designed as logical ‘business rules’ and sent to GP system suppliers to implement. The NHSIC decided to abandon both the GPSOC contract approach and the common query language, as they could not agree either with the Department and GP system suppliers. They then procured the extraction systems by negotiating direct with the GP clinical system suppliers. NHSIC is using a non-competitive procurement approach, plus the changes in design, contributed to the restrictive process for designing new extracts. The HSCIC, has continued to use the GPSOC framework to require data sharing between NHS systems. The new framework, effective from 2014, says that principal clinical system suppliers must provide an interface method for third-party systems to use. This would improve interoperability between systems in GP practices and the health community. The HSCIC cannot do the wide range and scale of data extracts the NHS requests, because of the design of the GPES system and restrictions in supplier contracts. Customers have requested over 100 different data extracts from GPES, but the HSCIC estimate they will be able to design only 24 new extracts in 2015-16. Figure shows a summary of the HSCIC’s process to develop a new extract, each of which the supplier designs and programmes from scratch. The HSCIC have limited flexibility to amend extracts once developed, for example to change a time period and the specific organisations it will extract data from. GPES will continue to operate in the short term, as its data is critical for determining payments to GPs. Its coverage of all practices in England cannot currently be replicated by other primary care data extraction systems. However, limited capacity and the difficulty of developing new extracts deters wider use. The HSCIC has acknowledged there is unlikely to be a long-term future for all or part of the GPES. However, they intend to reuse parts for a replacement system if possible. The HSCIC estimate that they will achieve less than two more years of use from the GPES in its current form, in contrast to the five-year minimum lifetime assumed for new IT systems. ","permalink":"https://congchan.github.io/posts/inf-course-note-software-architecture-process-and-management/","summary":"\u003cp\u003e爱丁堡大学信息学院课程笔记 Software Architecture, Process, and Management, Informatics, University of Edinburgh\u003c/p\u003e\n\u003cp\u003eReference:\n\u003ca href=\"https://msdn.microsoft.com/en-us/library/ff650706.aspx\"\u003emicrosoft\u003c/a\u003e\n\u003ca href=\"https://www.ibm.com/developerworks/rational/library/feb06/eeles/\"\u003eIBM\u003c/a\u003e\nSoftware Architecture in Practice (3rd edition), Bass, Clements, and Kazman\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"what-is-software-architecture\"\u003eWhat is Software Architecture?\u003c/h2\u003e\n\u003cp\u003eSoftware architecture is often described as the organization or structure of a system, where the system represents a collection of components that accomplish a specific function or set of functions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egrouping components into areas of \u003cstrong\u003econcern (layers)\u003c/strong\u003e: For example, the UI, business processing, and data access.\u003c/li\u003e\n\u003cli\u003efocus on interaction between the components and how different components work together.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在书中的定义：\u003c/p\u003e","title":"Inf Course Note - Software Architecture, Process, and Management"},{"content":"爱丁堡大学信息学院课程笔记 Software Testing, Informatics, University of Edinburgh\nReference: http://www.inf.ed.ac.uk/teaching/courses/st/2017-18/index.html Pezze and Young, Software Testing and Analysis: Process, Principles and Techniques, Wiley, 2007.\nWhy Software Testing? 1, 软件的漏洞, 错误和失效 Software Faults, Errors \u0026amp; Failures The problem start with Faults,\nFault(BUG): latent error, mistakes in programming.\ne.g add(x, y) = x * y. With the Faults in programs, if and only if executing add(x, y) = x * y, the fault being activated, and generate an Errors.\nError: An incorrect internal state that is the manifestation of some fault\nNow we has an effective Error, if and only if we use the values from add(x, y) = x * y to contribute to the program function (such as, assign it to some variables), then we get the Failure.\nFailure : External, observable incorrect behavior with respect to the requirements or other description of the expected behavior.\n总结: 软件的漏洞不一定会导致错误, 错误不一定会导致软件失效.\n2, 软件工程需要验证确认\n在软件项目管理、软件工程及软件测试中，验证及确认（verification and validation，简称V\u0026amp;V）是指检查软件是否匹配规格及其预期目的的程序。验证及确认也被视为一种软件质量管理，是软件开发过程的一部分，一般归类在软件测试中。\nValidation: 是否符合预期的目的，是否满足用户实际需求？\nVerification: meets the specification?\nVerification and Validation （V\u0026amp;V） start at the beginning or even before we decide to build a software product. V\u0026amp;V last far beyond the product delivery as long as the software is in use, to cope with evolution and adaptations to new conditions.\nThe distinction between the two terms is largely to do with the role of specifications. Validation is the process of checking whether the specification captures the customer\u0026rsquo;s needs, while verification is the process of checking that the software meets the specification. ![](/images/VandVtoolbox.jpg \u0026ldquo;V＆V包含的技术细节。 \u0026ldquo;modeling\u0026rdquo; 和 \u0026ldquo;model checking\u0026quot;指建立和分析软件行为的抽象模型。image from: http://www.easterbrook.ca/steve/2010/11/the-difference-between-verification-and-validation/\")\n3, 软件工程的可靠性 Dependability\nIn software engineering, dependability is the ability to provide services that can defensibly be trusted within a time-period\nAssess the readiness of a product.\nDifferent measures of dependability: • Availability measures the quality of service in terms of running versus down time • Mean time between failures (MTBF) measures the quality of the service in terms of time between failures • Reliability indicates the fraction of all attempted operations that complete successfully\nJUnits JUnit Terminology • A test runner is software that runs tests and reports results. Many implementations: standalone GUI, command line, integrated into IDE • A test suite is a collection of test cases. • A test case tests the response of a single method to a particular set of inputs. • A unit test is a test of the smallest element of code you can sensibly test, usually a single class.\n如何使用请参考Java 测试.\nTest class @Before public void init(): Creates a test fixture by creating and initialising objects and values.\n@After public void cleanUp(): Releases any system resources used by the test fixture. Java usually does this for free, but files, network connections etc. might not get tidied up automatically.\n@Test public void noBadTriangles(), @Test public void scaleneOk(), etc. These methods contain tests for the Triangle constructor and its isScalene() method.\nTest assert static void assertTrue(boolean test), static void assertTrue(String message, boolean test), static void assertFalse(boolean test), static void assertFalse(String message, boolean test)\n软件测试的核心问题和解决思路 A key problem in software testing is selecting and evaluating test cases.\nTest case: A test case is a set of inputs, execution conditions, and a pass/fail criterion. Test case specification is a requirement to be satisfied by one or more actual test cases. Test suite: a set of test cases. Adequacy criterion: a predicate that is true (satisfied) or false (not satisfied) of a \u0026lt; program, test suite \u0026gt; pair. Adequacy criterion is a set of test obligations, which can be derived from several sources of information, including • specifications (functional and model-based testing) • detailed design and source code (structural testing), • model of system • hypothesized defects (fault-based testing), • security testing.\nTest Case Selection and Adequacy Criteria How do we know when the test suite is enough? It is impossibal to provide adequate test suite for a system to pass. Instead, design rules to highlight inadequacy of test suites: if outcome break the rule, then there is bugs, if not, then not sure\u0026hellip;\nTest case specification: a requirement to be satisfied by one or more test cases.\nTest obligation: a partial test case specification, requiring some property deemed important to thorough testing. From: • Functional (black box specification Functional (black box, specification based): from software specifications • Structural (white or glass box): from code • Model-based: from model of system, models used in specification or design, or derived from code • Fault-based: from hypothesized faults (common bugs)\nAdequacy criterion: set of test obligations, a predicate that is true (satisfied) or false (not satisfied) of a (program, test suite) pair.\nA test suite satisfies an adequacy criterion if: • all the tests succeed (pass) • every test obligation in the criterion is satisfied by at least one of the test cases in the test suite.\nSatisfiability Sometimes no test suite can satisfy a criterion for a given program, e.g. defensive programming style includes \u0026ldquo;can\u0026rsquo;t happen\u0026rdquo; sanity checks.\nCoping with Unsatisfiability: Approach A, exclude any unsatisfiable obligation from the criterion. • Example: modify statement coverage to require execution only of statements that can be executed - But we can\u0026rsquo;t know for sure which are executable!\nApproach B, measure the extent to which a test suite approaches an adequacy criterion • Example: if a test suite satisfies 85 of 100 obligations we have reached 85% coverage.\nAn adequacy criterion is satisfied or not, a coverage measure is the fraction of satisfied obligations\nSubsumption relation Test adequacy criterion A subsumes test adequacy criterion B iff, for every program P, every test suite satisfying A with respect to P also satisfies B with respect to P.\ne.g. Exercising all program branches (branch coverage) subsumes exercising all program statements\nFunctional Testing Design functional test case: Generate test cases from specifications.\nSpecification: A functional specification is a description of intended program behavior.\nNot based on the internals of the code but program specifications, functional testing is also called specification-based or black-box testing 黑箱測試.\nThe core of functional test is systematic selection of test cases: partitioning the possible behaviors of the program into a finite number of homogeneous classes, where each such class can reasonably be expected to be consistently correct or incorrect. Test each category and boundaries between (experience suggests failures often lie at the boundaries).\nFunctional test case design is an indispensable base of a good test suite, complemented but never replaced by structural and fault-based testing, because there are classes of faults that only functional testing effectively detects. Omission of a feature, for example, is unlikely to be revealed by techniques that refer only to the code structure.\nPartition Strategies Failures are sparse in the whole input space, and dense in some specific regions, justified based on specification.\nRandom (uniform): • Pick possible inputs uniformly • Avoids designer bias: The test designer can make the same logical mistakes and bad assumptions as the program designer (especially if they are the same person) • But treats all inputs as equally valuable\nSystematic (non-uniform, Partition Testing Strategies): • Try to select inputs that are especially valuable • Usually by choosing representatives of classes that are apt to fail often or not at all • (Quasi-)Partition: separates the input space into classes whose union is the entire space (classes may overlap), sampling each class in the quasi-partition selects at least one input that leads to a failure, revealing the fault.\nSteps of systematic approaches to form test cases from specifications: 1, Decompose the specification. If the specification is large, break it into independently testable features (ITF) to be considered in testing: • An ITF is a functionality that can be tested independently of other functionalities of the software under test. It need not correspond to a unit or subsystem of the software. • ITFs are described by identifying all the inputs that form their execution environments. • ITFs are applied at different granularity levels, from unit testing through integration and system testing. The granularity of an ITF depends on the exposed interface and whichever granularity(unit or system) is being tested. 2, Identify Representative Classes of Values or Derive a Model • Representative values of each input • Representative behaviors of a model: simple input/output transformations don\u0026rsquo;t describe a system. We use models in program specification, in program design, and in test design 3, Generate Test Case Specifications with constraints: The test case specifications represented by the combinations (cartesian product) of all possible inputs or model behaviors, which must be restricted by ruling out illegal combinations and selecting a practical subset of the legal combinations.\nGiven a specification, there may be one or more techniques well suited for deriving functional test case. For example, the presence of several constraints on the input domain may suggest using a partitioning method with constraints, such as the category-partition method. While unconstrained combinations of values may suggest a pairwise combinatorial approach. If transitions among a finite set of system states are identifiable in the specification, a finite state machine approach may be indicated.\nCombinatorial approaches Combinatorial approaches to functional testing consist of a manual step of structuring the specification statement into a set of properties or attributes that can be systematically varied and an automatizable step of producing combinations of choices.\n总体思路： 1, Identify distinct attributes that can be varied: the data, environment, or configuration 2, Systematically generate combinations to be tested\nRational: test cases should be varied and include possible \u0026ldquo;corner cases\u0026rdquo;\nEnvironment describes external factors we need to configure in particular ways in order to specify and execute tests to fully exercise the system. Some common options: System memory, Locale.\nThere are three main techniques that are successfully used in industrial environments and represent modern approaches to systematically derive test cases from natural language specifications: • category-partition approach to identifying attributes, relevant values, and possible combinations; • Pairwise (n-way) combination test a large number of potential interactions of attributes with a relatively small number of inputs; • provision of catalogs to systematize the manual aspects of combinatorial testing.\nCombinatorial approaches 将test cases的粗暴合成分解成一个个步骤，通过解析和综合那些可以量化和监控(并得到工具部分支持)的活动来逐步拆解问题.\nA combinatorial approach may work well for functional units characterized by a large number of relatively independent inputs, but may be less effective for functional units characterized by complex interrelations among inputs.\nCategory-partition 和 pairwise partition 都是使用上面的总体思路，差别在于最后如何自动生成 test cases。\nCategory-partition 将穷举枚举作为自动生成combinations的基本方法，同时允许测试设计者添加限制组合数量增长的约束条件。当这些约束能够反映应用域中的真实约束（例如，category-partition中的\u0026quot;error\u0026quot;条目）时，能够非常有效地消除许多冗余组合。\nDecompose the specification into independently testable features for each feature: identify parameters, environment elements for each parameter and environment element: identify elementary characteristics (categories) Identify relevant/representative values: for each category identify representative (classes of) values normal values boundary values select extreme values within a class ((e.g., maximum and minimum legal values) select values outside but as close as possible to the class select interior (non-extreme) values of the class special values: 0 and 1, might cause unanticipated behavior alone or in combination with particular values of other parameters. error values: values outside the normal domain of the program Ignore interactions among values for different categories (considered in the next step) Introduce constraints: rule out invalid combinations. For single consgtraints, indicates a value class that test designers choose to test only once to reduce the number of test cases. 优点：Category partition testing gave us systematic approach -Identify characteristics and values (the creative step), generate combinations (the mechanical step).\n缺点：test suite size grows very rapidly with number of categories.\n不适合使用Category partition testing的情况：当缺乏应用领域的实际约束时，测试设计者为了减少组合数量被迫任意添加的约束（例如，\u0026ldquo;single\u0026quot;条目），此时不能很有效的减少组合数量。\nPairwise combination testing Most failures are triggered by single values or combinations of a few values.\n为n个测试类选择组合时，除了简单地枚举所有可能的组合外，更实际的组合方案是在集合n中取出k(k\u0026lt;n)项, 一般是二元组或三元组，总的 test cases 要包含所有 features 的两两（或三三）组合。生成测试用例时，先控制某一个变量逐一改变，记录配对了的变量，后续遇到重复的就可以忽略。这样即使没有加constraints也可以大大减少组合数（但我们也可以加constraints）。\n使用低阶组合构建测试用例时，可能会遗漏某些高阶组合的情况。\nBefinits of functional testing Functional testing is the base-line technique for designing test cases: • Timely: Often useful in refining specifications and assessing testability before code is written • Effective: finds some classes of fault (e.g.,missing logic) that can elude other approaches • Widely applicable: to any description of program behavior serving as spec, at any level of granularity from module to system testing. • Economical: typically less expensive to design and execute than structural (code-based) test cases\nEarly functional testing design: • Program code is not necessary: Only a description of intended behavior is needed • Often reveals ambiguities and inconsistency in spec • Useful for assessing testability, and improving test schedule and budget by improving spec • Useful explanation of specification, or in the extreme case (as in Extreme Programming), test cases are the spec\nFinite Models 建模主要解决两个工程问题: • 首先，不能等到实际的产品出来后才分析和测试。 • 其次，对实际产品进行彻底的测试是不切实际的，无论是否受制于所有可能的状态和输入。\n模型允许我们在开发早期就着手分析，并随着设计的发展重复分析，并允许我们应用比实际情况更广泛的分析方法。更重要的是，这些分析很多都是可以自动化的。\nModel program execution, emphasized control.\nA model is a representation that is simpler than the artifact it represents but preserves (or at least approximates) some important attributes of the actual artifact.\nA good model is: • compact: A model must be representable and manipulable in a reasonably compact form. • Predictive: well enough to distinguish between \u0026ldquo;good\u0026rdquo; and \u0026ldquo;bad\u0026rdquo; outcomes of analysis. • Semantically meaningful: interpret analysis results in a way that permits diagnosis of the causes of failure. • Sufficiently general: Models intended for analysis of some important characteristic must be general enough for practical use in the intended domain of application.\n模型的表达：使用有向图描述程序模型。通常我们将它们绘制为\u0026quot;方框和箭头\u0026quot;图，由一组节点N的组成的集合和它们间的关系E（即ordered pairs的集合），edges。节点表示某种类型的实体，例如源代码的步骤，类或区域。边表示实体之间的某种关系。\n模拟程序执行的模型，是该程序状态空间的抽象。通过抽象函数，程序运行状态空间中的状态与程序运行的finite state 模型中的状态相关联。但抽象函数无法完美呈现程序运行的所有细节，将实际的无限可能的状态折叠成有限必然需要省略一些信息，这就引入了不确定性nondeterminism。\n有什么软件模型的基本概念，又有哪些可以应用于测试和分析的模型？\nControal flow graph 程序中的单个步骤或方法的 Control flow 可以用 过程内流程图 intraprocedural control flow graph (CFG) 来表示. CFG 模拟通过单个过程或方法的可能运行路径, 是一个有向图，nodes 表示源代码的一个个区域，有向边 directed edges 表示程序可以在哪些代码区域间流转.\npublic static String collapseNewlines(String argStr) { char last = argStr.charAt(0); StringBuffer argBuf = new StringBuffer(); for (int cIdx = 0 ; cIdx \u0026lt; argStr.length(); cIdx++) { char ch = argStr.charAt(cIdx); if (ch != \u0026#39;\\n\u0026#39; || last != \u0026#39;\\n\u0026#39;) { argBuf.append(ch); last = ch; } } return argBuf.toString(); } 左边是上面代码对应的CFG，右边的表格是Linear Code Sequence and Jump (LCSJ)，表示从一个分支到另一个分支的控制流程图的子路径 Nodes = regions of source code (basic blocks) • Basic block = maximal program region with a single entry and single exit point • Often statements are grouped in single regions to get a compact model • Sometime single statements are broken into more than one node to model control flow within the statement Directed edges = possibility that program execution proceeds from the end of one region directly to the beginning of another\n为了便于分析，控制流程图通常会通过其他信息进一步加持。例如，后面介绍的数据流模型 data flow models 就是基于加持了有关变量被程序各个语句访问和修改的信息的CFG模型构建的.\nCall Graphs 过程间流程 Interprocedural control flow 也可以表示为有向图。最基本的模型是调用图 call graphs, nodes represent procedures (methods, C functions, etc.) and edges represent the \u0026ldquo;calls\u0026rdquo; relation.\n相较于CFG，调用图比有更多设计问题和权衡妥协， 因此基本调用图的表达方式是不固定的，特别是在面向对象的语言中，methods跟对象动态绑定。 调用图存在Overapproximation现象，比如尽管方法A.check()永远不会实际调用C.foo()，但是一个典型的调用图会认为这个调用是可能的。\nContext-sensitive call graph：调用图模型根据过程被调用的具体位置来表示不同行为。\npublic class Context { public static void main(String args[]) { Context c = new Context(); c.foo(3); c.bar(17); } void foo(int n) { int[] myArray = new int[ n ]; depends( myArray, 2) ; } void bar(int n) { int[] myArray = new int[ n ]; depends( myArray, 16) ; } void depends( int[] a, int n ) { a[n] = 42; } } Context sensitive analyses can be more precise than Context-insensitive analyses when the model includes some additional information that is shared or passed among procedures. But sensitive call graphs size grows exponentially, not fit for large program.\nFinite state machines 前面介绍的模型都是都是基于源代码抽象出来的。不过，模型的构建也常常先于或者独立于源代码，有限状态机 finite state machines 就是这种模型。\n有限状态机（finite-state machine，FSM）又称有限状态自动机，简称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。\n最简单的FSM由一个有限的状态集合和状态间的转移动作构成，可以有向图表示，节点表示状态，edges表示在状态间的转移需要的运算、条件或者事件。因为可能存在无限多的程序状态，所以状态节点的有限集合必须是具体编程状态的抽象。\nUsually we label the edge to indicate a program operation, condition, or event associated with the transition. We may label transitions with both an external event or a condition and with a program operation that can be thought of as a \u0026ldquo;response\u0026rdquo; to the event. Such a finite state machine with event/response labels on transitions is called a Mealy machine.\nIn the theory of computation, a Mealy machine is a finite-state machine whose output values are determined both by its current state and the current inputs. (This is in contrast to a Moore machine, whose output values are determined solely by its current state.)\nAn alternative representation of finite state machines, including Mealy machines, is the state transition table: There is one row in the transition table for each state node and one column for each event or input. If the FSM is complete and deterministic, there should be exactly one transition in each table entry. Since this table is for a Mealy machine, the transition in each table entry indicates both the next state and the response (e.g., d / emit means \u0026ldquo;emit and then proceed to state d\u0026rdquo;).\nStructural Testing Judging test suite thoroughness based on the structure of the program itself, it is still testing product functionality against its specification, but the measure of thoroughness has changed to structural criteria. Also known as \u0026ldquo;white-box\u0026rdquo;, \u0026ldquo;glass-box\u0026rdquo;, or \u0026ldquo;codebased\u0026rdquo; testing.\nMotivation: 1, If part of a program is not executed by any test case in the suite, faults in that part cannot be exposed. The part is a control flow element or combination, statements (or CFG nodes), branches (or CFG edges), fragments and combinations, conditions paths. 2, Complements functional testing, another way to recognize cases that are treated differently 3, Executing all control flow elements does not guarantee finding all faults: Execution of a faulty statement may not always result in a failure • The state may not be corrupted when the statement is executed with some data values • Corrupt state may not propagate through execution to eventually lead to failure 4, Structural coverage: Increases confidence in thoroughness of testing, removes some obvious inadequacies\nSteps:\nCreate functional test suite first, then measure structural coverage to identify see what is missing Interpret unexecuted elements may be due to natural differences between specification and implementation or may reveal flaws of the software or its development process inadequacy of specifications that do not include cases present in the implementation coding practice that radically diverges from the specification inadequate functional test suites Coverage measurements are convenient progress indicators, sometimes used as a criterion of completion.\nControl-flow Adequacy (expression coverage) A structural testing strategy that uses the program\u0026rsquo;s control flow as a model. Control flow elements include statements, branches, conditions, and paths.\nBut a set of correct program executions in which all control flow elements are exercised does not guarantee the absence of faults.\nTest based on control-flow are concerned with expression coverage.\nStatement testing Adequacy criterion: each statement (or node in the CFG) must be executed at least once. Because a fault in a statement can only be revealed by executing the faulty statement.\nCoverage: #(executed statements) / #(statements)\nMinimizing test suite size is seldom the goal, but small test cases make failure diagnosis easier.\nComplete statement coverage may not imply executing all branches in a program.\nBranch testing Adequacy criterion: each branch (edge in the CFG) must be executed at least once.\nCoverage: #(executed branches) / #(branches) Traversing all edges of a graph causes all nodes to be visited: test suites that satisfy the branch adequacy criterion for a program P also satisfy the statement adequacy criterion for the same program\nBut \u0026ldquo;All branches\u0026rdquo; can still miss conditions. Sample fault: missing operator (negation):digit_high == 1 || digit_low == -1, branch adequacy criterion can be satisfied by varying only part of the condition.\nCondition testing Basic condition adequacy criterion: each basic condition must be executed at least once.\nCoverage: #(truth values taken by all basic conditions) / 2 * #(basic conditions)\nBranch and basic condition are not comparable. Basic condition adequacy criterion can be satisfied without satisfying branch coverage\nBranch and condition adequacy: cover all conditions and all decisions\nCompound condition adequacy: • Cover all possible evaluations of compound conditions - A compound condition is either an atomic condition or some boolean formula of atomic conditions. For example, in the overall condition \u0026ldquo;A || (B \u0026amp;\u0026amp; C)\u0026rdquo; the set of compound conditions are \u0026ldquo;A\u0026rdquo;, \u0026ldquo;B\u0026rdquo;, \u0026ldquo;C\u0026quot;, \u0026quot;B \u0026amp;\u0026amp; C\u0026rdquo;, \u0026ldquo;A || (B \u0026amp;\u0026amp; C)\u0026rdquo;. • Cover all branches of a decision tree. • Number of test cases grows exponentially with the number of basic conditions in a decision ($2^N$).\n练习 - Write tests that provide statement, branch, and basic condition coverage over the following code:\nint search(string A[], int N, string what){ int index = 0; if ((N == 1) \u0026amp;\u0026amp; (A[0] == what)){ return 0; } else if (N == 0){ return -1; } else if (N \u0026gt; 1){ while(index \u0026lt; N){ if (A[index] == what) return index; else index++; } } return -1; } 先画出 CFG 图，再遍历：\nModified condition/decision adequacy criterion (MC/DC) Motivation: Effectively test important combinations of conditions, without exponential blow up in test suite size. (Important combinations: Each basic condition shown to independently affect the outcome of each decision)\n假如这些组合表明每一个条件都可以独立影响结果，那么就不要穷尽各种条件组合了，对于那些不影响结果的条件组合，测了也没有意义。\nRequires: • For each basic condition $C_i$, two test cases • 控制变量，只改变 $C_i$：values of all evaluated conditions except $C_i$ are the same • Compound condition as a whole evaluates to True for one and False for the other，结果的改变表明 $C_i$ 可以独立影响结果\nMC/DC: • basic condition coverage (C) • branch coverage (DC) • plus one additional condition (M): every condition must independently affect the decision\u0026rsquo;s output\nIt is subsumed by compound conditions and subsumes all other criteria discussed so far - stronger than statement and branch coverage. A good balance of thoroughness and test size (and therefore widely used).\nPath Testing Sometimes, a fault is revealed only through exercise of some sequence of decisions (i.e., a particular path through the program).\nPath coverage requires that all paths through the CFG are covered. In theory, path coverage is the ultimate coverage metric. But in practice, it is impractical if there is loop involed.\nAdequacy criterion: each path must be executed at least once: Coverage = #(Paths Covered) / #(Total Paths)\nPractical path coverage criteria: • The number of paths in a program with loops is unbounded - the simple criterion is usually impossible to satisfy • For a feasible criterion: Partition infinite set of paths into a finite number of classes • Useful criteria can be obtained by limiting •• the number of traversals of loops •• the length of the paths to be traversed •• the dependencies among selected paths\nBoundary Interior Coverage Groups paths that differ only in the subpath they follow when repeating the body of a loop. • Follow each path in the control flow graph up to the first repeated node • The set of paths from the root of the tree to each leaf is the required set of subpaths for boundary/interior coverage 把分支拆分为每一条可能的 path.\nLimitations: 1, The number of paths through non-loop branches (conditions) can still be exponential ($2^N$). 2, Choosing input data to force execution of one particular path may be very difficult, or even impossible if the conditions are not independent.\nLoop Boundary Coverage Since coverage of non-looping paths is expensive, we can consider a variant of the boundary/interior criterion that treats loop boundaries similarly but is less stringent with respect to other differences among paths.\nCriterion: A test suite satisfies the loop boundary adequacy criterion iff for every loop: • In at least one test case, the loop body is iterated zero times • In at least one test case, the loop body is iterated once • In at least one test case, the loop body is iterated more than once\nFor simple loops, write tests that:\nSkip the loop entirely. Take exactly one pass through the loop. Take two or more passes through the loop. (optional) Choose an upper bound N, and: M passes, where 2 \u0026lt; M \u0026lt; N (N-1), N, and (N+1) passes For Nested Loops:\nFor each level, you should execute similar strategies to simple loops. In addition: Test innermost loop first with outer loops executed minimum number of times. Move one loops out, keep the inner loop at \u0026ldquo;typical\u0026rdquo; iteration numbers, and test this layer as you did the previous layer. Continue until the outermost loop tested. For Concatenated Loops, one loop executes. The next line of code starts a new loop:\nThese are generally independent(Most of the time\u0026hellip;) If not, follow a similar strategy to nested loops. Start with bottom loop, hold higher loops at minimal iteration numbers. Work up towards the top, holding lower loops at \u0026ldquo;typical\u0026rdquo; iteration numbers. Linear Code Sequences and Jumps There are additional path-oriented coverage criteria that do not explicitly consider loops. Among these are criteria that consider paths up to a fixed length. The most common such criteria are based on Linear Code Sequence and Jump (LCSAJ) - sequential subpath in the CFG starting and ending in a branch.\nA single LCSAJ is a set of statements that come one after another (meaning no jumps) followed by a single jump. A LCSAJ starts at either the beginning of the function or at a point that can be jumped to. The LCSAJ coverage is what fraction of all LCSAJs in a unit are followed by your test suite.\nWe can require coverage of all sequences of LCSAJs of length N. Stronger criteria can be defined by requiring N consecutive LCSAJs to be covered - $TER_{N+2}$: 1, $TER_1$ is equivalent to statement coverage. 2, $TER_2$ is equivalent to branch coverage 3, $TER_3$ is LCSAJ coverage 4, $TER_4$ is how many pairs of LCSAJ covered \u0026hellip;\nCyclomatic adequacy (Complexity coverage) There are many options for the set of basis subpaths. When testing, count the number of independent paths that have already been covered, and add any new subpaths covered by the new test.\nYou can identify allpaths with a set of independent subpaths of size = the cyclomatic complexity. Cyclomatic coverage counts the number of independent paths that have been exercised, relative to cyclomatic complexity.\n• A path is representable as a bit vector, where each component of the vector represents an edge • \u0026ldquo;Dependence\u0026rdquo; is ordinary linear dependence between (bit) vectors\nIf e = #(edges), n = #(nodes), c = #(connected components) of a graph, it is $e - n + c$ for an arbitrary graph, $e - n + 2$ for a CFG.\nCyclomatic Complexity could be used to guess \u0026ldquo;how much testing is enough\u0026rdquo;. ○ Upper bound on number of tests for branch coverage. ○ Lower bound on number of tests for path coverage.\nAnd Used to refactor code. ○ Components with a complexity \u0026gt; some threshold should be split into smaller modules. ○ Based on the belief that more complex code is more fault-prone.\nProcedure call coverage The criteria considered to this point measure coverage of control flow within individual procedures - not well suited to integration or system testing, where connections between procedures(calls and returns) should be covered.\nChoose a coverage granularity commensurate with the granularity of testing - if unit testing has been effective, then faults that remain to be found in integration testing will be primarily interface faults, and testing effort should focus on interfaces between units rather than their internal details.\nProcedure Entry and Exit Testing - A single procedure may have several entry and exit points. • In languages with goto statements, labels allow multiple entry points. • Multiple returns mean multiple exit points.\nCall coverage: The same entry point may be called from many points. Call coverage requires that a test suite executes all possible method calls.\nSatisfying structural criteria The criterion requires execution of\nstatements that cannot be executed as a result of: defensive programming code reuse (reusing code that is more general than strictly required for the application) conditions that cannot be satisfied as a result of interdependent conditions paths that cannot be executed as a result of interdependent decisions Rather than requiring full adequacy, the \u0026ldquo;degree of adequacy\u0026rdquo; of a test suite is estimated by coverage measures.\nDependence and Data Flow Models 前面介绍的 Finite models (Control flow graph, call graph, finite state machines) 只是捕捉程序各部分之间依赖关系的其中一个方面。它们明确地表现控制流程，但不重视程序变量间的信息传递. Data flow models provide a complementary view, emphasizing and making explicit relations involving transmission of information.\nModels of data flow and dependence in software were originally developed in the field of compiler construction, where they were (and still are) used to detect opportunities for optimization.\nDefinition-Use Pairs (Def-Use Pairs) The most fundamental class of data flow model associates the point in a program where a value is produced (called a \u0026ldquo;definition\u0026rdquo;) with the points at which the value may be accessed (called a \u0026ldquo;use\u0026rdquo;).\nDefinitions - Variable declaration (often the special value \u0026ldquo;uninitialized\u0026rdquo;), Variable initialization, Assignment, Values received by a parameter. Use - Expressions, Conditional statements, Parameter passing, Returns. A Definition-Use pair is formed if and only if there is a definition-clear path between the Definition and the Use. A definition-clear path is a path along the CFG path from a definition to a use of the same variable without another definition of the variable in between.\n\u0026lt;D,U\u0026gt; pairs coverage: #(pairs covered)/ #(total number of pairs) If instead another definition is present on the path, then the latter definition kills the former.\nDefinition-use pairs record direct data dependence, which can be represented in the form of a graph - (Direct) Data Dependence Graph, with a directed edge for each definition-use pair.\nThe notion of dominators in a rooted, directed graph can be used to make this intuitive notion of \u0026ldquo;controlling decision\u0026rdquo; precise. Node M dominates node N if every path from the root of the graph to N passes through M.\nAnalyses: Reaching definition Definition-use pairs can be defined in terms of paths in the program control flow graph. • There is an association $(d,u)$ between a definition of variable $v$ at $d$ and a use of variable $v$ at $u$ if and only if there is at least one control flow path from $d$ to $u$ with no intervening definition of $v$. • Definition $v_d$ reaches $u$ ($v_d$ is a reaching definition at $u$). • If a control flow path passes through another definition $e$ of the same variable $v$, we say that $v_e$ kills $v_d$ at that point.\nPractical algorithms do not search every individual path. Instead, they summarize the reaching definitions at a node over all the paths reaching that node.\nAn algorithm for computing reaching definitions is based on the way reaching definitions at one node are related to reaching definitions at an adjacent node.\nSuppose we are calculating the reaching definitions of node n, and there is an edge $(p,n)$ from an immediate predecessor node $p$. We observe: • If the predecessor node $p$ can assign a value to variable $v$, then the definition $v_p$ reaches $n$. We say the definition $v_p$ is generated at $p$. • If a definition $v_d$ of variable $v$ reaches a predecessor node $p$, and if $v$ is not redefined at that node, then the definition is propagated on from $p$ to $n$.\nThese observations can be stated in the form of an equation describing sets of reaching definitions.\n/** Euclid\u0026#39;s algorithm */ public class GCD { public int gcd(int x, int y) { int tmp; // A: def x, y, tmp while (y != 0) { // B: use y tmp = x % y; // C: def tmp; use x, y x = y; // D: def x; use y y = tmp; // E: def y; use tmp } return x; // F: use x } } Reaching definitions at node E are those at node D, except that D adds a definition of x and replaces (kills) an earlier definition of x: $$ \\begin{equation} \\begin{split} Reach(E) \u0026= ReachOut(D) \\\\\\\\ ReachOut(D) \u0026= (Reach(D) \\backslash \\{x_A\\}) \\cup \\{x_D\\} \\end{split} \\end{equation} $$ Equations at the head of the while loop - node B, where values may be transmitted both from the beginning of the procedure - node A and through the end of the body of the loop - node E. The beginning of the procedure (node A) is treated as an initial definition of parameters and local variables: $$ \\begin{equation} \\begin{split} Reach(B) \u0026= ReachOut(A) \\cup ReachOut(E) \\\\\\\\ ReachOut(A) \u0026= gen(A) = \\{x_A, y_A, tmp_A \\} \\\\\\\\ ReachOut(E) \u0026= (Reach(E) \\backslash \\{y_A\\}) \\cup \\{y_D\\} \\end{split} \\end{equation} $$(If a local variable is declared but not initialized, it is treated as a definition to the special value \u0026ldquo;uninitialized.\u0026rdquo;)\nGeneral equations for Reach analysis: $$\\begin{equation} \\begin{split} Reach(n) \u0026= \\mathop{\\cup} \\limits_{m \\in pred(n)} ReachOut(m) \\\\\\\\ ReachOut(n) \u0026=(Reach(n) \\backslash kill(n)) \\cup gen(n) \\\\\\\\ \\end{split} \\end{equation}$$ $gen(n) = v_n$, $v$ is defined or modified at $n$; $kill(n) = v_x$, $v$ is defined or modified at $x, x \\ne n$.\nReaching definitions calculation: first initializing the reaching definitions at each node in the control flow graph to the empty set, and then applying these equations repeatedly until the results stabilize.\nAnalyses: Live and Avail Available expressions is another classical data flow analysis, used in compiler construction to determine when the value of a subexpression can be saved and reused rather than recomputed.\nAn expression is available at a point if, for all paths through the control flow graph from procedure entry to that point, the expression has been computed and not subsequently modified.\nAn expression is generated (becomes available) where it is computed and is killed (ceases to be available) when the value of any part of it changes (e.g., when a new value is assigned to a variable in the expression).\nThe expressions propagation to a node from its predecessors is described by a pair of set equations: $$\\begin{equation} \\begin{split} Avail(n) \u0026= \\mathop{\\cap} \\limits_{m \\in pred(n)} AvailOut(m) \\\\\\\\ AvailOut(n) \u0026=(Avail(n) \\backslash kill(n)) \\cup gen(n) \\\\\\\\ \\end{split} \\end{equation}$$ $gen(n)$, available, computed at $n$; $kill(n)$, has variables assigned at $n$.\nReaching definitions combines propagated sets using set union, since a definition can reach a use along any execution path. Available expressions combines propagated sets using set intersection, since an expression is considered available at a node only if it reaches that node along all possible execution paths.\nReaching definitions is a forward, any-path analysis; Available expressions is a forward, all-paths analysis.\nLive variables is a backward, any-path analysis that determines whether the value held in a variable may be subsequently used. Backward analyses are useful for determining what happens after an event of interest.\nA variable is live at a point in the control flow graph if, on some execution path, its current value may be used before it is changed, i.e. there is any possible execution path on which it is used.\n$$\\begin{equation} \\begin{split} Live(n) \u0026= \\mathop{\\cup} \\limits_{m \\in succ(n)} LiveOut(m) \\\\\\\\ LiveOut(n) \u0026=(Live(n) \\backslash kill(n)) \\cup gen(n) \\\\ \\end{split} \\end{equation}$$ $gen(n)$, $v$ is used at $n$; $kill(n)$, $v$ is modified at $n$.\nOne application of live variables analysis is to recognize useless definitions, that is, assigning a value that can never be used.\nIterative Solution of Dataflow Equations Initialize values (first estimate of answer) • For \u0026ldquo;any path\u0026rdquo; problems, first guess is \u0026ldquo;nothing\u0026rdquo;(empty set) at each node • For \u0026ldquo;all paths\u0026rdquo; problems, first guess is \u0026ldquo;everything\u0026rdquo; (set of all possible values = union of all \u0026ldquo;gen\u0026rdquo; sets)\nRepeat until nothing changes • Pick some node and recalculate (new estimate)\nFrom Execution to Conservative Flow Analysis We can use the same data flow algorithms to approximate other dynamic properties • Gen set will be \u0026ldquo;facts that become true here\u0026rdquo; • Kill set will be \u0026ldquo;facts that are no longer true here\u0026rdquo; • Flow equations will describe propagation\nExample: Taintedness (in web form processing) • \u0026ldquo;Taint\u0026rdquo;: a user-supplied value (e.g., from web form) that has not been validated • Gen: we get this value from an untrusted source here • Kill: we validated to make sure the value is proper\nData flow analysis with arrays and pointers The models and flow analyses described in the preceding section have been limited to simple scalar variables in individual procedures.\nArrays and pointers (dynamic references and the potential for aliasing) introduce uncertainty: Do different expressions access the same storage? • a[i] same as a[k] when i = k • a[i] same as b[i] when a = b (aliasing)\nThe uncertainty is accomodated depending on the kind of analysis • Any-path: gen sets should include all potential aliases and kill set should include only what is definitely modified • All path: vice versa\nScope of Data Flow Analysis 过程内 Intraprocedural: Within a single method or procedure, as described so far.\n过程之间 Interprocedural: Across several methods (and classes) or procedures\nCost/Precision trade-offs for interprocedural analysis are critical, and difficult: context sensitivity, flow-sensitivity.\nMany interprocedural flow analyses are flow-insensitive • $O(n^3)$ would not be acceptable for all the statements in a program. Though $O(n^3)$ on each individual procedure might be ok • Often flow-insensitive analysis is good enough\u0026hellip; considering type checking as an example\nReach, Avail, etc were flow-sensitive sensitive, intraprocedural analyses. • They considered ordering and control flow decisions • Within a single procedure or method, this is (fairly) cheap - $O(n^3)$ for $n$ CFG nodes.\nSummary of Data flow models Data flow models detect patterns on CFGs: Nodes initiating the pattern Nodes terminating it Nodes that may interrupt it Often, but not always, about flow of information (dependence) Pros: Can be impy g lemented by efficient iterative algorithms Widely applicable (not just for classic \u0026ldquo;data flow\u0026rdquo; properties) Limitations: Unable to distinguish feasible from infeasible paths Analyses spanning whole programs (e.g., alias analysis) must trade off precision against computational cost Data Flow Testing In structural testing, • Node and edge coverage don\u0026rsquo;t test interactions • Path-based criteria require impractical number of test cases: And only a few paths uncover additional faults, anyway • Need to distinguish \u0026ldquo;important\u0026rdquo; paths\nData flow testing attempts to distinguish \u0026ldquo;important\u0026rdquo; paths: Interactions between statements - Intermediate between simple statement and branch coverage and more expensive path-based structural testing.\nIntuition: Statements interact through data flow • Value computed in one statement used in another Value computed in one statement, used in another • Bad value computation revealed only when it is used\nAdequacy criteria: • All DU pairs: Each DU pair is exercised by at least one test case • All DU paths: Each simple (non looping) DU path is exercised by at least one test case • All definitions: For each definition, there is at least one test case which exercises a DU pair containing it - Every computed value is used somewhere\nLimits: Aliases, infeasible paths - Worst case is bad (undecidable properties, exponential blowup of paths), so 务实的 pragmatic compromises are required\nData flow coverage with complex structures Arrays and pointers • Under-estimation of aliases may fail to include some DU pairs • Over-estimation, may introduce unfeasible test obligations\nFor testing, it may be preferrable to accept under-estimation of alias set rather than over-estimation or expensive analysis • 有争议的 Controversial: In other applications (e.g., compilers), a conservative over-estimation of aliases is usually required • Alias analysis may rely on external guidance or other global analysis to calculate good estimates • Undisciplined use of dynamic storage, pointer arithmetic, etc. may make the whole analysis infeasible\nMutation testing Fault-based Testing, directed towards \u0026ldquo;typical\u0026rdquo; faults that could occur in a program.\nTake a program and test suite generated for that program (using other test techniques) Create a number of similar programs (mutants), each differing from the original in one small way, i.e., each possessing a fault The original test data are then run through the mutants Then mutants either: To be dead: test data detect all differences in mutants, the test set is adequate. Remains live if: it is equivalent to the original program (functionally identical although syntactically different - called an equivalent mutant) or, the test set is inadequate to kill the mutant. The test data need to be augmented (by adding one or more new test cases) to kill the live mutant. Numbers of mutants tend to be large (the number of mutation operators is large as they are supposed to capture all possible syntactic variations in a program), hence random sampling, selective mutation operators (Offutt).\nCoverage - mutation score: #(killed mutants) / #(all non-equivalent mutants) (or random sample).\nBenifits: • It provides the tester with a clear target (mutants to kill) • It does force the programmer to think of the test data that will expose certain kinds of faults • Probably most useful at unit testing level\nMutation operators could be built on • source code (body), • module interfaces (aimed at integration testing), • specifications: Petri-nets, state machines, (aimed at system testing)\nTools: MuClipse\nModel based testing Models used in specification or design have structure • Useful information for selecting representative classes of behavior; behaviors that are treated differently with respect to the model should be tried by a thorough test suite • In combinatorial testing, it is difficult to capture that structure clearly and correctly in constraints\nDevise test cases to check actual behavior against behavior specified by the model - \u0026ldquo;Coverage\u0026rdquo; similar to structural testing, but applied to specification and design models.\nDeriving test cases from finite state machines: From an informal specification, to a finite state machine, to a test suite\n\u0026ldquo;Covering\u0026rdquo; finite state machines • State coverage: Every state in the model should be visited by at least one test case • Transition coverage •• Every transition between states should be traversed by at least one test case. •• A transition can be thought of as a (precondition, postcondition) pair.\nModels are useful abstractions • In specification and design, they help us think and communicate about complex artifacts by emphasizing key features and suppressing details • Models convey structure and help us focus on one thing at a time\nWe can use them in systematic testing • If a model divides behavior into classes, we probably want to exercise each of those classes! • Common model-based testing techniques are based on state machines, decision structures, and grammars, but we can apply the same approach to other models.\nTesting Object Oriented Software Typical OO software characteristics that impact testing • State dependent behavior • Encapsulation • Inheritance • 多态性 Polymorphism and dynamic binding • Abstract and generic classes • Exception handling\nProcedural software, unit = single program, function, or procedure, more often: a unit of work that may correspond to one or more intertwined functions or programs.\nObject oriented software: • unit = class or (small) cluster of strongly related classes (e.g., sets of Java classes that correspond to exceptions) • unit testing = 类内测试 intra-class testing • integration testing = 类之间测试 inter-class testing (cluster of classes) • dealing with single methods separately is usually too expensive (complex scaffolding), so methods are usually tested in the context of the class they belong to.\nBasic approach is orthogonal: Techniques for each major issue (e.g., exception handling, generics, inheritance ) can be applied incrementally and independently. Intraclass State Machine Testing Basic idea: • The state of an object is modified by operations • Methods can be modeled as state transitions • Test cases are sequences of method calls that traverse the state machine model\nState machine model can be derived from specification (functional testing), code (structural testing), or both.\nTesting with State Diagrams: • A statechart (called a \u0026ldquo;state diagram\u0026rdquo; in UML) may be produced as part of a specification or design - May also be implied by a set of message sequence charts (interaction diagrams), or other modeling formalisms. • Two options: 1, Convert (\u0026ldquo;flatten\u0026rdquo;) into standard finite-state machine, then derive test cases 2, Use state diagram model directly\nIntraclass data flow testing Exercise sequences of methods • From setting or modifying a field value • To using that field value\nThe intraclass control flow graph - control flow through sequences of method calls: • Control flow for each method • node for class • edges: from node class to the start nodes of the methods; from the end nodes of the methods to node class.\nInterclass Testing The first level of integration testing for object-oriented software - Focus on interactions between classes\nBottom-up integration according to \u0026ldquo;depends\u0026rdquo; relation - A depends on B - Build and test B, then A\nStart from use/include hierarchy - Implementation-level parallel to logical \u0026ldquo;depends\u0026rdquo; relation • Class A makes method calls on class B • Class A objects include references to class B methods - but only if reference means \u0026ldquo;is part of\u0026rdquo; In software engineering, a class diagram in the Unified Modeling Language (UML) is a type of static structure diagram that describes the structure of a system by showing the system\u0026rsquo;s classes, their attributes, operations (or methods), and the relationships among objects.\nDependency is a weaker form of bond that indicates that one class depends on another because it uses it at some point in time. One class depends on another if the independent class is a parameter variable or local variable of a method of the dependent class.\nInteractions in Interclass Tests:\nProceed bottom-up Consider all combinations of interactions example: a test case for class Order includes a call to a method of class Model, and the called method calls a method of class Slot, exercise all possible relevant states of the different classes. problem: combinatorial explosion of cases so select a subset of interactions: arbitrary or random selection plus all significant interaction scenarios that have been previously identified in design and analysis: sequence + collaboration diagrams Using Structural Information: • Start with functional testing: the specification (formal or informal) is the first source of information • Then add information from the code (structural testing)\nInterclass structural testing Working \u0026ldquo;bottom up\u0026rdquo; in dependence hierarchy • Dependence is not the same as class hierarchy; not always the same as call or inclusion relation. • May match bottom-up build order\nStarting from leaf classes, then classes that use leaf classes,\u0026hellip;\nSummarize effect of each method: Changing or using object state, or both - Treating a whole object as a variable (not just primitive types)\nPolymorphism and dynamic binding One variable potentially bound to methods of different (sub-)classes.\nThe combinatorial approach: identify a set of combinations that cover all pairwise combinations of dynamic bindings.\nInheritance When testing a subclass, We would like to re-test only what has not been thoroughly tested in the parent class. But we should test any method whose behavior may have changed.\nReusing Tests with the Testing History Approach:\nTrack test suites and test executions determine which new tests are needed determine which old tests must be re-executed New and changed behavior \u0026hellip; new methods must be tested redefined methods must be tested, but we can partially reuse test suites defined for the ancestor other inherited methods do not have to be retested Abstract methods (and classes) - Design test cases when abstract method is introduced (even if it can t be executed yet)\nBehavior changes • Should we consider a method \u0026ldquo;redefined\u0026rdquo; if another new or redefined method changes its behavior? • The standard \u0026ldquo;testing history\u0026rdquo; approach does not do this • It might be reasonable combination of data flow (structural) OO testing with the (functional) testing history approach\nTesting exception handling Exceptions create implicit control flows and may be handled by different handlers.\nImpractical to treat exceptions like normal flow • too many flows: every array subscript reference, every memory, allocation, every cast, \u0026hellip; • multiplied by matching them to every handler that could appear immediately above them on the call stack. • many actually impossible\nSo we separate testing exceptions, and ignore program error exceptions (test to prevent them, not to handle them)\nWhat we do test: Each exception handler, and each explicit throw or re-throw of an exception.\nIntegration Testing Unit (module) testing is a foundation, unit level has maximum controllability and visibility.\nIntegration testing may serve as a process check • If module faults are revealed in integration testing, they signal inadequate unit testing • If integration faults occur in interfaces between correctly implemented modules, the errors can be traced to module breakdown and interface specifications. Integration test plan drives and is driven by the project \u0026ldquo;build plan\u0026rdquo;\nStructural orientation: Modules constructed, integrated and tested based on a hierarchical project structure - Top-down, Bottom-up, Sandwich, Backbone\nFunctional orientation: Modules integrated according to application characteristics or features - Threads, Critical module.\nA \u0026ldquo;thread\u0026rdquo; is a portion of several modules that together provide a user-visible program feature.\nComponent-based software testing Working Definition of Component • Reusable unit of deployment and composition • Characterized by an interface or contract • Often larger grain than objects or packages - A complete database system may be a component\nFramework • Skeleton or micro-architecture of an application • May be packaged and reused as a component, with \u0026ldquo;挂钩 hooks\u0026rdquo; or \u0026ldquo;插槽 slots\u0026rdquo; in the interface contract\nDesign patterns • Logical design fragments • Frameworks often implement patterns, but patterns are not frameworks. Frameworks are concrete, patterns are abstract\nComponent-based system • A system composed primarily by assembling components, often \u0026ldquo;Commercial off-the-shelf\u0026rdquo; (COTS) components • Usually includes application-specific \u0026ldquo;glue code\u0026rdquo;\nComponent Interface Contracts • Application programming interface (API) is distinct from implementation • Interface includes everything that must be known to use the component: More than just method signatures, exceptions, etc; May include non-functional characteristics like performance, capacity, security; May include dependence on other components.\nTesting a Component: Producer View • Thorough unit and subsystem testing • Thorough acceptance testing: Includes stress and capacity testing\nTesting a Component: User View • Major question: Is the component suitable for this application? • Reducing risk: Trial integration early\nSystem, Acceptance, and Regression Testing System Testing Characteristics: • Comprehensive (the whole system, the whole spec) • Based on specification of observable behavior: Verification against a requirements specification, not validation, and not opinions • Independent of design and implementation\nIndependence: Avoid repeating software design errors in system test design.\nMaximizing independence: • Independent V\u0026amp;V: System (and acceptance) test performed by a different organization. • Independence without changing staff: Develop system test cases early\nSystem tests are often used to measure progress. As project progresses, the system passes more and more system tests. Features exposed at top level as they are developed.\nSystem testing is the only opportunity to verify Global Properties - Performance, latency, reliability, \u0026hellip; Especially to find unanticipated effects, e.g., an unexpected performance bottleneck.\nContext-Dependent Properties is beyond system-global: Some properties depend on the system context and use, Example: • Performance properties depend on environment and configuration • Privacy depends both on system and how it is used • Security depends on threat profiles\nStress Testing When a property (e.g., performance or real-time response) is parameterized by use - requests per second, size of database,\u0026hellip; Extensive stress testing is required - varying parameters within the envelope, near the bounds, and beyond.\nOften requires extensive simulation of the execution environment, and requires more resources (human and machine) than typical test cases - Separate from regular feature tests, Run less often, with more manual control.\nCapacity, Security, Performance, Compliance, Documentation Testing.\nAcceptance testing Estimating dependability, measuring quality, not searching for faults. Requires valid statistical samples from operational profile(model), and a clear, precise definition of what is being measured.\nQuantitative dependability goals are statistical: • Reliability: Survival Probability - when function is critical during the mission time. • Availability: The fraction of time a system meets its specification - Good when continuous service is important but it can be delayed or denied • Failsafe: System fails to a known safe state • Dependability: Generalisation - System does the right thing at right time\nUsability, Reliability, Availability/Reparability Testing\nSystem Reliability The reliability $R_F(t)$ of a system is the probability that no fault of the class $F$ occurs (i.e. system survives) during time $t \\sim (t_{init}, t_{failure})$.\nFailure Probability $Q_F(t) = 1 -R_F(t)$.\nWhen the lifetime of a system is exponentially distributed, the reliability of the system is: $R(t) = e^{-\\lambda t}$ where the parameter $\\lambda$ is called the failure rate.\nMTTF: Mean Time To (first) Failure, or Expected Life. $ MTTF = E(t_f) = \\int_0^\\infty R(t)dt = \\frac{1}{\\lambda}$\nSerial System Reliability: Serially Connected Components. Assuming the failure rates of components are statistically independent, The overall system reliability: $$R_{ser}(t) = \\prod_{i=1}^n R_i(t) = e^{-t(\\lambda_{ser})} = e^{-t(\\sum_{i=1}^n \\lambda_i)}$$ $R_i(t) = e^{-\\lambda_i t}$ is reliability of a single component $i$.\nParallel System Reliability: Parallel Connected Components. $$R_{par}(t) = 1 - Q_{par}(t) = 1 - \\prod_{i=1}^n Q_i(t) = 1 - \\prod_{i=1}^n (1 - e^{-\\lambda_i t}) = 1 - \\prod_{i=1}^n (1 - R_i(t)) $$For example: · if one is to build a serial system with 100 components each of which had a reliability of 0.999, the overall system reliability would be $0.999^{100} = 0.905$. · Consider 4 identical modules are connected in parallel, System will operate correctly provided at least one module is operational. If the reliability of each module is 0.95, the overall system reliability is $1-(1-0.95)^4 = 0.99999375$.\nStatistical testing is necessary for critical systems (safety critical, infrastructure, \u0026hellip;), but difficult or impossible when operational profile is unavailable or just a guess, or when reliability requirement is very high.\nProcess-based Measures Based on similarity with prior projects, less rigorous than statistical testing.\nSystem testing process - Expected history of bugs found and resolved: • Alpha testing: Real users, controlled environment • Beta testing: Real users, real (uncontrolled) environment • May statistically sample users rather than uses • Expected history of bug reports\nRegression Testing Ideally, software should improve over time. But changes can both • Improve software, adding features and fixing bugs • Break software, introducing new bugs - regressions\nTests must be re-run after any changes.\nMake use of different techniques for selecting a subset of all tests to reduce the time and cost for regression testing.\nRegression Test Selection From the entire test suite, only select subset of test cases whose execution is relevant to changes.\nCode-based Regression Test Selection: Only execute test cases that execute changed or new code.\nControl-flow and Data-flow Regression Test Selection: Re-run test cases only if they include changed elements – elements may be modified control flow nodes and edges, or definition-use (DU) pairs in data flow. To automate selection: • Tools record changed elements touched by each test case - stored in database of regression test cases • Tools note changes in program • Check test-case database for overlap\nSpecification-based Regression Test Selection: • Specification-based prioritization: Execute all test cases, but start with those that related to changed and added features.\nTest Set Minimization Identify test cases that are redundant and remove them from the test suite to reduce its size. • Maximize coverage with minimum number of test cases. • Stop after a pre-defined number of iterations • Obtain an approximate solution by using a greedy heuristic\nTest Set Prioritisation • Sort test cases in order of increasing cost per additional coverage • Select the first test case • Repeat the above two steps until k test cases are selected or max cost is reached (whichever is first). Prioritized Rotating Selection: Execute some sooner than others, eventually execute all test cases. Possible priority schemes: • Round robin: Priority to least-recently-run test cases • Track record: Priority to test cases that have detected faults before - They probably execute code with a high fault density • Structural: Priority for executing elements that have not been recently executed - Can be coarse-grained: Features, methods, files.\nTest-Driven Development (TDD) Test-Driven Development (or test driven design) is a methodology.\n• Short development iterations. • Based on requirement and pre-written test cases. • Produces code necessary to pass that iteration\u0026rsquo;s test. • Refactor both code and tests. • The goal is to produce working clean code that fulfills requirements.\nPrinciple of TDD - Kent Beck defines: • Never write a single line of code unless you have a failing automated test. • Eliminate duplication\nTDD uses Black-box Unit test： 1， 明确功能需求。 2， 为功能需求编写 test。 3， 运行测试，按理应该无法通过测试（因为还没写功能程序）。 4， 编写实现该功能的代码，通过测试。 5， 可选：重构代码（和 test cases），使其更快，更整洁等等。 6， 可选：循环此步骤\nAutomating Test Execution Designing test cases and test suites is creative, but executing test cases should be automatic.\nExample Tool Chain for Test Case Generation \u0026amp; Execution: Combine \u0026hellip; • A combinatorial test case generation (genpairs.py) to create test data • DDSteps to convert from spreadsheet data to JUnit test cases • JUnit to execute concrete test cases\nScaffolding Code to support development and testing. • Test driver: A \u0026ldquo;main\u0026rdquo; program for running a test • Test stubs: Substitute for called functions/methods/objects.\nStub is an object that holds predefined data and uses it to answer calls during tests. It is used when we cannot or don\u0026rsquo;t want to involve objects that would answer with real data or have undesirable side effects. 代指那些包含了预定义好的数据并且在测试时返回给调用者的对象。Stub 常被用于我们不希望返回真实数据或者造成其他副作用的场景。\n• Test harness: Substitutes for other parts of the deployed environment\n• Comparison-based oracle: need predicted output for each input. Fine for a small number of hand-generated test cases, e.g. hand-written JUnit test cases.\n• Self-Checking Code as Oracle: oracle written as self-checks, possible to judge correctness without predicting results. Advantages and limits: Usable with large, automatically generated test suites, but often only a partial check.\n• Capture and Replay: If human interaction is required, capture the manually run test case, replay it automatically. With a comparison-based test oracle, behavior same as previously accepted behavior.\nSecurity Testing \u0026ldquo;Regular\u0026rdquo; testing aims to ensure that the program meets customer requirements in terms of features and functionality. Tests \u0026ldquo;normal\u0026rdquo; use cases - Test with regards to common expected usage patterns.\nSecurity testing aims to ensure that program fulfills security requirements. Often non-functional. More interested in misuse cases.\nTwo common approaches: • Test for known vulnerability types • Attempt directed or random search of program state space to uncover the \u0026ldquo;weird corner cases\u0026rdquo;\nPenetration testing • Manually try to \u0026ldquo;break\u0026rdquo; software • Typically involves looking for known common problems.\nFuzz testing Send semi-valid input to a program and observe its behavior. • Black-box testing - System Under Test (SUT) treated as a \u0026ldquo;black-box\u0026rdquo; • The only feedback is the output and/or externally observable behavior of SUT.\nInput generation • Mutation based fuzzing: Start with a valid seed input, and \u0026ldquo;mutate\u0026rdquo; it. Can typically only find the \u0026ldquo;low-hanging fruit\u0026rdquo; - shallow bugs that are easy to find. • Generation based fuzzing: Use a specification of the input format (e.g. a grammar) to automatically generate semi-valid inputs - Very long strings, empty strings, Strings with format specifiers, \u0026ldquo;extreme\u0026rdquo; format strings, Very large or small values, values close to max or min for data type, Negative values. Almost invariably gives better coverage, but requires much more manual effort.\nThe Dispatcher: running the SUT on each input generated by fuzzer module.\nThe Assessor: automatically assess observed SUT behavior to determine if a fault was triggered.\nConcolic testing Concolic execution workflow: 1, Execute the program for real on some input, and record path taken. 2, Encode path as query to SMT solver and negate one branch condition 3, Ask the solver to find new satisfying input that will give a different path.\nWhite-box testing method. • Input generated from control-structure of code to systematically explore different paths of the program. • Generational search (\u0026ldquo;whitebox fuzzing\u0026rdquo;): Performs concolic testing, but prioritizes paths based on how much they improve coverage.\nGreybox fuzzing ▪ Coverage-guided semi-random input generation. ▪ High speed sometimes beats e.g. concolic testing, but shares some limitations with mutation-based fuzzing (e.g. magic constants, checksums).\nSoftware Process Models - Software Development Waterfall model: Sequential, no feedback 1, Requirements 2, Design 3, Implementation 4, Testing 5, Release and maintenance\nV-model: modified version of the waterfall model • Tests are created at the point the activity they validate is being carried out. So, for example, the acceptance test is created when the systems analysis is carried out. • Failure to meet the test requires a further iteration beginning with the activity that has failed the validation\nBoehm\u0026rsquo;s Spiral Model: focuse on controlling project risk and attempting formally to address project risk throughout the lifecycle. • V\u0026amp;V activity is spread through the lifecycle with more explicit validation of the preliminary specification and the early stages of design. The goal here is to subject the early stages of design to V\u0026amp;V activity. • At the early stages there may be no code available so we are working with models of the system and environment and verifying that the model exhibits the required behaviours.\nExtreme Programming (XP): one of [Agile Processes] • Advocates working directly with code almost all the time. • The 12 principles of XP summarise the approach.\n1, Test-driven development; 2, The planning game; 3, On-site customer; 4, Pair programming; 5, Continuous integration; 6, Refactoring; 7, Small releases; 8, Simple design; 9, System metaphor; 10, Collective code ownership; 11, Coding standards; 12, 40-hour work week;\n• Development is test-driven. • Tests play a central role in refactoring activity. • \u0026ldquo;Agile\u0026rdquo; development mantra: Embrace Change.\nFacebook\u0026rsquo;s Process Model\nPerpetual development - a continuous development model. In this model, software will never be considered a finished product. Instead features are continuously added and adapted and shipped to users. Fast iteration is considered to support rapid innovation.\nPlanning and Monitoring the Process Monitoring: Judging progress against the plan.\nQuality process: Set of activities and responsibilities. Follows the overall software process in which it is embedded. • Example: waterfall software process ––\u0026gt; \u0026ldquo;V model\u0026rdquo;: unit testing starts with implementation and finishes before integration • Example: XP and agile methods ––\u0026gt; emphasis on unit testing and rapid iteration for acceptance testing by customers.\nStrategies vs. Plans Test and Analysis Strategy: • Lessons of past experience: an organizational asset built and refined over time • Body of explicit knowledge: amenable to improvement, reduces vulnerability to organizational change (e.g., loss of key individuals)\nElements of a Strategy: • Common quality requirements that apply to all or most products - unambiguous definition and measures • Set of documents normally produced during the quality process - contents and relationships • Activities prescribed by the overall process - standard tools and practices • Guidelines for project staffing and assignment of roles and responsibilities\nMain Elements of a Plan: • Items and features to be verified - Scope and target of the plan • Activities and resources - Constraints imposed by resources on activities • Approaches to be followed - Methods and tools • Criteria for evaluating results\nSchedule Risk • Critical path = chain of activities that must be completed in sequence and that have maximum overall duration • Critical dependence = task on a critical path scheduled immediately after some other task on the critical path\nRisk Planning • Generic management risk: personnel, technology, schedule • Quality risk: development, execution, requirements\nContingency Plan • Derives from risk analysis • Defines actions in response to bad news - Plan B at the ready\nProcess Monitoring • Identify deviations from the quality plan as early as possible and take corrective action\nProcess Improvement Orthogonal Defect Classification (ODC) • Accurate classification schema: for very large projects, to distill an unmanageable amount of detailed information • Two main steps 1, Fault classification: when faults are detected, when faults are fixed. 2, Fault analysis\nRoot Cause Analysis (RCA) • Technique for identifying and eliminating process faults • Four main steps 1, What are the faults? 2, When did faults occur? When, and when were they found? 3, Why did faults occur? 4, How could faults be prevented?\n","permalink":"https://congchan.github.io/posts/inf-course-note-software-testing/","summary":"\u003cp\u003e爱丁堡大学信息学院课程笔记 Software Testing, Informatics, University of Edinburgh\u003c/p\u003e\n\u003cp\u003eReference:\n\u003ca href=\"http://www.inf.ed.ac.uk/teaching/courses/st/2017-18/index.html\"\u003ehttp://www.inf.ed.ac.uk/teaching/courses/st/2017-18/index.html\u003c/a\u003e\nPezze and Young, Software Testing and Analysis: Process, Principles and Techniques, Wiley, 2007.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"why-software-testing\"\u003eWhy Software Testing?\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e1, 软件的漏洞, 错误和失效 Software Faults, Errors \u0026amp; Failures\u003c/strong\u003e\nThe problem start with Faults,\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFault(BUG): latent error, mistakes in programming.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003ee.g \u003ccode\u003eadd(x, y) = x * y\u003c/code\u003e.\nWith the Faults in programs, \u003cstrong\u003eif and only if\u003c/strong\u003e executing \u003ccode\u003eadd(x, y) = x * y\u003c/code\u003e, the fault being activated, and generate an Errors.\u003c/p\u003e","title":"Inf Course Note - Software Testing"},{"content":"Word2vec Mikolov et al.\nHow to represent meanings? 如何在数学上表达词义？\nVector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上语义相似的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于Distributional Hypothesis, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, Latent Semantic Analysis))和Predictive models(例如 neural net language models (NNLM))。\n具体的区别详见Baroni et al.. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。\nWord2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.).\n从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words ('the cat sits on the')预测目标单词(例如\u0026quot;mat\u0026quot;); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。\nWord2vec的算法流程是：\nStart with a sentence: “the quick brown fox jumps.” Use a sliding window across the sentence to create (context, target) pairs, where the target is the center word, and the context is the surrounding words: ([the, brown], quick) ([quick, fox], brown) ([brown, jumps], fox) Use a lookup embedding layer to convert the context words into vectors, and average them to get a single input vector that represents the full context. Using the context vector as input into a fully connected Neural Network layer with a Softmax transformation. This results in a probability for every word in your entire vocabulary for being the correct target given the current context. Minimize the cross-entropy loss where label = 1 for the correct target word, and label = 0 for all other words. 优化目标函数 NNLM 的训练是利用 最大似然 maximum likelihood (ML) 原则来最大化给定上文单词\\(h\\) (for \u0026ldquo;history\u0026rdquo;) 预测下一个词的概率 \\(w_t\\) (for \u0026ldquo;target\u0026rdquo;)。\n$$ \\begin{align} P(w_t | h) \u0026= \\text{softmax}(\\text{score}(w_t, h)) \\\\\\\\ \u0026= \\frac{\\exp \\{ \\text{score}(w_t, h) \\} } {\\sum_\\text{Word w' in Vocab} \\exp \\{ \\text{score}(w', h) \\} } \\end{align} $$其中 \\(\\text{score}(w_t, h)\\) 计算 word \\(w_t\\) 和 context \\(h\\) 的相关性 (一般用点乘).\n训练时，最大化\n$$ \\begin{align} J_\\text{ML} \u0026= \\log P(w_t | h) \\\\\\\\ \u0026= \\text{score}(w_t, h) - \\log \\left( \\sum_\\text{Word w' in Vocab} \\exp \\{ \\text{score}(w', h) \\} \\right). \\end{align} $$这么计算成本很高， 因为在每一训练步，需要为词汇表 \\(V\\) 中的每一个词汇 \\(w\u0026rsquo;\\) 计算在当前上下文 \\(h\\) 的分数概率。\nNegative sampling 但是，word2vec的目的是特征学习，而不是学习完整的概率语言模型。所以word2vec（CBOW和Skip gram一样）的训练目标函数其实是一个二分类模型(logistic regression)，给定一个上下文，在 \\(k\\) 个噪声词（根据算法选出）和一个真正的目标词汇\\(w_t\\)中识别出目标词\\(w_t\\)。如下图(以CBOW为例, Skip gram方向反过来)\n目标函数变为最大化: $$ J_\\text{NEG} = \\log Q_\\theta(D=1 |w_t, h) + k \\mathop{\\mathbb{E}}_{\\tilde w \\sim P_n} \\left[ \\log Q_\\theta(D = 0 |\\tilde w, h) \\right] $$where \\(Q_\\theta(D=1 | w, h)\\) is the binary logistic regression probability under the model of seeing the word \\(w\\) in the context \\(h\\) in the dataset \\(D\\), calculated in terms of the learned embedding vectors \\(\\theta\\). In practice we approximate the expectation by drawing \\(k\\) contrastive words from the noise distribution (i.e. we compute a Monte Carlo average).\n负采样是指每个训练样本仅更新模型权重的一小部分：\n负采样的选择是基于 unigram 分布 $f(w_i)$: 一个词作为负面样本被选择的概率与其出现的频率有关，更频繁的词更可能被选作负面样本。 $$ P(w_i) = \\frac{ {f(w_i)}^{3/4} }{\\sum_{j=0}^{n}\\left( {f(w_j)}^{3/4} \\right) } $$ 负采样带来的好处是\n训练速度不再受限于 vocabulary size 能够并行实现 模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。 模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。 通过这种方式把学习P分布的无监督学习任务改造为监督学习。 每个词由两个向量表示：\n$v_w$, 表示这个词作为中心词 (Focus Word) 时的样子。 $u_w$, 表示它作为另一个中心词的上下文 (Context Word) 时的样子。 这样, 对于一个中心词 $c$ 和外围词$o$: $$ P(o|c) = \\frac{exp(u^T_o v_c)}{\\sum_{w \\in V} \\left( {exp(u^T_w v_c)} \\right)} $$在C语言的源代码里，这些向量由两个数组 (Array) 分别负责： syn0数组，负责某个词作为中心词时的向量。是随机初始化的。\n// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369 for (a = 0; a \u0026lt; vocab_size; a++) for (b = 0; b \u0026lt; layer1_size; b++) { next_random = next_random * (unsigned long long)25214903917 + 11; syn0[a * layer1_size + b] = (((next_random \u0026amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size; } syn1neg数组，负责这个词作为上下文时的向量。是零初始化的。\n// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L365 for (a = 0; a \u0026lt; vocab_size; a++) for (b = 0; b \u0026lt; layer1_size; b++) syn1neg[a * layer1_size + b] = 0; 训练时，先选出一个中心词。在正、负样本训练的时候，这个中心词就保持不变 (Constant) 了。\n# https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L226 # Negative sampling. sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler( true_classes=labels_matrix, num_true=1, num_sampled=opts.num_samples, unique=True, range_max=opts.vocab_size, distortion=0.75, unigrams=opts.vocab_counts.tolist())) Noise Contrastive Estimation Noise Contrastive Estimation （NCE）是一种通过比较数据分布和定义的噪声分布来学习数据分布的方法。在实现上与负采样非常相似，但它增加了一些理论上的依据。NCE方法中的精髓在于把难以计算的高维Sofmax多分类损失函数转化成为了更容易计算的二分类损失函数。\n使用Logistic Regression (LogReg) model对输入来自一个类而不是另一类的log-odds赔率比率进行建模： $$ logit=\\log \\left(\\frac{p_{1}}{p_{2}}\\right)=\\log \\left(\\frac{p_{1}}{1-p_{1}}\\right) $$如果替换为正样例P和负样例Q，将试图学习的数据分布与噪声分布进行比较，这就是NCE $$ \\operatorname{logit}=\\log \\left(\\frac{\\mathrm{P}}{\\mathrm{Q}}\\right)=\\log (\\mathrm{P})-\\log (\\mathrm{Q}) $$ 我们不知道真正的分布P，但我们可以自由地指定Q分布来生成负样本。例如，以相等的概率对所有词汇进行采样，或者以一种考虑到一个单词在训练数据中频率的方式进行采样。总之Q是由我们决定的，所以计算log(Q)部分是可以直接统计出来的。随机从我们指定的分布Q中抽取负样例。直接通过神经网络预测正样本为正，负样本为负的方式训练网络，仅计算正目标词的网络输出值以及我们从噪声分布中随机采样的单词，仅更新对应的权重。\n具体步骤前三步和上面一样 create the same (context, target) pairs, and average the embeddings of the context words to get a context vector.\nIn Step 4, you do the same thing as in Negative Sampling: use the context embedding vector as input to the neural network, and then gather the output for the target word and a random sample of k negative samples from the noise distribution, Q. For the network output of each of the selected words, $z_i$, subtract $log(Q)$, $y_i = z_i - log(Q_i)$ Instead of a Softmax transformation, apply a sigmoid transformation, as in Logistic Regression: $$ \\hat{p}_i=\\sigma\\left(y_i\\right)=\\frac{1}{1-e^{-y_i}} $$ Label the correct target word with label=1 and the negative samples with label=0. Use these as training samples for a Logistic Regression, and minimize the Binary Cross Entropy Loss: $$ BCE = \\frac{1}{N} \\sum_i^N l_i \\log (\\hat{p}_i) + (1 - l_i) \\log (1 - \\hat{p}_i) $$ $N = k+1$ (number of negative samples plus the target word), $l_i$ are the labels for if it’s the target or a negative sample, and Equation are the outputs of the sigmoid as defined above. 图表征学习 参考漫谈表征学习 - 张相於的文章 - 知乎\n互联网场景下常见的ID类特征有大量信息冗余。ID类特征本身含有的信息是非常少的，每一个维度就是一个ID，而每一个ID上本身是一个取值只有0或1的二元特征，并没有什么提炼汇总的空间，它的冗余主要体现在大量ID之间存在或强或弱的相关性。这种情况下，要相对这海量的ID进行降维有两大类思路：第一类思路是将ID进行归类，即将个体ID降维到类别ID，这里的典型代表是LDA这类的主题聚类方法；第二类是不直接进行ID归类，而是将ID投射到一个新的低维空间中，在这个空间中ID间可计算相似度，拥有更丰富的相似度空间，这类方法的典型代表是word2vec等序列学习的方法。\n实践中常用的图表征学习方法基本上都可以溯源到word2vec中的词向量训练方法，其中又以SGNS为主。\nSGNS方法通过序列构造训练样本的方式，通过负采样完成模型的高效求解。\n这套方法中模型只在样本中指明了不同节点之间应该是什么关系，应该亲密还是疏远，同时给出一组特征用来进行这种亲密度的判断，但有趣的地方在于就在这组特征：这里给的是一组完全随机初始化的，每个维度没有什么明确含义的特征，这和其他常用模型有着两点本质区别：每一维特征没有既定输入的特征值每一维特征没有明确的含义作为对比，我们在电商CTR模型中可能会用一维特征叫做价格，同时这个特征在每条样本上也会有明确的取值，但在SGNS这套方法中这两点都被打破了。\n如果说传统机器学习算法是给定特征值，学习特征权重，那么图表征学习就是在同时学习特征值和特征权重。但仔细一想，事实也并非完全如此，样本中也并非完全没有输入特征值，其实每条样本都有一个信息量高度聚集的输入特征值，那就是节点本身，或者说是节点ID，所以从某种角度来看，整个图表征学习的过程就是把节点ID这一信息量大，但却稀疏性高，缺乏节点间共享能力的特征，分解到了一组向量上面，使得这组向量能够还原原始节点ID所持有的信息量，而原始节点ID的信息全部通过样本中两两节点的关系来体现，所以学习得到的向量也能够体现原始节点的两两关系。更重要的是，这样分解后的向量在不同节点有了大量共享重合的维度，具有了很好的泛化能力。从这个角度来看，图表征学习就像是一个“打碎信息”的过程：将原本高度聚集在一个ID上的信息打碎之后分配在一个向量上，使得向量中分散的信息通过内积运算仍然能够最大程度还原原始ID的信息量。\nSGNS方法很好地解决了样本构造和海量softmax计算这两个图表征学习中最重要的问题，因此对后面的其他图表征学习算法都产生了很大的影响，像DeepWalk以及node2vec等方法，基本都保留SGNS方法的整体框架，只是在样本构造方式上做了优化。此外，SGNS算法还有比较强的调优空间，这主要体现在损失函数的构造上，negative sampling本质上是nce方法（noise contrastive estimate）的一种具体实现，而nce方法中的精髓在于他把难以计算的标准化多分类损失函数转化成为了更容易计算的二分类损失函数，这一精髓思想也在很多后续工作中得到了发扬.\n例如Airbnb发表在KDD 2018上的工作中，就把nce loss中负样本的部分进行了扩展:\nSGNS中标准的正负样本loss 另外两个项则是使用类似负采样的思想构造出来的，指明了想要把什么样的样本和什么样的样本区分开来。 指出用户点击过的房子；应该和最终预订的房子比较像， 用户点击的房子要和同区域内曝光未点击的房子比较不像。 相比于LDA用于表征学习\n以SGNS为代表的表征学习方法有着一些比较明显的优势。\n首先是与深度学习架构的原生兼容性。SGNS本质上就是一个特殊的浅层神经网络，其优化过程也是基于反向梯度传播的，所以SGNS得到的结果可以很方便地作为输入嫁接到一个更复杂的深度网络中继续训练，在之前结果上继续细粒度调优。\n其次是图表征学习方法捕捉到的信息要比LDA更加丰富，或者说加入了很多LDA这类方法没有包含的信息，最为典型的就是节点间的顺序信息以及局部图结构信息：LDA看待文档中的词是没有顺序和局部性的，只有文档和词的相互对应关系，但是图表征学习却非常看重这一信息，能够学习到局部顺序信息，这一点对于一些对于信息要求高的下游应用是很有用的。\n再次，以node2vec和deepwalk为代表的方法可以同时在同一个图结构中学到多种不同类型物品的表征，且这些物品在同一个语义空间中。例如可以同时将用户、物品甚至物品的属性放在一起进行学习，只要他们能够成一个图结构即可，物品和用户的表征是由于在训练过程中是无差别的，因此是在同一个语义空间中的，可以进行内积运算直接计算相似度。但LDA算法的理论基础决定了它一次只适合描述一种类型的物品，或者最多同时学习出文档和主题这两种类型的表示，而且即使这样，这两种表示也不在同一语义空间中，因此无法直接进行相关性运算，也无法当做同一组特征嵌入到其他模型中，在应用面和易用性方面相对受限。\n最后就是工具层面的影响，在计算机行业，最流行的方法不一定是理论上最先进的，但一定是工具最方便的，古今中外，概莫能外。在TensorFlow、Pytorch等工具流行之前，基本都是每个算法一个专用工具，典型的例如libsvm、liblinear等，一个算法如果没有一个高效易用的实现是很难被广泛应用的，包括word2vec的广泛应用也有很大一部分要归功于其工具的简单高效，而tf这些可微分编程工具的出现，使得新算法的开发和优化变得更加简单，那么像LDA这样没有落入到可微分编程范畴内的方法自然就相对受到冷落了。上面这些原因的存在，使得图表征学习成为了事实标准以及未来趋势，但LDA为代表的生成式表征学习方法仍然会在合适的场合继续发挥作用，并且也有可能会和可微分编程框架发生融合，焕发第二春。\n用TensorFlow实现word2vec Negative Sampling 可以利用原理近似的 noise-contrastive estimation (NCE) loss, 已经在TF的tf.nn.nce_loss()实现了.\nBuilding the graph 初始化一个在-1: 1之间随机均匀分布的矩阵\nembeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) NCE loss 依附于 logistic regression 模型。为此, 我们需要定义词汇中每个单词的weights和bias。\nnce_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) nce_biases = tf.Variable(tf.zeros([vocabulary_size])) 参数已经就位, 接下来定义模型图.\nThe skip-gram model 有两个输入. 一个是以word indice表达的一个batch的context words, 另一个是目标单词。为这些输入创建placeholder节点, 以便后续馈送数据。\n# Placeholders for inputs train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) 利用embedding_lookup来高效查找word indice对应的vector.\nembed = tf.nn.embedding_lookup(embeddings, train_inputs) 使用NCE作为训练目标函数来预测target word:\n# Compute the NCE loss, using a sample of the negative labels each time. loss = tf.reduce_mean( tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels=train_labels, inputs=embed, num_sampled=num_sampled, num_classes=vocabulary_size)) 然后添加计算梯度和更新参数等所需的节点。\n# We use the SGD optimizer. optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss) Training the model 使用feed_dict推送数据到placeholders, 调用tf.Session.run\nfor inputs, labels in generate_batch(...): feed_dict = {train_inputs: inputs, train_labels: labels} _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict) Evaluating the model Embeddings 对于 NLP 中的各种下游预测任务非常有用。可以利用analogical reasoning, 也就是预测句法和语义关系来简单而直观地评估embeddings, 如king is to queen as father is to ?\n# https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L292 def build_eval_graph(self): \u0026#34;\u0026#34;\u0026#34;Build the eval graph.\u0026#34;\u0026#34;\u0026#34; # Eval graph # Each analogy task is to predict the 4th word (d) given three # words: a, b, c. E.g., a=italy, b=rome, c=france, we should # predict d=paris. # The eval feeds three vectors of word ids for a, b, c, each of # which is of size N, where N is the number of analogies we want to # evaluate in one batch. analogy_a = tf.placeholder(dtype=tf.int32) # [N] analogy_b = tf.placeholder(dtype=tf.int32) # [N] analogy_c = tf.placeholder(dtype=tf.int32) # [N] # Normalized word embeddings of shape [vocab_size, emb_dim]. nemb = tf.nn.l2_normalize(self._emb, 1) # Each row of a_emb, b_emb, c_emb is a word\u0026#39;s embedding vector. # They all have the shape [N, emb_dim] a_emb = tf.gather(nemb, analogy_a) # a\u0026#39;s embs b_emb = tf.gather(nemb, analogy_b) # b\u0026#39;s embs c_emb = tf.gather(nemb, analogy_c) # c\u0026#39;s embs # We expect that d\u0026#39;s embedding vectors on the unit hyper-sphere is # near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim]. target = c_emb + (b_emb - a_emb) # Compute cosine distance between each pair of target and vocab. # dist has shape [N, vocab_size]. dist = tf.matmul(target, nemb, transpose_b=True) # For each question (row in dist), find the top 4 words. _, pred_idx = tf.nn.top_k(dist, 4) # Nodes for computing neighbors for a given word according to # their cosine distance. nearby_word = tf.placeholder(dtype=tf.int32) # word id nearby_emb = tf.gather(nemb, nearby_word) nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True) nearby_val, nearby_idx = tf.nn.top_k(nearby_dist, min(1000, self._options.vocab_size)) Reference https://tensorflow.google.cn/tutorials/representation/word2vec Learning word embeddings efficiently with noise-contrastive estimation Learning word embeddings efficiently with noise-contrastive estimation http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ A Gentle Introduction to Noise Contrastive Estimation ","permalink":"https://congchan.github.io/posts/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3word2vec/","summary":"\u003cp\u003eWord2vec \u003ca href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\"\u003eMikolov et al.\u003c/a\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"how-to-represent-meanings\"\u003eHow to represent meanings?\u003c/h2\u003e\n\u003cp\u003e如何在数学上表达词义？\u003c/p\u003e\n\u003cp\u003eVector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上\u003cstrong\u003e语义相似\u003c/strong\u003e的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于\u003ca href=\"https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis\"\u003eDistributional Hypothesis\u003c/a\u003e, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, \u003ca href=\"https://en.wikipedia.org/wiki/Latent_semantic_analysis\"\u003eLatent Semantic Analysis\u003c/a\u003e))和Predictive models(例如 \u003ca href=\"http://www.scholarpedia.org/article/Neural_net_language_models\"\u003eneural net language models (NNLM)\u003c/a\u003e)。\u003c/p\u003e\n\u003cp\u003e具体的区别详见\u003ca href=\"http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf\"\u003eBaroni et al.\u003c/a\u003e. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。\u003c/p\u003e\n\u003cp\u003eWord2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in \u003ca href=\"https://arxiv.org/pdf/1301.3781.pdf\"\u003eMikolov et al.\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003e从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words (\u003ccode\u003e'the cat sits on the'\u003c/code\u003e)预测目标单词(例如\u003ccode\u003e\u0026quot;mat\u0026quot;\u003c/code\u003e); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。\u003c/p\u003e","title":"深入理解word2vec"},{"content":"循环神经网络 当人类阅读时，会根据对之前单词的理解和记忆来辅助理解当前看到的每个单词。也就是人能够很好地处理语言的长距离依赖特性（long-term dependency）。在自然语言处理任务中，很多传统的模型无法做到这一点，比如前馈神经网络；而传统的n-gram模型固然可以通过把把n系数增大来捕捉长距离依赖，但带来的非常巨大的内存消耗。\n循环神经网络（Recurrent Neural Networks, RNNs)可以看做是多个共享参数的前馈神经网络不断叠加的结果 ![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png \u0026ldquo;A recurrent neural network and the unfolding in time of the computation involved in its forward computation. \u0026ldquo;image from: http://colah.github.io\u0026rdquo;)\n这里的核心是想办法解码历史信息, 即通过递归方程$s_i = R(x_i, s_{i−1})$让$s_i$解码序列$x_{1:n}$. 比如把所有历史信息累加就是一种非常简单粗暴的方式, 这样得到的是连续词袋模型(continuous-bag-of-words model)$s_i = R_{CBOW}(x_i, s_{i-1}) = x_i + s_{i−1}$, 虽然简单，但这种RNN其实忽略了数据的时序性质。\n一般意义上的RNN是指Elman Network or Simple-RNN (S-RNN)(Elman [1990]), $s_i = R_{SRNN}(x_i, s_{i-1}) = g(x_iW^x + s_{i−1}W^s + b)$, 也就是把历史信息先进行线性变换(乘以矩阵), 再和bias加起来, 再通过一个非线性激活函数(tanh或ReLU). 添加了线性变换再进行非线性激活, 使网络对输入的顺序变得敏感。\n在使用时, 给定输入序列（单词序列或语音）得出输出序列的过程如下：\n把每个词$x_{t}$(以向量表示)逐个输入RNN 每一时间步$t$都有对应的隐含状态$s_t$，用于解码历史信息: $s_t = g(Ux_t + Ws_{t-1} + b)$. 每一时间步都可以有一个输出（虽然大部分应用只用到最后一时间步）$o(t)$： 例如，语言模型想要预测下一个单词，那么输出就是在词汇表上的概率分布向量，$o_t = softmax(Vs_t)$. 其中，各个时间步共享几个参数矩阵（$U, V, W$） In addition to the above normal many to many structure RNNs, there are other non-sequence input or output: Many to one, e.g. when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. One to many: Music generation. 除了应用于语言模型, RNNs 还可以应用于 · tagging, e.g. part-of-speech tagging, named entity recognition (many to many RNNs) · sentence classification, e.g. sentiment classification (many to one RNNs) · generate text, e.g. speech recognition, machine translation, summarization\nRNNs Backpropagation Backpropagation Through Time (BPTT): Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps.\nRNNs trained with BPTT have difficulties learning long-term dependencies (e.g. dependencies between steps that are far apart) due to what is called the vanishing/exploding gradient problem.\n梯度消失与爆炸 The Vanishing/Exploding Gradient problem。\nRNNs shares the same matrix (w, u, etc.) at each time step during forward prop and backprop. 求导数时, 根据链式法则, loss对各参数的导数会转换为loss对输出y的导数, 乘以y对隐含层的导数, 乘以隐含层相对隐含层之间的导数, 再乘以隐含层对参数的导数.\n不同隐含层（举例如$h_t$和$h_k$）之间如果相隔太远, $h_t$对$h_k$的导数就变成多个jacobian矩阵的相乘， 对各个jacobian范数（norms）进行分析后，发现$h_t$对$h_k$的导数值在训练过程中会很快变得很极端（非常小或者非常大）。\nGradient作为传导误差以帮助系统纠正参数的关键角色，如果本身变得接近于0或者nan，那么我们就无法判断t和t+n的数据的依赖性（是没有依赖？还是因为vanish of gradient？还是因为参数设置错误？）。梯度衰减会直接降低模型学习长距离依赖关系的能力，给定一个时间序列，例如文本序列，循环神经网络较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。\n在使用RNN学习language model的时候，非常容易出现梯度爆炸，解决办法是使用 gradient clipping 梯度裁剪，就是通过把梯度映射到另一个大小的空间，以限制梯度范数的最大值On the difficulty of training Recurrent Neural Networks。\n虽然梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。一个缓解梯度衰减的方案是使用更好的参数初始化方案和激活函数（ReLUs）A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.\n不过更主流的解决梯度衰减的方案是使用更复杂的rnn隐含单元: Gated Recurrent Units (GRU) introduced by Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation and LSTMs.\n门控循环网络 因为梯度消失的问题，RNN的解码能力是很有限的。S-RNN架构的一个明显缺陷是对历史信息的记忆是不受控制，在每一时间步的计算，读写整个记忆状态$s_t$。而门控循环网络，比如Long Short-Term Memory（LSTMs），Gated Recurring Unit（GRUs），使用门的概念，让网络拥有控制哪些信息需要记录, 哪些需要丢弃的能力。如何实现这种门呢? 考虑一种介于[0, 1]中间的因子, 让这种因子与各种状态信息相乘, 可以为每个状态信息独自训练一个因子, 也就是由简单的神经网络(非线性激活函数Sigmoid)来控制.\n是否允许信息通过（打开）或不通过（关闭）取决于其门控单元内部Sigmoid激活层的点乘运算。Sigmoid函数值介于0和1之间，可用于描述允许通过单元格的信息量。\nLSTM架构将状态向量$s_i$分成两半，其中一半被视为“记忆单元”$C$, 而另一半被视为一般的工作存储单元-隐含状态$h$。\n1, LSTM用遗忘门来决定从前一时间步的记忆单元中丢弃哪些信息，控制当前记忆单元应该忘记多少来自前一步状态$h_{t-1}$的信息量，标记为遗忘信息。遗忘门由一个sigmoid层学习而来 2, 用输入门 Input gate (a sigmoid hidden layer) 来决定有多少新信息是值得储存的（当前时间步$t$）。输入门控制哪些信息需要更新. 再通过一个隐含层(tanh/relu)生成新的候选信息向量$\\widetilde{C}_t$. 输入门和遗忘门一起，控制每一步的信息存储和改写, 将遗忘信息和候选信息组合在一起作为更新信息，作为当前时间步的新记忆单元，$C_{t}$.\n3, 最后，用一个输出门 Output gate (a sigmoid layer) 来控制多少记忆单元作为当前步的工作隐含状态$h_t$。先通过一个tanh激活层把当前记忆单元$C_t$推送为[-1, 1]之间的值, 再乘以输出门.\n总的来说, LSTM有遗忘门, 输入门和输出门这三个门. 加上其中的更新信息, 形式上LSTM有四个神经网络, 输入都是上一步隐含状态和当前步的输入向量的.\nGRUs LSTMs分别使用记忆单元和隐含状态来控制信息流, 其中最新的隐含状态需要用到最新的记忆单元来计算，每一步输出只用到隐含状态的信息，也就说记忆单元本质上是为隐含状态的更新服务的。而记忆单元自身就用到了三个神经网络来计算，而且可以看到记忆单元和隐含状态的计算似乎有一些冗余。比如LSTM的遗忘门和更新门，更新和遗忘本身就是相伴相随的，对于一个记忆单元来说，本身的容量是固定的，需要更新多少信息，自然也意味着需要遗忘多少信息，也就是可以用一个神经网络来控制。随着数据量的增大和神经网络增大，是否可以把一些门合并简化？\nGRUs (Cho, et al. (2014))就抛弃了记忆单元的设定, 只用隐含状态 $h$ 来表达信息流. GRUs首先根据当前的输入词向量和隐含状态计算更新门$z_t$和一个重置门$r_t$。核心是更新门$z_t$，用于控制更新多少新的候选信息$\\widetilde{h}_t$, 同时意味着只保留$(1 - z_t)$的旧信息$h_{t-1}$. 隐含状态的候选信息$\\widetilde{h}_t$用重置门$r_t$计算.\nLSTMs的遗忘门和输入门的合作核心是更新信息. 而GRUs将LSTMs的遗忘门和输入门合并成一个更新门, 抛弃了输出门的概念, 让更新门负责计算新的隐含状态. 这样GRUs内部总共只有两个门, 三个神经网络, 某种程度上简化了LSTMs模型。\nGRU intuition\n重置门赋予了模型丢弃与未来无关的信息的能力。若重置门接近于0，则忽略之前的记忆，仅储存新加入的信息. 更新门控制过去的状态对现在的影响程度（即决定更新多少），如果接近于$1$，则 $h_t=z_t\\cdot h_{t-1}$, 等同于把过去的信息完整复制到未来，相应地缓解梯度衰减。 短距离依赖的单元，过去的信息仅保留很短的时间，重置门一般很活跃，也就是数值在0和1之间频繁变动。 长距离依赖的单元，重置门较稳定（保留过去的记忆较长时间），而更新门较活跃。 不同RNNs变种的比较 Vanilla RNNs Execution:\nRead the whole register h Update the whole register GRU Execution:\nSelect a readable subset Read the subset Select a writable subset Update the subset 门控循环神经网络的训练 把参数矩阵初始化为正交 把遗忘门的bias初始化为1，默认不遗忘 别忘了梯度裁剪 注意dropout在RNNs中的应用不同于DNN和CNN Bidirectional RNNs Bidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs. 参考资料 斯坦福cs224n http://web.stanford.edu/class/cs224n\nhttp://colah.github.io\nNeural Network Methods in Natural Language Processing, by Yoav Goldberg\n","permalink":"https://congchan.github.io/posts/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","summary":"\u003ch2 id=\"循环神经网络\"\u003e循环神经网络\u003c/h2\u003e\n\u003cp\u003e当人类阅读时，会根据对之前单词的理解和记忆来辅助理解当前看到的每个单词。也就是人能够很好地处理语言的长距离依赖特性（long-term dependency）。在自然语言处理任务中，很多传统的模型无法做到这一点，比如前馈神经网络；而传统的n-gram模型固然可以通过把把n系数增大来捕捉长距离依赖，但带来的非常巨大的内存消耗。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e循环神经网络（Recurrent Neural Networks, RNNs)可以看做是多个\u003cstrong\u003e共享参数\u003c/strong\u003e的前馈神经网络不断叠加的结果\n![](\u003ca href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\"\u003ehttp://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\u003c/a\u003e \u0026ldquo;A recurrent neural network and the unfolding in time of the computation involved in its forward computation. \u0026ldquo;image from: \u003ca href=\"http://colah.github.io\"\u003ehttp://colah.github.io\u003c/a\u003e\u0026rdquo;)\u003c/p\u003e\n\u003cp\u003e这里的核心是想办法解码历史信息, 即通过递归方程$s_i = R(x_i, s_{i−1})$让$s_i$解码序列$x_{1:n}$. 比如把所有历史信息累加就是一种非常简单粗暴的方式, 这样得到的是连续词袋模型(continuous-bag-of-words model)$s_i = R_{CBOW}(x_i, s_{i-1}) = x_i + s_{i−1}$, 虽然简单，但这种RNN其实忽略了数据的时序性质。\u003c/p\u003e\n\u003cp\u003e一般意义上的RNN是指Elman Network or Simple-RNN (S-RNN)(\u003ccode\u003eElman [1990]\u003c/code\u003e), $s_i = R_{SRNN}(x_i, s_{i-1}) = g(x_iW^x + s_{i−1}W^s + b)$, 也就是把历史信息先进行线性变换(乘以矩阵), 再和bias加起来, 再通过一个非线性激活函数(tanh或ReLU). 添加了线性变换再进行非线性激活, 使网络对输入的顺序变得敏感。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg\" title=\"image from: Nature\"\u003e\n在使用时, 给定输入序列（单词序列或语音）得出输出序列的过程如下：\u003c/p\u003e","title":"循环神经网络"},{"content":"What you will get from this Python digest: 1, Learn advanced python programming. 2, Learn new concepts, patterns, and methods that will expand your programming abilities, helping move you from a novice to an expert programmer. 3, Practice going from a problem description to a solution, using a series of assignments.\nOperator Emulating numeric types In-place operation: One modifies the data-structure itself\nobject.__iadd__(self, other) object.__isub__(self, other) object.__imul__(self, other) object.__imatmul__(self, other) object.__itruediv__(self, other) object.__ifloordiv__(self, other) object.__imod__(self, other) object.__ipow__(self, other[, modulo]) object.__ilshift__(self, other) object.__irshift__(self, other) object.__iand__(self, other) object.__ixor__(self, other)¶ object.__ior__(self, other) These methods are called to implement the augmented arithmetic assignments. These methods should attempt to do the operation in-place (modifying self) and return the result (which could be, but does not have to be, self). If x is an instance of a class with an __iadd__() method, x += y is equivalent to x = operator.iadd(x, y)\nB = np.arange(12).reshape(4,3) for b in B: b += 1 print(B) # B will be changed Object oriented Programming Class Name Guidline underscore (_): • For storing the value of last expression in interpreter. • For ignoring the specific values. (so-called “I don’t care”) • To use as ‘Internationalization(i18n)’ or ‘Localization(l10n)’ functions. • To separate the digits of number literal value.\nTo give special meanings and functions to name of variables or functions • single_leading_underscore: weak \u0026ldquo;internal use\u0026rdquo; indicator, declaring private variables, functions, methods and classes in a module. Anything with this convention are ignored in from module import *. • single_trailing_underscore: used by convention to avoid conflicts with Python keyword • __double_leading_underscore: when naming a class attribute, invokes name mangling (inside class FooBar, __boo becomes _FooBar__boo; see Designing for inheritance) • double_leading_and_trailing_underscore: \u0026ldquo;magic\u0026rdquo; objects or attributes that live in user-controlled namespaces. E.g. init, import or file. Never invent such names; only use them as documented. See Magic Attributes\nDesigning for inheritance If your class is intended to be subclassed, and you have attributes that you do not want subclasses to use, consider naming them with double leading underscores and no trailing underscores. This invokes Python\u0026rsquo;s name mangling algorithm, where the name of the class is mangled into the attribute name. This helps avoid attribute name collisions should subclasses inadvertently contain attributes with the same name. • Note 1: Note that only the simple class name is used in the mangled name, so if a subclass chooses both the same class name and attribute name, you can still get name collisions. • Note 2: Name mangling can make certain uses, such as debugging and getattr(), less convenient. However the name mangling algorithm is well documented and easy to perform manually. • Note 3: Not everyone likes name mangling. Try to balance the need to avoid accidental name clashes with potential use by advanced callers.\nDescriptor Magic Attributes __init__ for initialization purpose.\nobject.__dict__: A dictionary or other mapping object used to store an object’s (writable) attributes. Basically it contains all the attributes which describe the object under question. It can be used to alter or read the attributes. __call__\nIs Python call-by-value or call-by-reference? Neither.\nIn Python, (almost) everything is an object. What we commonly refer to as \u0026ldquo;variables\u0026rdquo; in Python are more properly called names. A variable is not an alias for a location in memory. Rather, it is simply a binding to a Python object, likewise, \u0026ldquo;assignment\u0026rdquo; is really the binding of a name to an object. Each binding has a scope that defines its visibility, usually the block in which the name originates. \u0026ndash; https://jeffknupp.com/blog/2012/11/13/is-python-callbyvalue-or-callbyreference-neither/\nPython实际上有两种对象。\n一种是可变对象，表现出随时间变化的行为。可变对象的变更对与它绑定的所有名称都可见，如 Python list。 一种是不可变对象，值在创建后无法修改。 跟java的 immutable reference类似的是 Python tuple：虽然 tuple 不可变，那仅是针对其自身所绑定固定的对象而言tuple(list1, list2)，但tuple包含的元素对象list1, list2本身有自己的可变属性. 所以Python的方法调用中,foo(bar)只是在foo的作用域内创建一个与参数bar的绑定。\n如果bar指向可变对象，当foo更改时，这些更改可以在函数foo的作用域外可见。 如果bar指向一个不可变的对象，foo只能在其自身本地空间中创建一个名称bar并将其绑定到其他对象。 Solving Problem A general process to solve problem with three steps: understand, specify and design. 1, Start with a vague understanding that you refine into a formal specification of a problem. In this step you want to take inventory of the concepts you are dealing with. 2, Specify how this problem can be made amenable to being coded. What is the input and output? What output is desirable? 3, Design working code\n?? \u0026mdash;-(1 Vague Understanding)\u0026ndash;\u0026gt;Formal specification of a problem \u0026mdash;(2 Specify)\u0026mdash;\u0026gt;Amendable specification\u0026mdash;(3 Design)\u0026mdash;\u0026gt;Working Code\nProgram Design and Development Dimensions of programming Correctness, Efficiency, Features, Elegance Each part takes time, learn to make Tradeoff: During the process, generally Correctness comes first. Test But pursuing the 100% Correctness is not the best choice. There is a balance of tradeoff, and sometimes saving some time and efforts to improving the Efficiency or adding more Features may be a better option. Elegance is good for maintaining and improving the program, which means saving for the future. Refactoring - moving along the Elegance direction without changing the other dimensions. DRY: don\u0026rsquo;t repeat yourself Reuse: save time and code lines, also reduce the possibility of mistake Coding Style For Python, https://www.python.org/dev/peps/pep-0008 has emerged as the style guide that most projects adhere to; it promotes a very readable and eye-pleasing coding style. Here are the most important points extracted:\nUse 4-space indentation, and no tabs.\n4 spaces are a good compromise between small indentation (allows greater nesting depth) and large indentation (easier to read). Tabs introduce confusion, and are best left out.\nWrap lines so that they don’t exceed 79 characters.\nThis helps users with small displays and makes it possible to have several code files side-by-side on larger displays.\nUse blank lines to separate functions and classes, and larger blocks of code inside functions.\nWhen possible, put comments on a line of their own.\nUse docstrings.\nUse spaces around operators and after commas, but not directly inside bracketing constructs: a = f(1, 2) + g(3, 4).\nName your classes and functions consistently; the convention is to use CamelCase for classes and lower_case_with_underscores for functions and methods. Always use self as the name for the first method argument (see A First Look at Classes for more on classes and methods).\nDon’t use fancy encodings if your code is meant to be used in international environments. Python’s default, UTF-8, or even plain ASCII work best in any case.\nLikewise, don’t use non-ASCII characters in identifiers if there is only the slightest chance people speaking a different language will read or maintain the code.\nDocstring An easy way to associate documentation with a function.\nDocumentation Strings conventions The first line should always be a short, concise summary of the object’s purpose. The second line should be blank The following lines should be one or more paragraphs describing the object’s calling conventions, its side effects, etc. The following Python file shows the declaration of docstrings within a Python source file: \u0026#34;\u0026#34;\u0026#34;Assuming this is file mymodule.py, then this string, being thefirst statement in the file, will become the \u0026#34;mymodule\u0026#34; module\u0026#39;sdocstring when the file is imported.\u0026#34;\u0026#34;\u0026#34; class MyClass(object): \u0026#34;\u0026#34;\u0026#34;The class\u0026#39;s docstring\u0026#34;\u0026#34;\u0026#34; def my_method(self): \u0026#34;\u0026#34;\u0026#34;The method\u0026#39;s docstring\u0026#34;\u0026#34;\u0026#34; def my_function(): \u0026#34;\u0026#34;\u0026#34;The function\u0026#39;s docstring\u0026#34;\u0026#34;\u0026#34; The following is an interactive session showing how the docstrings may be accessed: \u0026gt;\u0026gt;\u0026gt; import mymodule\u0026gt;\u0026gt;\u0026gt; help(mymodule) Assuming this is file mymodule.py then this string, being thefirst statement in the file will become the mymodule modulesdocstring when the file is imported \u0026gt;\u0026gt;\u0026gt; help(mymodule.MyClass)The class\u0026#39;s docstring\u0026gt;\u0026gt;\u0026gt; help(mymodule.MyClass.my_method)The method\u0026#39;s docstring\u0026gt;\u0026gt;\u0026gt; help(mymodule.my_function)The function\u0026#39;s docstring\u0026gt;\u0026gt;\u0026gt; Test It is important that each part of the specification gets turned into a piece of code that implements it and a test that tests it.\nExtreme values assert Insert debugging assertions into a program. Assertions are not a substitute for unit tests or system tests, but rather a complement.\nUsing Assertions Effectively Places to consider putting assertions: checking parameter types, classes, or values checking data structure invariants checking \u0026ldquo;can\u0026rsquo;t happen\u0026rdquo; situations (duplicates in a list, contradictory state variables.) after calling a function, to make sure that its return is reasonable Time Track which part of the code is the bottle neck of efficiency\n\u0026gt;\u0026gt; python -m cProfile file.py import cProfile, cProfile.run('test()') Aspect-oriented programming correct efficiency Tracking time: to find out the bottle neck function or algorithm Rethinking the implementation of the bottle neck Fewer * Easier/smaller: Divide and Conquer debugging Each part is done with some line of codes. Instead of mix different part of the code together, it would be better to define them as different function/class. Try to seperate them as much as possible. Function There are many special and useful function implementation and control flow in python: lambda, map, filter, reduce, generator, etc..\nLambda λ, istead of defining function with def and a specific function name, Lambda provide a convinent way to define a function using its own native logic and methematical expression. The benifits are • A small function could be defined wihtin the same code structure without seperating out a specific def function • Without bothering creating any proper funciton name for a small anonymous function.\nLambda implementation 1, Like nested function definitions, lambda functions can reference variables from the containing scope, returning a function from another function. This is often used to create function wrappers, such as Python\u0026rsquo;s decorators.\n# uses a lambda expression to return a function \u0026gt;\u0026gt;\u0026gt; def make_incrementor(n): ... return lambda x: x + n ... \u0026gt;\u0026gt;\u0026gt; f = make_incrementor(42) # f is declared as a lambda function \u0026#34;lambda x: x+42\u0026#34; with parameter n = 42 \u0026gt;\u0026gt;\u0026gt; f(0) # call f with x=0 to return the 42 \u0026gt;\u0026gt;\u0026gt; f(1) 43 This is like creating a compiler to save process cost: some parameters like default values or initial values are compiled into the compiler, program process these parameter only once, then this compiler as a function could be called many times with other input parameters which varies every time the compiler is being called(like user input values).\n2, Pass a small function as an argument, sorting or max by an alternate key\n\u0026gt;\u0026gt;\u0026gt; pairs = [(1, \u0026#39;one\u0026#39;), (2, \u0026#39;two\u0026#39;), (3, \u0026#39;three\u0026#39;), (4, \u0026#39;four\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; pairs.sort(key=lambda pair: pair[1]) \u0026gt;\u0026gt;\u0026gt; pairs [(4, \u0026#39;four\u0026#39;), (1, \u0026#39;one\u0026#39;), (3, \u0026#39;three\u0026#39;), (2, \u0026#39;two\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; l =[(\u0026#39;x\u0026#39;,2),(\u0026#39;y\u0026#39;,4),(\u0026#39;z\u0026#39;,0)] \u0026gt;\u0026gt;\u0026gt; max(l, key = lambda x: x[0]) \u0026gt;\u0026gt;\u0026gt; (\u0026#39;z\u0026#39;, 0) Lambda with logic control flow\nLambda x,y: False if x\u0026lt;y else x+y Filter Construct a list from the elements of an iterable for which function returns true. If iterable is a string or a tuple, the result also has that type; otherwise it is always a list.\nfilter(function, iterable) equals to if function is None: [item for item in iterable if item] if not: [item for item in iterable if function(item)] mult3 = filter(lambda x: x % 3 == 0, [1, 2, 3, 4, 5, 6, 7, 8, 9]) \u0026raquo;\u0026gt; [3, 6, 9] See itertools.ifilter() and itertools.ifilterfalse() for iterator versions of this function, including a variation that filters for elements where the function returns false. Map Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel\n\u0026gt;\u0026gt;\u0026gt; map(lambda x: x % 2, [1, 2, 3, 4, 5, 6, 7, 8, 9]) \u0026gt;\u0026gt;\u0026gt; [1, 0, 1, 0, 1, 0, 1, 0, 1] Reduce Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value.\nIn [1]: reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) Out[1]: 15 # ((((1+2)+3)+4)+5) In [1]: reduce(lambda a, b: \u0026#39;{}, {}\u0026#39;.format(a, b), [1, 2, 3, 4, 5, 6, 7, 8, 9]) Out[1]: \u0026#39;1, 2, 3, 4, 5, 6, 7, 8, 9\u0026#39; List/Dict/Set Comprehensions List comprehensions: [ s for r, s in cards if r in 'JQK' ] Dictionary comprehensions: {x: x ** 2 for x in range(5) if x % 2 == 0} Set comprehensions: {int(sqrt(x)) for x in range(30)} And in general, we can have any number of for statements, if statements, more for statements, more if statements. The whole is read from left to right Generator Expressions Unlike the for loop in the list comprehensions which walk through the whole loop, generator will walk one step in the for loop if a next() is called.\nThe advantage is less indentation stop the loop early easier to edit Implementation of generator: g = (sq(x) for x in range(10) if x%2 == 0). The generator function is a promise, but no computation has been done yet. next(g) to call a one-time calculation. When reaching the end of for-loop in the generator, the next(g) comment will return a false called \u0026ldquo;StopIteration\u0026rdquo;. To avoid the \u0026ldquo;StopIteration\u0026rdquo; false Use a outer for statement: for xx in g: ... convert the generator to list: list(g) Generator functions Using a yield expression in a function definition is sufficient to cause that definition to create a generator function instead of a normal function.\nYield expressions The yield expression is only used when defining a generator function, and can only be used in the body of a function definition. Yield implementation def ints(start, end=None): i = start while i \u0026lt;= end or end is None: yield i i = i + 1 def fab(max): n, a, b = 0, 0, 1 while n \u0026lt; max: yield b # print b a, b = b, a + b n = n + 1 Iterator The true beneath For Statemet is iterable\nfor x in itmes: print x What the whole truth is:\nit = iter(items) try: while True: x = next(it) print x except StopIteration: pass Overall, Python calls the thing that can be iterated over in a for loop an iterable. Strings and lists are examples of iterables, and so are generators.\nitertools library - Functions creating iterators for efficient looping. any(iterable): Return True if any element of the iterable is true. If the iterable is empty, return False.\nUnpacking Argument Lists The * operator simply unpacks the tuple or list and passes them as the positional arguments to the function.\n\u0026gt;\u0026gt;\u0026gt; list(range(3, 6)) # normal call with separate arguments [3, 4, 5] \u0026gt;\u0026gt;\u0026gt; args = [3, 6] \u0026gt;\u0026gt;\u0026gt; list(range(*args)) # call with arguments unpacked from a list [3, 4, 5] Handling different types of argument (*polymorphism) An argument could be different type: timedcalls(n,fn), if n is int isinstance(n,int), it means controling the how many times fn was called, while n is float, it means controling the total runtime of fn called\neval() Decorator Motivation: when applying a transformation to a function def f(self): ...definition...; f = dec(f), it becomes less readable with longer methods. It also seems less than pythonic to name the function three times for what is conceptually a single declaration.\nThe solution is to place the decoration in the function\u0026rsquo;s declaration:\n@dec def foo(cls): pass @property property(fget=None, fset=None, fdel=None, doc=None) A property object has three methods, getter(), setter(), and delete() to specify fget, fset and fdel at a later point.\nsome_object = property(get_some_object,set_some_object) equals to\nsome_object = property() # make empty property some_object = some_object.getter(get_some_object) # assign fget some_object = some_object.setter(set_some_object) # assign fset Decorator as tools • Debug tool: help developping, count calls times, count excecute time • Performance: make the programme faster, such as dynamic programming algorithm • Expressiveness: doc string, explaining funciton • Trace: help to monitor the execution of the program, such as each level result printed with different indentation\nDisable decorator: dec = disabled, make the decorator disabled.\nRegular Expression import re\nReference: A Regular Expression Matcher\nIn C language, any number start with \u0026lsquo;0\u0026rsquo; is interpreted as an octal number( base-8 number system ): \u0026lsquo;012\u0026rsquo; -\u0026gt; int 10; \u0026lsquo;09\u0026rsquo; -\u0026gt; invalid\nSpecial characters • * match 0 or more repetitions of the preceding character. ab* will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s. • ? Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. ab? will match either ‘a’ or ‘ab’. • . (Dot) matches any single character • ^ (Caret) Matches the start of the string • $ Matches the end of the string or just before the newline at the end of the string, foo matches both ‘foo’ and ‘foobar’, while the regular expression foo$ matches only ‘foo’ • + match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.\nCommonly used expression • Upper case letter '[A-Z]' • Any alphanumeric character [a-zA-Z0-9_] • Decimal digit [0-9] • Non-digit character [^0-9] • Whitespace character [ \\t\\n\\r\\f\\v]\nsearch(string[, pos[, endpos]]): Scan through string looking for a location where this regular expression produces a match, and return a corresponding MatchObject instance. Return None if no position in the string matches the pattern.\nre.findall(pattern, string, flags=0)：Return all non-overlapping matches of pattern in string, as a list of strings.\nString Formatting Modulo(%): String and Unicode objects have one unique built-in operation: the % operator (modulo). This is also known as the string formatting or interpolation operator. Given format % values (where format is a string or Unicode object), % conversion specifications in format are replaced with zero or more elements of values. %d:\tSigned integer decimal. %s:\tString (converts any python object using str()). print '%d: %s' % (1, 'animal') \u0026raquo; 1: animal\nPython data structure Numpy indexing Ellipsis: The same as .... Special value used mostly in conjunction with extended slicing syntax for user-defined container data types. a = [1,2,3], a[...] is actually the same as a\nNone: extends one more demention by further slicing the corresponding c into smallest units.\nt = np.arange(27).reshape(3,3,3), #t shape is (3,3,3) t[None,].shape # (1, 3, 3, 3) t[...,None].shape # (3, 3, 3, 1) t[:, None,:].shape # (3, 1, 3, 3) t[:,:, None].shape # (3, 3, 1, 3) Reference • CS212 Design of Computer Program @Udacity, Course Wiki\nSyllabus Lesson 1: How to think to solve problem Lesson 2: Python features; Instrumentation Lesson 3: Build function as tools; Define language; Grammar Lesson 4: Dealing with Complexity Through Search Lesson 5: Dealing with Uncertainty Through Probability\n• The Python Tutorial • Open Book Project: How to Think Like a Computer Scientist: Learning with Python\n","permalink":"https://congchan.github.io/posts/python-digest/","summary":"\u003cp\u003eWhat you will get from this Python digest:\n1, Learn advanced python programming.\n2, Learn new concepts, patterns, and methods that will expand your programming abilities, helping move you from a novice to an expert programmer.\n3, Practice going from a problem description to a solution, using a series of assignments.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"operator\"\u003e\u003ca href=\"https://docs.python.org/2/library/operator.html\"\u003eOperator\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"emulating-numeric-types\"\u003eEmulating numeric types\u003c/h3\u003e\n\u003cp\u003eIn-place operation: One modifies the data-structure itself\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eobject.__iadd__(self, other)\nobject.__isub__(self, other)\nobject.__imul__(self, other)\nobject.__imatmul__(self, other)\nobject.__itruediv__(self, other)\nobject.__ifloordiv__(self, other)\nobject.__imod__(self, other)\nobject.__ipow__(self, other[, modulo])\nobject.__ilshift__(self, other)\nobject.__irshift__(self, other)\nobject.__iand__(self, other)\nobject.__ixor__(self, other)¶\nobject.__ior__(self, other)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThese methods are called to implement the augmented arithmetic assignments. These methods should attempt to do the operation in-place (modifying self) and return the result (which could be, but does not have to be, self).\nIf x is an instance of a class with an \u003ccode\u003e__iadd__()\u003c/code\u003e method, \u003ccode\u003ex += y\u003c/code\u003e is equivalent to \u003ccode\u003ex = operator.iadd(x, y)\u003c/code\u003e\u003c/p\u003e","title":"Python Digest"},{"content":" CSAPP 非常巧妙的把程序设计及优化、数字电路基础、指令集体系、汇编语言、存储器体系结构、链接与装载、进程、虚存等来自不同学科的核心知识点和在一起，并以程序员的视角呈现; 告诉我们作为一个程序员，究竟需要对计算机的硬件了解到什么程度？\n本笔记是 CMU CSAPP 的学习笔记, 使用 CMU 15-213, UW CSE351 的课程视频, lab, 作业, project 辅助练习.\nComputer Systems: A Programmer\u0026rsquo;s Perspective (csapp), 豆瓣-深入理解计算机系统 卡内基梅隆大学 CMU 15-213 Introduction to Computer Systems (ICS) 华盛顿大学 UW CSE351: The Hardware/Software Interface 信息的表达与操作 Information is Bits + Context. Study systems by tracing the lifetime of the hello program, from the time it is created by a programmer, until it runs on a system, prints its simple message, and terminates.\n#include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello, world\\n\u0026#34;); } The source program is a sequence of bits, each with a value of 0 or 1, organized in 8-bit chunks(bytes). Each byte represents some text character in the program.\nAll information in a system — including disk files, programs stored in memory, user data stored in memory, and data transferred across a network—is represented as a bunch of bits.\n整数加减乘位移 Most machines shift and add faster than multiply, compiler translate multiply to shift and add automatically.\nPower-of-2 Multiply with Shift: u \u0026lt;\u0026lt; k gives $u \\times 2^k$, u * 24 = u * 32 - u * 8 = (u \u0026lt;\u0026lt; 5) - (u \u0026lt;\u0026lt; 3)\nSigned Power-of-2 Divide with Shift: x \u0026gt;\u0026gt; k using arithmetic shift(补1) gives $x / 2^k$, when u \u0026lt; 0, say y = -15213 = b[11000100 10010011], y \u0026gt;\u0026gt; 1 = b[11100010 01001001] = -7607, the rounding is downward, which is not the same as the convention toward zero.\nCorrect Power-of-2 Divide by adding bias: $x / 2^k$, computed as $(x + 2^k - 1) / 2^k$, in C (x\u0026lt;0 ? x+(1\u0026lt;\u0026lt;k)-1 : x) \u0026gt;\u0026gt; k\n浮点数 IEEE floating-point standard represents a number in a form $V = (−1)^s × M × 2^E$\nsign s: determines whether the number is negative (s = 1) or positive (s = 0) exponent E weights the value by a (possibly negative) power of 2, encoded by the k-bit exponent field exp. significand M: a fractional binary number that ranges either [1, 2 − ϵ) or [0, 1 − ϵ), encoded by the n-bit fraction field frac Case 1: Normalized Values E is interpreted as representing a signed integer in biased form, E = e − Bias, where e is the unsigned number, Bias equals to $2^{k−1}−1$. The significand is defined to be M = 1 + f, where f is the fraction field, 0 ≤ f \u0026lt; 1, M = $1.f_{n−1}f_{n−2}...f_0$\nCase 2: Denormalized Values Exponent field is all zeros, the exponent value is E = 1 − Bias, and the significand value is M = f, M = $0.f_{n−1}f_{n−2}...f_0$ Denormalized numbers serve two purposes.\nprovide a way to represent numeric value 0, +0.0, bit pattern all zeros, s = M = f = 0 −0.0, bit pattern all zeros except s = 1. represent numbers that are very close to 0.0: possible numeric values are spaced evenly near 0.0 Case 3: Special Values Exponent field is all ones.\nWhen the fraction field is all zeros, the resulting values represent infinity, either +∞ when s = 0, or −∞ when s = 1. Infinity can represent results that overflow When the fraction field is nonzero, the resulting value is NaN Programs are traslated by other programs into different forms The hello program begins as a high-level C program because it can be read and understood by human beings in that form. However, in order to run hello.c on the system, the individual C statements must be translated by other programs into a sequence of low-level machine-language instructions.\nThese instructions are then packaged in a form called an executable object program and stored as a binary disk file. Object programs are also referred to as executable object files.\nThe programs that perform the four phases (preprocessor, compiler, assembler, and linker) are known collectively as the compilation system.\nPreprocessing phase.The preprocessor (cpp) modifies the original C program according to directives that begin with the # character. Compilation phase. The compiler (cc1) translates the text file hello.i into the text file hello.s, which contains an assembly-language program. Assembly language is useful because it provides a common output language for different compilers for different high-level languages. Assembly phase. Next, the assembler (as) translates hello.s into machinelanguage instructions, packages them in a form known as a relocatable object program, and stores the result in the object file hello.o. The hello.o file is a binary file whose bytes encode machine language instructions rather than characters. Linking phase. The printf function resides in a separate precompiled object file called printf.o, which must somehow be merged with our hello.o program. The linker (ld) handles this merging. 大多数 JVM 将内存区域划分为 Method Area（Non-Heap）（方法区）, Heap（堆）, Program Counter Register（程序计数器）, VM Stack（虚拟机栈/JAVA方法栈）, Native Method Stack（ 本地方法栈 ），其中Method Area和Heap是线程共享的，VM Stack，Native Method Stack和Program Counter Register是非线程共享的。\n程序计数器是一个比较小的内存区域，用于指示当前线程所执行的字节码执行到了第几行，是线程隔离的 虚拟机栈描述的是Java方法执行的内存模型，用于存储局部变量，操作数栈，动态链接，方法出口等信息，是线程隔离的 原则上讲，所有的对象都在堆区上分配内存，是线程之间共享的 方法区域存放了所加载的类的信息（名称、修饰符等）、类中的静态变量、类中定义为final类型的常量、类中的Field信息、类中的方法信息，当开发人员在程序中通过Class对象中的getName、isInterface等方法来获取信息时，这些数据都来源于方法区域，同时方法区域也是全局共享的，在一定的条件下它也会被GC，当方法区域需要使用的内存超过其允许的大小时，会抛出OutOfMemory的错误信息 一个一般性的 Java 程序工作过程：\n一个 Java 源程序文件，会被编译为字节码文件（以 class 为扩展名），每个java程序都需要运行在自己的JVM上，然后告知 JVM 程序的运行入口，再被 JVM 通过字节码解释器加载运行。 程序开始运行后，开始涉及各内存区域： JVM初始运行的时候都会分配好 Method Area（方法区）和Heap（堆）， 而JVM每遇到一个线程，就为其分配一个Program Counter Register（程序计数器）, VM Stack（虚拟机栈）和Native Method Stack（本地方法栈），当线程终止时，三者所占用的内存空间也会被释放掉。 这也是为什么把内存区域分为线程共享和非线程共享的原因，非线程共享的那三个区域的生命周期与所属线程相同，而线程共享的区域与JAVA程序运行的生命周期相同，所以这也是系统垃圾回收的场所只发生在线程共享的区域（实际上对大部分虚拟机来说知发生在Heap上）的原因。\nProcessors read and interpret instructions stored in memory The hello.c source program has been translated by the compilation system into an executable object file called hello that is stored on disk, to run the executable file on Unix:\nunix\u0026gt; ./hello hello, world unix\u0026gt; The shell is a command-line interpreter that prints a prompt, waits for you to type a command line, and then performs the command.\nHardware organization of a systems Hardware organization of a typical system.\nBuses Electrical conduits that carry bytes of information back and forth between the components. Buses are typically designed to transfer fixed-sized chunks of bytes known as words. USB: Universal Serial bus.\nInput/output (I/O) devices The system’s connection to the external world. Each I/O deviceisconnected to the I/O bus by either a controller or an adapter：\nControllers are chip sets in the device itself or on the system’s main printed circuit board (often called the motherboard). An adapter is a card that plugs into a slot on the motherboard. Main Memory A temporary storage device that holds both a program and the data it manipulates while the processor is executing the program.\nPhysically, main memory consists of a collection of dynamic random access memory (DRAM) chips. Logically, memory is organized as a linear array of bytes, each with its own unique address (array index) starting at zero Processor: Central Processing Unit (CPU) PC: Program counter, a word-sized storage device (or register) at CPU core. At any point in time, the PC points at (contains the address of) some machine-language instruction in main memory. Register: a quickly accessible location available to CPU, Register file: an array of registers, each with its own unique name. Arithmetic/logic unit: ALU computes new data and address values. A processor repeatedly executes the instruction pointed at by the program counter and updates the program counter to point to the next instruction. The processor reads the instruction from memory pointed at by the PC, interprets the bits in the instruction, performs some simple operation dictated by the instruction, and then updates the PC to point to the next instruction.\nCPU operations examples Load: Copy a byte or a word from main memory into a register, overwriting the previous contents of the register.\nStore(write): Copy a byte or a word from a register to a location in main memory, overwriting the previous contents of that location.\nOperate: Copy the contents of two registers to the ALU, perform an arithmetic operation on the two words, and store the result in a register, overwriting the previous contents of that register.\nJump: Extract a word from the instruction itself and copy that word into the program counter (PC), overwriting the previous value of the PC.\nBranch greater than (BGT): compares two registers and decides whether to branch (target would be the address to branch to), i.e. it is implementing the \u0026ldquo;if\u0026rdquo; decision.\nRunning a programs Initially, the shell program is waiting for user types a command. As we type the characters “./hello” at the keyboard, the shell program reads each one into a register, and then stores it in memory. When we hit the enter key on the keyboard, the shell knows that we have finished typing the command. The shell then loads the executable hello file by executing a sequence of instructions that copies the code and data in the hello object file from disk to main memory. The data include the string of characters “hello, world\\n” that will eventually be printed out. Using a technique known as direct memory access (DMA), the data travels directly from disk to main memory, without passing through the processor. Once the code and data in the hello object file are loaded into memory, the processor begins executing the machine-language instructions in the hello program’s main routine. These instructions copy the bytes in the hello, world\\n string from memory to the register file, and from there to the display device, where they are displayed on the screen. Caches An important lesson from this simple example is that a system spends a lot of time moving information from one place to another. From a programmer’s perspective, much of this copying is overhead that slows down the “real work” of the program. Because of physical laws, larger storage devices are slower than smaller storage devices. Speed that processor read from: register \u0026gt; memory \u0026gt; disk.\nIt is easier and cheaper to make processors run faster than it is to make main memory run faster. To deal with the processor-memory gap, system designers include smaller faster storage devices called cache memories (or simply caches) that serve as temporary staging areas for information that the processor is likely to need in the near future.\nThe L1 and L2 caches are implemented with a hardware technology known as static random access memory (SRAM). Newer and more powerful systems even have three levels of cache: L1, L2, and L3.\nBy setting up caches to hold data that is likely to be accessed often, we can perform most memory operations using the fast caches.\nStorage Devices Form a Hierarchy Operating system The operating system has two primary purposes: (1) to protect the hardware from misuse by runaway applications, and (2) to provide applications with simple and uniform mechanisms for manipulating complicated and often wildly different low-level hardware devices.\nThink of the operating system as a layer of software interposed between the application program and the hardware, with fundamental abstractions: processes, virtual memory, and files. Process进程 A process is the operating system’s abstraction for a running program. Multiple processes can run concurrently on the same system by having the processor switch (context switching) among them, and each process appears to have exclusive use of the hardware.\nThe os keeps track of all the state information that the process needs in order to run. This state, i.e. the context, includes information such as the current values of the PC, the register file, and the contents of main memory.\nWhen the operating system decides to transfer control from the current process to some new process, it performs a context switch by saving the context of the current process, restoring the context of the new process, and then passing control to the new process. The new process picks up exactly where it left off. Virtual Memory Virtual memory is an abstraction that provides each process with the illusion that it has exclusive use of the main memory. Each process has the same uniform view of memory, which is known as its virtual address space.\nIn Linux, the topmost region of the address space is reserved for code and data in the operating system that is common to all processes. The lower region of the address space holds the code and data defined by the user’s process. Starting with the lowest addresses and working our way up:\nProgram code and data: Fixed in size once the process begins running. The code and data areas are initialized directly from the contents of an executable object file, in our case the hello executable. Run-time heap: expands and contracts dynamically at run time as a result of calls to C standard library routines such as malloc and free. Shared libraries: holds the code and data for shared libraries such as the C standard library and the math library. User stack: the compiler uses to implement function calls. Each time we call a function, the stack grows. Each time we return from a function, it contracts. Kernel virtual memory: The kernel is the part of the operating system that is always resident in memory. Application programs are not allowed to read or write the contents of the top region of the address space (which is reserved for the kernel) or to directly call functions defined in the kernel code. Thread线程 In computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system.\nIn most cases a thread is a component of a process. Multiple threads can exist within one process, executing concurrently and sharing resources such as memory, while different processes do not share these resources.\nThreads are an increasingly important programming model because of the requirement for concurrency in network servers, because it is easier to share data between multiple threads than between multiple processes, and because threads are typically more efficient than processes.\nFiles A file is a sequence of bytes. Every I/O device, including disks, keyboards, displays, and even networks, is modeled as a file. All input and output in the system is performed by reading and writing files, using a small set of system calls known as Unix I/O.\nConcurrency and Parallelism Concurrency: general concept of a system with multiple, simultaneous activities. Parallelism: the use of concurrency to make a system run faster.\nParallelism could be achieved in different levels of abstraction in computer system. There are three common levels (from the highest to the lowest level in the system hierarchy):\nThread-Level Concurrency Building on the process abstraction, we are able to devise systems where multiple programs execute at the same time, leading to concurrency. With threads, we can even have multiple control flows executing within a single process.\nWhen we construct a system consisting of multiple processors all under the control of a single operating system kernel, we have a multiprocessor system\nMulti-core processors: Several CPUs (referred to as “cores”) integrated onto a single integrated-circuit chip.\nHyperthreading: Sometimes called simultaneous multi-threading, is a technique that allows a single CPU to execute multiple flows of control.\ninstruction-level parallelism At a much lower level of abstraction, modern processors can execute multiple instructions at one time.\nSingle-Instruction, Multiple-Data (SIMD) Parallelism At the lowest level, special hardware that allows a single instruction to cause multiple operations to be performed in parallel.\nMemory, Data, \u0026amp; Addressing 十进制，2进制，16进制:\nA single byte consists of 8 bits. 二进制 value ranges from 000000002 to 111111112, 十进制 value ranges from 010 to 25510 二进制表示法过于冗长，而使用十进制表示法，与bits进行模式转换非常繁琐。 十六进制，hexadecimal numbers: Hexadecimal (or simply “hex”) uses digits ‘0’ through ‘9’ along with characters ‘A’ through ‘F’ to represent 16 possible values. Values range from 0016 to FF16. 内存：\nA machine-level program views memory as a very large array of bytes, referred to as virtual memory. Every byte of memory is identified by a unique number, known as its address. The set of all possible addresses is known as the virtual address space - 进程可用的虚拟地址范围称为该进程的“虚拟地址空间”。 这个虚拟地址空间只是一个呈现给机器级程序的虚拟概念。实际的实现需要用到随机访问存储器（RAM），磁盘存储，特殊的硬件和操作系统软件的组合来构建相对于程序而言的单片字节数组。\nAddress and Pointers 地址是内存的位置，指针是一种包含地址的数据对象。\nByte ordering: Endianness\nlittle endian - where the least significant byte comes first, followed by most Intel-compatible machines. big endian - where the most significant byte comes first, followed by most machines from IBM and Sun Microsystems Many recent microprocessors are bi-endian, meaning that they can be configured to operate as either little- or big-endian machines. Integer and floating point numbers 把多个bits组合起来，通过解码，可以表达有限集合内的所有元素。比如二进制数字系统可以表示正整数。\nThree most important representations of numbers.\nUnsigned encodings：based on traditional binary notation, representing numbers greater than or equal to 0. Two’s-complement encodings: the most common way to represent signed integers, that is, numbers that may be either positive or negative. Floating-point encodings: base-two version of scientific notation for representing real numbers. C Methods Naming data types with typedef: C的typedef声明用于给数据类型命名。这对提高代码可读性有很大的帮助，因为深层嵌套类型声明可能难以解读。 typedef int *int_pointer; int_pointer ip; 等同于int *ip; Formatted printing with printf(fprintf and sprintf): provides a way to print information with considerable control over the formatting details. The first argument is a format string. Each character sequence starting with ‘%’ indicates how to format the next argument. %d - 输出十进制整数, %f - 浮点数, %c - 字符 while any remaining arguments are values to be printed. sizeof(T) returns the number of bytes required to store an object of type T void *malloc(size_t size)分配请求的内存(size in bytes)并返回一个指向它的指针(如果请求失败，则返回NULL)。 Addresses and pointer in C 指针是C的核心功能，可以引用数据结构元素（包括数组）。就像变量一样，指针有两个组成部分：值和类型。该值指示某个对象的位置，而其类型指示该位置处存储什么类型的对象（例如，整数或浮点数）。\n\u0026amp; - “address of\u0026quot;, return a pointer; Variable declarations： int x, find location in memory in which to store integer. Pointer declarations use *: int *pointer, declares a variable pointer that is a pointer pointing to an object of type integer. Assignment to a pointer: pointer = \u0026amp;x, assigns pointer to point to the address where x is stored. To use the value pointed to by a pointer, use *: if pointer = \u0026amp;x, then x = *pointer +1 is the same as x = x + 1 假如x是一个对象, 那么*(\u0026amp;x)=*\u0026amp;x = x Pointers and arrays C通过数组将标量数据聚合为更大的数据类型。In C, we can dereference a pointer with array notation, and we can reference array elements with pointer notation. C有一个不常见的特性, 就是我们可以生成指向数组内的元素的指针，并使用这些指针来执行算术运算。\nT A[N]; 首先，它在内存中分配一个L*N大小的连续区域, 其中L是数据类型T的大小（以bytes为单位）. 数组的元素可以使用 0 ~ N-1 之间的整数索引来访问 A[i];\n其次，它引入了一个标识符A，可以作为指向数组开头的指针;\n在指针上进行算术运算时，其实际的索引值会根据指针引用的数据类型的大小进行缩放, 即假设A的值是xa, 那么A+i的值就是xa + L * i, A[i] = *(A+i);\n其他 #define 指令允许在源代码中定义宏 macro。这些宏定义允许在整个代码中声明常量值。 宏定义不是变量，不能像变量那样通过程序代码进行更改。创建表示数字，字符串或表达式的常量时，通常使用此语法。\n定义常数：#define CNAME value or #define CNAME (expression)。CNAME是常数的名称。大多数C程序员用大写字母来定义常量名，但这不是C语言的要求。expression就是被分配给常量的表达式。如果表达式包含运算符，则该表达式必须括在括号内。\n","permalink":"https://congchan.github.io/posts/computer-systems-a-programmers-perspective-csapp-cmu-15213/","summary":"\u003cblockquote\u003e\n\u003cp\u003eCSAPP 非常巧妙的把程序设计及优化、数字电路基础、指令集体系、汇编语言、存储器体系结构、链接与装载、进程、虚存等来自不同学科的核心知识点和在一起，并以程序员的视角呈现; 告诉我们作为一个程序员，究竟需要对计算机的硬件了解到什么程度？\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e本笔记是 CMU CSAPP 的学习笔记, 使用 CMU 15-213, UW CSE351 的课程视频, lab, 作业, project 辅助练习.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"http://csapp.cs.cmu.edu/\"\u003eComputer Systems: A Programmer\u0026rsquo;s Perspective (csapp)\u003c/a\u003e, 豆瓣-\u003ca href=\"https://book.douban.com/subject/26912767/\"\u003e深入理解计算机系统\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.cs.cmu.edu/~213/\"\u003e卡内基梅隆大学 CMU 15-213 Introduction to Computer Systems (ICS)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://courses.cs.washington.edu/courses/cse351/\"\u003e华盛顿大学 UW CSE351: The Hardware/Software Interface\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"信息的表达与操作\"\u003e信息的表达与操作\u003c/h2\u003e\n\u003cp\u003eInformation is Bits + Context. Study systems by tracing the lifetime of the hello program, from the time it is created by a programmer, until it runs on a system, prints its simple message, and terminates.\u003c/p\u003e","title":"Computer Systems - A Programmer's Perspective (CSAPP) - CMU 15213"},{"content":"文本分类 文本分类是很多业务问题中广泛使用到的NLP/监督机器学习（ML）。文本分类的目标是自动将文本/文档分类为一个或多个预定义类别。目前的成熟思路是用词向量解码文本，然后使用传统机器学习模型或者深度神经网络模型来做分类。\n文本分类是学术界和工业界非常活跃的研究领域。本文主要介绍用于文本分类的几种神经网络模型方法，并比较它们的性能，代码实现主要基于Keras。文中代码都在这个DeepTextGitHub项目中.\n文本分类的一些示例包括：\n从社交媒体中了解受众情绪（😁 😐 😥） 检测垃圾邮件和非垃圾邮件 自动标记客户查询 将新闻文章📰分类为预定义主题 端到端文本分类流水线 端到端文本分类流水线由以下组件组成：\n训练文本：输入文本，有监督模型能够通过已标注数据来学习和预测所需的类。 特征向量：特征向量是用于解码输入数据特征的信息的向量。 标签：预定义的类别/类，作为模型预测的目标。 算法模型：能够处理文本分类的算法（在我们的例子中：CNN，RNN，HAN, Fasttext） 预测：已经在历史数据集上训练过的模型，可以用于执行标签预测。 这里使用汽车消费者的评测数据集，在tsv文件中, 第一列是序号对我们没用, 第二列是label(0, 1)，分别代表（消极，积极）评价，第三列是文本.\n1\t操控性舒服、油耗低，性价比高 0\t动力的确有点点让我相信了up的确是个代步车而已! 1\t1。车的外观很喜欢。2。省油，现在磨合期7.3，相信以后还会下降。 1\t内饰的做工和用料同级别同价位最厚道的 0\t减震系统太硬！ 数据处理使用的类，具体见代码链接\nclass DataProcessor(object): \u0026#34;\u0026#34;\u0026#34; Base class for data converters for sequence classification data sets. helper funcitons [read_tsv, read_text, read_json] \u0026#34;\u0026#34;\u0026#34; ... class SampleProcessor(DataProcessor): \u0026#34;\u0026#34;\u0026#34; Sample processor for the classification data set. Tranform the text to tensor for training if use pre-train model, need vocabulary file usage: process data files \u0026gt;\u0026gt;\u0026gt; processer = SampleProcessor(config, ) provide your own data in list format [train_X, train_Y, test_X, test_Y] \u0026gt;\u0026gt;\u0026gt; processer = SampleProcessor(config, data) \u0026#34;\u0026#34;\u0026#34; 词向量 使用包含外部知识的embedding表达字词是目前的主流方法，经典的如word2vec，GLoVe，较新进的 ELMo，BERT，等预训练向量，集成了关于单词的新信息（词汇和语义），这些信息已经在非常大的数据集上进行了训练和提炼。\n在这里的模型，都允许我们直接载入外部的 embedding 参数。\n特别是提供了通过预训练的BERT获取中文单词的向量表达的接口. 最好是使用在自己文本上fine-tune过的预训练BERT模型.\n@staticmethod def load_bert_embedding(vob_size, emb_size, word2id): \u0026#34;\u0026#34;\u0026#34; Get bert pre-trained representation, for example, pre-trained chinese_L-12_H-768_A-12, the hidden_size is 768 \u0026#34;\u0026#34;\u0026#34; ... return rep_matrix 输入你的词汇表, 返回各个词汇对应的向量, 以词典形式返回. 内部的工作机制是把每一个单词都用拼接起来, 之间用BERT的句子分隔符[SEP]隔开. 在返回的token level 的向量中重新pool出各个词汇的表达. 这个方法具体的效果有待验证.\nFasttext文本分类 Fasttext 非常适合处理一些显而易见，不需要推理，情况比较单纯的文本分类问题。它就是一个词袋模型，把文本所有单词的向量pool在一起，得出整个文本的向量表达，这个文本向量使用softmax分类器得出不同标签的概率分布。为了捕捉词之间的顺序，fasttext加入了ngram特征。详细推荐看这两篇文章\nEnriching Word Vectors with Subword Information, P. Bojanowski, E. Grave, A. Joulin, T. Mikolov Bag of Tricks for Efficient Text Classification, A. Joulin, E. Grave, P. Bojanowski, T. Mikolov 代码链接 def fasttext(max_length, emb_size, max_words, class_num, pre_train_emb=None): \u0026#34;\u0026#34;\u0026#34; return single label classification fasttext model paper: Bag of Tricks for Efficient Text Classification The original paper use average pooling. In many Kaggle application, Max Pooling is found to be useful \u0026#34;\u0026#34;\u0026#34; input = Input(shape=(max_length,), dtype=\u0026#39;int32\u0026#39;, name=\u0026#39;input\u0026#39;) embeddings_initializer = \u0026#39;uniform\u0026#39; if pre_train_emb is not None: embeddings_initializer = initializers.Constant(pre_train_emb) embed_input = Embedding(output_dim=emb_size, dtype=\u0026#39;float32\u0026#39;, input_dim=max_words + 1, input_length=max_length, embeddings_initializer=embeddings_initializer, trainable=True )(input) drop_out_input = Dropout(0.5, name=\u0026#39;dropout_layer\u0026#39;)(embed_input) ave_pool = GlobalAveragePooling1D()(drop_out_input) max_pool = GlobalMaxPooling1D()(drop_out_input) concat_pool = concatenate([ave_pool, max_pool]) output = Dense(class_num, activation=\u0026#39;softmax\u0026#39;, name=\u0026#39;output\u0026#39;)(concat_pool) model = Model(inputs=[input], outputs=output) model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model 对于中文文本，如果数据集不是很干净的话（比如有错别字），考虑使用特殊超参的fasttext。一般来说fasttext在英文中的char+ngram的窗口大小一般取值3 ~ 6，但是在处理中文时，为了去除输入中的噪声，那么可以把这个窗口限制为1~2，因为小窗口有利于模型去捕获错别字（错误词一般都是其中的一个字表达成同音异形的另一个字），比如小ngram窗口fasttext学出来的\u0026quot;似乎\u0026quot;近似词很有可能包含\u0026quot;是乎\u0026quot;等内部包含错别字的词，这样等于让fasttext拥有了识别错别字的词的能力。\n卷积神经网络（CNN）文本分类 CNN通常用于计算机视觉，但它们最近已应用于各种NLP任务，结果很有前景。\n简要地说，在文本数据上使用CNN时，当检测到特殊的 pattern，每个卷积的结果都将触发。通过改变内核的大小并连接它们的输出，你可以自己检测多个大小的模式（2, 3或5个相邻的单词）。Patterns 可以是表达式（如 ngrams），因此CNN可以在句子中识别它们而不管它们的位置如何。 参数使用 128 个 filters，大小从1到4。模型架构如图 代码链接\ndef text_cnn(max_length, emb_size, max_words, class_num, pre_train_emb=None): \u0026#34; textCNN model \u0026#34; ... cnn1_1 = Conv1D(128, 1, padding=\u0026#39;same\u0026#39;, strides=1)(drop_out_layer) ... cnn1 = GlobalMaxPooling1D()(cnn1_2_at) cnn2_1 = Conv1D(128, 2, padding=\u0026#39;same\u0026#39;, strides=1)(drop_out_layer) ... cnn2 = GlobalMaxPooling1D()(cnn2_2_at) cnn3_1 = Conv1D(128, 4, padding=\u0026#39;same\u0026#39;, strides=1)(drop_out_layer) ... cnn3 = GlobalMaxPooling1D()(cnn3_2_at) concat_cnn = concatenate([cnn1, cnn2, cnn3], axis=-1) ... return model 用于text的CNN不仅更容易并行化运算，而且很容易成为一个数据集上的很强的baseline（除非这个分类任务很难）。根据数据的情况选择模型，如果ngram特征很重要，使用textCNN，如果文本长距离依赖比较明显，考虑使用RNN。\nRNN用于文本分类 RNN用于文本分类的话，seq2one 架构，把不定长序列解码为定长向量，再把这个输出向量用softmax函数计算出各标签的概率分布。RNN(LSTM/GRU)因为处理长文本的能力较弱，目前一般需要加上注意力机制。这里暂时简单粗暴的用双向GRU来定义核心的encoder.\ndef text_rnn(max_length, emb_size, max_words, class_num, pre_train_emb=None): \u0026#34; Text RNN model using GRU cell\u0026#34; return _bilstm_attention(max_length, emb_size, max_words, class_num, False, pre_train_emb) def text_rnn_attention(max_length, emb_size, max_words, class_num, pre_train_emb=None): \u0026#34; Text RNN model using GRU cell with attention mechanism\u0026#34; return _bilstm_attention(max_length, emb_size, max_words, class_num, True, pre_train_emb) RCNN Hierarchical Attention Network (HAN) Reference Enriching Word Vectors with Subword Information, P. Bojanowski, E. Grave, A. Joulin, T. Mikolov Bag of Tricks for Efficient Text Classification, A. Joulin, E. Grave, P. Bojanowski, T. Mikolov https://arxiv.org/abs/1408.5882 Yoon Kim http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ by Denny Britz. Understanding convolutional neural networks for nlp https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f Hierarchical Attention Networks for Document Classification ","permalink":"https://congchan.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/","summary":"\u003ch2 id=\"文本分类\"\u003e文本分类\u003c/h2\u003e\n\u003cp\u003e文本分类是很多业务问题中广泛使用到的NLP/监督机器学习（ML）。文本分类的目标是自动将文本/文档分类为一个或多个预定义类别。目前的成熟思路是用词向量解码文本，然后使用传统机器学习模型或者深度神经网络模型来做分类。\u003c/p\u003e\n\u003cp\u003e文本分类是学术界和工业界非常活跃的研究领域。本文主要介绍用于文本分类的几种神经网络模型方法，并比较它们的性能，代码实现主要基于Keras。文中代码都在这个\u003ca href=\"https://github.com/congchan/DeepText\"\u003eDeepText\u003c/a\u003eGitHub项目中.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e文本分类的一些示例包括：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e从社交媒体中了解受众情绪（😁 😐 😥）\u003c/li\u003e\n\u003cli\u003e检测垃圾邮件和非垃圾邮件\u003c/li\u003e\n\u003cli\u003e自动标记客户查询\u003c/li\u003e\n\u003cli\u003e将新闻文章📰分类为预定义主题\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"端到端文本分类流水线\"\u003e端到端文本分类流水线\u003c/h2\u003e\n\u003cp\u003e端到端文本分类流水线由以下组件组成：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e训练文本：输入文本，有监督模型能够通过已标注数据来学习和预测所需的类。\u003c/li\u003e\n\u003cli\u003e特征向量：特征向量是用于解码输入数据特征的信息的向量。\u003c/li\u003e\n\u003cli\u003e标签：预定义的类别/类，作为模型预测的目标。\u003c/li\u003e\n\u003cli\u003e算法模型：能够处理文本分类的算法（在我们的例子中：CNN，RNN，HAN, Fasttext）\u003c/li\u003e\n\u003cli\u003e预测：已经在历史数据集上训练过的模型，可以用于执行标签预测。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这里使用汽车消费者的评测数据集，在\u003ccode\u003etsv\u003c/code\u003e文件中, 第一列是序号对我们没用, 第二列是\u003ccode\u003elabel(0, 1)\u003c/code\u003e，分别代表\u003ccode\u003e（消极，积极）\u003c/code\u003e评价，第三列是文本.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e1\t操控性舒服、油耗低，性价比高\n0\t动力的确有点点让我相信了up的确是个代步车而已!\n1\t1。车的外观很喜欢。2。省油，现在磨合期7.3，相信以后还会下降。\n1\t内饰的做工和用料同级别同价位最厚道的\n0\t减震系统太硬！\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e数据处理使用的类，具体见\u003ca href=\"https://github.com/congchan/DeepText/blob/a33fe1b8e895916b26bc658f0a02ac8253291d8a/data_process.py#L29\"\u003e代码链接\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDataProcessor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eobject\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34; Base class for data converters for sequence classification data sets.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        helper funcitons [read_tsv, read_text, read_json]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eSampleProcessor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eDataProcessor\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34; Sample processor for the classification data set.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Tranform the text to tensor for training\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        if use pre-train model, need vocabulary file\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        usage:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e            process data files\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e            \u0026gt;\u0026gt;\u0026gt; processer = SampleProcessor(config, )\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e            provide your own data in list format [train_X, train_Y, test_X, test_Y]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e            \u0026gt;\u0026gt;\u0026gt; processer = SampleProcessor(config, data)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"词向量\"\u003e词向量\u003c/h3\u003e\n\u003cp\u003e使用包含外部知识的embedding表达字词是目前的主流方法，经典的如word2vec，GLoVe，较新进的 ELMo，BERT，等预训练向量，集成了关于单词的新信息（词汇和语义），这些信息已经在非常大的数据集上进行了训练和提炼。\u003c/p\u003e","title":"神经网络用于文本分类"},{"content":"信息抽取 1997年MUC会议（MUC-7） 召开时，评测任务已经增加到5个： ① 场景模板（scenario template, ST）填充：定义了描述场景的模板及槽填充规范； ② 命名实体（named entity, NE）识别：识别出文本中出现的专有名称和有意义的数量短语， 并加以归类； ③ 共指（coreference, CR）关系确定：识别出给定文本中的参照表达（ referring expressions），并确定这些表达之间的共指关系； ④ 模板元素（template element, TE）填充：类似于人名和组织机构名识别，但是要求系统必须识别出实体的描述和名字，如果一个实体在文本中被提到了多次，使用了几种可能的描述和不同的名字形式，要求系统都要把它们识别出来，一个文本中的每个实体只有一个模板元素［Grishman and Sundheim, 1996］； ⑤ 模板关系（template relation, TR）：确定实体之间与特定领域无关的关系。\n1999年起美国NIST组织了自动内容抽取（automatic content extraction, ACE）评测会议，旨在研究和 开发自动内容技术以支持对三种不同来源文本（普通文本、经语音识别后得到的文本、 由OCR识别得到的文本）的自动处理，以实现新闻语料中出现的实体、关系、事件等内容的自动抽取。评测任务设计: 实体检测与跟踪（entity detection and tracking, EDT）、数值检测与识别（value detection and recognition, VDR）、时间识别和规范化（time expression recognition and normalization, TERN）、关系检测与描述（relation detection and characterization, RDC）、事件检测与描述（event detection and characterization, EDC）和实体翻译（entity translation, ET）等。\nTF-IDF 关键词抽取 import jieba.analyse jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) sentence 为待提取的文本 topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20 withWeight 为是否一并返回关键词权重值，默认值为 False allowPOS 仅包括指定词性的词，默认值为空，即不筛选. 如电商评论指定要形容词\n关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径, 用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径 自定义语料库示例见 https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big 用法示例见 https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py\n关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径, 用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径 自定义语料库示例见 https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt 用法示例见 https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py\n关键词一并返回关键词权重值 https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py\nTextRank 论文：TextRank: Bringing Order into Texts.\n将待抽取关键词的文本进行分词 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图 计算图中节点的PageRank，是无向带权图 数据量越大，构建的图越精准 jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 默认过滤词性。jieba.analyse.TextRank() 新建自定义实例\n开放式信息抽取 处理的文本领域不再限定于规范的新闻文本或者某一领域文本，而是不限定领域的网络文本, 不仅需要考虑文本 特征，同时需要综合考虑网页结构特征和用户行为特征等。\n开放式实体抽取 开放式实体抽取关注的是从海量、冗余、不规范的网络数据源上抽取出符合某个语义类的实体列表，侧重于抽取。\n基于这样的假设：同类实体在网络上具有相似的网页结构或者相似的上下文特征。因此可以根据给出的特定语义类的若干实体（“种子”），找出该语义类包含的其他实体，其中特定语义类的标签可能是显式，也可能是隐式给出的。如给出“中国、美国、俄罗斯”这三个实体，要求找出“国家”这个语义类的其他实体诸如“德国、法国、日本”等。\n训练步骤包含两部分：候选实体获取和候选实体置信度计算和排序。\n具体训练过程：通常从种子实体出发，通过分析种子实体在语料中的上下文特征得到模板，根据模板得到更多候选实体，选取置信度高的候选实体作为新种子进行迭代，满足一定条件后停止迭代， 返回 历次置信度高的候选实体作为结果输出。\n抽取比识别在任务上更加底层，实体抽取的结果可以作为列表支撑实体的识别。\n对于中文而言，当不存在网页结构特征时，实体抽取任务变得更加困难，其中一个重要原因来自汉语分词，未知实体往往在分词过程中被分开。\n实体消歧 Entity disambiguation的难点在于指称项多样性（name variation）和指称项歧义（name ambiguity）。 指称项多样性指一个实体概念可以用多种命名性指称项指称，如全称、别称、简称、拼写错误、多语言名称等。\n单语言的实体消歧问题的主要方法：\n实体聚类消歧法：对每一个实体指称项抽取其上下文特征（包括词、实体等），并将其表示成特征向量；然后计算实体指称项之间的相似度；计算基于指称项之间的相似度时，可采用一定聚类算法将其聚类，将每个类看作一个实体概念。这种方法的核心任务是计算实体指称项之间的相似度， 传统的方法是利用上下文的词信息建立词袋模型（bag-of-words, BOW）。 针对人名消歧，采用基于图的算法，利用社会化关系的传递性考虑隐藏的实体关系知识。 利用知识资源，如Wikipedia、Web上的链接信息、命名实体的同现信息、领域特定语料库等，来提升实体消歧的效果。 实体链接消歧法：实体链接（entity linking）也称实体分辨或实体解析（entity resolution），或记录链接（record linkage）。基于实体链接消歧法的目的是解决基于聚类的实体消歧法不能显式地给出实体语义信息的问题，其基本任务是：给定一个实体指称项，将其链接到知识库中的实体概念上。实体链接的核心任务仍是计算实体指称项和候选实体之间的相似度，选择相似度最大的候选实体作为链接的目标实体。 实体消歧仍面临很多难题，包括空目标实体问题（NIL entity problem）（即实体知识库中不包含某指称项的目标实体）、知识库覆盖度有限、来自互联网的知识源可靠性差和知识库使用方法单一（集中于使用单文档特征）等。\n开放式实体关系抽取 实体关系通常采用采用三元组表示：(Arg1, Pred, Arg2)， 其中，Arg1表示实体，Arg2表示实体关系值，通常也是实体，Pred表示关系名称，通常为动词、名词或者名词短语。\n","permalink":"https://congchan.github.io/posts/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/","summary":"\u003ch2 id=\"信息抽取\"\u003e信息抽取\u003c/h2\u003e\n\u003cp\u003e1997年MUC会议（MUC-7） 召开时，评测任务已经增加到5个：\n① 场景模板（scenario template, ST）填充：定义了描述场景的模板及槽填充规范；\n② 命名实体（named entity, NE）识别：识别出文本中出现的专有名称和有意义的数量短语， 并加以归类；\n③ 共指（coreference, CR）关系确定：识别出给定文本中的参照表达（ referring expressions），并确定这些表达之间的共指关系；\n④ 模板元素（template element, TE）填充：类似于人名和组织机构名识别，但是要求系统必须识别出实体的描述和名字，如果一个实体在文本中被提到了多次，使用了几种可能的描述和不同的名字形式，要求系统都要把它们识别出来，一个文本中的每个实体只有一个模板元素［Grishman and Sundheim, 1996］；\n⑤ 模板关系（template relation, TR）：确定实体之间与特定领域无关的关系。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e1999年起美国NIST组织了自动内容抽取（automatic content extraction, ACE）评测会议，旨在研究和\n开发自动内容技术以支持对三种不同来源文本（普通文本、经语音识别后得到的文本、 由OCR识别得到的文本）的自动处理，以实现新闻语料中出现的实体、关系、事件等内容的自动抽取。评测任务设计:\n实体检测与跟踪（entity detection and tracking, EDT）、数值检测与识别（value detection and recognition, VDR）、时间识别和规范化（time expression recognition and normalization, TERN）、关系检测与描述（relation detection and characterization, RDC）、事件检测与描述（event detection and characterization, EDC）和实体翻译（entity translation, ET）等。\u003c/p\u003e\n\u003ch3 id=\"tf-idf-关键词抽取\"\u003eTF-IDF 关键词抽取\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejieba.analyse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ejieba\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eanalyse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eextract_tags\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esentence\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etopK\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e20\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewithWeight\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eallowPOS\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003esentence\u003c/code\u003e 为待提取的文本\n\u003ccode\u003etopK\u003c/code\u003e 为返回几个 TF/IDF 权重最大的关键词，默认值为 \u003ccode\u003e20\u003c/code\u003e\n\u003ccode\u003ewithWeight\u003c/code\u003e 为是否一并返回关键词权重值，默认值为 \u003ccode\u003eFalse\u003c/code\u003e\n\u003ccode\u003eallowPOS\u003c/code\u003e 仅包括指定词性的词，默认值为空，即不筛选. 如电商评论指定要形容词\u003c/p\u003e","title":"信息抽取"},{"content":" Algorithms, Part I, https://online.princeton.edu/course/algorithms-part-i Algorithms, Part II, https://online.princeton.edu/course/algorithms-part-ii Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne https://algs4.cs.princeton.edu/ Union−Find Considering the dynamic connectivity problem, modeling of multiple objects connected in a space/network.\nApplications involve manipulating objects of all types. ・Pixels in a digital photo. ・Computers in a network. ・Friends in a social network. ・Transistors in a computer chip.\nGiven a set of N objects.\nunion(a, b): connect two objects. connected(p, q): is two objects connected? find(p): Find component identifier for p (0 to N – 1) Modeling the objects: array.\nModeling the connections: Maximal set of objects that are mutually connected - Connected components.\nData structure:\nQuick find Integer array id[] of length N, two objects are connected iff they have the same id. Defect: union too expensive, $\\in \\Theta(N^2)$.\nQuick-union Integer array id[] of length N, id[i] is parent of i, root of i is id[id[id[...id[i]...]]] (until it doesn’t change). The find is recursive.\n/** chase parent pointers until reach root * (depth of i array accesses) */ private int find(int i) { while (i != id[i]) i = id[i]; return i; } Defect: Trees can get tall, find too expensive, $\\in \\Theta(N)$.\nWeighted quick-union Modify quick-union to avoid tall trees. Balance by linking root of smaller tree to root of larger tree. Maintain extra array sz[i] to keep track of size of each tree (number of objects). find: time proportional to depth of p and q, the depth of any node x is at most $\\log N$,\nWeighted quick-union with path compression Making all the nodes that examined directly link to its root. Keeps tree almost completely flat.\n/** Make every other node in path point to its grandparent * (thereby halving path length). */ private int root(int i) { while (i != id[i]) { id[i] = id[id[i]]; i = id[i]; } return i; } Amortized analysis: `[Hopcroft-Ulman, Tarjan]` Starting from an empty data structure, any sequence of M union-find ops on N objects makes $≤ c ( N + M \\lg \\ast N )$ array accesses. $\\lg \\ast N$ is [Iterated logarithm](https://en.wikipedia.org/wiki/Iterated_logarithm), for $N = 2^{65536}$, $\\lg \\ast N = 5$. In theory, WQUPC is not quite linear. In practice, WQUPC is linear. Amazing fact. [Fredman-Saks] No linear-time algorithm exists.\nElement Sort Two elementary sorting methods: selection sort and insertion sort. Shellsort is a variation of one of them.\nThe objective is to rearrange the items such that their keys are in ascending order.\nIn Java, the abstract notion of a key is captured by the Comparable interface. The Comparable interface provides an elegant API for callback when Java need to compare keys.\nSome background knowlege:\nCost model, please refer to Asymptotic Analysis Sorting cost model. How many compares and exchanges, or array accesses, for a sorting. Memory. There are sorting algorithms that sort in place (no extra memory except perhaps for a small function-call stack or a constant number of instance variables), and those that need enough extra memory to hold another copy of the array to be sorted. Selection Sort Repeatedly selecting the smallest remaining item:\nFind the smallest item in the array, and exchange it with the first entry. Find the next smallest item and exchange it with the second entry. Continue until the entire array is sorted. Selection sort uses ~$n^2/2$ compares and n exchanges to sort an array of length n.\nInsertion Sort Works like people sort Pokers: consider the cards one at a time, inserting each into its proper place among those already considered (keeping them sorted). In a computer implementation, we need to make space for the current item by moving larger items one position to the right, before inserting the current item into the vacated position.\nFor randomly ordered arrays of length N with distinct keys, insertion sort uses ~$N^2/4$ compares and ~$N^2/4$ exchanges on the average. The worst case is ~ $N^2/2$ compares and ~ $N^2/2$ exchanges and the best case is $N-1$ compares and 0 exchanges.\nInsertion sort works well for certain types of nonrandom arrays that often arise in practice, even if they are huge. An inversion is a pair of keys that are out of order in the array. For instance, E X A M P L E has 11 inversions: E-A, X-A, X-M, X-P, X-L, X-E, M-L, M-E, P-L, P-E, and L-E. If the number of inversions in an array is less than a constant multiple of the array size, we say that the array is partially sorted.\nShellsort Shellsort gains speed by allowing exchanges of entries that are far apart, to produce partially sorted arrays that can be efficiently sorted, eventually by insertion sort.\nThe idea is to rearrange the array to give it the property that taking every $h_{th}$ entry (starting anywhere) yields a sorted sequence. Such an array is said to be h-sorted.By h-sorting for some large values of h, we can move entries in the array long distances and thus make it easier to h-sort for smaller values of h. Using such a procedure for any increment sequence of values of h that ends in 1 will produce a sorted array:\nThe number of compares used by shellsort with the increments 1, 4, 13, 40, 121, 364, \u0026hellip; is O(N^{3/2}).\nMerge Sort Merging means combining two ordered arrays to make one larger ordered array. Merge sort is an utility of divide and conquer paradigm.\nMergesort guarantees to sort an array of N items in time proportional to $N \\log N$, no matter what the input. But it uses extra space proportional to N. Specifically, mergesort uses between $1/2 N \\lg N$ and $N \\lg N$ compares and at most $6 N \\lg N$ array accesses to sort any array of length N.\nAbstract in-place merge: The method merge(a, lo, mid, hi) in Merge.java puts the results of merging the subarrays a[lo..mid] with a[mid+1..hi] into a single ordered array, leaving the result in a[lo..hi]. While it would be desirable to implement this method without using a significant amount of extra space, such solutions are remarkably complicated.\npublic class Merge { private static void merge(Comparable[] a, Comparable[] aux, int lo, int mid, int hi) { assert isSorted(a, lo, mid); // precondition: a[lo..mid] sorted assert isSorted(a, mid+1, hi); // precondition: a[mid+1..hi] sorted for (int k = lo; k \u0026lt;= hi; k++) // copy aux[k] = a[k]; int i = lo, j = mid+1; for (int k = lo; k \u0026lt;= hi; k++) // merge { if (i \u0026gt; mid) a[k] = aux[j++]; else if (j \u0026gt; hi) a[k] = aux[i++]; else if (less(aux[j], aux[i])) a[k] = aux[j++]; else a[k] = aux[i++]; } assert isSorted(a, lo, hi); // postcondition: a[lo..hi] sorted } private static void sort(Comparable[] a, Comparable[] aux, int lo, int hi) { if (hi \u0026lt;= lo) return; int mid = lo + (hi - lo) / 2; sort(a, aux, lo, mid); sort(a, aux, mid+1, hi); merge(a, aux, lo, mid, hi); } public static void sort(Comparable[] a) { aux = new Comparable[a.length]; sort(a, aux, 0, a.length - 1); } } } Proposition. Mergesort uses at most $N lg N$ compares and $6 N lg N$ array accesses to sort any array of size N. Mergesort uses extra space proportional to N\nA sorting algorithm is in-place if it uses $≤ c \\log N$ extra memory. Ex. Insertion sort, selection sort, shellsort.\nMergesort: Practical improvements Use insertion sort for small subarrays (7).\nprivate static void sort(Comparable[] a, Comparable[] aux, int lo, int hi) { if (hi \u0026lt;= lo + CUTOFF - 1) { Insertion.sort(a, lo, hi); return; } int mid = lo + (hi - lo) / 2; sort (a, aux, lo, mid); sort (a, aux, mid+1, hi); merge(a, aux, lo, mid, hi); } Stop if already sorted: Is biggest item in first half ≤ smallest item in second half?\nprivate static void sort(Comparable[] a, Comparable[] aux, int lo, int hi) { if (hi \u0026lt;= lo) return; int mid = lo + (hi - lo) / 2; sort (a, aux, lo, mid); sort (a, aux, mid+1, hi); if (!less(a[mid+1], a[mid])) return; merge(a, aux, lo, mid, hi); } Eliminate the copy to the auxiliary array. Save time (but not space) by switching the role of the input and auxiliary array in each recursive call.\nprivate static void merge(Comparable[] a, Comparable[] aux, int lo, int mid, int hi) { int i = lo, j = mid+1; for (int k = lo; k \u0026lt;= hi; k++) // merge from a[] to aux[] { if (i \u0026gt; mid) aux[k] = a[j++]; else if (j \u0026gt; hi) aux[k] = a[i++]; else if (less(a[j], a[i])) aux[k] = a[j++]; else aux[k] = a[i++]; } } private static void sort(Comparable[] a, Comparable[] aux, int lo, int hi) { if (hi \u0026lt;= lo) return; int mid = lo + (hi - lo) / 2; // switch roles of aux[] and a[] sort (aux, a, lo, mid); sort (aux, a, mid+1, hi); merge(a, aux, lo, mid, hi); } Top-down mergesort A recursive mergesort implementation based on this abstract in-place merge. Bottom-up mergesort Do all the merges of tiny arrays on one pass, then do a second pass to merge those arrays in pairs, and so forth, continuing until we do a merge that encompasses the whole array.\nWe start by doing a pass of 1-by-1 merges then a pass of 2-by-2 merges (merge subarrays of size 2 to make subarrays of size 4), then 4-by-4 merges, and so forth. Proposition: No compare-based sorting algorithm can guarantee to sort N items with fewer than $lg(N!)$ ~ $N \\lg N$ compares. Proposition. Mergesort is an asymptotically optimal compare-based sorting algorithm. That is, both the number of compares used by mergesort in the worst case and the minimum number of compares that any compare-based sorting algorithm can guarantee are ~N lg N.\npublic class MergeBU { private static void merge(...) { /* as before */ } public static void sort(Comparable[] a) { int N = a.length; Comparable[] aux = new Comparable[N]; for (int sz = 1; sz \u0026lt; N; sz = sz+sz) for (int lo = 0; lo \u0026lt; N-sz; lo += sz+sz) merge(a, aux, lo, lo+sz-1, Math.min(lo+sz+sz-1, N-1)); } } About 10% slower than recursive, top-down mergesort on typical systems\nMergesort Applications Java sort for objects. Perl, C++ stable sort, Python stable sort, Firefox JavaScript, \u0026hellip;\nCounting inversions: An inversion in an array a[] is a pair of entries a[i] and a[j] such that i \u0026lt; j but a[i] \u0026gt; a[j]. Given an array, design a linearithmic algorithm to count the number of inversion.\n在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007\npublic int InversePairs(int [] array) { int len = array.length; int[] aux = new int[len]; for (int k = 0; k \u0026lt; len; k++) // copy aux[k] = array[k]; return InversePairsSort(array, aux, 0, len - 1); } private int InversePairsSort(int[] a, int[] b, int s, int e) { if (e \u0026lt;= s) return 0; int mid = (e + s) / 2; int n1 = InversePairsSort(b, a, s, mid) % 1000000007; int n2 = InversePairsSort(b, a, mid + 1, e) % 1000000007; return (n1 + n2 + InversePairsMerge(a, b, s, mid, e)) % 1000000007; } private int InversePairsMerge(int[] a, int[] b, int s, int mid, int e) { int i = mid, j = e, n = 0; for (int k = e; k \u0026gt;= s; k--) {// merge from a to b if (i \u0026lt; s) b[k] = a[j--]; else if (j \u0026lt;= mid) b[k] = a[i--]; else if (a[i] \u0026gt; a[j]) { n += j - mid; if(n \u0026gt;= 1000000007)//数值过大求余 n %= 1000000007; b[k] = a[i--]; } else b[k] = a[j--]; } return n; } count while mergesorting.\nShuffling a linked list: Given a singly-linked list containing n items, rearrange the items uniformly at random. Your algorithm should consume a logarithmic (or constant) amount of extra memory and run in time proportional to $n \\log n$ in the worst case:\nDesign a linear-time subroutine that can take two uniformly shuffled linked lists of sizes $n_1$ and $n_2$ and combined them into a uniformly shuffled linked lists of size $n_1 + n_2$.\nQuick Sort Basic plan:\nShuffle the array: Random shuffle. Probabilistic guarantee against worst case. Partition so that, for some j entry a[j] is in place no larger entry to the left of j no smaller entry to the right of j Sort each piece recursively Patition:\nRepeat until i and j pointers cross. Scan i from left to right so long as (a[i] \u0026lt; a[lo]) Scan j from right to left so long as (a[j] \u0026gt; a[lo]) Exchange a[i] with a[j] When pointers cross. Exchange a[lo] with a[j] public class Quick { // return index of item now known to be in place private static int partition(Comparable[] a, int lo, int hi) { int i = lo, j = hi+1; while (true) { while (less(a[++i], a[lo])) // find item on left to swap if (i == hi) break; while (less(a[lo], a[--j])) // find item on right to swap if (j == lo) break; if (i \u0026gt;= j) break; // check if pointers cross exch(a, i, j); } exch(a, lo, j); // swap with partitioning item return j; } private static void sort(Comparable[] a, int lo, int hi) { if (hi \u0026lt;= lo) return; int j = partition(a, lo, hi); sort(a, lo, j-1); sort(a, j+1, hi); } public static void sort(Comparable[] a) { StdRandom.shuffle(a); sort(a, 0, a.length - 1); } } Best case. Number of compares is ~ $N \\lg N$ Worst case. Number of compares is ~ $½N^2$ Average case. Number of compares is ~ $1.39 N \\lg N$, 39% more compares than mergesort. But faster than mergesort in practice because of less data movement.\nQuicksort: practical improvements Insertion sort small subarrays (10 items), could delay insertion sort until one pass at end.\nprivate static void sort(Comparable[] a, int lo, int hi) { if (hi \u0026lt;= lo + CUTOFF - 1) { Insertion.sort(a, lo, hi); return; } int j = partition(a, lo, hi); sort(a, lo, j-1); sort(a, j+1, hi); } Median of sample: Best choice of pivot item = median. Estimate true median by taking median of sample. Median-of-3 (random) items\nprivate static void sort(Comparable[] a, int lo, int hi) { if (hi \u0026lt;= lo) return; int m = medianOf3(a, lo, lo + (hi - lo)/2, hi); swap(a, lo, m); int j = partition(a, lo, hi); sort(a, lo, j-1); sort(a, j+1, hi); } Quicksort with duplicate keys ・Algorithm goes quadratic unless partitioning stops on equal keys! ・1990s C user found this defect in qsort() Mistake. Put all items equal to the partitioning item on one side. Consequence. ~ $½N^2$ compares when all keys equal. Recommended. Stop scans on items equal to the partitioning item. Consequence. ~ $N \\lg N$ compares when all keys equal.\n3-way partitioning: Dutch national flag problem. Partition array into 3 parts so that: ・Entries between lt and gt equal to partition item v. ・No larger entries to left of lt. ・No smaller entries to right of gt.\nLet v be partitioning item a[lo] Scan i from left to right. (a[i] \u0026lt; v): exchange a[lt] with a[i]; increment both lt and i (a[i] \u0026gt; v): exchange a[gt] with a[i]; decrement gt (a[i] == v): increment i private static void sort(Comparable[] a, int lo, int hi) { if (hi \u0026lt;= lo) return; int lt = lo, gt = hi; Comparable v = a[lo]; int i = lo; while (i \u0026lt;= gt) { int cmp = a[i].compareTo(v); if (cmp \u0026lt; 0) exch(a, lt++, i++); else if (cmp \u0026gt; 0) exch(a, i, gt--); else i++; } sort(a, lo, lt - 1); sort(a, gt + 1, hi); } Quicksort Applications Java sort for primitive types. C qsort, Unix, Visual C++, Python, Matlab, Chrome JavaScript, \u0026hellip;\nSelection: Order statistics, Find the \u0026ldquo;top k.\u0026rdquo; Given an array of N items, find a $k^{th}$ smallest item. Ex. Min(k = 0), max(k = N - 1), median(k = N/2).\nQuick-select\nPartition array so that: Entry a[j] is in place No larger entry to the left of j No smaller entry to the right of j Repeat in one subarray, depending on j; finished when j equals k. public static Comparable select(Comparable[] a, int k) { StdRandom.shuffle(a); int lo = 0, hi = a.length - 1; while (hi \u0026gt; lo) { int j = partition(a, lo, hi); if (j \u0026lt; k) lo = j + 1; else if (j \u0026gt; k) hi = j - 1; else return a[k]; } return a[k]; } Quick-select takes linear time on average.\nPriority Queues 优先队列可用于快速地（O(1)）返回最大或者最小的值。\npublic class MaxPQ\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;\u0026gt; { MaxPQ() create an empty priority queue MaxPQ(Key[] a) create a priority queue with given keys void insert(Key v) insert a key into the priority queue Key delMax() return and remove the largest key boolean isEmpty() is the priority queue empty? Key max() return the largest key int size() number of entries in the priority queue } 应用: ・Event-driven simulation. [customers in a line, colliding particles] ・Numerical computation. [reducing roundoff error] ・Data compression. [Huffman codes] ・Graph searching. [Dijkstra\u0026rsquo;s algorithm, Prim\u0026rsquo;s algorithm] ・Number theory. [sum of powers] ・Artificial intelligence. [A* search] ・Statistics. [maintain largest M values in a sequence] ・Operating systems. [load balancing, interrupt handling] ・Discrete optimization. [bin packing, scheduling] ・Spam filtering. [Bayesian spam filter]\n比如对于数据流, 需要用优先队列保存最大的M个值, 因为内存不足以储存数据流全部数据.\nBinary Heap Heap-ordered binary tree: 父节点比其所有子节点都大（或都小）。根节点为最大值的binary heap称之为最大堆, 根节点为最小值的称之为最小堆.\n以最大堆为例, 使用数组来表达：\n索引从1开始, 按照层次遍历顺序存储节点. 最大值就是根节点a[1]\\ 可以使用数组索引遍历树 节点a[k]的父节点是a[k/2] 节点a[k]的子节点为a[2k], a[2k+1] 当子节点的值比父节点大时, 需要不断调换二者的值, 直到不再有子节点比父节点大的情况存在:\nprivate void swim(int k) { while (k \u0026gt; 1 \u0026amp;\u0026amp; less(k/2, k)) { exch(k, k/2); k = k/2; } } 反之当父节点比子节点小时:不断把父节点和较大的子节点调换, 直到恢复 heap order:\nprivate void sink(int k) { while (2*k \u0026lt;= N) { int j = 2*k; if (j \u0026lt; N \u0026amp;\u0026amp; less(j, j+1)) j++; if (!less(k, j)) break; exch(k, j); k = j; } } 插入操作, 需要先把新节点放在末端, 然后swim. 至多1 + lgN比较:\npublic void insert(Key x) { pq[++N] = x; swim(N); } 删除操作, 需要把根节点和尾节点调换, 然后sink, 至多2lgN比较\npublic Key delMax() { Key max = pq[1]; exch(1, N--); sink(1); pq[N+1] = null; return max; } public class MaxPQ\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;\u0026gt; { private Key[] pq; private int N; public MaxPQ(int capacity) { pq = (Key[]) new Comparable[capacity+1]; } public boolean isEmpty() { return N == 0; } public void insert(Key key) public Key delMax() { /* see previous code */ } private void swim(int k) private void sink(int k) { /* see previous code */ } private boolean less(int i, int j) { return pq[i].compareTo(pq[j]) \u0026lt; 0; } private void exch(int i, int j) { Key t = pq[i]; pq[i] = pq[j]; pq[j] = t; } } 如果要实现最小堆, 那么就用greater()替代less() Heap Sort 可以使用heap数据结构来排序一个数组，核心步骤是两个\n创建一个最大堆 然后不断拿出当前最大值，放置于后面. public class Heap { public static void sort(Comparable[] a) { // bottom-up方法创建堆, // 虽然是 heap order, 但不一定是sorted order int N = a.length; for (int k = N/2; k \u0026gt;= 1; k--) sink(a, k, N); while (N \u0026gt; 1) { // 把当前最大值调换到N位置 exch(a, 1, N); // 最大值放在尾部 sink(a, 1, --N); // 恢复heap order } } private static void sink(Comparable[] a, int k, int N) { /* as before */ } private static boolean less(Comparable[] a, int i, int j) { /* as before */ } private static void exch(Comparable[] a, int i, int j) { /* as before */ } } In-place sorting algorithm with N log N worst-case, but not stable\n排序算法汇总比较 Pigeonhole sort 鸽巢排序(基数分类)\nPigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the length of the range of possible key values (N) are approximately the same. It requires O(n + N) time.\n给定要排序的数组，设置一个辅助数组作为初始的空“鸽笼”，通过原始数组的范围为每个键值设置一个鸽笼。 遍历原始数组，将每个值放入与其键对应的鸽笼中，这样每个鸽笼最终都包含该键的所有值的列表。 按顺序迭代鸽笼数组，并将非空鸽笼中的元素依次放回原始数组中。 /* Java program to implement Pigeonhole Sort */ public class GFG { public static void pigeonhole_sort(int arr[], int n) { int min = arr[0]; int max = arr[0]; int range, i, j, index; for(int a=0; a\u0026lt;n; a++) { if(arr[a] \u0026gt; max) max = arr[a]; if(arr[a] \u0026lt; min) min = arr[a]; } range = max - min + 1; int[] phole = new int[range]; //Arrays.fill(phole, 0); for(i = 0; i \u0026lt; n; i++) phole[arr[i] - min]++; index = 0; for(j = 0; j \u0026lt; range; j++) while(phole[j] \u0026gt; 0) arr[index++] = j + min; } public static void main(String[] args) { GFG sort = new GFG(); int[] arr = {8, 3, 2, 7, 4, 6, 8}; System.out.print(\u0026#34;Sorted order is : \u0026#34;); sort.pigeonhole_sort(arr,arr.length); for(int i=0 ; i\u0026lt;arr.length ; i++) System.out.print(arr[i] + \u0026#34; \u0026#34;); } } 类似于counting sort\nBucket sort 桶排序(箱排序 bin sort)主要用于均匀分布区间值的排序，如浮点数排序，\ndistributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort, a generalization of pigeonhole sort, and is a cousin of radix sort in the most-to-least significant digit flavor. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity estimates involve the number of buckets.\nWorst-case performance ${\\displaystyle O(n^{2})}$ Best-case performance ${\\displaystyle \\Omega (n+k)}$ Average performance\t${\\displaystyle \\Theta (n+k)}$\n设置一个初始为空的“桶”数组。 Scatter：遍历原始数组，将每个对象分发到桶中。 对每个非空桶进行排序。 Gather：按顺序访问桶并将所有元素放回原始数组中。 Graph 图：由边连接的成对的顶点集。\n无向图，有向图\n","permalink":"https://congchan.github.io/posts/algorithms-princeton/","summary":"\u003cul\u003e\n\u003cli\u003eAlgorithms, Part I, \u003ca href=\"https://online.princeton.edu/course/algorithms-part-i\"\u003ehttps://online.princeton.edu/course/algorithms-part-i\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eAlgorithms, Part II, \u003ca href=\"https://online.princeton.edu/course/algorithms-part-ii\"\u003ehttps://online.princeton.edu/course/algorithms-part-ii\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eAlgorithms, 4th Edition by Robert Sedgewick and Kevin Wayne \u003ca href=\"https://algs4.cs.princeton.edu/\"\u003ehttps://algs4.cs.princeton.edu/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"unionfind\"\u003eUnion−Find\u003c/h2\u003e\n\u003cp\u003eConsidering the dynamic connectivity problem, modeling of multiple objects connected in a space/network.\u003c/p\u003e\n\u003cp\u003eApplications involve manipulating objects of all types.\n・Pixels in a digital photo.\n・Computers in a network.\n・Friends in a social network.\n・Transistors in a computer chip.\u003c/p\u003e\n\u003cp\u003eGiven a set of N objects.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eunion(a, b)\u003c/code\u003e: connect two objects.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econnected(p, q)\u003c/code\u003e: is two objects connected?\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efind(p)\u003c/code\u003e: Find component identifier for \u003ccode\u003ep\u003c/code\u003e (0 to N – 1)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eModeling the objects: array.\u003c/p\u003e","title":"Algorithms - Princeton"},{"content":"程序员或者其他需要码字多的人，经常要使用编辑器如sublime、atom 和 Typora等。如果每次都要用鼠标点击才能用sublime打开文件，或者在编辑器中新建文件，那么就会有点麻烦！但你可以用一句命令解决！\n配置在Git Bash中用各种文本编辑器打开文件或者直接新建文件。这里以atom为例。\n常规步骤 打开Git Bash并cd到你的目标文件夹, 或者直接在目标文件中右键打开Git Bash. atom xxx.md 就会在弹出的atom窗口中打开名为xxx.md的markdown文件, 如果没有这个文件, 会自动创建一个. 适用于其他类型文件, 如.java等. 如果想用sublime, 可以用subl xxx.java, 同理notepad++ 可以用 notepad++ xxx.java等。 (若出现错误,看下面) 若系统无法识别命令 一般使用sublime或者notepad++的用户, 可能会出现error: 系统无法识别命令...之类的, 可以这么解决:\n方法1 新建一个文件命名为subl（注意不能有后缀名），内容：\n#!/bin/sh \u0026#34;D:\\Sublime Text 3\\sublime_text.exe\u0026#34; $1 \u0026amp; 第一行指明这是个 shell 脚本. 第二行的字符串是sublime的安装目录, 示例只是我电脑的目录, 注意这里要改为你自己的目录, 第二行的$1 是取的命令之后输入的参数 第二行的\u0026amp;是此命令在后台打开，这样sublime打开之后，就不会阻塞你的git bash\n文件保存到 C:\\Program Files (x86)\\Git\\mingW32\\bin 目录下(你的git目录可能与我的不一样，注意改成你自己的)\n同理适用于其他编辑器，比如用chrome打开.html文件等。如果不想每次都新建一个文件，可以用下面的方法2。\n方法2 找到 C:\\Users\\你的计算机名目录，如果你的计算机名是Administrator，那么你就要去C:\\Users\\Administrator目录下, 这里一般存放着windows系统的我的文档, 桌面等文件夹. 在该目录下用Git Bash输入notepad .bashrc, 这会用windows记事本新建并打开一个文件.bashrc，这个文件没有名称只有后缀名。.bashrc里面可以给Git Bash设置命令的别名, 设置路径等。 在.bashrc文件加入下面一行文本alias notepad++=\u0026quot;/D/Notepad++/notepad++.exe\u0026quot;, 这里你需要修改为你电脑的安装路径。alias就是别名的意思，当我们执行notepad++的时候，实际执行的是=后面的语句. 重新打开Git Bash, 设置才能生效，如果不想关掉在打开的话，可以直接在bash下输入source ~/.bashrc就可以立刻加载修改后的设置，设置立即生效。 现在在bash下输入notepad++ test.py, 就直接打开了notepad++并创建了这个叫test的Python文件。这里的别名不一定非要取notepad++，随你想叫什么都行。 同理也可以扩展到别的文本编辑器，alias atom=\u0026quot;atom的路径\u0026quot;, alias sublime=\u0026quot;sublime的路径\u0026quot;等. 最后还要注意一点，上面所说的路径最好不要有空格，括号等，否则会造成命令无效.\n.bashrc还有很多有用的配置,可以根据需要进行扩展. 比如很多程序猿会选择修改删除命令rm(此命令不加任何参数的话，会直接删除文件, 可能会造成误删的后果)。这个时候可以给rm加个参数-i，意为在删除的时候给出提示。在文件.bashrc里添加这行代码alias rm=\u0026quot;rm -i\u0026quot;。但这里不建议这么做，因为rm=\u0026quot;rm -i\u0026quot;是一个定时炸弹，在使用它之后，习惯了之后, 你会本能地期望rm在删除文件之前会提示你。但是，总有一天你可能会用一个没有rm alias 别名的系统, 这时若你也直接随手一甩rm, 本以为会有提示, 结果发现数据真的被删除了。\n在任何情况下，预防文件丢失或损坏的好方法就是进行备份。\n所以如果你想个性化删除命令, 最好不要动rm，而是创建属于你的命令，比如trash, myrm, delete等, 用alias trash='/bin/rm -irv'会创建一条把文件放入垃圾回收站的命令.\n","permalink":"https://congchan.github.io/posts/bash-%E7%9B%B4%E6%8E%A5%E5%90%AF%E5%8A%A8-sublime-%E6%88%96-atom-%E7%AD%89%E7%BC%96%E8%BE%91%E5%99%A8%E4%BB%A5%E6%89%93%E5%BC%80%E6%88%96%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6/","summary":"\u003cp\u003e程序员或者其他需要码字多的人，经常要使用编辑器如sublime、atom 和 Typora等。如果每次都要用鼠标点击才能用sublime打开文件，或者在编辑器中新建文件，那么就会有点麻烦！但你可以用一句命令解决！\u003c/p\u003e\n\u003cp\u003e配置在Git Bash中用各种文本编辑器打开文件或者直接新建文件。这里以atom为例。\u003c/p\u003e\n\u003ch2 id=\"常规步骤\"\u003e常规步骤\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e打开Git Bash并\u003ccode\u003ecd\u003c/code\u003e到你的目标文件夹, 或者直接在目标文件中右键打开Git Bash.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eatom xxx.md\u003c/code\u003e 就会在弹出的atom窗口中打开名为\u003ccode\u003exxx.md\u003c/code\u003e的markdown文件, 如果没有这个文件, 会自动创建一个.\u003c/li\u003e\n\u003cli\u003e适用于其他类型文件, 如\u003ccode\u003e.java\u003c/code\u003e等.\u003c/li\u003e\n\u003cli\u003e如果想用sublime, 可以用\u003ccode\u003esubl xxx.java\u003c/code\u003e, 同理notepad++ 可以用 \u003ccode\u003enotepad++ xxx.java\u003c/code\u003e等。 (若出现错误,看下面)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"若系统无法识别命令\"\u003e若系统无法识别命令\u003c/h2\u003e\n\u003cp\u003e一般使用sublime或者notepad++的用户, 可能会出现\u003ccode\u003eerror: 系统无法识别命令...\u003c/code\u003e之类的, 可以这么解决:\u003c/p\u003e\n\u003ch3 id=\"方法1\"\u003e方法1\u003c/h3\u003e\n\u003cp\u003e新建一个文件命名为\u003ccode\u003esubl\u003c/code\u003e（注意不能有后缀名），内容：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#!/bin/sh\n\u0026#34;D:\\Sublime Text 3\\sublime_text.exe\u0026#34; $1 \u0026amp;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e第一行指明这是个 shell 脚本.\n第二行的字符串是sublime的安装目录, 示例只是我电脑的目录, 注意这里要改为你自己的目录,\n第二行的$1 是取的命令之后输入的参数\n第二行的\u0026amp;是此命令在后台打开，这样sublime打开之后，就不会阻塞你的git bash\u003c/p\u003e\n\u003cp\u003e文件保存到 \u003ccode\u003eC:\\Program Files (x86)\\Git\\mingW32\\bin\u003c/code\u003e 目录下(你的git目录可能与我的不一样，注意改成你自己的)\u003c/p\u003e\n\u003cp\u003e同理适用于其他编辑器，比如用\u003ccode\u003echrome\u003c/code\u003e打开\u003ccode\u003e.html\u003c/code\u003e文件等。如果不想每次都新建一个文件，可以用下面的方法2。\u003c/p\u003e\n\u003ch3 id=\"方法2\"\u003e方法2\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e找到 \u003ccode\u003eC:\\Users\\你的计算机名\u003c/code\u003e目录，如果你的计算机名是Administrator，那么你就要去\u003ccode\u003eC:\\Users\\Administrator\u003c/code\u003e目录下, 这里一般存放着windows系统的\u003ccode\u003e我的文档, 桌面\u003c/code\u003e等文件夹.\u003c/li\u003e\n\u003cli\u003e在该目录下用Git Bash输入\u003ccode\u003enotepad .bashrc\u003c/code\u003e, 这会用windows记事本新建并打开一个文件\u003ccode\u003e.bashrc\u003c/code\u003e，这个文件没有名称只有后缀名。\u003ccode\u003e.bashrc\u003c/code\u003e里面可以给Git Bash设置命令的别名, 设置路径等。\u003c/li\u003e\n\u003cli\u003e在.bashrc文件加入下面一行文本\u003ccode\u003ealias notepad++=\u0026quot;/D/Notepad++/notepad++.exe\u0026quot;\u003c/code\u003e, 这里你需要修改为你电脑的安装路径。\u003ccode\u003ealias\u003c/code\u003e就是别名的意思，当我们执行\u003ccode\u003enotepad++\u003c/code\u003e的时候，实际执行的是\u003ccode\u003e=\u003c/code\u003e后面的语句.\u003c/li\u003e\n\u003cli\u003e重新打开Git Bash, 设置才能生效，如果不想关掉在打开的话，可以直接在bash下输入\u003ccode\u003esource ~/.bashrc\u003c/code\u003e就可以立刻加载修改后的设置，设置立即生效。\n现在在bash下输入\u003ccode\u003enotepad++ test.py\u003c/code\u003e, 就直接打开了notepad++并创建了这个叫test的Python文件。这里的别名不一定非要取\u003ccode\u003enotepad++\u003c/code\u003e，随你想叫什么都行。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e同理也可以扩展到别的文本编辑器，\u003ccode\u003ealias atom=\u0026quot;atom的路径\u0026quot;\u003c/code\u003e, \u003ccode\u003ealias sublime=\u0026quot;sublime的路径\u0026quot;\u003c/code\u003e等. 最后还要注意一点，上面所说的路径最好不要有空格，括号等，否则会造成命令无效.\u003c/p\u003e","title":"Bash 直接启动 sublime 或 atom 等编辑器以打开或新建文件"},{"content":"本篇介绍 topic modeling, 以及一个经典的算法Latent Dirichlet allocation, 文本挖掘与语义理解的集大成者(至少在深度学习统治之前). 当然LDA不仅仅局限于文本, 还可应用于涉及大量数据集的各种问题，包括协同过滤，基于内容的图像检索和生物信息学等领域的数据。\nTopic Modelling 大规模文本挖掘的核心问题, 就是用数学模型代替人力来理解文本语义，目标是找到对集合成员（如一堆文本）的数学/统计描述，以便能够对这些大型集合进行高效处理，同时保留对基本任务（如分类，检测，摘要以及相似性和相关性判断）有用的基本统计关系。\n在这方面的研究方法很多，特别是信息检索(IR)领域. 一个基本方法是将语料库中的每个文档向量化，向量中的每个实数代表计数率。比如经典的tf-idf方法，用Document-Term Matrix来表达不同词在不同文档出现的情况差异, 一般term就是word作为features, 所以在这里我们表示document-word matrix(DWM), 就是DWM[i][j] = The number of occurrences of word_j in document_i. Doc 1: I have a fluffy cat. Doc 2: I see a fluffy dog.\nDWM I have a fluffy cat see dog doc1 1 1 1 1 1 0 0 doc2 1 0 1 1 0 1 1 然后进行normalization, 去和 inverse document frequency count(IDF)进行比较. IDF统计每个词在整个文档集合中出现的总次数, 通常转化为log scale, 并进行适当的normalization.\n这个矩阵把文档表示为向量，使得不同文档之间可以从几何上衡量相似性，根据相似性聚类文本.\n虽然tf-idf有很多很好的特性, 但是它的降维程度非常有限, 而且无法揭示文档间或文档内的统计结构.\n比如我们需要知道文本包含什么信息, 却又不清楚什么信息是重要的, 所以我们希望能把信息也归纳成几类。我们称这种信息为主题, 一种粗粒度的信息. 那么就有了一个很重要的任务, 就是挖掘出这一堆文本包含的主题都有哪几大类. 每个文本都可能包含多种不同主题, 而且包含的侧重也不一样, 所以进一步的, 我们希望能够挖掘出每个文本的主题分布, 也就是主题类别在各个文本中的权重. 这种对文本信息的挖掘和理解方法, 称之为主题建模(Topic Modelling). 其核心思想是认为词不是由文档直接生成，而是由文档先生成主题，主题再生成词。\n因为主题建模不再是用词频来表达, 而是用主题权重{Topic_i: weight(Topic_i, T) for Topic_i in Topics}来表达文档在K个主题上的分布。用K维的向量来表征文档，本质上是降维。\n此时主题数量就是一个超参数, 通过主题建模，构建了单词的clusters而不是文本的clusters。因此，文本被表达为多个主题的混合，每个主题都有一定的权重。这种做法在机器学习的框架中可称之为隐变量模型，因为它引入了一个观测数据中不存在的变量，也就是主题这个变量。它首先假设存在这样一个隐变量，并假设了隐变量和观测变量之间的关系，然后通过模型训练得到隐变量和观测变量之间的具体关系，最终模型的产出包括隐变量的分布本身，以及更重要的，隐变量和观测变量之间的关系。\n主题建模也可以理解为文本主题的tagging任务, 只是无监督罢了.\nLatent Semantic Analysis (LSA) 通过引入Latent的概念，把主题表达为隐藏的信息, 也就是假设主题已经存在, 只是我们看不到. LSA使用DWM矩阵的SVD奇异值分解来确定tf-idf特征空间中的线性子空间，该子空间捕获了集合中的大部分variance。\n假设单词使用中存在一些latent的结构, 由于单词选择的多样性而被掩盖了. 与其将文本表示为单词的t维空间中的向量，不如将相似词有效地组合在一起的\u0026quot;概念\u0026quot;(topic), 作为维度, 将文本（以及词本身）表示为低维空间中的向量. 这些低维的轴就是通过PCA得出的Principal Components 然后就可以在 latent semantic 空间中为下游任务服务, 如计算文本相似度(通过内积等cosine相似度计算). 把DWM矩阵表达为 DTM(Document Topic Matrix) 和 TWM(Topic Word Matrix) 两个矩阵, 它们维度更小，相乘的结果应该尽可能近似原始的DWM矩阵。\n假设词汇$V$有$1024$个, 文档$W$有$64$篇, 用 $DWM = W \\times V$来表达Document-Word Matrix, 需要$64 \\times 1024 = 65,536$的参数量. 假如我们设定topic参数为$8$, 那么就可以用$DWM = DTM \\times TWM $来近似表达Document-Term Matrix, 参数量减少为$64 \\times 8 + 8 \\times 1024 = 8,704$, 缩减了将近$90\\%$\n所以LSA核心思想是构造 Document-Term Matrix 的低阶近似.\n用 tf-idf 计算加权DWM. tf-idf(term frequency–inverse document frequency)是DWM矩阵的一种经典加权表达。 然后对DWM矩阵进行 Singular Value Decomposition (SVD). 降维的意义不仅仅是减少下游任务的计算负担：\ntf-idf向量一般很长很庞大。因此降维操作对于聚类或分类等进一步计算是能节省很多资源。 原始DTM矩阵被认为是有噪声的：近似矩阵被解释为去噪矩阵（比原始矩阵更好的矩阵）。 假定原始的DTM矩阵相对于“真实的”DTM矩阵过于稀疏，降维也可以看作一种泛化。也就是说，原始矩阵仅列出每个文档中实际的单词，而我们可能会对与每个文档相关的所有单词感兴趣-如同义词等。 (car), (truck), (flower)} --\u0026gt; {(1.3452 * car + 0.2828 * truck), (flower)}\nProbabilistic Latent Semantic Analysis (PLSA) 也称之为aspect model, 尝试从统计学的角度改进LSA. 将文档中的每个单词建模为混合模型中的样本. 其中这个混合模型混合的成分是multinomial随机变量，可以视为“主题”的表示形式。因此，每个单词都是由单个主题生成的，每一个文档中的不同单词可能从不同的主题生成。每个文档都表示为这些混合成分根据不同比例混合的列表，从而简化为固定主题集的概率分布.\n把潜在的topics视作 Latent variable 隐变量z, 而文本Documents和词汇Words就是观察变量 observed variables. 共现(co‐occurrence)的数据都关联有隐含的话题类别, 做出条件独立假设, D和W是基于隐变量z的条件独立变量, $$ P(w|d) = \\sum_{z\\in Z} P(w|z)P(z|d)$$$$ P(d, w) = P(d)P(w|d) = P(d) \\sum_{z\\in Z} P(w|z)P(z|d) \\\\\\ = \\sum_{z\\in Z} P(d) P(w|z)P(z|d) \\\\\\ = \\sum_{z\\in Z} P(z) P(w|z)P(d|z) $$\n利用隐变量可以解决稀疏性问题，也就是避免文档中未出现的word的概率为零, 可以理解为一种平滑.\n使用EM算法: E-Step: 计算隐变量的posterior probabilities,\n$P(z|d, w) = \\frac{ P(z) P(w|z)P(d|z) }{ \\sum_{z'\\in Z} P(z') P(w|z')P(d|z') }$ M-Step: 更新参数\n$P(w|z) \\propto \\sum_{d \\in D} n(d, w) P(z|d, w) $ $P(d|z) \\propto \\sum_{w \\in W} n(d, w) P(z|d, w) $ $P(z) \\propto \\sum_{d \\in D}\\sum_{w \\in W} n(d, w) P(z|d, w) $ PLSA是一种生成式的概率模型. PLSA的$P(w, d)$可以解释为LSA中的$P = U Σ V^T$, 其中$U$包含$P(d|z)$, $Σ$作为$P(z)$的对角矩阵, $V$包含$P(w|z)$\nPLSA有助于处理多义词(Polysemous words), 通过$P(w|z)$排序比较, 比如SEGMENT在topic1中更靠近image, 意味着Image region; 在topic2中更靠近sound, 意味着Phonetic segment.\n虽然PPCA也是概率模型, 但是PPCA假设了正态分布(normal distribution), 局限性很大. PLSA将每个共现的概率建模为条件独立的多项式分布(multinomial distributions)的混合. 多项式分布在此领域是更好的选择。\n因为有了$p(z|d)$充当特定文档的主题混合权重, pLSA可以捕获文档包含多个主题的可能性. 但是, $d$是训练集文档列表的虚拟索引, 因此$d$是一个多项式随机变量，其值可能与训练文档一样多, 这样pLSA仅针对训练集的文档学习主题混合$p(z|d)$, 对于训练集之外的document而言, 不知道如何分配概率. 等于说pLSA并不是一个定义明确的文档级别的概率生成模型。\n除此之外, 因为使用训练集文档索引的分布，另一个困难就是需要估计的参数数量随着训练集数量增加而线性增加. 具体地说, 一个k-topic pLSA的参数是$k$个latent topic上大小为$V$的多项式分布, 以及$M$个mixtures, 参数量是$kV + kM$，随着$M$线性增长。所以pLSA容易过拟合(虽然可以用Tempered EM算法来稍微缓解).\nLatent Dirichlet allocation(LDA) 回顾LSA和pLSA, 都基于“词袋”的假设。从概率论的角度来说，是对文档中单词有exchangeability的假设（Aldous，1985）。此外，尽管很少正式地陈述这些方法，但这些方法还假定文档是可交换的。语料库中文档的特定顺序也可以忽略。Finetti（1990）提出的经典表示定理认为任何可交换随机变量的集合都具有表示为混合分布的形式 - 通常是无限混合。因此，如果我们希望考虑文档和单词的可交换表示形式，则需要考虑能够同时捕获单词和文档的可交换性的混合模型。这就是Latent Dirichlet allocation. David Blei, Andrew Ng, and Michael Jordan. 2003.这篇文章的动机.\nLDA对主题分布的基本设定是, 每个文档被表达为latent variables(topics)的随机混合, 其中各个topics可以由单词的概率分布来描述.\n对于语料库$D$中的每个文档$\\mathbb{w}$, LDA假设如下的生成过程:\n选择参数$N ∼ Poisson(ξ)$, 用Dirichlet分布$Dir(\\alpha)$生成一个多项式分布参数$θ$, 即$p(θ|\\alpha)$ 对于文档中的每一个词$w_n$: 基于多项式概率分布$Multinomial(θ)$选择一个topic$z_n$, 即$p(z_n |θ)$ 基于$p(w_n | z_n, β)$, 即以topic $z_n$为条件的multinomial概率, 选择一个词$w_n$ 这个过程做了几个假设. 一个是, $\\beta$作为单词概率的参数, 是一个$k \\times V$的矩阵, $\\beta_{ij} = p(w^j = 1 | z^i = 1)$, 是需要估计的固定变量. 这里要注意，$N$是独立于所有其他数据生成变量（$θ$和$z$）, 因此是一个辅助变量，通常会忽略它的随机性。其余的假设有兴趣可以去读论文.\n给定了参数$\\alpha$和$\\beta$, 可以估计topic mixture θ，一组$N$个主题$\\mathbb{z}$和一组$N$个单词$\\mathbb{w}$的联合分布: $$ p(θ,\\mathbb{z}, \\mathbb{w}|α,β) = p(θ|α) \\prod^N_{n=1} p(z_n | \\theta) p(w_n|z_n, \\beta) $$$p(z_n |θ)$在这里就是第$i$个$\\theta_i$, 这个独特的$i$使得$z^i_n = 1$. 沿着$θ$求积分并在$z$上求和，得到文档的 marginal distribution $$p(\\mathbb{w}|α,β) = \\int p(θ|α) \\Bigg( \\prod\\limits^N_{n=1} \\sum\\limits_{z_n} p(z_n | \\theta) p(w_n|z_n, \\beta) \\Bigg) d\\theta$$最后，取各个文档的marginal distribution的乘积，得到整个语料库(corpus)的概率 $$ p(D|α,β) = \\prod\\limits^M_{d=1} p(\\mathbb{w}|α,β) $$参数$\\alpha$和$\\beta$是语料库级别的参数，假定在生成语料库的过程中只采样一次。$\\theta_d$是文档级别的变量, 每个文档采样一次. $z_{dn}$和$w_{dn}$是词级别的变量, 每个文档的每个词采样一次.\nLDA通过将主题混合权重视为k-parameter隐随机变量，而不是与训练集显式关联的一大套参数, 解决pLSA的缺陷。而且k-topic的LDA模型参数量是$k + kV$, 不会随着训练语料库的增加而增长.\n如果在几何上比较和理解pLSA和LDA, 模型都可以视为在words的分布空间上操作, 每个这样的分布都可以看作(V-1)-simplex(称之为word simplex)上的一个点. 如图, 假设有3个单词, 假设选择k=3的topic simplex包含在三个单词的word simplex中。word simplex的三个角对应于三个特殊的分布([1, 0, 0], [0, 1, 0], [0, 0, 1])，即其中各有一个单词的概率为1。topic simplex的三个点对应于三个不同的单词分布(比如类似[0.7, 0.2, 0.1], [0.05, 0.9, 0.05], [0.3, 0.05, 0.65])。\n最简单的unigram模型在word simplex上找到一个点，并假设语料库中的所有单词都来自相应的分布。而隐变量模型考虑(选择)word simplex上的k个点(在图中是k=3个)，并基于这些点形成sub-simplex，即topic simplex。\npLSA模型假定训练集文档的每个单词各来自一个随机选择的主题。主题本身来自document-specific的主题分布，即topic simplex上的一个个点x。每个文档都有一个这样的分布；因此，文档训练集定义了关于topic simplex的经验分布。 LDA假定，不管是训练集还是测试集的文档, 每个单词都是由随机选择的主题生成的，该主题是从一个以随机选择的$θ_d$为参数的分布中得出的。参数$θ_d$的采样方法是每个文档采样一个topic simplex的平滑分布, 就是图中的等高线。 LDA推理和参数估计 LDA推理关键的一步是计算给定的一个文档的隐变量的后验分布(posterior distribution): $$ p(θ, \\mathbb{z} | \\mathbb{w}, α, β) = \\frac{p(θ, \\mathbb{z}, \\mathbb{w} | α, β)}{p( \\mathbb{w} | α, β)} $$其中的$p(\\mathbb{w}|α,β)$由于latent topics的求和中$θ$和$β$之间的耦合而变得很难求解(Dickey, 1983). 尽管因为后验分布导致精确的推理是很难，但对于LDA，可以考虑使用各种近似算法，包括Laplace逼近，变分(variational)逼近和Markov chain Monte Carlo(Jordan, 1999)。\n论文中介绍了一种convexity-based variational inference方法, 基本思想是利用Jensen’s inequality获得log likelihood的可调下限(Jordan, et al., 1999)\n使用迭代逼近来计算DLA模型：\n初始化：每个单词随机分配给一个主题。 循环遍历每个单词，基于以下信息将单词重新分配给一个主题： training: repeat until converge assign each word in each document to one of T topics. For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t) Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d) LDA的实际应用 LDA聚类的结果经常被用来做聚类，典型的如文档的聚类，而其他表征学习学到的ID表征，同样可以用来做ID的聚类，例如用词向量做词的聚类等。能生成向量表示，并且能用来聚类，从这个角度来看，表征学习和LDA这类生成式模型的用途是一样。\n以LDA为代表的生成式模型，或者叫生成式表征学习方法的应用面也非常的广，只要我们能把问题抽象成“文档+词”这样的结构，LDA几乎都可以给出一个有效的表示，例如“用户和群组”、“用户和POI”、“用户和商品”等关系都可以解构为“文档和词”这样的关系，从而可使用LDA模型计算主题和表征。此外，即使是在word2vec及其通用方法横空出世之后，LDA类方法应用仍然非常广泛。\nLDA模型实战案例\n总结 主题建模的算法:\n(p)LSA: (Probabilistic) Latent Semantic Analysis – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra. SVD假设了Gaussian distributed. LDA: latent Dirichlet allocation, 假设了multinomial distribution。 LDA是pLSA的generalization, LDA的hyperparameter设为特定值的时候，就specialize成 pLSA 了。从工程应用价值的角度看，这个数学方法的generalization，允许我们用一个训练好的模型解释任何一段文本中的语义。而pLSA只能理解训练文本中的语义。（虽然也有ad hoc的方法让pLSA理解新文本的语义，但是大都效率低，并且并不符合pLSA的数学定义。）这就让继续研究pLSA价值不明显了。\nNMF – Non-Negative Matrix Factorization Generalized matrix decomposition 实际上是 collaborative filtering 的 generalization，是用户行为分析和文本语义理解的共同基础. 参考资料 https://nlpforhackers.io/topic-Modelling/ Steyvers and Griffiths (2007). Probabilistic topic models. Distributional semantic models and topic models have been extensively investigated not just in NLP, but also as models of human cognition. This paper provides a brief introduction to topic models as cognitive models. A much more thorough investigation can be found in Griffiths, Steyvers, and Tenenbaum (2007). Latent Semantic Analysis (LSA) for Text Classification Tutorial Intuitive Guide to Latent Dirichlet Allocation ","permalink":"https://congchan.github.io/posts/topic-modelling-%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1%E4%BB%A5%E5%8F%8A%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B/","summary":"\u003cp\u003e本篇介绍 topic modeling, 以及一个经典的算法Latent Dirichlet allocation, 文本挖掘与语义理解的集大成者(至少在深度学习统治之前). 当然LDA不仅仅局限于文本, 还可应用于涉及大量数据集的各种问题，包括协同过滤，基于内容的图像检索和生物信息学等领域的数据。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"topic-modelling\"\u003eTopic Modelling\u003c/h2\u003e\n\u003cp\u003e大规模文本挖掘的核心问题, 就是用数学模型代替人力来理解文本语义，目标是找到对集合成员（如一堆文本）的数学/统计描述，以便能够对这些大型集合进行高效处理，同时保留对基本任务（如分类，检测，摘要以及相似性和相关性判断）有用的基本统计关系。\u003c/p\u003e\n\u003cp\u003e在这方面的研究方法很多，特别是信息检索(IR)领域. 一个基本方法是将语料库中的每个文档向量化，向量中的每个实数代表计数率。比如经典的tf-idf方法，用\u003cstrong\u003eDocument-Term Matrix\u003c/strong\u003e来表达不同词在不同文档出现的情况差异, 一般term就是word作为features, 所以在这里我们表示document-word matrix(DWM), 就是\u003ccode\u003eDWM[i][j] = The number of occurrences of word_j in document_i\u003c/code\u003e.\nDoc 1: I have a fluffy cat.\nDoc 2: I see a fluffy dog.\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDWM\u003c/th\u003e\n          \u003cth\u003eI\u003c/th\u003e\n          \u003cth\u003ehave\u003c/th\u003e\n          \u003cth\u003ea\u003c/th\u003e\n          \u003cth\u003efluffy\u003c/th\u003e\n          \u003cth\u003ecat\u003c/th\u003e\n          \u003cth\u003esee\u003c/th\u003e\n          \u003cth\u003edog\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003edoc1\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e0\u003c/td\u003e\n          \u003ctd\u003e0\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003edoc2\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e0\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e0\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e然后进行normalization, 去和 inverse document frequency count(IDF)进行比较. IDF统计每个词在整个文档集合中出现的总次数, 通常转化为log scale, 并进行适当的normalization.\u003c/p\u003e","title":"Topic Modelling - 主题建模以及隐变量模型"},{"content":"参考 CS229: Machine Learning, Stanford\n什么是机器学习？目前有两个定义。\n亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。\n汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义： E：经验，即历史的数据集。 T：某类任务。 P：任务的绩效衡量。 若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习 “如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。 例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。\nSupervised Learning Linear Regression Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y Intercept term: to simplify notation, introduce the convention of letting x0 = 1 Cost function J(θ): a function that measures, for each value of the θ’s, how close the h(x(i))’s are to the corresponding y(i)’s Purpose: to choose θ so as to minimize J(θ). Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ). LMS(least mean squares) algorithm: gradient descent learning rate error term batch gradient descent：looks at every example in the entire training set on every step stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only. particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent. The normal equations performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θj’s, and setting them to zero. To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices\nMatrix derivatives: the gradient ∇Af(A) is itself an m-by-n matrix, whose (i, j)-element is ∂f/∂Aij Least squares revisited: Given a training set, define the design matrix X to be the m-by-n matrix (actually m-by-n + 1, if we include the intercept term) that contains the training examples’ input values in its rows, let y be the m-dimensional vector containing all the target values from the training set, used the fact that the trace of a real number is just the real number( trace operator, written “tr.” For an n-by-n matrix A, the trace of A is defined to be the sum of its diagonal entries: trA = ΣAii To minimize J, find its derivatives with respect to θ: ∇θJ(θ) = XTXθ − XTy To minimize J, we set its derivatives to zero, and obtain the normal equations: XTXθ = XTy Thus the value of θ that minimizes J(θ) is given in closed form by the equation: θ = (XTX)-1XTy Probabilistic interpretation why the least-squares cost function J is a reasonable choice? With a set a probabilistic assumptions, under which least-squares regression is derived as a very natural algorithm.\nLocally weighted linear regression (LWR) algorithm assuming there is sufficient training data, makes the choice of features less critical.\nIn the original linear regression algorithm, to make a prediction at a query point x (i.e., to evaluate h(x)), we would: Fit θ to minimize Σi(y(i) − θTx(i))2. Output θTx. The locally weighted linear regression algorithm does the following: Fit θ to minimize Σiw(i)(y(i) − θTx(i))2. Output θTx. Here, the w(i)’s are non-negative valued weights Intuitively, if w(i) is large for a particular value of i, then in picking θ, we’ll try hard to make (y(i) − θTx(i))2 small. If w(i) is small, then the error term will be pretty much ignored in the fit. A fairly standard choice for the weights is w(i) = exp(-(x(i)-x)2 / 2τ2 ) if |x(i)-x| is small, then w(i) ≈ 1; if large, then w(i) is small. Hence, θ is chosen giving a much higher “weight” to the (errors on) training examples close to the query point x. The parameter τ controls how quickly the weight of a training example falls off with distance of its x(i), from the query point x; τ is called the bandwidth parameter Classification and logistic regression Logistic regression logistic function or the sigmoid function: g(z) = (1 + e−z)-1. g(z) tends towards 1 as z → ∞, and g(z) tends towards 0 as z → −∞. derivative of the sigmoid function: g(z)\u0026rsquo; = g(z)(1 - g(z)) endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likelihood: Similar to our derivation in the case of linear regression, we can use gradient ascent to maximize the likelihood. updates will therefore be given by θ := θ + α∇θℓ(θ). (Note the positive rather than negative sign in the update formula, since we’re maximizing,rather than minimizing, a function now.) This therefore gives us the stochastic gradient ascent rule: θj := θj + α(y(i)− hθ(x(i)))x(i)j If we compare this to the LMS update rule, we see that it looks identical; but this is not the same algorithm, because hθ(x(i)) is now defined as a non-linear function of θTx(i). * Nonetheless, it’s a little surprising that we end up with the same update rule for a rather different algorithm and learning problem. Is this coincidence, or is there a deeper reason behind this? Check GLM models. Generalized Linear Models The exponential family Bernoulli distributions Gaussianexponential distributions multinomial Poisson (for modelling count-data) beta and the Dirichlet (for distributions over probabilities) Constructing GLMs Ordinary Least Squares Logistic Regression Softmax Regression Softmax Regression Consider a classification problem in which the response variable y can take on any one of k values, so y ∈ {1, 2, . . . , k}. We will thus model it as distributed according to a multinomial distribution.\nparameterize the multinomial with only k − 1 parameters, φ1, . . . , φk−1, where φi = p(y = i; φ), and p(y = k; φ) = 1 − Σki=1φi. To express the multinomial as an exponential family distribution, we will definee T(y) ∈ Rk-1： * η = [log(φ1/φk),\u0026hellip;,log(φk-1/φk)], the ηi’s are linearly related to the x’s. * softmax function: a mapping from the η’s to the φ’s: φi = eηi / Σkj=1eηi softmax regression: the model, which applies to classification problems where y ∈ {1, . . . , k}: p(y = i|x; θ) = φi = eθTi x / Σkj=1eθTi x This hypothesis will output the estimated probability that p(y = i|x; θ), for every value of i = 1, . . . , k. parameter fitting: obtain the maximum likelihood estimate of the parameters by maximizing ℓ(θ) in terms of θ, using a method such as gradient ascent or Newton’s method. Naive Bayes classification 朴素贝叶斯 以二元分类为例: 根据A和B各自的先验概率和条件概率, 算出针对某一特征事件的后验概率, 然后正则化(正则化后两个后验概率之和为1, 但不影响对事件的触发对象是A或B的判断)\nWhy naïve: 忽略了事件发生的顺序, 故称之为\u0026quot;朴素\u0026quot; Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了 详见加速自然语言处理-朴素贝叶斯 Problem: how to deal with continuous values features? Use Gaussian Naive Bayes. Gaussian Naive Bayes With real-valued inputs, we can calculate the mean and standard deviation of input values (x) for each class to summarize the distribution. This means that in addition to the probabilities for each class, we also store the mean μ and standard deviations σ of each feature for each class.\nThe class conditional probability P(x|c) is estimated by probability density of the normal distribution : Algorithm – continuous Xi (but still discrete Y) Train Naïve Bayes (examples) for each class value yk: estimate P(Yk) for each attribute Xi: estimate class conditional mean, variance Classify(xnew): Ynew \u0026lt;- argmax(k) ∏P(xi|Yk)P(Yk) Short: classes with the same distribution Missing data instances in NB Ignore attribute in instance where its value is missing compute likelihood based on observed attribtues no need to “fill in” or explicitly model missing values based on conditional independence between attributes Generative and Discriminative Algorithm: Generative classifiers learn a model of the joint probability, p(x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(y|x), and then picking the most likely label y. Discriminative classifiers model the posterior p(y|x) directly, or learn a direct map(hypothesis/functions) from inputs x to the class labels. Generative models advantage: Can be good with missing data, naive Bayes handles missing data good for detecting outliers to generate likely input (x,y). Decision trees 决策树 Algorithm: ID3 algorithm Decision trees with continuous attributes: Create split based on threshold ID3 algorithm Recursive Split( node, {examples} ): 1. A \u0026lt;- the best attribute for splitting the {examples} 2. For each value of A, create new child node 3. Split training {examples} to child nodes 4. For each child node, subset: * If subset is pure - stop * Else: split(child_node, {subset} ) How to decide which attribute is the best to split on: Entropy\nEntropy Use log2 here is to represent concepts of information - on average how many bits needed to tell X split purity To represent two classes, need one bit \u0026ldquo;0, 1\u0026rdquo;, to represent 4 classes, need 2 bits \u0026ldquo;00, 01, 10, 11\u0026rdquo; If x is pure(one class only), entropy is 0. Information Gain: Expected drop in entropy after split, Gain( P, C) = Entropy(parent) - Σw*Entropy(children), w is weighted average matrix., A is the split attribute Problems: tend to pick attributes with lots of values, could not generalize well on new data. use GainRation: for attribute A with many different values V, the SplitEntropy will be large, Overfitting in Decision Trees the tree split too deep to try to classify almost every single sample. As a result the model could not predict new data well.\nSub-tree replacement pruning For each node: Pretend remove node + all children from the tree Measure performance on validation set Remove node that results in greatest improvement Repeat until further pruning is harmful Decision boundary Logistic Regression and trees differ in the way that they generate decision boundaries\nDecision Trees bisect the space into smaller and smaller regions, Logistic Regression fits a single line/hyperplane to divide the space exactly into two. Random Decision forest Grow K different decision trees: pick a random subset Sr of training examples grow a full ID3 tree (no prunning): When splitting: pick from d\u0026laquo;D random attributes Computing gain based on Sr instead of full set repeat for r =1…K Given a new data point X: classify X using each of the trees T1 …. Tk use majority vote: class predicted most often SVM Intuition: Suppose there is a good hyperplane to seperate data set, h(x)=g(wTx+b), (relation with fully connected layer and activation funciton in DNN). Want functional margin of hyperplane to be large: for dataset (xi,yi), functional margin γi = yi(wTxi+b), if yi=1, need wTxi+b\u0026raquo;0, if yi=-1, need wTxi+b\u0026laquo;0. Thus γi\u0026gt;0 means the classification is correct. Geometric margins: Define the hyperplane as wTx+b=0, the normal of the hyperplane is w/||w||, thus a point A(xi)\u0026rsquo;s, which represents the input x(i) of some training example with label y(i) = 1, projection on the hyperplane is point B = xi - γi·w/||w||, where γi is xi\u0026rsquo;s distance to the decision boundary. Thus wT(xi - γi·w/||w||) + b=0 =\u0026gt; γi = (w/||w||)Txi+ b/||w||. More generally, the geometric margin of (w, b) with respect to a training example (xi, yi) is γi = yi· (w/||w||)Txi+ b/||w|| If ||w|| = 1, then the functional margin equals the geometric margin The optimal margin classifier: Given a training set, a natural desideratum is to try to find a decision boundary that maximizes the minimum (geometric) margin, i.e want min(γi) as large as possible. Via some transformation, the object turns to minimize ||w||2, subject to y(i)·(wTxi+b) ≥ 1, Lagrange duality: solving constrained optimization problems. w = Σαiyixi, αi is Lagrange multipliers. Support vector: The points with the smallest margins. The number of support vectors can be much smaller than the size the training set Training: fit our model’s parameters to a training set, and now wish to make a prediction at a new point input x. We would then calculate wTx + b, and predict y = 1 if and only if this quantity is bigger than zero. . In order to make a prediction, we have to calculate it which depends only on the inner product between x and the points in the training set. Moreover, αi’s will all be zero except for the support vectors. Thus, many of the terms in the sum above will be zero, and we need to find only the inner products between x and the support vectors (of which there is often only a small number) in order to make our prediction. The inner product \u0026lt;xi,x\u0026gt; could be replaced by kernel k(xi,x) Kernels Define the “original” input value x as the input attributes of a problem. When that is mapped to some new set of quantities that are then passed to the learning algorithm, we’ll call those new quantities the input features. φ denote the feature mapping, which maps from the attributes to the features. E.g. φ(x) = [x, x^2, x^3] given a feature mapping φ, we define the corresponding Kernel to be K(x, z) = φ(x)Tφ(z) Often, φ(x) itself may be very expensive to calculate (perhaps because it is an extremely high dimensional vector, require memory), K(x, z) may be very inexpensive to calculate. We can get SVMs to learn in the high dimensional feature space given by φ, but without ever having to explicitly find or represent vectors φ(x). E.g. Based on Mercer’s Theorem, you can either explicitly map the data with a φ and take the dot product, or you can take any kernel and use it right away, without knowing nor caring what φ looks like Keep in mind however that the idea of kernels has significantly broader applicability than SVMs. Specifically, if you have any learning algorithm that you can write in terms of only inner products \u0026lt;x, z\u0026gt; between input attribute vectors, then by replacing this with K(x, z) where K is a kernel, you can allow your algorithm to work efficiently in the high dimensional feature space corresponding to K. SVM vs. Logistic regression Logistic regression focuses on maximizing the probability of the data. The further the data lies from the separating hyperplane (on the correct side), the happier LR is. An SVM don’t care about getting the right probability, i.e the right P(y=1|x), but only care about P(y=1|x)/P(y=0|x)≥ c. It tries to find the separating hyperplane that maximizes the distance of the closest points to the margin (the support vectors). If a point is not a support vector, it doesn’t really matter. P(y=1|x)/P(y=0|x) \u0026gt; c, if c=1, that means P(y=1|x) \u0026gt; P(y=0|x), thus y=1, take log of both side, and plug in P(y=1|x) = sigmoid(wTx + b), P(y=0|x)=1-P(y=1|x), recall the sigmoid, we get wTx + b \u0026gt; 0 Underlying basic idea of linear prediction is the same, but error functions differ, the r = P(y=1|x)/P(y=0|x) = exp(wTx + b), different classifiers assigns different cost to r If cost(r)=log(1 + 1/r), this is logistic regression If cost(r)=max(0, 1-log(r))=max(0, 1-(wTx + b)), then SVM Logistic regression (non-sparse) vs SVM (hinge loss, sparse solution) Linear regression (squared error) vs SVM (ϵ insensitive error) K Nearest Neighbour Intuition: predict based on nearby/similar training data.\nAlgorithm: for a test data compute its distance to every training example xi select k closest training instances prediction: For Classification: predict as the most frequent label among the k instances. For regression: predict as the mean of label among the k instances. Choose k large k: everything classified as the most probable class small k: highly variable, unstable decision boundaries affects “smoothness” of the boundary Use train-validation to choose k Distance meansures: Euclidian: symmetric, spherical, treats all dimensions equally, but sensitive to extreme differences in single attribtue Hamming: number of attribtues that differ Resolve ties: random prior: pick class with greater prior nearest: use 1-NN classifier to decide Missing values: have to fill in the missing values, otherwise cannot compute distance. Pro and cons: Almost no assumptions about data easy to update in online setting: just add new item to training set Need to handle missing data: fill-in or create a special distance Sensitive to outliers Sensitve to lots of irrelevant aeributes (affect distance) Computationally expensive: need to compute distance to all examples O(nd) - Vectorization Faster knn: K-D Trees, Inverted lists, Locality-sensitive hashing K-D Trees low-dimensional, real-valued data\nA kd-tree is a binary tree data structure for storing a fi\fnite set of points from a k-dimensional space. Build the tree: Pick random dimension, Find median, Split data Nearest neighbor search: Traverse the whole tree, BUT make two modifications to prune to search space: Keep variable of closest point C found so far. Prune subtrees once their bounding boxes say that they can’t contain any point closer than C Search the subtrees in order that maximizes the chance for pruning Inverted lists high-dimensional, discrete data, sparse\nApplication: text classification, most attribute values are zero (sparseness), training: list all training examples that contain particular attribute Testing: merge inverted list for attribtues presented in the test set, and choose those instances in the new inverted list as the neighbours Locality-sensitive hashing high-d, discrete or real-valued\nUnsupervised learning 无监督学习 Clustering K-means split data into a specified number of populations\nInput: K (number of clusters in the data) Training set {x1, x2, x3 \u0026hellip;, xn) Algorithm: Randomly initialize K cluster centroids as {μ1, μ2, μ3 \u0026hellip; μK}, now centroid could represent cluster. Repeat until converge: Inner loop 1: repeatedly sets the c(i) variable to be the index of the closes variable of cluster centroid closes to xi, i.e. take ith example, measure squared distance to each cluster centroid, assign c(i)to the closest cluster(centroid) Inner loop 2: For each cluster j, new centroid c(j) = average mean of all the points assigned to the cluster j in previous step. Target (Distortion) function: J(c,μ)=Σ|| xi-μi ||^2, coordinate ascent, decrease monotonically, thus guarantee to converge. What if there\u0026rsquo;s a centroid with no data: Remove that centroid, so end up with K-1 classes, Or, randomly reinitialize it, not sure when though\u0026hellip; How to choose cluster numbers: scree plot to find the best k. Hierarchical K-means A Top-down approach run k-means algorithm on the original dataset for each of the resulting clusters, recursively run k-means Pro cons: Fast nearby points may end up in different clusters Agglomerative Clustering A bottom up algorithm:\n1. starts with a collections of singleton clusters 2. repeat until only one cluster is left: 1. Find a pair of clusters that is closest 2. Merge the pair of clusters into one new cluster 3. Remove the old pair of clusters Need to define a distance metric over clusters Produce a dendrogram: Hierarchical tree of clusters slow Gaussian Mixtures For non-Gaussian distribution data, assume it is a mixture of several(k) Gaussians.\nAlgorithm: EM EM algorithm strategy will be to repeatedly construct a lower-bound on ℓ(E-step) based on Jensen’s inequality, and then optimize that lower-bound(M-step).\nE step: For each i, let Qi be some distribution over the z’s (ΣzQi(z) = 1, Qi(z) ≥ 0). z(i) indicating which of the k Gaussians each x(i) had come from, get P(Z)=φ, then compute the conditional probability wj as P(x|Z) via Gaussian Naive Bayes: M step: maximize, with respect to our parameters φ, µ, Σ, the quantity, by updating parameter(φ, µ, σ) 举例：start with two randomly placed Gaussians (μa, σa), (μb, σb), assume a uniform prior (P(a)=P(b)=0.5), iterate until convergence: E-step: for each point: P(b|xi), P(a|xi)=1-P(b|xi) , does it look like it came from b or a? M-step: adjust (μa, σa) and (μb, σb) to fit points soft assigned to them, The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the “hard” cluster assignments c, we instead have the “soft” assignments w. Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea. How to pick k: cannot discover K, likelihood keeps growing with K K-means vs. EM Dimensionality Reduction Pros: reflects human intuitions about the data allows estimating probabilities in highadimensional data: no need to assume independence etc. dramatic reduction in size of data: faster processing (as long as reduction is fast), smaller storage Cons too expensive for many applications (Twitter, web) disastrous for tasks with fine-grained classes understand assumptions behind the methods (linearity etc.): there may be better ways to deal with sparseness Factor analysis If the features n ≫ m, or n≈m, in such a problem, it might be difficult to model the data even with a single Gaussian, 更别提高斯混合了. Because the variance matrix Σ becomes singular - non invertable.\nPrincipal Components Analysis PCA, automatically detect and reduce data to lower dimension k, k \u0026laquo; n, preserve dimenson that affects class separability most.\nAlgorithm: Pre-process: data normalization to 0 mean and unit variance, Steps (3-4) may be omitted if we had apriori knowledge that the different attributes are all on the same scale to project data into a k-dimensional subspace (k \u0026lt; n), we should choose e1,\u0026hellip; ek to be the top k eigenvectors of Σ. The e’s now form a new, orthogonal basis for the data. To represent a training data point x with d dimension into this basis (k dimension), e1Tx,\u0026hellip;ekTx The vectors u1,\u0026hellip;, uk are called the first k principal components of the data. Eigenvalue λi = variance along ei. Pick ei that explain the most variance by sorting eigenvectors s.t. λ1 ≥ λ2 ≥…≥ λn pick first k eigenvectors which explain 90% or 95% of the total variance Σλ(i). Maximize the variance of projection of x onto a unit vector u, Application: eigenfaces Linear Discriminant Analysis LDA\nIdea: pick a new dimension that gives maximum separation between means of projected classes minimum variance within each projected class How: eigenvectors based on between-class and within-class covariance matrices LDA not guaranteed to be better for Classification assumes classes are unimodal Gaussians fails when discriminatory information is not in the mean, but in the variance of the data Singular Value Decomposition Generalization and evaluation Receiver Operating Characteristic ROC, plot TPR(Sensitivity) vs. FPR(Specificity) as t varies from ∞ to -∞, shows performance of system across all possible thresholds A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner. Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test AUC: area under ROC curve, popular alternative to Accuracy Confidence interval tell us how closed our estimation\nE = probability that misclassify a random instance: Take a random set of n instances, how many misclassified? Equal to Binomial distribution with mean = nE, variance = nE(1-E) Efuture: the next instance\u0026rsquo;s probability of misclassified = average #misclassifed = variance / n = mean E= E(1-E)/n, small variance means big confidence interval, a Gaussian distribution with one variance distance extend from mean will cover 2/3 future test sets p% Confidence interval for future error, 95% confidence interval needs about 2 variance extends from mean. .\n","permalink":"https://congchan.github.io/posts/machine-learning-note-cs229-stanford/","summary":"\u003cp\u003e参考\n\u003ca href=\"http://cs229.stanford.edu/notes\"\u003eCS229: Machine Learning, Stanford\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e什么是机器学习？目前有两个定义。\u003c/p\u003e\n\u003cp\u003e亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。\u003c/p\u003e\n\u003cp\u003e汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义：\nE：经验，即历史的数据集。\nT：某类任务。\nP：任务的绩效衡量。\n若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习\n“如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。\n例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"supervised-learning\"\u003e\u003ca href=\"http://cs229.stanford.edu/notes/cs229-notes1.pdf\"\u003eSupervised Learning\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"linear-regression\"\u003eLinear Regression\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWeights(parameters) θ: parameterizing the space of linear functions mapping from X to Y\u003c/li\u003e\n\u003cli\u003eIntercept term: to simplify notation, introduce the convention of letting x\u003csub\u003e0\u003c/sub\u003e = 1\u003c/li\u003e\n\u003cli\u003eCost function J(θ): \u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/linearR_cost.png\"\u003e  a function that measures, for each value of the θ’s, how close the h(x\u003csup\u003e(i)\u003c/sup\u003e)’s are to the corresponding y\u003csup\u003e(i)\u003c/sup\u003e’s\u003c/li\u003e\n\u003cli\u003ePurpose: to choose θ so as to minimize J(θ).\u003c/li\u003e\n\u003cli\u003eImplementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"lmsleast-mean-squares-algorithm\"\u003eLMS(least mean squares) algorithm:\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003egradient descent\u003c/li\u003e\n\u003cli\u003elearning rate\u003c/li\u003e\n\u003cli\u003eerror term\u003c/li\u003e\n\u003cli\u003ebatch gradient descent：looks at every example in the entire training set on every step\u003c/li\u003e\n\u003cli\u003estochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to\nthe gradient of the error with respect to that single training example only.\u003c/li\u003e\n\u003cli\u003eparticularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"the-normal-equations\"\u003eThe normal equations\u003c/h4\u003e\n\u003cp\u003eperforming the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θ\u003csub\u003ej\u003c/sub\u003e’s, and setting them to zero.\nTo enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices\u003c/p\u003e","title":"Machine Learning Note - cs229 - Stanford"},{"content":"Scikit-learn 提供一套实用的工具，用于解决机器学习中的实际问题，并配合适当的方法来制定解决方案。\n涉及数据和模型简介，决策树，误差的作用，最小化误差，回归拟合，逻辑回归，神经网络，感知器，支持向量机，朴素贝叶斯，降维，K均值，简单高斯混合模型，分层聚类，模型评估。\n实验和代码在GitHub; 练习作业答案可以参考GitHub\n","permalink":"https://congchan.github.io/posts/machine-learning-with-scikit-learn-sklearn-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/","summary":"\u003cp\u003eScikit-learn 提供一套实用的工具，用于解决机器学习中的实际问题，并配合适当的方法来制定解决方案。\u003c/p\u003e\n\u003cp\u003e涉及数据和模型简介，决策树，误差的作用，最小化误差，回归拟合，逻辑回归，神经网络，感知器，支持向量机，朴素贝叶斯，降维，K均值，简单高斯混合模型，分层聚类，模型评估。\u003c/p\u003e\n\u003cp\u003e实验和代码在\u003ca href=\"https://github.com/JamesOwers/iaml2017\"\u003eGitHub\u003c/a\u003e;\n练习作业答案可以参考\u003ca href=\"https://github.com/ShootingSpace/Machine-learning-practical-with-scikit-learn\"\u003eGitHub\u003c/a\u003e\u003c/p\u003e","title":"Machine Learning with Scikit-learn (Sklearn) 机器学习实践"},{"content":"语言模型 语言模型Language modeling（LM）最初是针对语音识别问题而开发的, 现在广泛用于其他NLP应用中, 比如机器翻译需要利用LM来给翻译出的句子打分.\n假设我们有一个语料库 - 某种语言的句子的无限集合$\\mathcal{V^+}$（这些句子是由有限的词$\\mathcal{V}$组成的）。例如，我们可能从网上获得大量文本。给定了此语料库，我们想估计LM的参数。这些参数包含语料库中所有单词的有限集合$\\mathcal{V}$, 以及句子的概率分布函数$p(x_1, x_2, ..., x_n)$，必须满足\nFor any $\\langle x_1...x_n \\rangle \\in \\mathcal{V^+}$, $p(x_1, x_2, ..., x_n) ≥ 0$ $\\sum_{\\langle x_1...x_n \\rangle \\in \\mathcal{V^+}}p(x_1, x_2, ..., x_n) = 1$ 比如，当$\\mathcal{V}$只有cat, eat, fish, 那么它组合成的句子按照人类的评价标准, 通顺程度从高到低是: cat eat fish, fish eat cat, cat fish eat, eat cat fish, eat fish cat, fish cat eat. 这些是可能出现的句子(还没出现的不代表未来不会出现), 从概率分布的角度看待, 这些句子的概率之和是1, 因为这三个词只能组成这几个句子. 而LM的意义就在于能够赋予cat eat fish最大的概率, 代替人来判断句子是否准确, 通俗的说是一个句子通顺打分机器.\n广义的语言模型, 可以计算任何连续的单词或者任何其他序列数据（比如语音）出现的概率, 当然是以参数的训练样本的角度来看待。除了为每个词序列指定概率之外，语言模型还指定给定的单词（或单词序列）跟随前面的单词序列的似然概率。\n语言模型本身即是一种概率模型. 概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟一个事件发生的概率。\nLM的任务就是为单词序列$w_{1:n}$分配概率$P(w_{1:n})$, 等同于给序列的每个位置预测可能出现的单词，给定前面的单词（作为条件），预测下一个单词出现的概率 P(w|w1, w2, w3...)。听起来有点像词性标注(Tagging)\u0026hellip; 事实上最初为语言建模开发的参数估计技术也给词性标注做了不少贡献.\n利用链式法则, $$P(w_{1:n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})P(w_4|w_{1:3})...P(w_n|w_{1:n-1}),$$ 最后一项基于n-1个词的条件概率计算难度非常大。为了简化LM参数的训练，利用k阶马尔可夫假设，声明序列的下一个词仅依赖于前k个词。如利用一阶马尔可夫假设得到P(transparent | the water is so ) ≈ P(transparent | so).\n使用马尔可夫假设简化前面的乘链: $$\\begin{align} P(w_{1:n}) \u0026= \\prod_{i=1}^n P(w_i | w_1, ..., w_{i-1}) \\\\\\\\ \u0026\\propto \\prod_{i=1}^n P(w_i | w_{i-k}, ..., w_{i-1}) \\end{align}$$ 在语料处理时，开头的句子前面需要相应的加上k个补丁符号\u0026lt;s\u0026gt;，才能计算第一个词的条件概率。LM也是一种生成模型, 一般是在句子末尾加上特殊符号\u0026lt;/s\u0026gt;表示句子结束, 以方便生成任务时判断句子的生成结束.\n固然Markov假设对于任意k阶都是有偏差的（毕竟句子可以有任意长的依赖性），但仍可以使用较小的k建模出较强的LM，并且几十年来一直是语言建模的主要方法。\n对于LM参数中每一项似然概率的估算，可以使用最大似然估计（MLE）：\n$$P(w_{i}=m|w_{i-k:i-1}) = \\frac{Count(w_{i-k:i})}{Count(w_{i-k:i-1})}$$这个就是经典的N-gram模型。\nN-Gram语言模型 N-Gram语言模型是基于N-1阶马尔可夫假设且由MLE估算出的LM。N-GramLM 预测下一个单词出现概率仅条件于前面的(N-1)个单词, 以The students opened their books为例:\nBi-gram: 统计$P(w_{i}=m|w_{i-1})$, P(students | the), P(opened | students), \u0026hellip;, 属于马尔可夫一阶模型, 即当前t时间步的状态仅跟t-1相关. Tri-gram: P(students | \u0026lt;/s\u0026gt; The), P(opened | The students), 马尔可夫二阶模型 Four-gram: 依此类推 特殊的Uni-gram: 统计$P(w_i)$, P(the), P(students), \u0026hellip;, 此时整个模型退化为词袋模型, 不再属于马尔可夫模型, 而是基于贝叶斯假设, 即各个单词是条件独立的. 所以一般N-gram是指N\u0026gt;1的.\nN-Gram模型因为使用MLE估算参数，缺点很明显：\n无法很好地解决NLP中的长距离依赖现象, 比如一般表现比较好的Trigram语言模型，没有考虑到两步之外的词 没有考虑词的相似性，泛化能力差。比如在训练集出现了The cat is walking in the bedroom,理论上应该泛化到给A dog was running in a room, 因为dog和cat(resp. “the” and “a”, “room” and “bedroom”, etc\u0026hellip;)有类似的语义和语法定位. N-gram只是在测试语料库与训练语料库比较相似时表现才比较好。否则基于训练语料训练出来的参数肯定无法很好地评估测试语料，就像人无法对其不认识的语言做任何语法句法上的评价。 稀疏问题1：大多数高阶Gram几乎不会出现，虽然u v w在训练语料中从来没有出现过, 但我们不能简单地把P(w | u, v)定义为0，因为语言是千变万化的，有些词组虽然少见但不代表不存在。句子的概率是由各个gram似然概率相乘而来，如果仅仅因为一个词组出现次数为0就导致整个句子概率变为0, 那显然是不合理的. 稀疏问题2：部分低阶gram没有出现过，低阶gram的次数作为MLE公式中分母变为0，那计算就没法进行下去了. 一般而言，N越高，模型表现越好，但是更大的N使稀疏问题变得更糟。通常人们不会取大于5的N。 需要存储所有可能的N-Gram，所以模型的大小是 O(exp(n)), 需要大量的内存，而其实大部分都是出现次数为0. 平滑 针对数据稀疏问题（0概率的问题）, 可以使用各种平滑处理（Smoothing）.\n加一（Laplace）平滑：最简单的平滑法，为所有事件（不管有没出现过）的频次加一，这样保证了没有0概率事件出现。这种平滑效果很差，因为齐夫定律Zipf's law的关系\nZipf's law：在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。\n会有很多长尾单词很少甚至几乎没有出现过, 所以在总数为1的概率池子里, 为了给这些长尾单词分配至少频次1的概率, 需要从真正出现的单词(所谓真实发生的事件)中分走很多概率.\n因此可以给Laplace平滑加入控制因子，变为 Add alpha smoothing。更多平滑方案参考UoE-anlp\n语言模型评估方法 既然LM是用于评估句子是否准确的模型，那么在评价LM好坏时，就要看它在测试集上的表现如何。给定测试集包含$m$个句子$x^{(1)}, x^{(2)}, ..., x^{(m)}$, 各个句子的长度分别为$n_i$. LM给这些测试集句子评估的概率大小为\n$$\\prod_{i=1}^m p(x^{(i)})$$ 这个数值越高，说明LM评估测试集句子的质量越好。注意, 测试集必须是完全没有参与模型训练, 且是在人类标准中是好的句子.\n但在实际使用中, 我们往往使用上面这个概率的一种变换 - 困惑度（Perplexity）来评价LM的质量. 首先取整个测试语料库的对数概率除以测试语料库中的单词总数$M$: $$l = \\frac{1}{M} \\log_2 \\prod_{i=1}^m p(x^{(i)}) = \\frac{1}{M} \\sum_{i=1}^m \\log_2 p(x^{(i)})$$ 然后得到 $$\\begin{align} Perplexity \u0026= 2^{-l} \\\\\\\\ \u0026= 2^{-\\frac{1}{M} \\sum_{i=1}^m \\log_2 p(x^{(i)})}\\\\\\\\ \\\\\\\\ \u0026= t^{-1} \\end{align}$$ 其中，$t = \\sqrt[\\leftroot{-2}\\uproot{2}M]{\\prod_{i=1}^m p(x^{(i)})}$, 作为测试集概率的几何平均. 例如，如果困惑等于100，则$t = 0.01$，表明几何平均值为0.01. 可以看到, Perplexity的值越小，语言模型建模测试集的能力就越好.\n概率取对数转换可以避免数值下溢，可以把乘法转换为加法, 计算也更快.\n困惑度为何就是一种好的衡量标准呢？对于任何一个任务，我们需要定义Baseline模型作为基准，如果后续有一个新的模型，但无法超过此baseline，那么我们认为这个新的模型是没有进步的。对于语言建模这一个任务，最无脑最简单的baseline，就是假设每一个位置的每个单词出现概率相等，这就是最大熵分布，即假设此baseline对这个任务一无所知，所有位置所有单词在它眼里都是没区别的(均匀分布)。如果词汇集(包含\u0026lt;/s\u0026gt;)大小为N, 那么\n$$P_{i \\in T}(w_i | w_{1:i-1}) = \\frac{1}{N},$$ 此时的困惑度等于N, 即在均匀概率分布模型下，困惑度等于词汇量的大小。显而易见任何一个有效模型的困惑度必须小于类别个数. 此时困惑度可以理解为模型的有效词汇量：例如，词汇量大小为10,000, 而模型的困惑度为120，那么这大致说明有效的词汇量只有大概120个。最佳情况下，模型总是把测试集的概率预测为 1, 此时困惑度为 1。最坏情况下，概率预测为 0, 此时困惑度为正无穷。Baseline模型总是预测所有类别的概率都相同, 此时困惑度为词汇量大小（类别个数）。\n目前很多神经网络框架计算语言模型的损失函数都是用交叉熵损失函数并取对数, 要得到perplexity，只需要把这个loss取指数运算。\n那么困惑度一般都是多大呢？Goodman (“A bit of progress in language modeling”, figure 2) 评估了在英语数据上的unigram，bigram和trigram语言模型，词汇量为50,000。Goodman的报告结果显示，trigram模型的困惑度约为74，bigram模型为137，unigram模型为955。相比于Baseline模型困惑度50,000，trigram模型显然有了巨大的改进，且比bigram和unigram模型也有很大的改进。而更强大的SOTA神经语言模型，可以在wikitext-2数据集上跑出40以下的困惑度。\n神经网络语言模型 神经网络模型解决了传统语言模型的一些缺点：它们允许越来越长的距离依赖，而参数数量仅线性增加，它们减少了手动设计backoff顺序的需要，并且它们支持跨不同上下文的泛化。\nBengio et al. [2003]提出的神经网络语言模型(NNLM, 确切的说是前馈神经网络语言模型), 把文本处理成n个k-gram词窗口$w_{i:i+k-1}$, 每个词转换为词镶嵌的形式$\\mathcal{v}(w) \\in \\mathcal{R}^{d_w}$, 一整个窗口的词向量拼接为矩阵向量$x = [\\mathcal{v}(w_0); ...; \\mathcal{v}(w_{k-1})]$, 作为输入数据输入到一个1到2层的感知机.\n训练数据的处理一般这么操作, 每个句子的开头加上\u0026lt;s\u0026gt;, 末尾加上\u0026lt;/s\u0026gt;, 然后按照k大小的长度一段段截断成k-gram词窗口$w_{i:i+k-1}$. 每一段k-gram的词拼接为一个向量$x = (C(w_{i}), C(w_{i+1}), ···, C(w_{i+k-1}))$, 作为一个训练样本, 其末尾的下一个词$w_{i+k}$作为样本对应的预测标签$y_i = \\mathcal{v}(w_{i+k})$. 训练时，以输出的词向量概率分布向量和对应正确标签的 one-hot-vector 间的 cross-entropy loss 为损失函数.\n神经网络的参数数量比传统的N-gram少，因为其每增加一个词，参数就多$d_w$, 也就是线性增加, 而N-gram是多项式增加速率. 并且NNLM的参数矩阵对所有输入都是共享的, 这进一步减少了参数量. 虽然如此, NNLM的训练时间还是比N-gram LM长.\n神经网络语言模型的泛化能力更好，因为相似的词具有相似的特征向量，并且因为概率函数（模型参数）是这些特征值的平滑函数，所以特征的微小变化相应地引起概率的微小变化。\n真正影响NNLM计算效率的是输出层的softmax计算, 因为训练样本的词汇量$\\mathcal{V}$往往很大. 输出层的softmax需要与隐含层参数矩阵$W^2 \\in \\mathcal{R}^{d_{hid} \\times \\mathcal{V}}$进行昂贵的矩阵向量乘法, 然后进行$\\mathcal{V}$次对数操作. 这部分计算占据了大部分运行时间，使得大词汇量的NNLM建模令人望而却步。\n后续发展的NNLM普遍使用循环神经网络（RNN, LSTM）来代替简单的前馈神经网络。循环神经网络可以理解为多层前馈神经网络叠加, 但各神经网络隐含层的参数是共享的. 句子逐词输入循环神经网络, 也就是循环神经网络使用同样参数方程来处理每一个词, 因此循环神经网络的参数量比前馈神经网络更少. 使用循环神经网络作为LM模型时, 同样最后一层还是使用softmax输出层。不同的是输入不再局限于定长的kgram词窗口，LSTM理论上可以接受无限长序列, 但事实上LSTM的记忆能力也是有限的, 太长就会遗忘掉前面的信息.\n对大词汇量语言模型的尝试 Hierarchical softmax [Morin and Bengio, 2005]\nSelf-normalizing aproaches, 比如 noise-contrastive estimation (NCE) [Mnih and Teh, 2012, Vaswani et al., 2013] 或者在训练目标函数中加入正则化项 [Devlin et al., 2014].\n有关处理大输出词汇表的这些和其他技术的良好评论和比较，请参阅 Chen et al. [2016].\n参考资料 class notes by Michael Collins: http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf Neural Network Methods in Natural Language Processing, by Yoav Goldberg\nA Neural Probabilistic Language Model, Yoshua Bengio, 2003\n","permalink":"https://congchan.github.io/posts/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","summary":"\u003ch2 id=\"语言模型\"\u003e语言模型\u003c/h2\u003e\n\u003cp\u003e语言模型Language modeling（LM）最初是针对语音识别问题而开发的, 现在广泛用于其他NLP应用中, 比如机器翻译需要利用LM来给翻译出的句子打分.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e假设我们有一个语料库 - 某种语言的句子的无限集合$\\mathcal{V^+}$（这些句子是由有限的词$\\mathcal{V}$组成的）。例如，我们可能从网上获得大量文本。给定了此语料库，我们想估计LM的参数。这些参数包含语料库中所有单词的有限集合$\\mathcal{V}$, 以及句子的概率分布函数$p(x_1, x_2, ..., x_n)$，必须满足\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFor any $\\langle x_1...x_n \\rangle \\in \\mathcal{V^+}$, $p(x_1, x_2, ..., x_n) ≥ 0$\u003c/li\u003e\n\u003cli\u003e$\\sum_{\\langle x_1...x_n \\rangle \\in \\mathcal{V^+}}p(x_1, x_2, ..., x_n) = 1$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e比如，当$\\mathcal{V}$只有\u003ccode\u003ecat, eat, fish\u003c/code\u003e, 那么它组合成的句子按照人类的评价标准, 通顺程度从高到低是: \u003ccode\u003ecat eat fish\u003c/code\u003e, \u003ccode\u003efish eat cat\u003c/code\u003e, \u003ccode\u003ecat fish eat\u003c/code\u003e, \u003ccode\u003eeat cat fish\u003c/code\u003e, \u003ccode\u003eeat fish cat\u003c/code\u003e, \u003ccode\u003efish cat eat\u003c/code\u003e. 这些是可能出现的句子(还没出现的不代表未来不会出现), 从概率分布的角度看待, 这些句子的概率之和是\u003ccode\u003e1\u003c/code\u003e, 因为这三个词只能组成这几个句子. 而LM的意义就在于能够赋予\u003ccode\u003ecat eat fish\u003c/code\u003e最大的概率, 代替人来判断句子是否准确, 通俗的说是一个句子通顺打分机器.\u003c/p\u003e","title":"语言模型"},{"content":"Bit Map Bit-map用一个bit位来标记某个元素对应的Value， 而Key即是该元素。由于采用了Bit为单位来存储数据，因此在存储空间方面，可以大大节省。\n假设我们要对0-7内的5个元素4,7,2,5,3排序（假设这些元素没有重复）。那么我们就可以采用Bit-map的方法来达到排序的目的。要表示8个数，我们就只需要8个Bit（1Bytes），\n首先我们开辟1Byte的空间，将这些空间的所有Bit位都置为0，0 0 0 0 0 0 0 0. 然后遍历这5个元素，首先第一个元素是4，那么就把4对应的位置设为1, p+(i/8)|(0x01\u0026lt;\u0026lt;(i%8)), 这里默认为Big-ending, 0 0 0 0 1 0 0 0. 然后再处理第二个元素7，将第八位置为1,，接着再处理第三个元素，一直到最后处理完所有的元素，将相应的位置为1，这时候的内存的Bit位的状态0 0 1 1 1 1 0 1 遍历一遍Bit区域，把1的索引依次输出（2，3，4，5，7），这样就达到了排序的目的。 算法的关键是如何确定十进制的数映射到二进制bit位的map图。算法占用很少内存，比如N=10000000；只需占用内存为N/8=1250000Byte=1.25M。缺点是不能有重复数据。\nMap映射表 假设需要排序或者查找的总数N=10000000，那么我们需要申请内存空间的大小为int a[1 + N/32]，其中：a[0]在内存中占32位, 可以对应十进制数0-31，依次类推： bitmap表为：\na[0]---------\u0026gt;0-31 a[1]---------\u0026gt;32-63 a[2]---------\u0026gt;64-95 a[3]---------\u0026gt;96-127 .......... 十进制数需要转换为对应的bit位\n位移转换 将十进制数转换为对应的bit位, 申请一个int一维数组，作为32列的二维数组，\nint a[0] |0000000000000000000000000000000000000| int a[1] |0000000000000000000000000000000000000| ……………… int a[N] |0000000000000000000000000000000000000| 例如十进制0，对应在a[0]第一位： 00000000000000000000000000000001\n求十进制0-N对应在数组a的索引：十进制0-31，对应a[0]，先由十进制数n转换为与32的余可转化为对应在数组a中的索引0。比如n=24,那么 n/32=0，则24对应a[0]。又比如n=60, 那么n/32=1，则60对应a[1]。 求0-N对应0-31中的数：十进制0-31就对应0-31，而32-63则对应也是0-31，即给定一个数n可以通过模32求得对应0-31中的数。 利用移位0-31使得对应32bit位为1. 找到对应0-31的数为M, 左移M位：即2 ^ M, 置1. Bloom Filter 为了降低键值冲突的概率，Bloom Filter使用了多个哈希函数：创建一个m位BitSet，先将所有位初始化为0，然后选择k个不同的哈希函数。第i个哈希函数对字符串str哈希的结果记为h(i, str)，且h(i, str)的范围是0到m-1 。\n对于字符串str，分别计算h(1, str), h(2, str), ... h(k, str), 以这些哈希值作为索引, 将BitSet的对应位置的位设为1, 这样就把str映射到BitSet的k个二进制位了.\n如果要检查某string是否已经被记录在BitSet中, 只需要计算其哈希值数组, 并检查BitSet上对应位置的值是否为1, 若对应位置中有任何一个不是1, 那么该字符串一定没有被记录过, 若全部对应位置都为1, 那么按照false positive认为该字符串已经被记录过了(但不是100%肯定).\n删除操作会影响到其他字符串。如果需要删除字符串的功能，使用Counting bloomfilter(CBF)，这是一种Bloom Filter的变体，CBF将Bloom Filter每一个Bit改为一个计数器，这样就可以实现删除字符串的功能了。\nBloom Filter跟单哈希函数Bit-Map不同之处在于：Bloom Filter使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。\n所以Bloom Filter适用以下几个特点：\n只要返回数据不存在，则肯定不存在。 返回数据存在，但只能是大概率存在。 不能清除其中的数据。 BloomFilter的应用很多，比如数据库、爬虫（用爬虫抓取网页时对网页url去重）、防缓存击穿等。特别是需要精确知道某个数据不存在时做点什么事情就非常适合布隆过滤。 Goolge在BigTable中就使用了BloomFilter，以避免在硬盘中寻找不存在的条目。\n实现 Java实现\n作者：crossoverJie 链接：https://zhuanlan.zhihu.com/p/50926087 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 public class BloomFilters { /** * 数组长度 */ private int arraySize; /** * 数组 */ private int[] array; public BloomFilters(int arraySize) { this.arraySize = arraySize; array = new int[arraySize]; } /** * 写入数据 * @param key */ public void add(String key) { int first = hashcode_1(key); int second = hashcode_2(key); int third = hashcode_3(key); array[first % arraySize] = 1; array[second % arraySize] = 1; array[third % arraySize] = 1; } /** * 判断数据是否存在 * @param key * @return */ public boolean check(String key) { int first = hashcode_1(key); int second = hashcode_2(key); int third = hashcode_3(key); int firstIndex = array[first % arraySize]; if (firstIndex == 0) { return false; } int secondIndex = array[second % arraySize]; if (secondIndex == 0) { return false; } int thirdIndex = array[third % arraySize]; if (thirdIndex == 0) { return false; } return true; } /** * hash 算法1 * @param key * @return */ private int hashcode_1(String key) { int hash = 0; int i; for (i = 0; i \u0026lt; key.length(); ++i) { hash = 33 * hash + key.charAt(i); } return Math.abs(hash); } /** * hash 算法2 * @param data * @return */ private int hashcode_2(String data) { final int p = 16777619; int hash = (int) 2166136261L; for (int i = 0; i \u0026lt; data.length(); i++) { hash = (hash ^ data.charAt(i)) * p; } hash += hash \u0026lt;\u0026lt; 13; hash ^= hash \u0026gt;\u0026gt; 7; hash += hash \u0026lt;\u0026lt; 3; hash ^= hash \u0026gt;\u0026gt; 17; hash += hash \u0026lt;\u0026lt; 5; return Math.abs(hash); } /** * hash 算法3 * @param key * @return */ private int hashcode_3(String key) { int hash, i; for (hash = 0, i = 0; i \u0026lt; key.length(); ++i) { hash += key.charAt(i); hash += (hash \u0026lt;\u0026lt; 10); hash ^= (hash \u0026gt;\u0026gt; 6); } hash += (hash \u0026lt;\u0026lt; 3); hash ^= (hash \u0026gt;\u0026gt; 11); hash += (hash \u0026lt;\u0026lt; 15); return Math.abs(hash); } } Guava 实现\n@Test public void guavaTest() { long star = System.currentTimeMillis(); BloomFilter\u0026lt;Integer\u0026gt; filter = BloomFilter.create( Funnels.integerFunnel(), 10000000, 0.01); for (int i = 0; i \u0026lt; 10000000; i++) { filter.put(i); } Assert.assertTrue(filter.mightContain(1)); Assert.assertTrue(filter.mightContain(2)); Assert.assertTrue(filter.mightContain(3)); Assert.assertFalse(filter.mightContain(10000000)); long end = System.currentTimeMillis(); System.out.println(\u0026#34;执行时间：\u0026#34; + (end - star)); } 构造方法有两个比较重要的参数，一个是预计存放多少数据，一个是可以接受的误报率。Guava 会通过你预计的数量以及误报率帮你计算出你应当会使用的数组大小 numBits 以及需要计算几次 Hash 函数 numHashFunctions 。\n@VisibleForTesting static \u0026lt;T\u0026gt; BloomFilter\u0026lt;T\u0026gt; create( Funnel\u0026lt;? super T\u0026gt; funnel, long expectedInsertions, double fpp, Strategy strategy) { checkNotNull(funnel); checkArgument( expectedInsertions \u0026gt;= 0, \u0026#34;Expected insertions (%s) must be \u0026gt;= 0\u0026#34;, expectedInsertions); checkArgument(fpp \u0026gt; 0.0, \u0026#34;False positive probability (%s) must be \u0026gt; 0.0\u0026#34;, fpp); checkArgument(fpp \u0026lt; 1.0, \u0026#34;False positive probability (%s) must be \u0026lt; 1.0\u0026#34;, fpp); checkNotNull(strategy); if (expectedInsertions == 0) { expectedInsertions = 1; } /* * TODO(user): Put a warning in the javadoc about tiny fpp values, since the resulting size * is proportional to -log(p), but there is not much of a point after all, e.g. * optimalM(1000, 0.0000000000000001) = 76680 which is less than 10kb. Who cares! */ long numBits = optimalNumOfBits(expectedInsertions, fpp); int numHashFunctions = optimalNumOfHashFunctions(expectedInsertions, numBits); try { return new BloomFilter\u0026lt;T\u0026gt;(new LockFreeBitArray(numBits), numHashFunctions, funnel, strategy); } catch (IllegalArgumentException e) { throw new IllegalArgumentException(\u0026#34;Could not create BloomFilter of \u0026#34; + numBits + \u0026#34; bits\u0026#34;, e); } } put有不同的策略，如MURMUR128_MITZ_64()策略根据 murmur3_128 方法的到一个 128 位长度的 byte[]。分别取高低 8 位的到两个 hash 值(lowerEight, upperEight)。再根据初始化时的到的执行 hash 的次数进行 hash 运算。\n/** * This strategy uses all 128 bits of {@link Hashing#murmur3_128} when hashing. It looks different * than the implementation in MURMUR128_MITZ_32 because we\u0026#39;re avoiding the multiplication in the * loop and doing a (much simpler) += hash2. We\u0026#39;re also changing the index to a positive number by * AND\u0026#39;ing with Long.MAX_VALUE instead of flipping the bits. */ MURMUR128_MITZ_64() { @Override public \u0026lt;T\u0026gt; boolean put( T object, Funnel\u0026lt;? super T\u0026gt; funnel, int numHashFunctions, LockFreeBitArray bits) { long bitSize = bits.bitSize(); byte[] bytes = Hashing.murmur3_128().hashObject(object, funnel).getBytesInternal(); long hash1 = lowerEight(bytes); long hash2 = upperEight(bytes); boolean bitsChanged = false; long combinedHash = hash1; for (int i = 0; i \u0026lt; numHashFunctions; i++) { // Make the combined hash positive and indexable bitsChanged |= bits.set((combinedHash \u0026amp; Long.MAX_VALUE) % bitSize); combinedHash += hash2; } return bitsChanged; } } LockFreeBitArray就是真正存放数据的底层数据结构。利用了一个 AtomicLongArray data 来存放数据。所以 set() 时候也是对这个 data 做处理。\n/** * Models a lock-free array of bits. * * \u0026lt;p\u0026gt;We use this instead of java.util.BitSet because we need access to the array of longs and we * need compare-and-swap. */ static final class LockFreeBitArray { private static final int LONG_ADDRESSABLE_BITS = 6; final AtomicLongArray data; private final LongAddable bitCount; LockFreeBitArray(long bits) { this(new long[Ints.checkedCast(LongMath.divide(bits, 64, RoundingMode.CEILING))]); } // Used by serialization LockFreeBitArray(long[] data) { checkArgument(data.length \u0026gt; 0, \u0026#34;data length is zero!\u0026#34;); this.data = new AtomicLongArray(data); this.bitCount = LongAddables.create(); long bitCount = 0; for (long value : data) { bitCount += Long.bitCount(value); } this.bitCount.add(bitCount); } /** Returns true if the bit changed value. */ boolean set(long bitIndex) { if (get(bitIndex)) { return false; } int longIndex = (int) (bitIndex \u0026gt;\u0026gt;\u0026gt; LONG_ADDRESSABLE_BITS); long mask = 1L \u0026lt;\u0026lt; bitIndex; // only cares about low 6 bits of bitIndex long oldValue; long newValue; do { oldValue = data.get(longIndex); newValue = oldValue | mask; if (oldValue == newValue) { return false; } } while (!data.compareAndSet(longIndex, oldValue, newValue)); // We turned the bit on, so increment bitCount. bitCount.increment(); return true; } } 在 set() 之前先通过 get() 判断这个数据是否存在于集合中，如果已经存在则直接返回告知客户端写入失败。接下来就是通过位运算进行位或赋值。get() 方法的计算逻辑和 set() 类似，只要判断为 0 就直接返回存在该值。\n","permalink":"https://congchan.github.io/posts/java-bitmap-%E5%92%8C-bloom-filter/","summary":"\u003ch2 id=\"bit-map\"\u003eBit Map\u003c/h2\u003e\n\u003cp\u003eBit-map用一个bit位来标记某个元素对应的Value， 而Key即是该元素。由于采用了Bit为单位来存储数据，因此在存储空间方面，可以大大节省。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e假设我们要对0-7内的5个元素\u003ccode\u003e4,7,2,5,3\u003c/code\u003e排序（假设这些元素没有重复）。那么我们就可以采用Bit-map的方法来达到排序的目的。要表示8个数，我们就只需要8个Bit（1Bytes），\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e首先我们开辟1Byte的空间，将这些空间的所有Bit位都置为0，\u003ccode\u003e0 0 0 0 0 0 0 0\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e然后遍历这5个元素，首先第一个元素是4，那么就把4对应的位置设为1, \u003ccode\u003ep+(i/8)|(0x01\u0026lt;\u0026lt;(i%8))\u003c/code\u003e, 这里默认为Big-ending, \u003ccode\u003e0 0 0 0 1 0 0 0\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e然后再处理第二个元素7，将第八位置为1,，接着再处理第三个元素，一直到最后处理完所有的元素，将相应的位置为1，这时候的内存的Bit位的状态\u003ccode\u003e0 0 1 1 1 1 0 1\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e遍历一遍Bit区域，把\u003ccode\u003e1\u003c/code\u003e的索引依次输出（\u003ccode\u003e2，3，4，5，7\u003c/code\u003e），这样就达到了排序的目的。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e算法的关键是如何确定十进制的数映射到二进制bit位的map图。算法占用很少内存，比如N=10000000；只需占用内存为N/8=1250000Byte=1.25M。缺点是不能有重复数据。\u003c/p\u003e\n\u003ch3 id=\"map映射表\"\u003eMap映射表\u003c/h3\u003e\n\u003cp\u003e假设需要排序或者查找的总数\u003ccode\u003eN=10000000\u003c/code\u003e，那么我们需要申请内存空间的大小为\u003ccode\u003eint a[1 + N/32]\u003c/code\u003e，其中：\u003ccode\u003ea[0]\u003c/code\u003e在内存中占32位, 可以对应十进制数0-31，依次类推：\nbitmap表为：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ea[0]---------\u0026gt;0-31\na[1]---------\u0026gt;32-63\na[2]---------\u0026gt;64-95\na[3]---------\u0026gt;96-127\n..........\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e十进制数需要转换为对应的bit位\u003c/p\u003e\n\u003ch3 id=\"位移转换\"\u003e位移转换\u003c/h3\u003e\n\u003cp\u003e将十进制数转换为对应的bit位, 申请一个\u003ccode\u003eint\u003c/code\u003e一维数组，作为32列的二维数组，\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eint a[0]    |0000000000000000000000000000000000000|\n\nint a[1]    |0000000000000000000000000000000000000|\n\n………………\n\nint a[N]    |0000000000000000000000000000000000000|\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e例如十进制0，对应在\u003ccode\u003ea[0]\u003c/code\u003e第一位： \u003ccode\u003e00000000000000000000000000000001\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e求十进制\u003ccode\u003e0-N\u003c/code\u003e对应在数组\u003ccode\u003ea\u003c/code\u003e的索引：十进制\u003ccode\u003e0-31\u003c/code\u003e，对应\u003ccode\u003ea[0]\u003c/code\u003e，先由十进制数n转换为与32的余可转化为对应在数组\u003ccode\u003ea\u003c/code\u003e中的索引\u003ccode\u003e0\u003c/code\u003e。比如n=24,那么 n/32=0，则24对应\u003ccode\u003ea[0]\u003c/code\u003e。又比如n=60, 那么n/32=1，则60对应\u003ccode\u003ea[1]\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e求\u003ccode\u003e0-N\u003c/code\u003e对应\u003ccode\u003e0-31\u003c/code\u003e中的数：十进制0-31就对应0-31，而32-63则对应也是0-31，即给定一个数n可以通过模32求得对应0-31中的数。\u003c/li\u003e\n\u003cli\u003e利用移位0-31使得对应32bit位为1. 找到对应0-31的数为M, 左移M位：即\u003ccode\u003e2 ^ M\u003c/code\u003e, 置1.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"bloom-filter\"\u003eBloom Filter\u003c/h3\u003e\n\u003cp\u003e为了降低键值冲突的概率，Bloom Filter使用了多个哈希函数：创建一个m位BitSet，先将所有位初始化为0，然后选择k个不同的哈希函数。第i个哈希函数对字符串str哈希的结果记为\u003ccode\u003eh(i, str)\u003c/code\u003e，且\u003ccode\u003eh(i, str)\u003c/code\u003e的范围是0到m-1 。\u003c/p\u003e","title":"Java BitMap 和 Bloom Filter"},{"content":"避免歧义的编码 在构建压缩编码的对应关系时，我们使用不同的数量的位来编码不同的字符.\n比如摩斯密码. 如果单纯使用这种对应关系，会出现一些问题， 如•••−−−•••会产生歧义: SOS? V7? IAMIE? EEWNI? 所以在实际使用中, 密码使用一些间隔来分隔代码字。\n那么对于不同的压缩编码, 有什么常用方法来避免歧义？ 方法是确保没有一个编码是另一个编码的前缀。比如\n使用固定长度编码。 为每个编码添加特殊的stop char。 使用一种具备广泛使用性的prefix-free编码。 用什么数据结构来设计prefix-free编码?\n用Trie构造编码 一个二叉(0, 1)Trie: 叶节点是字符, 根节点到叶节点的路径就是编码.\n压缩: 方法1：从叶开始; 按照路径到达根; 反向打印bits。 方法2：创建键-值对的符号表。\n解压:\n从根节点开始, 根据位值是0还是1在Trie图上游走, 直到走到叶节点，则解压出一个字符 返回根节点, 继续第一步, 直到跑完所有编码. private static class Node implements Comparable\u0026lt;Node\u0026gt; { private final char ch; // used only for leaf nodes private final int freq; // used only for compress private final Node left, right; public Node(char ch, int freq, Node left, Node right) { this.ch = ch; this.freq = freq; this.left = left; this.right = right; } public boolean isLeaf() { return left == null \u0026amp;\u0026amp; right == null; } // compare Nodes by frequency public int compareTo(Node that) { return this.freq - that.freq; } // Runtime - Linear in input size N public void expand() { Node root = readTrie(); // read in encoding trie int N = BinaryStdIn.readInt(); // read in number of chars for (int i = 0; i \u0026lt; N; i++) { Node x = root; while (!x.isLeaf()) { if (!BinaryStdIn.readBoolean()) x = x.left; else x = x.right; } BinaryStdOut.write(x.ch, 8); } BinaryStdOut.close(); } } 如何读取一个Trie：根据Trie的前序遍历序列重构.\nprivate static Node readTrie() { if (BinaryStdIn.readBoolean()) { char c = BinaryStdIn.readChar(8); return new Node(c, 0, null, null); } Node x = readTrie(); Node y = readTrie(); return new Node(\u0026#39;\\0\u0026#39;, 0, x, y); } 如何把Trie写为序列：以前序遍历的方式写Trie；额外用一个位标记是否叶节点。\nprivate static void writeTrie(Node x) { if (x.isLeaf()) { BinaryStdOut.write(true); BinaryStdOut.write(x.ch, 8); return; } BinaryStdOut.write(false); writeTrie(x.left); writeTrie(x.right); } 用哈夫曼算法构建最优编码 就是用Huffman算法. Huffman算法是把最短的编码赋给出现频率最高的字符, 把最长的编码留给出现频率较低的字符. 在Trie上的效果就变成频率最高的字符路径最短, 长路径都留给频率低的字符. 这样总的效果就是使用了更少的数据位来表达同样的信息.\n统计输入的各个字符的频率freq[i]。 为每个char i构建一个具有权重freq[i]的Trie(子节点为null), 从此节点开始 重复以下过程直到融合为一个trie(根节点)： 选择当前权重最小的两Tries, freq[i]和freq[j], 其中i \u0026lt;= j, freq[i] \u0026lt;= freq[j] 给它们创建父节点, 权重为freq[i] + freq[j], 两个子Trie和其父节点合并为一个Trie, 而且路径0(左边)总是指向较小的子Trie, 路径1(右边)指向较大的. private static Node buildTrie(int[] freq) { MinPQ\u0026lt;Node\u0026gt; pq = new MinPQ\u0026lt;Node\u0026gt;(); // initialize PQ with singleton tries for (char i = 0; i \u0026lt; R; i++) if (freq[i] \u0026gt; 0) pq.insert(new Node(i, freq[i], null, null)); while (pq.size() \u0026gt; 1) { // merge two smallest tries Node x = pq.delMin(); Node y = pq.delMin(); Node parent = new Node(\u0026#39;\\0\u0026#39;, x.freq + y.freq, x, y); pq.insert(parent); } return pq.delMin(); } 通过这个算法, 可以保证频率最高(权重最大)的字符的叶节点就是最左叶节点, 一般编码为0, 其他依次类推. 可以证明Huffman算法生成的最优prefix-free编码.\n完整代码见\nImplementation. ・Pass 1: tabulate char frequencies and build trie. ・Pass 2: encode file by traversing trie or lookup table\nRunning time. Using a binary heap ⇒ N + R log R. N input size, R alphabet size.\n对于具有n个叶子节点的哈夫曼树，一共需要2*n-1个节点: 二叉树有三种类型节点，即子节点数为2的节点，为1的节点和为0的叶节点。而哈夫曼树的非叶子节点是由两个节点生成的，因此不能出现只有单子节点的节点，如果叶子节点个数为n, 那么非叶子节点的个数为n-1.\n哈夫曼编码广泛应用于jpeg, pdf, MP3, MP4等文件编码中.\n在神经网络中, 哈夫曼树也被用于构建层级Softmax.\n一个使用Huffman Encoding的实例： https://github.com/congchan/cs106b-programming-abstraction/tree/master/HW6_Huffman%20Encoding/Huffman/src\n","permalink":"https://congchan.github.io/posts/%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9-%E5%93%88%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81/","summary":"\u003ch2 id=\"避免歧义的编码\"\u003e避免歧义的编码\u003c/h2\u003e\n\u003cp\u003e在构建压缩编码的对应关系时，我们使用不同的数量的位来编码不同的字符.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e比如摩斯密码\u003cimg loading=\"lazy\" src=\"/images/Morse_Code.png\" title=\"Chart of the Morse code letters and numerals.\"\u003e. 如果单纯使用这种对应关系，会出现一些问题， 如\u003ccode\u003e•••−−−•••\u003c/code\u003e会产生歧义: \u003ccode\u003eSOS\u003c/code\u003e? \u003ccode\u003eV7\u003c/code\u003e? \u003ccode\u003eIAMIE\u003c/code\u003e? \u003ccode\u003eEEWNI\u003c/code\u003e? 所以在实际使用中, 密码使用一些间隔来分隔代码字。\u003c/p\u003e\n\u003cp\u003e那么对于不同的压缩编码, 有什么常用方法来避免歧义？\n方法是确保没有一个编码是另一个编码的前缀。比如\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e使用固定长度编码。\u003c/li\u003e\n\u003cli\u003e为每个编码添加特殊的stop char。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e使用一种具备广泛使用性的prefix-free编码\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e用什么数据结构来设计prefix-free编码?\u003c/p\u003e\n\u003ch3 id=\"用trie构造编码\"\u003e用Trie构造编码\u003c/h3\u003e\n\u003cp\u003e一个二叉(\u003ccode\u003e0, 1\u003c/code\u003e)Trie: 叶节点是字符, 根节点到叶节点的路径就是编码.\u003cimg loading=\"lazy\" src=\"/images/huffman_trie.png\" title=\"image from: https://www.coursera.org/learn/algorithms-part2/\"\u003e\u003c/p\u003e\n\u003cp\u003e压缩:\n方法1：从叶开始; 按照路径到达根; 反向打印bits。\n方法2：创建\u003ccode\u003e键-值\u003c/code\u003e对的符号表。\u003c/p\u003e\n\u003cp\u003e解压:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e从根节点开始, 根据位值是0还是1在Trie图上游走, 直到走到叶节点，则解压出一个字符\u003c/li\u003e\n\u003cli\u003e返回根节点, 继续第一步, 直到跑完所有编码.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eComparable\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003echar\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ech\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"c1\"\u003e// used only for leaf nodes\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003efreq\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"c1\"\u003e// used only for compress\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eleft\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eright\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eNode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003echar\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ech\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003efreq\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eleft\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eright\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ech\u003c/span\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ech\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003efreq\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003efreq\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eleft\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eleft\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eright\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eright\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eisLeaf\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eleft\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enull\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eright\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enull\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// compare Nodes by frequency\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ecompareTo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ethat\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003efreq\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ethat\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003efreq\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Runtime - Linear in input size N\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eexpand\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e       \u003c/span\u003e\u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eroot\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ereadTrie\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// read in encoding trie\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e       \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eN\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eBinaryStdIn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ereadInt\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// read in number of chars\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e       \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eN\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e       \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eroot\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eisLeaf\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e             \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003eBinaryStdIn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ereadBoolean\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eleft\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e             \u003c/span\u003e\u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eright\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"n\"\u003eBinaryStdOut\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ech\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e       \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e       \u003c/span\u003e\u003cspan class=\"n\"\u003eBinaryStdOut\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eclose\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e如何读取一个Trie：根据Trie的前序遍历序列重构.\u003cimg loading=\"lazy\" src=\"/images/preorder_traversal_trie.png\" title=\"image from: https://www.coursera.org/learn/algorithms-part2/\"\u003e\u003c/p\u003e","title":"信息处理 - 数据压缩 - 哈夫曼编码"},{"content":"数据压缩 压缩数据以节省储存空间，节省传输时间。同时很多文件都有很多冗余信息，这为压缩提供了很多可能性。\n通用文件压缩 ·文件：GZIP，BZIP，7z ·Archivers：PKZIP ·文件系统：NTFS，HFS +，ZFS\n多媒体 ·图像：GIF，JPEG ·声音：MP3 ·视频：MPEG，DivX™，HDTV\n通讯 ·ITU-T T4 Group 3 Fax ·V.42bis调制解调器 ·Skype\n数据库\n压缩率 Compression ratio = Bits in Compressed B / bits in B.\n自然语言的压缩率为50-75％或更高.\n读写二进制 public class BinaryStdIn { boolean readBoolean() // read 1 bit of data and return as a boolean value char readChar() // read 8 bits of data and return as a char value char readChar(int r) // read r bits of data and return as a char value // similar methods for byte (8 bits); short (16 bits); int (32 bits); long and double (64 bits) boolean isEmpty() // is the bitstream empty? void close() // close the bitstream } public class BinaryStdOut { void write(boolean b) // write the specified bit void write(char c) // write the specified 8-bit char void write(char c, int r) // write the r least significant bits of the specified char // similar methods for byte (8 bits); short (16 bits); int (32 bits); long and double (64 bits) void close() // close the bitstream } 比如使用三种方法表达12/31/1999 1, A character stream (StdOut),\nStdOut.print(month + \u0026#34;/\u0026#34; + day + \u0026#34;/\u0026#34; + year); 00110001 1 00110010 2 00101111 / 00110111 3 00110001 1 00101111 / 00110001 1 00111001 9 00111001 9 00111001 1 共 80bits 2, Three ints (BinaryStdOut)\nBinaryStdOut.write(month); BinaryStdOut.write(day); BinaryStdOut.write(year); 00000000 00000000 00000000 00001100 12 00000000 00000000 00000000 00011111 31 00000000 00000000 00000111 11001111 1999 共96bits 3，A 4-bit field, a 5-bit field, and a 12-bit field (BinaryStdOut)\nBinaryStdOut.write(month, 4); BinaryStdOut.write(day, 5); BinaryStdOut.write(year, 12); 1100 12 11111 13 0111110 01111 1999 共21bits\n通用数据压缩算法？ 不存在的，因为假如真的存在一种可以压缩所有比特串的算法，那么该算法就可以继续压缩已经被它压缩过的数据，那意味着所有比特串可以被压缩为0比特.\nRun-length encoding Simple type of redundancy in a bitstream. Long runs of repeated bits： 0000000000000001111111000000011111111111 Compression, 4-bit counts to represent alternating runs of 0s and 1s: 15 0s, then 7 1s, then 7 0s, then 11 1s. 1111 0111 0111 1011\npublic class RunLength { // maximum run-length count private final static int R = 256; // number of bits per count private final static int LG_R = 8; /** * Reads a sequence of bits from standard input; compresses * them using run-length coding with 8-bit run lengths; and writes the * results to standard output. */ public static void compress() { char run = 0; boolean old = false; while (!BinaryStdIn.isEmpty()) { boolean b = BinaryStdIn.readBoolean(); if (b != old) { BinaryStdOut.write(run, LG_R); run = 1; old = !old; } else { // 如果长度超过最大值, 写入0 if (run == R-1) { BinaryStdOut.write(run, LG_R); run = 0; BinaryStdOut.write(run, LG_R); } run++; } } BinaryStdOut.write(run, LG_R); BinaryStdOut.close(); } /** * Reads a sequence of bits from standard input (that are encoded * using run-length encoding with 8-bit run lengths); decodes them; * and writes the results to standard output. */ public static void expand() { boolean bit = false; while (!BinaryStdIn.isEmpty()) { int run = BinaryStdIn.readInt(lgR); for (int i = 0; i \u0026lt; run; i++) BinaryStdOut.write(bit); bit = !bit; } BinaryStdOut.close(); } } ","permalink":"https://congchan.github.io/posts/%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/","summary":"\u003ch2 id=\"数据压缩\"\u003e数据压缩\u003c/h2\u003e\n\u003cp\u003e压缩数据以节省储存空间，节省传输时间。同时很多文件都有很多冗余信息，这为压缩提供了很多可能性。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e通用文件压缩\n·文件：GZIP，BZIP，7z\n·Archivers：PKZIP\n·文件系统：NTFS，HFS +，ZFS\u003c/p\u003e\n\u003cp\u003e多媒体\n·图像：GIF，JPEG\n·声音：MP3\n·视频：MPEG，DivX™，HDTV\u003c/p\u003e\n\u003cp\u003e通讯\n·ITU-T T4 Group 3 Fax\n·V.42bis调制解调器\n·Skype\u003c/p\u003e\n\u003cp\u003e数据库\u003c/p\u003e\n\u003ch3 id=\"压缩率\"\u003e压缩率\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eCompression ratio = Bits in Compressed B / bits in B\u003c/code\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e自然语言的压缩率为50-75％或更高.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"读写二进制\"\u003e读写二进制\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epublic class BinaryStdIn {\n    boolean readBoolean() // read 1 bit of data and return as a boolean value\n    char readChar() // read 8 bits of data and return as a char value\n    char readChar(int r) // read r bits of data and return as a char value\n    // similar methods for byte (8 bits); short (16 bits); int (32 bits); long and double (64 bits)\n    boolean isEmpty() // is the bitstream empty?\n    void close() // close the bitstream\n}\n\npublic class BinaryStdOut {\n    void write(boolean b) // write the specified bit\n    void write(char c) // write the specified 8-bit char\n    void write(char c, int r) // write the r least significant bits of the specified char\n    // similar methods for byte (8 bits); short (16 bits); int (32 bits); long and double (64 bits)\n    void close() // close the bitstream\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e比如使用三种方法表达\u003ccode\u003e12/31/1999\u003c/code\u003e\n1, A character stream (StdOut),\u003c/p\u003e","title":"信息处理 - 数据压缩"},{"content":"数组中有一个数字出现的次数超过数组长度的一半，例如输入一个长度为9的数组1,2,3,2,2,2,5,4,2。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。因为这个数出现次数超过了数组长度一半以上, 那么它就是数组中出现次数最多的数, 故谓之众数.\nclass Solution: def MoreThanHalfNum_Solution(self, numbers): # write code here most = numbers[0] count = 1 for item in numbers: if item == most: count += 1 else: count -= 1 if count \u0026lt; 0: most = item count = 1 return 0 if numbers.count(most) \u0026lt;= len(numbers) / 2 else most 众数问题 众数问题可以推广泛化：给定大小为n的整数数组，找到所有出现超过n / m次的元素。这种问题可以使用 Boyer-Moore 算法解决.\nThe Boyer–Moore majority vote algorithm is an algorithm for finding the majority of a sequence of elements using linear time and constant space. It is named after Robert S. Boyer and J Strother Moore, who published it in 1981, and is a prototypical example of a streaming algorithm.\n如果存在众数元素，该算法会找到众数元素：对于出现次数一半以上的元素。但是，如果没有众数，算法将不会检测到该事实，并且仍将输出其中一个元素。\n这个时候需要第二次遍历数据, 验证在第一次通过中找到的元素是否真正占众数。\n比如找到所有出现超过n / 3次的元素, 最多只可能有2个, 可以用长度为2的数据结构(这里选择map)来记录众数.\nclass Solution: def majorityElement(self, nums): \u0026#34;\u0026#34;\u0026#34; :type nums: List[int] :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; m = 2 cand = [0] * m freq = {} for item in nums: if len(freq) \u0026lt; m: freq[item] = 1 + freq.get(item, 0) elif item in freq: freq[item] += 1 else: for k in list(freq): freq[k] -= 1 if freq[k] \u0026lt;= 0: freq.pop(k) return [k for k in freq if nums.count(k) \u0026gt; len(nums) // (m + 1)] ","permalink":"https://congchan.github.io/posts/%E4%BC%97%E6%95%B0%E9%97%AE%E9%A2%98-boyermoore-majority-vote-algorithm/","summary":"\u003cp\u003e数组中有一个数字出现的次数超过数组长度的一半，例如输入一个长度为9的数组\u003ccode\u003e1,2,3,2,2,2,5,4,2\u003c/code\u003e。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。因为这个数出现次数超过了数组长度一半以上, 那么它就是数组中出现次数最多的数, 故谓之\u003cstrong\u003e众数\u003c/strong\u003e.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eSolution\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eMoreThanHalfNum_Solution\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enumbers\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"c1\"\u003e# write code here\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003emost\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enumbers\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ecount\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eitem\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003enumbers\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eitem\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"n\"\u003emost\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"n\"\u003ecount\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"n\"\u003ecount\u003c/span\u003e \u003cspan class=\"o\"\u003e-=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ecount\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"n\"\u003emost\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eitem\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"n\"\u003ecount\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003enumbers\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecount\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emost\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e \u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enumbers\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"n\"\u003emost\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"众数问题\"\u003e众数问题\u003c/h2\u003e\n\u003cp\u003e众数问题可以推广泛化：给定大小为\u003ccode\u003en\u003c/code\u003e的整数数组，找到所有出现超过\u003ccode\u003en / m\u003c/code\u003e次的元素。这种问题可以使用 Boyer-Moore 算法解决.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe Boyer–Moore majority vote algorithm is an algorithm for finding the majority of a sequence of elements using linear time and constant space. It is named after Robert S. Boyer and J Strother Moore, who published it in 1981, and is a prototypical example of a streaming algorithm.\u003c/p\u003e","title":"众数问题 - Boyer–Moore majority vote algorithm"},{"content":"各种树的变种 为了适应不同的应用场景, 人们使用不同的树结构来实现符号表.\n九宫格输入法 对于手机的九宫格输入法, 简单的实现方式是多次敲击: 通过反复按键输入一个字母，直到出现所需的字母。\n但 http://www.t9.com/ 的 T9 texting 支持更高效的输入方法: ・Find all words that correspond to given sequence of numbers. ・Press 0 to see all completion options. Ex. hello ・多次敲击: 4 4 3 3 5 5 5 5 5 5 6 6 6 ・T9: 4 3 5 5 6\n可以使用 8-way trie 来实现.\n三元搜索Trie R较大的R-way trie的空间效率不高，读取比较大的文件往往导致内存不足。但弊端是开辟出的数组内存利用率其实不高。现在很多系统都使用Unicode，分支可高达65,536. 所以需要更高效的方法。\nTernary search tries: ・Store characters and values in nodes (not keys). ・Each node has 3 children: smaller (left), equal (middle), larger (right). Search in a TST: Follow links corresponding to each character in the key. ・If less, take left link; if greater, take right link. ・If equal, take the middle link and move to the next key character.\npublic class TST\u0026lt;Value\u0026gt; { private Node root; private class Node { private Value val; private char c; private Node left, mid, right; } public void put(String key, Value val) { root = put(root, key, val, 0); } private Node put(Node x, String key, Value val, int d) { char c = key.charAt(d); if (x == null) { x = new Node(); x.c = c; } if (c \u0026lt; x.c) x.left = put(x.left, key, val, d); else if (c \u0026gt; x.c) x.right = put(x.right, key, val, d); else if (d \u0026lt; key.length() - 1) x.mid = put(x.mid, key, val, d+1); else x.val = val; return x; } public boolean contains(String key) { return get(key) != null; } public Value get(String key) { Node x = get(root, key, 0); if (x == null) return null; return x.val; } private Node get(Node x, String key, int d) { if (x == null) return null; char c = key.charAt(d); if (c \u0026lt; x.c) return get(x.left, key, d); else if (c \u0026gt; x.c) return get(x.right, key, d); else if (d \u0026lt; key.length() - 1) return get(x.mid, key, d+1); else return x; } } TSTs比hashing更快（特别是对于搜索缺失键的情况）。\n基数树 Radix Tree, 也叫 Patricia trie (Practical Algorithm to Retrieve Information Coded in Alphanumeric), crit-bit tree, 压缩前缀树: ・Remove one-way branching. ・Each node represents a sequence of characters. ・Implementation: one step beyond this course. ![](/images/radix_trie.png \u0026ldquo;put(\u0026ldquo;shells\u0026rdquo;, 1); put(\u0026ldquo;shellfish\u0026rdquo;, 2); image from: https://algs4.cs.princeton.edu/\") 对于基数树的每个节点，如果该节点是唯一的子树的话，就和父节点合并。\nApplications. ・Database search. ・P2P network search. ・IP routing tables: find longest prefix match. ・Compressed quad-tree for N-body simulation. ・Efficiently storing and querying XML documents.\n后缀树 后缀树（Suffix tree）指字符串后缀的基数树: 一个String S的后缀树是一个边（edge）被标记为字符串的树。因此每一个S的后缀都唯一对应一条从根节点到叶节点的路径。这样就形成了一个S的后缀的基数树。\nApplications. ・Linear-time: longest repeated substring, longest common substring, longest palindromic substring, substring search, tandem repeats, …. ・Computational biology databases (BLAST, FASTA).\n字符符号表总结 Red-black BST. ・Performance guarantee: log N key compares. ・Supports ordered symbol table API.\nHash tables. ・Performance guarantee: constant number of probes. ・Requires good hash function for key type.\nTries. R-way, TST. ・Performance guarantee: log N characters accessed. ・Supports character-based operations. You can get at anything by examining 50-100 bits\n","permalink":"https://congchan.github.io/posts/%E4%B8%8D%E5%90%8C%E6%A0%91%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%AC%A6%E5%8F%B7%E8%A1%A8/","summary":"\u003ch2 id=\"各种树的变种\"\u003e各种树的变种\u003c/h2\u003e\n\u003cp\u003e为了适应不同的应用场景, 人们使用不同的树结构来实现符号表.\u003c/p\u003e\n\u003ch3 id=\"九宫格输入法\"\u003e九宫格输入法\u003c/h3\u003e\n\u003cp\u003e对于手机的九宫格输入法, 简单的实现方式是多次敲击: 通过反复按键输入一个字母，直到出现所需的字母。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e但 \u003ca href=\"http://www.t9.com/\"\u003ehttp://www.t9.com/\u003c/a\u003e 的 T9 texting 支持更高效的输入方法:\n・Find all words that correspond to given sequence of numbers.\n・Press 0 to see all completion options.\n\u003cimg loading=\"lazy\" src=\"/images/t9.png\"\u003e\nEx. hello\n・多次敲击: 4 4 3 3 5 5 5 5 5 5 6 6 6\n・T9: 4 3 5 5 6\u003c/p\u003e\n\u003cp\u003e可以使用 8-way trie 来实现.\u003c/p\u003e\n\u003ch3 id=\"三元搜索trie\"\u003e三元搜索Trie\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eR\u003c/code\u003e较大的R-way trie的空间效率不高，读取比较大的文件往往导致内存不足。但弊端是开辟出的数组内存利用率其实不高。现在很多系统都使用Unicode，分支可高达\u003ccode\u003e65,536\u003c/code\u003e. 所以需要更高效的方法。\u003c/p\u003e\n\u003cp\u003eTernary search tries:\n・Store characters and values in nodes (not keys).\n・Each node has 3 children: smaller (left), equal (middle), larger (right).\n\u003cimg loading=\"lazy\" src=\"/images/tst.png\" title=\"image from: https://www.coursera.org/learn/algorithms-part2/\"\u003e\nSearch in a TST: Follow links corresponding to each character in the key.\n・If less, take left link; if greater, take right link.\n・If equal, take the middle link and move to the next key character.\u003c/p\u003e","title":"不同树结构的字符串符号表"},{"content":"符号表 在计算机科学中，符号表是一种用于语言翻译器（例如编译器和解释器）中的数据结构。在符号表中，程序源代码中的每个标识符都和它的声明或使用信息绑定在一起，比如其数据类型、作用域以及内存地址。 常用哈希表来实现.\n符号表的应用非常广泛, 可用于实现Set, Dictionary, 文件索引, 稀疏向量/矩阵等数据结构和相关的运算操作, 还有其他如过滤查询(Exception filter), 一致性查询(concordance queries)等操作.\n字符符号表就是专门针对字符操作的符号表, API: Prefix match - Keys with prefix sh: she, shells, and shore. Wildcard match - Keys that match .he: she and the. Longest prefix - Key that is the longest prefix of shellsort: shells.\npublic interface StringST\u0026lt;Value\u0026gt; { StringST(); create a symbol table with string keys void put(String key, Value val); put key-value pair into the symbol table Value get(String key); value paired with key void delete(String key); delete key and corresponding value Iterable\u0026lt;String\u0026gt; keys(); all keys Iterable\u0026lt;String\u0026gt; keysWithPrefix(String s); keys having s as a prefix Iterable\u0026lt;String\u0026gt; keysThatMatch(String s); keys that match s (where . is a wildcard) String longestPrefixOf(String s); longest key that is a prefix of s } 以Trie为基础的字符符号表 algs4中提供了用 R-way trie 来实现符号表(symbol table)例子:\npublic class TrieST\u0026lt;Value\u0026gt; { private static final int R = 256; // extended ASCII private Node root = new Node(); private static class Node { private Object value; private Node[] next = new Node[R]; } public void put(String key, Value val) { root = put(root, key, val, 0); } private Node put(Node x, String key, Value val, int d) { if (x == null) x = new Node(); if (d == key.length()) { x.value = val; return x; } char c = key.charAt(d); x.next[c] = put(x.next[c], key, val, d+1); return x; } public boolean contains(String key) { return get(key) != null; } public Value get(String key) { Node x = get(root, key, 0); if (x == null) return null; return (Value) x.val; } private Node get(Node x, String key, int d) { if (x == null) return null; if (d == key.length()) return x; char c = key.charAt(d); return get(x.next[c], key, d+1); } } 按顺序迭代所有键： ·中序遍历trie，找到的键添加到队列中 ·维护从根到当前节点路径的字符序列\npublic Iterable\u0026lt;String\u0026gt; keys() { Queue\u0026lt;String\u0026gt; queue = new Queue\u0026lt;String\u0026gt;(); collect(root, \u0026#34;\u0026#34;, queue); return queue; } private void collect(Node x, String prefix, Queue\u0026lt;String\u0026gt; q) { if (x == null) return; if (x.val != null) q.enqueue(prefix); for (char c = 0; c \u0026lt; R; c++) collect(x.next[c], prefix + c, q); } 前缀匹配 Find all keys in a symbol table starting with a given prefix. Ex. Autocomplete in a cell phone, search bar, text editor, or shell. ・User types characters one at a time. ・System reports all matching strings.\npublic Iterable\u0026lt;String\u0026gt; keysWithPrefix(String prefix) { Queue\u0026lt;String\u0026gt; queue = new Queue\u0026lt;String\u0026gt;(); Node x = get(root, prefix, 0); collect(x, prefix, queue); return queue; } 最长前缀 Find longest key in symbol table that is a prefix of query string. Ex. To send packet toward destination IP address, router chooses IP address in routing table that is longest prefix match.\n・Search for query string. ・Keep track of longest key encountered.\npublic String longestPrefixOf(String query) { int length = search(root, query, 0, 0); return query.substring(0, length); } private int search(Node x, String query, int d, int length) { if (x == null) return length; if (x.val != null) length = d; if (d == query.length()) return length; char c = query.charAt(d); return search(x.next[c], query, d+1, length); } ","permalink":"https://congchan.github.io/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%AC%A6%E5%8F%B7%E8%A1%A8%E5%92%8C%E4%B8%89%E5%85%83%E6%90%9C%E7%B4%A2trie/","summary":"\u003ch2 id=\"符号表\"\u003e符号表\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在计算机科学中，符号表是一种用于语言翻译器（例如编译器和解释器）中的数据结构。在符号表中，程序源代码中的每个标识符都和它的声明或使用信息绑定在一起，比如其数据类型、作用域以及内存地址。\n常用哈希表来实现.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e符号表的应用非常广泛, 可用于实现Set, Dictionary, 文件索引, 稀疏向量/矩阵等数据结构和相关的运算操作, 还有其他如过滤查询(Exception filter), 一致性查询(concordance queries)等操作.\u003c/p\u003e\n\u003cp\u003e字符符号表就是专门针对字符操作的符号表, API:\nPrefix match - Keys with prefix \u003ccode\u003esh\u003c/code\u003e: \u003ccode\u003eshe\u003c/code\u003e, \u003ccode\u003eshells\u003c/code\u003e, and \u003ccode\u003eshore\u003c/code\u003e.\nWildcard match - Keys that match \u003ccode\u003e.he\u003c/code\u003e: \u003ccode\u003eshe\u003c/code\u003e and \u003ccode\u003ethe\u003c/code\u003e.\nLongest prefix - Key that is the longest prefix of \u003ccode\u003eshellsort\u003c/code\u003e: \u003ccode\u003eshells\u003c/code\u003e.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003einterface\u003c/span\u003e \u003cspan class=\"nc\"\u003eStringST\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eValue\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eStringST\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecreate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esymbol\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003etable\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ewith\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003estring\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eValue\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eval\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eput\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003epair\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003einto\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ethe\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esymbol\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003etable\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eValue\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003epaired\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ewith\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003edelete\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003edelete\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eand\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecorresponding\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eIterable\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ekeys\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eall\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eIterable\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ekeysWithPrefix\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ehaving\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eas\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eprefix\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eIterable\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ekeysThatMatch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ethat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ematch\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003es\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ewhere\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eis\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ewildcard\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003elongestPrefixOf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elongest\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ethat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eis\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eprefix\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eof\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"以trie为基础的字符符号表\"\u003e以Trie为基础的字符符号表\u003c/h3\u003e\n\u003cp\u003ealgs4中提供了用 R-way trie 来实现符号表(symbol table)例子:\u003c/p\u003e","title":"字符串符号表和三元搜索Trie"},{"content":"虽然KMP可以用于单模式匹配问题，但如果是多模式问题, KMP的性能就得不到保证。比如根据墙内法律要求, 墙内的搜索引擎需要过滤敏感词后才能合法运营。敏感词的数量不少, 如果要求包含敏感词的网页不能被搜索到, 那么搜索引擎在爬取网页信息时, 就要标记网页的文本中是否包含任意个敏感词.\n这就是典型的多模匹配问题. 这种情况下如果使用Trie，那么需要遍历网页的每一个字符位置，对每一个位置进行Trie前缀匹配。如果词典的词语数量为N，每个词语长度为L，文章的长度为M，那么需要进行的计算次数是在N*M*L这个级别的. 即使把词语的长度L简化为常数级别的, 整个算法的复杂度也至少是$O(n^2)$.\nAC自动机 可以看到，KMP算法可以避免back up（在检查字符的过程中不需要回头），而Trie可以存储多个模式的信息。如果把二者结合在一起，也许能从性能上解决多模式（任意位置）匹配问题。这就是Aho–Corasick算法（AC自动机）。\nAho–Corasick算法是由Alfred V. Aho和Margaret J.Corasick 发明的字符串搜索算法，用于在输入的一串字符串中匹配有限组字典中的子串。它与普通字符串匹配的不同点在于同时与所有字典串进行匹配。算法均摊情况下具有近似于线性的时间复杂度，约为字符串的长度加所有匹配的数量。\n所以算法的关键就是通过Trie把多个模式构建为一个DFA（Deterministic finite state automaton），然后让模式串末尾对应的状态作为一个DFA的终止节点。这样，对于一个要检查的长字符串（如一段网页内容），让这个字符串在DFA上跑一趟，每一个字符表示一种跳转方式，如果这段字符能够跳到任何一个终结节点, 那么就表明这段字符串匹配了至少一个模式, 如果整段字符跑完都没到达终结节点, 那么这个网页就是\u0026quot;和谐的\u0026quot;.\n在单模式匹配中, 用KMP构建的DFA是比较简单的, 从左到右, 开头的状态就是开始状态, 结尾的状态就是结束状态: 而多模式匹配中, 在Trie的结构基础上构建出来的DFA更像一个DFA的样子: Trie中的节点, 就类似于DFA中的状态. 如果让字符串shis在上面跑, 假如仅仅是靠Trie(也即是没有虚线标识的转移), 那么第一次从字符串的第一个字符s开始转移, 经过转移路径0 - 85 - 90之后就转不动了, 因为Trie记录的模式中没有shi, 这个时候得back up, 从第二个位置h开始再匹配一遍. 这个过程中就产生重复匹配, 而参考KMP的思路, 在匹配shi的过程中, 其实已经挖掘出了hi这个子串了, 而这个子串是跟模式his对应的, 如果有办法不回头继续匹配下去就能提高性能了.\n而DFA中虚线的失败转移就是用来解决这个问题的: 当走到状态90时, 前面有了小部分子串h刚好对应状态74, 这个时候用虚线作为失败转移, 转移到74, 在状态74中寻找下一个转移i, 这样就实现了不回头继续匹配了.\n因为AC自动机是在Trie的基础上添加边, 用于指示各个节点经过不同字符后跳转到哪个节点, 结果就变成了图, 所以也叫做Trie图.\n要构建AC自动机:\n首先要把所有模式都吃进一个Trie中(最近看多进击的巨人了), 构建出一个由不同实线串联起来的状态机, 其中代表刚好吻合一个模式的状态标记为终结节点(如上图绿色节点) 然后补全其他字符的转移(失败转移), 用虚线表示. 补全了所有字符的转移方式, 才能让字符串永不回头地匹配下去, 避免了back up, 保证性能. 问题的关键在如何补全所有的状态转移.\n补全状态转移 这里要在Trie结构中定义一个后缀节点的概念: Trie中对应路径(已有模式)去掉部分前缀字符后剩余的后缀字符在Trie中对应的结点. 比如上图中, h作为sh的一个后缀, h对应的Trie节点74就是sh对应节点90的后缀节点. 等于说, 节点和其后缀节点对应的模式有一部分后缀是相同.\n如果知道了每一个节点的后缀节点, 那么在匹配的过程中, 在任一位置匹配失败, 都可以通过失败转移的方式转移到后缀节点, 继续进行后续匹配, 而不会遗漏, 因为后缀节点对应这个目前为止已匹配字符的某一部分后缀. 等于说, 后缀节点告诉我们, 在字符串中出现与模式不同的字符串时(匹配失败), 如何转移到其他状态.\n所以问题的关键又变成了如何求后缀节点.\n求后缀节点 观察Trie结构可以发现两个要点\n字符串任何一个位置对应的状态节点，一定比它的后缀节点更深，比如前面例子中状态节点90在第二层, 而其后缀节点74在第一层. 这点也是理所当然的, 毕竟后缀比较短. 从动态规划的角度考虑, 字符串任一位置i对应的状态节点的后缀节点一定是k\u0026lt;i的节点中的某一个. 因为每一个状态i都是由其父节点j通过某一个字符c转移而来, 那么i的后缀节点一定是j的后缀节点通过同样的字符c转移而来. 或者说, 如果j的后缀节点是jj, 那么j和jj有着相同的后缀, 它们通过同样的转移字符c转移后, 二者到达的节点也一定有着相同的后缀. 比如上面Ushers自动机例子中, 如果用字符串sshis来跑, 那么ssh对应的状态90, 是由前缀ss通过字符h转移而来. 因为ssh的后缀节点, 同样是某一个有共同后缀的字符(h或者sh)对应的状态(在这里是h对应的74). 可以发现74是由根节点0通过同样的字符h转移而来的. 反过来说, 节点0就是节点90的父节点85的后缀节点.\n在多个模式中, 如果有某模式的前缀刚好是另一模式的子串(后缀). 比如上面Ushers自动机例子中, 模式her(或者he)的前缀he就是模式she的子串, 则会二者存在失败转移的关联. 如果没有, 那么就跳回初始状态节点.\n所以补全所有状态转移的具体实现方法就是运用动态规划的原理:\n从Trie根节点开始, 逐层往下补全每一层的状态转移, 也就是宽度优先遍历(BFS), 这样下层的状态转移就可以利用上层的结果. 动态规划的转移方程可以描述为: 每一个通过字符c转移而来的状态节点i的后缀节点 = i的父节点的后缀节点通过c转移到的状态节点 初始状态包含两部分: 一个是根节点(初始状态0), 它的后缀节点就是它自己, 另一个是第一层的状态节点, 如85, 74, 因为它们对应的是长度为1的字符, 没有后缀, 所以它们的后缀节点也是根节点0. 在实现中还要注意, 后缀结点为标记结点的结点也需要被标记. 因为在状态转移过程中, 如果某个虚线转移刚好转移到终结节点, 但在字符串遍历的过程中, 并没有选择走这一条线, 就会忽略了这个终结节点, 导致匹配失败, 或者多走了更多的路. 比如在上面的例子中, 如果把模式she改为shee, 91不再是终结节点, 而是延伸到92为终结节点, 91的后缀节点是76. 如果用字符串sshe来跑这个DFA, 就会出现走到最后字符e时, 在节点91结束, 匹配失败. 所以需要把91也标记为终结节点.\n实现代码 /** 把字典通过insert把所有单词插入Trie树， * 然后通过setSuffix()构建出对应的Trie图， * 然后从Trie图的根节点开始，沿着文章str的每一个字符，走出对应的边， * 直到遇到一个标记结点或者整个str都遍历完成 */ public static class Trie { private TrieNode trie; Queue\u0026lt;TrieNode\u0026gt; queue; public Trie() { trie = new TrieNode(null, \u0026#39; \u0026#39;); queue = new LinkedList\u0026lt;\u0026gt;(); } public void insert(String word) { TrieNode curNode = trie; for (char x : word.toCharArray()) { curNode = insert(curNode, x); } curNode.setLast(true); } /** insert char x, means create a new node in the x edge. * return created node */ private TrieNode insert(TrieNode node, char x) { if (node.get(x) == null) { node.set(x); } return node.get(x); } /** BFS on the trie */ public void setSuffix() { queue.add(trie); while (!queue.isEmpty()) { /** poll() removes the present head. http://www.tutorialspoint.com/java/util/linkedlist_poll.htm */ TrieNode node = queue.poll(); setSuffix(node); complementDFA(node); } } /** Set node\u0026#39;s suffix, complement lacking edge * */ private TrieNode setSuffix(TrieNode node) { if (node.root == null) { // Trie root node.suffix = node; } else if (node.root.root == null) { node.suffix = node.root.suffix; } else { node.suffix = node.root.suffix.get(node.fromIndex); } if (node.suffix.isLast) { node.isLast = true; } return node.suffix; } /** Complement DFA according to suffix */ private void complementDFA(TrieNode node) { if (node.isLast) { return; } for (int i = 0; i \u0026lt; node.edges.length; i++) { if (node.edges[i] == null) { if (node.root == null) { node.edges[i] = node; } else { node.edges[i] = node.suffix.edges[i]; } } else { queue.add(node.edges[i]); } } } public boolean search(String s) { boolean contains = false; TrieNode curNode = trie; for (int i = 0; i \u0026lt; s.length(); i++) { char x = s.charAt(i); curNode = curNode.get(x); if (curNode.isLast) { contains = true; break; } } return contains; } public static class TrieNode { static final int R = 26; static final int ATO0 = 97; boolean isLast; TrieNode[] edges; TrieNode root; char fromIndex; TrieNode suffix; public TrieNode(TrieNode root, char from) { this.root = root; fromIndex = from; edges = new TrieNode[R]; isLast = false; } public TrieNode get(char ch) { return edges[ch - ATO0]; } /** instantiate the ch child in edges */ public void set(char ch) { edges[ch - ATO0] = new TrieNode(this, ch); } public void setLast(boolean isLast) { this.isLast = isLast; } } public static void main(String[] args) { Scanner in = new Scanner(System.in); Trie t = new Trie(); String[] X = {\u0026#34;sb\u0026#34;, \u0026#34;dsb\u0026#34;, \u0026#34;cjdsb\u0026#34;, \u0026#34;qnmlgb\u0026#34;}; for (String x : X) { t.insert(x); } t.setSuffix(); String s = \u0026#34;aadbaaadaaac\u0026#34;; if (t.search(s)) { System.out.println(\u0026#34;YES\u0026#34;); } else { System.out.println(\u0026#34;NO\u0026#34;); } } } ","permalink":"https://congchan.github.io/posts/%E5%92%8C%E8%B0%90-%E5%A4%9A%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95-ac%E8%87%AA%E5%8A%A8%E6%9C%BA/","summary":"\u003cp\u003e虽然KMP可以用于\u003ca href=\"/NLP-01-string-searching-algorithm-01-kmp\"\u003e单模式匹配问题\u003c/a\u003e，但如果是多模式问题, KMP的性能就得不到保证。比如根据墙内法律要求, 墙内的搜索引擎需要过滤敏感词后才能合法运营。敏感词的数量不少, 如果要求包含敏感词的网页不能被搜索到, 那么搜索引擎在爬取网页信息时, 就要标记网页的文本中是否包含任意个敏感词.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e这就是典型的多模匹配问题. 这种情况下如果使用Trie，那么需要遍历网页的每一个字符位置，对每一个位置进行Trie前缀匹配。如果词典的词语数量为N，每个词语长度为L，文章的长度为M，那么需要进行的计算次数是在\u003ccode\u003eN*M*L\u003c/code\u003e这个级别的. 即使把词语的长度L简化为常数级别的, 整个算法的复杂度也至少是$O(n^2)$.\u003c/p\u003e\n\u003ch2 id=\"ac自动机\"\u003eAC自动机\u003c/h2\u003e\n\u003cp\u003e可以看到，KMP算法可以避免back up（在检查字符的过程中不需要回头），而Trie可以存储多个模式的信息。如果把二者结合在一起，也许能从性能上解决多模式（任意位置）匹配问题。这就是Aho–Corasick算法（AC自动机）。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAho–Corasick算法是由Alfred V. Aho和Margaret J.Corasick 发明的字符串搜索算法，用于在输入的一串字符串中匹配有限组字典中的子串。它与普通字符串匹配的不同点在于同时与所有字典串进行匹配。算法均摊情况下具有近似于线性的时间复杂度，约为字符串的长度加所有匹配的数量。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e所以算法的关键就是通过Trie把多个模式构建为一个DFA（Deterministic finite state automaton），然后让模式串末尾对应的状态作为一个DFA的终止节点。这样，对于一个要检查的长字符串（如一段网页内容），让这个字符串在DFA上跑一趟，每一个字符表示一种跳转方式，如果这段字符能够跳到任何一个终结节点, 那么就表明这段字符串匹配了至少一个模式, 如果整段字符跑完都没到达终结节点, 那么这个网页就是\u0026quot;和谐的\u0026quot;.\u003c/p\u003e\n\u003cp\u003e在单模式匹配中, 用KMP构建的DFA是比较简单的, 从左到右, 开头的状态就是开始状态, 结尾的状态就是结束状态:\n\u003cimg loading=\"lazy\" src=\"/images/build_dfa.png\" title=\"image from: https://www.coursera.org/learn/algorithms-part2/\"\u003e\n而多模式匹配中, 在Trie的结构基础上构建出来的DFA更像一个DFA的样子:\n\u003cimg loading=\"lazy\" src=\"/images/ushers_dfa.png\" title=\"经典的ushers自动机，模式串是he/ she/ his /hers, 忽略了部分到根节点的转移边. image from: https://baike.baidu.com/pic\"\u003e\nTrie中的节点, 就类似于DFA中的状态. 如果让字符串\u003ccode\u003eshis\u003c/code\u003e在上面跑, 假如仅仅是靠Trie(也即是没有虚线标识的转移), 那么第一次从字符串的第一个字符\u003ccode\u003es\u003c/code\u003e开始转移, 经过转移路径\u003ccode\u003e0 - 85 - 90\u003c/code\u003e之后就转不动了, 因为Trie记录的模式中没有\u003ccode\u003eshi\u003c/code\u003e, 这个时候得back up, 从第二个位置\u003ccode\u003eh\u003c/code\u003e开始再匹配一遍. 这个过程中就产生重复匹配, 而参考KMP的思路, 在匹配\u003ccode\u003eshi\u003c/code\u003e的过程中, 其实已经挖掘出了\u003ccode\u003ehi\u003c/code\u003e这个子串了, 而这个子串是跟模式\u003ccode\u003ehis\u003c/code\u003e对应的, 如果有办法不回头继续匹配下去就能提高性能了.\u003c/p\u003e\n\u003cp\u003e而DFA中虚线的失败转移就是用来解决这个问题的: 当走到状态\u003ccode\u003e90\u003c/code\u003e时, 前面有了小部分子串\u003ccode\u003eh\u003c/code\u003e刚好对应状态\u003ccode\u003e74\u003c/code\u003e, 这个时候用虚线作为失败转移, 转移到\u003ccode\u003e74\u003c/code\u003e, 在状态\u003ccode\u003e74\u003c/code\u003e中寻找下一个转移\u003ccode\u003ei\u003c/code\u003e, 这样就实现了不回头继续匹配了.\u003c/p\u003e","title":"“和谐” - 多模式匹配算法 - AC自动机"},{"content":"Trie 也称字典树，名称来源于Retrieval，支持$O(n)$插入和查询操作，以空间换取时间的数据结构. 用于词频统计和输入统计领域, 可以高效地存储大规模的字典数据, 也可以用于模糊匹配, 搜索最长前缀词等.\nA trie, also called digital tree, radix tree or prefix tree is a kind of search tree - an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node.\n![](/images/Trie_example.png \u0026ldquo;A trie for keys \u0026ldquo;A\u0026rdquo;,\u0026ldquo;to\u0026rdquo;, \u0026ldquo;tea\u0026rdquo;, \u0026ldquo;ted\u0026rdquo;, \u0026ldquo;ten\u0026rdquo;, \u0026ldquo;i\u0026rdquo;, \u0026ldquo;in\u0026rdquo;, and \u0026ldquo;inn\u0026rdquo;. Image from https://en.wikipedia.org/wiki/Trie\")\nTrie Trie没有规定每一个节点的分支数量, 用R-way Trie来表示分支数量为R的Trie. 对于不同的应用, 可以设置不同的R.\n字符（模糊）匹配与拼写检查 应用例子是在一本字典中查找特定前缀的所有单词. 简化的例子是在英文字典中, 根据查询前缀, 返回相同前缀的所有单词数. 同样的结构可以用来检查拼写错误. 那么只需要在每一个节点存储该节点以下所有单词数就行了. 每一个节点包含一个长度26的数组，以方便快速定位对应的26个字母, 类似B-tree:\n/** * from https://algs4.cs.princeton.edu/code/edu/princeton/cs/algs4/TrieSET.java.html * @author Robert Sedgewick * @author Kevin Wayne */ public class TrieSET implements Iterable\u0026lt;String\u0026gt; { private static final int R = 256; // extended ASCII private Node root; // root of trie private int n; // number of keys in trie // R-way trie node private static class Node { private Node[] next = new Node[R]; private boolean isString; } /** * Initializes an empty set of strings. */ public TrieSET() { } /** * Does the set contain the given key? * @param key the key * @return {@code true} if the set contains {@code key} and * {@code false} otherwise * @throws IllegalArgumentException if {@code key} is {@code null} */ public boolean contains(String key) { if (key == null) throw new IllegalArgumentException(\u0026#34;argument to contains() is null\u0026#34;); Node x = get(root, key, 0); if (x == null) return false; return x.isString; } private Node get(Node x, String key, int d) { if (x == null) return null; if (d == key.length()) return x; char c = key.charAt(d); return get(x.next[c], key, d+1); } /** * Adds the key to the set if it is not already present. * @param key the key to add * @throws IllegalArgumentException if {@code key} is {@code null} */ public void add(String key) { if (key == null) throw new IllegalArgumentException(\u0026#34;argument to add() is null\u0026#34;); root = add(root, key, 0); } private Node add(Node x, String key, int d) { if (x == null) x = new Node(); if (d == key.length()) { if (!x.isString) n++; x.isString = true; } else { char c = key.charAt(d); x.next[c] = add(x.next[c], key, d+1); } return x; } /** * Returns the number of strings in the set. * @return the number of strings in the set */ public int size() { return n; } /** * Is the set empty? * @return {@code true} if the set is empty, and {@code false} otherwise */ public boolean isEmpty() { return size() == 0; } /** * Returns all of the keys in the set, as an iterator. * To iterate over all of the keys in a set named {@code set}, use the * foreach notation: {@code for (Key key : set)}. * @return an iterator to all of the keys in the set */ public Iterator\u0026lt;String\u0026gt; iterator() { return keysWithPrefix(\u0026#34;\u0026#34;).iterator(); } /** * Returns all of the keys in the set that start with {@code prefix}. * @param prefix the prefix * @return all of the keys in the set that start with {@code prefix}, * as an iterable */ public Iterable\u0026lt;String\u0026gt; keysWithPrefix(String prefix) { Queue\u0026lt;String\u0026gt; results = new Queue\u0026lt;String\u0026gt;(); Node x = get(root, prefix, 0); collect(x, new StringBuilder(prefix), results); return results; } private void collect(Node x, StringBuilder prefix, Queue\u0026lt;String\u0026gt; results) { if (x == null) return; if (x.isString) results.enqueue(prefix.toString()); for (char c = 0; c \u0026lt; R; c++) { prefix.append(c); collect(x.next[c], prefix, results); prefix.deleteCharAt(prefix.length() - 1); } } /** * Returns all of the keys in the set that match {@code pattern}, * where . symbol is treated as a wildcard character. * @param pattern the pattern * @return all of the keys in the set that match {@code pattern}, * as an iterable, where . is treated as a wildcard character. */ public Iterable\u0026lt;String\u0026gt; keysThatMatch(String pattern) { Queue\u0026lt;String\u0026gt; results = new Queue\u0026lt;String\u0026gt;(); StringBuilder prefix = new StringBuilder(); collect(root, prefix, pattern, results); return results; } private void collect(Node x, StringBuilder prefix, String pattern, Queue\u0026lt;String\u0026gt; results) { if (x == null) return; int d = prefix.length(); if (d == pattern.length() \u0026amp;\u0026amp; x.isString) results.enqueue(prefix.toString()); if (d == pattern.length()) return; char c = pattern.charAt(d); if (c == \u0026#39;.\u0026#39;) { for (char ch = 0; ch \u0026lt; R; ch++) { prefix.append(ch); collect(x.next[ch], prefix, pattern, results); prefix.deleteCharAt(prefix.length() - 1); } } else { prefix.append(c); collect(x.next[c], prefix, pattern, results); prefix.deleteCharAt(prefix.length() - 1); } } /** * Returns the string in the set that is the longest prefix of {@code query}, * or {@code null}, if no such string. * @param query the query string * @return the string in the set that is the longest prefix of {@code query}, * or {@code null} if no such string * @throws IllegalArgumentException if {@code query} is {@code null} */ public String longestPrefixOf(String query) { if (query == null) throw new IllegalArgumentException(\u0026#34;argument to longestPrefixOf() is null\u0026#34;); int length = longestPrefixOf(root, query, 0, -1); if (length == -1) return null; return query.substring(0, length); } // returns the length of the longest string key in the subtrie // rooted at x that is a prefix of the query string, // assuming the first d character match and we have already // found a prefix match of length length private int longestPrefixOf(Node x, String query, int d, int length) { if (x == null) return length; if (x.isString) length = d; if (d == query.length()) return length; char c = query.charAt(d); return longestPrefixOf(x.next[c], query, d+1, length); } /** * Removes the key from the set if the key is present. * @param key the key * @throws IllegalArgumentException if {@code key} is {@code null} */ public void delete(String key) { if (key == null) throw new IllegalArgumentException(\u0026#34;argument to delete() is null\u0026#34;); root = delete(root, key, 0); } private Node delete(Node x, String key, int d) { if (x == null) return null; if (d == key.length()) { if (x.isString) n--; x.isString = false; } else { char c = key.charAt(d); x.next[c] = delete(x.next[c], key, d+1); } // remove subtrie rooted at x if it is completely empty if (x.isString) return x; for (int c = 0; c \u0026lt; R; c++) if (x.next[c] != null) return x; return null; } } 如果要问题扩展为返回所有相同前缀的单词，那么就要在插入字典时，在对应单词结尾的节点标记颜色。\n提高扩展性 用固定长度为26的数组来处理英文，好处是数组内存占用小，索引时也不需要搜索，直接用字符码作为索引。也可以根据ASCII码进一步扩大数组长度以支持更多字符。\n为了提高可扩展性，可以考虑用其他更灵活的数据结构来替代数组，比如HashMap，同时把HashMap放进一个TrieNode类。这样以后要修改核心的存储结构，只需要改动TrieNode即可，其余的接口不用改。\npublic static class Trie { private TrieNode node; public Trie() { this.node = new TrieNode(); } public void insert(String word) { TrieNode curNode = node; for (char x : word.toCharArray()) { curNode = curNode.set(x); } } public int search(String prefix) { TrieNode curNode = node; for (char x : prefix.toCharArray()) { if (curNode.get(x) == null) { return 0; } curNode = curNode.get(x); } return curNode.count; } public static class TrieNode { HashMap\u0026lt;Character, TrieNode\u0026gt; map; private int count; private char value; public TrieNode() { count = 0; map = new HashMap\u0026lt;\u0026gt;(); } public TrieNode(Character val) { count = 1; this.value = val; map = new HashMap\u0026lt;\u0026gt;(); } public TrieNode get(char ch) { return map.get(ch); } public TrieNode set(char ch) { TrieNode t = map.get(ch); if (t == null) { t = new TrieNode(ch); this.map.put(ch, t); } else { t.count++; } return t; } public int getCount() { return this.count; } public char getValue() { return this.value; } } } HashMap的寻址虽然会靠字符码作为地址的数组慢一点点，但也是非常快的:$O(\\log N)$。但HashMap本身是比较耗内存的数据结构, 所以如果知道要处理的数据是在特定范围内的, 比如节点就是在256个字符中, 那么还是不要不用HashMap.\n","permalink":"https://congchan.github.io/posts/%E5%8D%95%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%E4%B8%8E%E6%8B%BC%E5%86%99%E6%A3%80%E6%9F%A5-trie/","summary":"\u003cp\u003eTrie 也称字典树，名称来源于Re\u003cfont color=\"red\"\u003etrie\u003c/font\u003eval，支持$O(n)$插入和查询操作，以空间换取时间的数据结构. 用于词频统计和输入统计领域, 可以高效地存储大规模的字典数据, 也可以用于模糊匹配, 搜索最长前缀词等.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA \u003cstrong\u003etrie\u003c/strong\u003e, also called \u003cstrong\u003edigital tre\u003c/strong\u003ee, \u003cstrong\u003eradix tree\u003c/strong\u003e or \u003cstrong\u003eprefix tree\u003c/strong\u003e is a kind of search tree - an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest. Hence, keys are not necessarily associated with every node.\u003c/p\u003e","title":"单模式匹配与拼写检查 - Trie"},{"content":"Rabin-Karp Fingerprint Rabin-Karp fingerprint(RK) 基于 modular hashing：\nCompute a hash of pattern characters 0 to M - 1. For each i, compute a hash of text characters i to M + i - 1. If pattern hash = text substring hash, check for a match. 如果在一一比较中对text的每个子串都重新计算hash，那么速度比暴力算法还慢。\n所以算法的关键在于如何高效地计算哈希值：Horner\u0026rsquo;s method - M阶多项式hash的线性时间方法\n$$a^b \\pmod c = (a \\pmod c)^b$$ 引理： $$(a \\times b) \\pmod c = [( a \\pmod c ) \\times (b \\pmod c) ] \\pmod c$$即积的取余等于取余的积的取余.\n具体证明可参考位操作 - 快速幂/快速幂取余\n根据modulars算术的基本性质，在每个算术运算后除以$Q$取余数，和执行完所有算术运算后再取除以Q的余数的结果一样。比如加法 $a+b = c$, then $a\\pmod N+b\\pmod N \\equiv c$. 其中$\\equiv$表示Congruence, 即$15 \\equiv -9\\pmod{12}$表示$15$和$9$对$12$取余的余数相同. 同理\nIf $a\\equiv b\\pmod N$, then $a+k \\equiv b+k \\pmod N$ for any integer $k$. If $a\\equiv b\\pmod N$, and $c\\equiv d\\pmod N$, then $a+c \\equiv b+d \\pmod N$. If $a \\equiv b\\pmod N$, then $-a \\equiv -b\\pmod N$. 乘法运算的取余满足:\nIf $a \\cdot b = c$, then $a\\pmod N\\cdot b\\pmod N \\equiv c \\pmod{N}$. If $a \\equiv b \\pmod{N}$, then $ka \\equiv kb \\pmod{N}$ for any integer $k$. If $a \\equiv b \\pmod{N}$ and $c \\equiv d \\pmod{N}$, then $ac \\equiv bd \\pmod{N}$. Exponentiation:\nIf $a\\equiv b\\pmod{N}$, then $a^k \\equiv b^k \\pmod{N}$ for any positive integer kk. 使用多项式 Hash 计算pattern的hash，$h(d) = \\Sigma d_i \\times b^i \\pmod Q$： // Compute hash for M-digit key private long hash(String key, int M) { long h = 0; for (int j = 0; j \u0026lt; M; j++) h = (R * h + key.charAt(j)) % Q; return h; } RK算法的核心是，在pattern和text的对比中，如何动态地利用前面位置i已计算的值来计算新位置i+1的哈希. 给定$x_i$，如何计算$x_{i+1}$： $$x_i = t_i R^{M-1} + T_{i+1}R^{M-2} + ... + t_{i+M-1}R^0$$ 其中M-digit, base-R integer, modulo Q $$x_{i+1} = t_{i+1}R^{M-1} + T_{i+2}R^{M-2} + ... + t_{i+M}R^0$$ $$x_{i+1} = (x_i - t_i R^{M-1}) R + t_{i+M}$$根据这个关系可知, 我们不必动态维护$x_i$值，而只需维护其除$Q$的余数即可。而且，$R^{M-1}$是可以预先计算的.\n同时为了避免出现负数, 需要在每次运算中加一次Q(不影响取余结果) $$\\begin{equation} \\begin{aligned} (x_i + 1) \\pmod Q \u0026= (x_i + Q - t_i \\times RM) \\pmod Q \\\\ \u0026= (x_i - t_i \\times RM) \\pmod Q + (t_i \\times Q) \\pmod Q \\end{aligned} \\end{equation} $$于是得出具体的搜索方法\npublic class RabinKarp { private String pat; // pattern (only needed for Las Vegas) private long patHash; // pattern hash value private int M; // pattern length private long Q; // a large prime private int R = 256; // alphabet size private long RM; // R^(M-1) % Q public RabinKarp(String pat) { this.pat = pat; // save pattern (only needed for Las Vegas) this.M = pat.length(); Q = longRandomPrime(); // See Exercise 5.3.33. RM = 1; for (int i = 1; i \u0026lt;= M-1; i++) // Compute R^(M-1) % Q for use RM = (R * RM) % Q; // in removing leading digit. patHash = hash(pat, M); } /** check for hash collision using rolling hash function */ public int search(String txt) { // Search for hash match in text. int N = txt.length(); long txtHash = hash(txt, M); if (patHash == txtHash) return 0; // Match at beginning. for (int i = M; i \u0026lt; N; i++) { // Remove leading digit, add trailing digit, check for match. txtHash = (txtHash + Q - RM*txt.charAt(i-M) % Q) % Q; txtHash = (txtHash*R + txt.charAt(i)) % Q; if (patHash == txtHash) if (check(i - M + 1)) return i - M + 1; // match } return N; // no match found } } Monte Carlo Correctness 存在哈希冲突, 如果要保证100%的字符匹配准确, 需要在hash匹配后, 进行一次字符的比对, 这就是Las Vegas版本的RK算法。而Monte Carlo版本的RK算法通过把映射hash表的Q值取尽可能大(比如long值$10^{20}$), 使得hash冲突概率降得尽可能低(如低至$1/Q = 10^{-20}$).\nboolean check(int i) // Monte Carlo (See text.) { return true; } // For Las Vegas, check pat vs txt(i..i-M+1). 二者对比:\nMonte Carlo version. Return match if hash match. Always runs in linear time. Extremely likely to return correct answer (but not always!). Las Vegas version. Check for substring match if hash match; continue search if false collision.Always returns correct answer. Extremely likely to run in linear time (but worst case is M N). In theory, if Q is a sufficiently large random prime (about $M N^2$), then the probability of a false collision is about 1 / N. In practice, choose Q to be a large prime (but not so large to cause overflow). Under reasonable assumptions, probability of a collision is about 1 / Q.\n最长回文子串 字符串哈希还可以用于计算判断最长回文子串，需要分别预处理正着和倒着的哈希值; 判断是否可行时枚举回文中心（对称轴），哈希判断两侧是否相等。\n总结 算法可以拓展到二维模式匹配, 多模式匹配等问题.\n","permalink":"https://congchan.github.io/posts/%E5%AD%97%E7%AC%A6%E6%90%9C%E7%B4%A2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95-03-rabin-karp-fingerprint-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%93%88%E5%B8%8C/","summary":"\u003ch2 id=\"rabin-karp-fingerprint\"\u003eRabin-Karp Fingerprint\u003c/h2\u003e\n\u003cp\u003eRabin-Karp fingerprint(RK) 基于 modular hashing：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompute a hash of pattern characters \u003ccode\u003e0\u003c/code\u003e to \u003ccode\u003eM - 1\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eFor each \u003ccode\u003ei\u003c/code\u003e, compute a hash of text characters \u003ccode\u003ei\u003c/code\u003e to \u003ccode\u003eM + i - 1\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eIf pattern hash = text substring hash, check for a match.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e如果在一一比较中对text的每个子串都重新计算hash，那么速度比暴力算法还慢。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e所以算法的关键在于如何高效地计算哈希值：Horner\u0026rsquo;s method - M阶多项式hash的线性时间方法\u003c/p\u003e\n$$a^b \\pmod c = (a \\pmod c)^b$$\u003cp\u003e\n引理：\n\u003c/p\u003e\n$$(a \\times b) \\pmod c = [( a \\pmod c ) \\times (b \\pmod c) ] \\pmod c$$\u003cp\u003e即积的取余等于取余的积的取余.\u003c/p\u003e","title":"字符搜索匹配算法 03 Rabin-Karp Fingerprint \u0026 字符串哈希"},{"content":" In computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.\n字符串搜索/匹配算法在大规模文本应用中有非常重要的作用，比如文章敏感词搜索，多关键词过滤搜索等。如果使用暴力搜索，则时间复杂度很高（若 m 为关键字的长度， n 为要待搜索的字符串长度， k为关键字数量，则复杂度为$O(n \\times m \\times k)$。而好的算法可以让这些问题的时间复杂度大大降低。\n常用的算法有Knuth–Morris–Pratt(KMP), Boyer-Moore(BM), Rabin-Karp(RK), Trie, Trie图, AC自动机等.\n一个实例 匹配时，想象我们拿着模式字符串pat=ABABAC, 像尺子一样从左到右对齐依次匹配如图的txt。\n从txt[i=0]开始, 把pat的开头pat[j=0]对齐txt[0], 开始比较pat[0]和txt[0],\n发现不匹配, 暴力的算法是从txt下一个字符重新开始i=1, 同时把尺子也右移一位对齐新的txt起始点. 从i=3开始, 发现一开始可以匹配上(pat[j=0] == txt[3]), 那么保持尺子不动, 开始比较pat[j+1]和txt[i+1], 结果不匹配. 从i=4开始, 情况也类似, 而且发现连续匹配上了pat[++j]和txt[++i], 假如运气好, 我们能匹配完整个尺子, 那么达到目的. 可惜在i=7时失败了. 问题的关键就是i=3和i=7这里, 特别是i=7, 假如还是用暴力解法1操作, 那么需要重新比对txt[i=5,6,7...]. 但前面已经匹配了一半的尺子了, 那么其实我们已经知道了txt的后缀txt[4,5,6]和尺子的前缀pat[0,1,2]重合, 我们能否利用这个信息来优化算法? 按照前面的分析, 每一个已匹配的前缀等于txt中已匹配的后缀, 那么txt后缀后面可能接的字符有R种, 我们可以提前计算每一个已匹配txt后缀后接每一种字符时, 应该怎么做.\n自动机匹配字符 我们可以建立一个有限自动机，该自动机会扫描文本字符串T以查找模式P的所有出现。这些字符串匹配自动机非常高效：只对每个文本字符检查一次，每个文本字符花费固定的时间, 匹配开销是O(n)。但是需要预处理pattern以构建自动机。\n为了方便说明, 先定义一个有关对应 pattern $P$的后缀函数(Suffix function)$\\sigma(x)$, 该函数返回$P$和字符串$x$的后缀重叠的最长前缀的长度.\n比如对于$P = ab$, $\\sigma(ccaca) = 1$, $\\sigma(ccab) = 2$, 空字符$\\epsilon$可以是任何字符的前缀, $\\sigma(\\epsilon) = 0$,\n定义自动机用于读取字符串:\n自动机的状态$\\Phi(w)$表示读取字符$w$后达到的状态$M$, $\\Phi (\\epsilon) = q_0$. 定义自动机的有限状态集为${0, 1, ..., m}$, 起始状态$q_0 = 0$, 最终接收状态是$m$; 设置转移函数$\\delta$为$\\delta(q, a) = \\sigma(P_qa)$, 这么定义是为了追踪当前已经匹配的pattern最长前缀. $\\Phi (wa) = \\delta(\\Phi(w), a)$ 该自动机满足不变性 $\\Phi(T_i) = \\sigma(T_i) = q$. 那么当自动机在状态q时读入下一个条件字符T[i+1] = a, 自动机要把状态转移到和$T_ia$的后缀匹配的$P$最长前缀对应的状态$\\sigma(T_ia)$. 因为$P_q$是$P$和$T_i$的后缀匹配的最长前缀, 因此不严谨地推理出$\\sigma(T_ia) = \\sigma(P_qa)$.\nDFA KMP Knuth版本的KMP算法, 就是用**确定性有限自动机（Deterministic Finite Automaton, DFA）**来匹配字符：\n有限数量的状态（包括开始和停止）, 中间状态对应txt的后缀和pattern的重叠。 确定性: 字母表每个字符对应一个状态转移, 每个状态转移都对应一个确定的字符。 只接受能通往停止状态的转换序列。 DFA以dfa[i][j]状态矩阵的数据结构应用于KMP算法,\n每一行对应一种字符, 字符指示了跳转的条件 每一列对应一种后缀: 第j列的含义是, txt后缀已经匹配了pat[..., j-1]后根据各行条件字符所应跳转到的状态 每一列只有一个字符行是匹配跳转, 匹配跳转永远是跳转到j+1状态(列), 其他都是不匹配跳转. 确定性状态: j = 0..len(pat), 在DFA的各个确定性状态中跳转, 保证了当前不管处于哪个状态, j等于已经成功匹配的pattern前缀长度, 也就是pattern的前缀pattern[0...j]刚好是txt的最长匹配后缀txt[0..i]. 匹配的过程, 被抽象为把txt的字符从左到右依次输入DFA, 并根据每次读入的字符决定跳去哪个状态, 如果能够到达终点状态, 那就是有一个完整的匹配.\n如ABCAABABABAB对应0→1→2→0→1→1→2→3→4→5→4→5→4\n在匹配的过程中, 假设已经对比了txt[i]和pat[j], 那么state = dfa[txt[i]][j]就是转移到下一个状态state, 接下来需要对比txt[i+1]和pat[state].\n如果匹配, 那么只需要继续检查txt下一个字符i++, 此时我们要求dfa[pat.charAt(j)][j] = j+1, 也就是和pattern的下一个字符对比. 如果不匹配, 我们不仅已经知道了txt[i], 也知道了txt的后j-1个字符刚好是pattern的前j-1个字符 回到这个实例中: 在遇到不匹配时, 有了dfa就可以知道在ABA_后面分别遇到A, B和C时应该如何对齐尺子, 同时不需要回溯, 也就是i++. 比如i=7时, 是属于ABA_接C, 对应dfa[C][3]=0, 同时把尺子的开头对齐到i=8, 继续比对就好了, 不用担心前面txt[5,6,7]会不会遗漏什么.\npublic int search(String txt) { int i, j, N = txt.length(); for (i = 0, j = 0; i \u0026lt; N \u0026amp;\u0026amp; j \u0026lt; M; i++) j = dfa[txt.charAt(i)][j]; if (j == M) return i - M; // found else return N; // not found } Running time: at most N character accesses to Simulate DFA on text.\nKMP因为保证了字符串遍历指针只会前进不会后退, 所以可以接受stream输入.\n构建DFA 首先需要了解DFA状态矩阵的特性, 那就是每一列的状态j只能在0到j+1转移, 也就是只能在第一列和右边相邻一列之间跳转. 这个特性, 和匹配过程中pattern的移位对齐方式一致. 抽象的DFA只能从左往右逐个生成状态. 所以我们从左往右生成/完善DFA, 也就说从左列往右列构建dfa[][]矩阵. 在构建第j个状态时, 我们只需要知道DFA如何读取前j-1个字符进行转移，因此我们始终可以从前面部分构建的半成品DFA中获得下一步状态所需的信息。\n对于patternABABAC, 其5个前缀列表的跳转方向对应了DFA的5个状态0, ..., 5.\n第一列(状态0)也就是对应的前缀pat[-1] = ...在遇到pat[0] = A时才能前进, 其他行都是维持原状态0不变. 关键是如何确定其他不匹配字符的跳转状态 j 0 1 2 3 4 5 A B A B A C ------------- A | 1 - 3 - 5 - B | 0 2 - 4 - - C | 0 - - - - 6 设想DFA还在构建中, 只是刚刚完成第一列. 此时可应对匹配pattern至多前两个前缀(...和A...)的情况.\n然后我们需要构建第1列:\n设想如果txt[...i]的后缀匹配了前缀pat[j-1] = A...后, 进入了状态dfa[A][0] = 1, 但此时1列还没构建; 如果下一个字符txt[i+1]=C和pat[j]对比发现不匹配时，我们需要在DFA此时状态栏dfa[C][1]存储正确的跳转. 如何得出正确跳转? 就是其转移目标等同于用暴力算法得到结果. 暴力算法选择用txt[i+1]和pat[0]比对, 此时等价于把txt[i+1] = C输入当前构建中的DFA, 会定位到dfa[C][0], 跳转到0. 这个过等价于给txt后缀txt[i+1] = ...AC寻找和pattern重叠的前缀...(没有重叠, 状态0), 等于pattern开头移位对齐到txt[i+2].\n此时的DFA虽然还是半成品, 只包含了pat[0]这部分信息, 却可以据此推测出pat[.][1]的信息;\n同理, 假如txt[i+1]=A, 可以推测出pat[A][1] = dfa[A][0] = 1; 这样我们就推理出了DFA第二个状态在R字符空间的所有确定性转移.\nX ↓ j 0 1 2 3 4 5 A B A B A C ------------- A | 1 1 3 - 5 - B | 0 2 - 4 - - C | 0 0 - - - 6 如果用X临时变量来存储上一次复制的状态(0), 那么上面的操作概括为dfa[c][j] = dfa[c][X];, 其中j代表当前待构建的状态列, c代表不匹配的条件字符. 这里X也指示了pattern移位对齐的位置, 在Robert Sedgewick的书中, 也把这种对齐称之为DFA restart position.\n依次类推\u0026hellip;\n假设我们推理到状态5, (根据前面的流程此时X=3), 同样设想txt[i, ..., i+4]后缀已经匹配了前缀pat[0, ..., j-1] = ABABA...后, 假如下一个字符txt[i+5]=B和pat[j]对比不匹配. 暴力检索需要回溯后缀txt[i+1, ..., i+4] = ...BABA, 输入DFA可定位到状态3, 然后复制dfa[B][3] = 4; pattern和txt后缀txt[i+1, ...] = ...BABAB重叠的前缀是ABAB..., 直接可推测出移位pattern和txt[i+6]对齐的索引是4.\n总结算法: 构建每一个状态state j时, 表示模式pat的前j个字符已经匹配. 用dfa[r][j]表示在状态state j时, 遇到下一个字符r时应该转移到哪个状态：\n对于不匹配的字符，copy dfa[][X] to dfa[][j]. 对于匹配的字符，dfa[pat.charAt(j)][j] = j+1. Update X. public dfaKMP(String pat) { this.pat = pat; M = pat.length(); dfa = new int[R][M]; dfa[pat.charAt(0)][0] = 1; for (int X = 0, j = 1; j \u0026lt; M; j++) { for (int c = 0; c \u0026lt; R; c++) dfa[c][j] = dfa[c][X]; // copy mismatch cases dfa[pat.charAt(j)][j] = j+1; // set match cases X = dfa[pat.charAt(j)][X]; // update restart state } } Running time. M character accesses (but space/time proportional to R M).\nNFA KMP Knuth版本的KMP问题是当R也就是字符集较大时, 比如有一个像Unicode这样的字符集，即使pattern仅由几个不同的字母组成，存储该DFA所需的内存也会很大。\n改进方法是由Knuth的学生Pratt提出的用**非确定性有限自动机（Non-deterministic Finite Automaton, NFA）**来解决. 既然导致DFA矩阵过大的原因是字符集R过大, 那么可否考虑把所有not match的字符归为一种不匹配转移? 这样在FSM中，任何状态下都只有两种转移：匹配和不匹配。很容易看出，所需的额外空间相对于M（pattern的长度）是线性的。这个版本的KMP算法就是最后三位作者联名发表的论文呈现的方法.\n代价就是当出现不匹配字符时无法保证确定性, 因为导致不匹配的字符有很多种, 所以是一种NFA。因为状态序列和pattern序列一一对应, 而且只有两种状态转移, 其中一种是确定性的, 确定性的转移永远是text[i] == pat[j], state += 1, 所以只需要保存不匹配状态转移, 也就是用一维数组保存在每一个状态遇到不匹配时应该转移到哪一个状态.\nBorder 不匹配时如何转移是nfa模型的关键. KMP使用一个辅助数组提前存储有关pattern自身的信息(这种信息称之为Border), 以达到在线计算自动机转移函数的目的. 定义一个有关pattern $P$的Border(或者称之为Prefix function), 该函数封装了有关$P$如何匹配偏移的$P$的知识。\n比如这种情况:\n0 1 2 3 4 5 6 7 8 9 ... a b c a b c a b d a b c a b d a b c a b d 如果提前知道了pattern[...,4]的后缀ab和其前缀一样, 那么就可以直接把重合的前缀移位对齐过去. 也就是需要提前计算pattern每一个前缀p[...i]的前后缀重复情况.\n构建patternABABAC的前缀列表, 根据索引位置分别编号为0...6: 0 ε... 什么都没匹配 1 A... 2 AB... 3 ABA... 4 ABAB... 5 ABABA... 6 ABABAC 完全匹配\n每一个前缀自身都可能存在前后缀重复的情况. 比如前缀ABAB...就有前后缀AB是重合的, 也就是AB既是ABAB...的proper prefix又是其proper suffix. 定义Border[k], 每一个位置计算了前缀pattern[...k]的最长的重叠前后缀r的宽度: $$r = x_0, ..., x_{i-1} = x_{k-i}, ..., x_{k-1}$, 其中$i \\in \\\\{0, ..., k-1 \\\\}$$比如abacab的border是ε和ab. ε的宽度为0.\n在预处理阶段，确定pattern每个前缀的最宽border的宽度。然后在搜索阶段，可以根据已匹配的前缀来计算移位距离。\n首先要知道border的性质:\nLet r, s be borders of a string x, where |r| \u0026lt; |s|. Then r is a border of s. If s is the widest border of x, the next-widest border r of x is obtained as the widest border of s etc.\n使用指针i指定pat前缀和指针j指向候选border, 如果pat[i] = pat[j], 那么border[i]就能扩展至j, 也就是border[i] = j\nj i ↓ ↓ ---# ... ---? ↑ b[i] 当出现不匹配时, 通过border的递归特性选择更小的border, 逐个尝试. 比如$\\pi^{*}[5] = \\\\{3, 1, 0 \\\\}$.\nint[] border(String pat) { int i = 0, j = -1, m = pat.length(); int[] b = new int[m + 1]; b[i] = j; while (i \u0026lt; m) { while (j \u0026gt;= 0 \u0026amp;\u0026amp; pat.charAt(i) != pat.charAt(j)) j = b[j]; i++; j++; b[i] = j; } return b; } running time $\\Theta(m)$\nnfa具象化为border数组, 在匹配时, 对齐移位距离由p已经匹配的前缀的最宽border来确定。\n匹配 如果把上述预处理算法应用于字符串pt = pattern + text, 如果pt的某个前缀x有宽为m = len(pattern)的border, 则意味着有匹配. 这样就得出类似上面预处理算法的匹配算法.\nint borderSearch(String txt, String pat, int[] border) { int i = 0, j = 0, n = txt.length(), m = pat.length(); while (i \u0026lt; n) { while (j \u0026gt;= 0 \u0026amp;\u0026amp; txt.charAt(i) != pat.charAt(j)) j = border[j]; i++; j++; if (j == m) { return i-j; // j = border[j]; // continue searching } } return -1; } Running time: at most N character accesses to Simulate NFA on text.\n内部的while循环中, 如果在位置j处发生不匹配，则考虑pattern长度j的已匹配前缀的最宽border。根据border宽度b[j]对齐偏移继续匹配。如果再次发生不匹配，则考虑下一个最宽的border，依此类推，直到匹配或没有边界（j = -1）为止。\nReference https://www.inf.hs-flensburg.de/lang/algorithmen/pattern/kmpen.htm http://yuex.in/post/2017/06/kmp-beauty.html ","permalink":"https://congchan.github.io/posts/%E5%AD%97%E7%AC%A6%E6%90%9C%E7%B4%A2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95-01-knuthmorrisprattkmp/","summary":"\u003cblockquote\u003e\n\u003cp\u003eIn computer science, string-searching algorithms, sometimes called string-matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e字符串搜索/匹配算法在大规模文本应用中有非常重要的作用，比如文章敏感词搜索，多关键词过滤搜索等。如果使用暴力搜索，则时间复杂度很高（若 m 为关键字的长度， n 为要待搜索的字符串长度， k为关键字数量，则复杂度为$O(n \\times m \\times k)$。而好的算法可以让这些问题的时间复杂度大大降低。\u003c/p\u003e\n\u003cp\u003e常用的算法有Knuth–Morris–Pratt(KMP), Boyer-Moore(BM), Rabin-Karp(RK), Trie, Trie图, AC自动机等.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"一个实例\"\u003e一个实例\u003c/h2\u003e\n\u003cp\u003e匹配时，想象我们拿着模式字符串\u003ccode\u003epat=ABABAC\u003c/code\u003e, 像尺子一样从左到右对齐依次匹配如图的\u003ccode\u003etxt\u003c/code\u003e。\u003cimg loading=\"lazy\" src=\"/images/kmp.png\" title=\"image from: https://algs4.cs.princeton.edu/\"\u003e\u003c/p\u003e\n\u003cp\u003e从\u003ccode\u003etxt[i=0]\u003c/code\u003e开始, 把\u003ccode\u003epat\u003c/code\u003e的开头\u003ccode\u003epat[j=0]\u003c/code\u003e对齐\u003ccode\u003etxt[0]\u003c/code\u003e, 开始比较\u003ccode\u003epat[0]\u003c/code\u003e和\u003ccode\u003etxt[0]\u003c/code\u003e,\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e发现不匹配, 暴力的算法是从txt下一个字符重新开始\u003ccode\u003ei=1\u003c/code\u003e, 同时把尺子也右移一位对齐新的txt起始点.\u003c/li\u003e\n\u003cli\u003e从\u003ccode\u003ei=3\u003c/code\u003e开始, 发现一开始可以匹配上(\u003ccode\u003epat[j=0] == txt[3]\u003c/code\u003e), 那么保持尺子不动, 开始比较\u003ccode\u003epat[j+1]\u003c/code\u003e和\u003ccode\u003etxt[i+1]\u003c/code\u003e, 结果不匹配.\u003c/li\u003e\n\u003cli\u003e从\u003ccode\u003ei=4\u003c/code\u003e开始, 情况也类似, 而且发现连续匹配上了\u003ccode\u003epat[++j]\u003c/code\u003e和\u003ccode\u003etxt[++i]\u003c/code\u003e, 假如运气好, 我们能匹配完整个尺子, 那么达到目的. 可惜在\u003ccode\u003ei=7\u003c/code\u003e时失败了.\u003c/li\u003e\n\u003cli\u003e问题的关键就是\u003ccode\u003ei=3\u003c/code\u003e和\u003ccode\u003ei=7\u003c/code\u003e这里, 特别是\u003ccode\u003ei=7\u003c/code\u003e, 假如还是用暴力解法\u003ccode\u003e1\u003c/code\u003e操作, 那么需要重新比对\u003ccode\u003etxt[i=5,6,7...]\u003c/code\u003e. 但前面已经匹配了一半的尺子了, 那么其实我们已经知道了txt的\u003cstrong\u003e后缀\u003c/strong\u003e\u003ccode\u003etxt[4,5,6]\u003c/code\u003e和尺子的\u003cstrong\u003e前缀\u003c/strong\u003e\u003ccode\u003epat[0,1,2]\u003c/code\u003e重合, 我们能否利用这个信息来优化算法?\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e按照前面的分析, 每一个已匹配的前缀等于txt中已匹配的后缀, 那么txt后缀后面可能接的字符有\u003ccode\u003eR\u003c/code\u003e种, 我们可以提前计算每一个\u003cstrong\u003e已匹配txt后缀\u003c/strong\u003e后接每一种字符时, 应该怎么做.\u003c/p\u003e","title":"字符搜索匹配算法 01 - Knuth–Morris–Pratt(KMP)"},{"content":"Boyer-Moore算法 在从头到尾对齐txt的字符时, 每一次对齐, BM算法反方向从尾到头检查patternP=BAABBAA和txt字符是否匹配。匹配过程中如果发现txt[i] != P[j], 也就说二者子串不相等, 如...XAA != ...BAA, 且字符txt[i] = X在P中不存在时，可以把P开头对齐到txt[i+7], 一次就跳过了7个位置. 模式相较于文本一般较短, 所以模式中包含的字符种类相对也比较少, 那么这样的跳跃出现情况就很可观了, 可以大大加速匹配.\n不过一般来说, 可能txt的某些子串会和P的其他子串重合，不失一般性我们需要像Knuth-Morris-Pratt算法一样的重启位置数组。\nBad Character Heuristic(BCH) 重启点位: 预先计算各个字符c在Pattern的最rightmost的位置(若无则-1), 这些位置告诉我们如果是txt中的字符c导致不匹配, pattern可以右移的距离.\nbadCharacterPreprocess(String pat) { // Compute skip table. this.pat = pat; int M = pat.length(); int R = 256; right = new int[R]; for (int c = 0; c \u0026lt; R; c++) right[c] = -1; // -1 for chars not in pattern for (int j = 0; j \u0026lt; M; j++) // rightmost position for right[pat.charAt(j)] = j; // chars in pattern } 有了right数组后, 一个例子说明匹配过程:\ni txt: ...TLE... P: NEEDLE NEEDLE j 在匹配时如果发现字符不匹配txt[i+j] != P[j]，分三种情况考虑:\nMismatch character T not in pattern: increment i one character beyond T, i += j+1. Mismatch character in pattern: mismatch character N in pattern, align text N with rightmost pattern N, i += j - right[N], 此时不会导致回退. Mismatch character in pattern (but heuristic no help): mismatch character E in pattern, align text E with rightmost pattern E ? 此时会导致回退, 也就说j - right[E] \u0026lt; 0, 这种情况就直接i += 1. public int search(String txt) { // Search for pattern in txt. int N = txt.length(); int M = pat.length(); int skip; for (int i = 0; i \u0026lt;= N-M; i += skip) { // Does the pattern match the text at position i ? skip = 0; for (int j = M-1; j \u0026gt;= 0; j--) { if (pat.charAt(j) != txt.charAt(i+j)) { skip = Math.max(1, j - right[txt.charAt(i+j)]); break; } } if (skip == 0) return i; // found. } return N; // not found. } Substring search with the Boyer-Moore bad character heuristic takes about ~ N / M character compares to search for a pattern of length M in a text of length N. Worst-case can be as bad as ~ M N.\nGood Suffix Heuristics(GSH) 通过类似KMP的Border特性来提高效率，可以将最坏情况改善为O(~3N)字符比较。只不过这里需要的映射关系是一个已经匹配的border对应(包含在)pattern哪一个后缀中, 用shift[]数组记录. 对于pattern的一个后缀pat[j,...]的borderpat[j,...k] = pat[l, ...], 当不匹配时, 根据已匹配的borderpat[l, ...], 可以偏移l - k位置以对齐到pat[j,...k]. 当完成一个完整匹配后, GSH方法可以基于shift[]决定下一次对齐偏移的距离. 除此之外, 在匹配中如果发现不匹配字符, pattern的偏移距离可以在GSH和BCH给出的建议距离中选择最大者.\nborder: A border is a substring which is both proper suffix and proper prefix\nCase 1. P有子串和后缀ab重复, 对齐重复的子串\n0 1 2 3 4 5 6 7 8 9 ... a b a a b a b a c b a c a b a b c a b a b Case 2. P没有子串和后缀bab重叠, 但P的前缀ab与t的后缀ab匹配\n0 1 2 3 4 5 6 7 8 9 ... a a b a b a b a c b a a b b a b a b b a b Case 3. P没有重复子串, 可以跳过整个P的长度\n0 1 2 3 4 5 6 7 8 9 ... a b c a b a b a c b a c b a a b c b a a b 对于Case 1, 预处理时使用一个保存border position的数组bp, 每个元素bp[i]都包含模式P中从索引i开始的后缀的最宽border起始位置, 所以bp元素永远指向其右边的某一个位置。与Knuth-Morris-Pratt预处理算法相似，每个border都是通过检查是否匹配来向左扩展已知的较短边界来计算的。\n当边界不能向左扩展时, 那就意味着在发生不匹配时, 可以把左边的border偏移过来。相应的偏移距离j - i保存在数组shift[j]中，前提是对应位置尚未被占用, 如果已经有占用(shift[i] != 0), 则意味着较短的后缀具有相同border的情况; 然后让border指针j跳转到bp[j], 因为bp[j]存下来之前计算的pat[j,...]的border, 同时我们知道pat[i] == pat[j], 这样保证了pat[i...]和pat[j...]有共同的前缀. 如此这般直到j \u0026gt; m. shift数组的值仅由那些无法扩增的border决定, 因为如果可以扩增的话, 意味着pat[i-1] = pat[j-1] != txt[k-1], 此时按照j - i来移位不过是继续出现不匹配.\ni: 0 1 2 3 4 5 6 7 p: a b b a b a b b: 5 6 4 5 6 7 7 8 s: 0 0 0 0 2 0 4 1 如bp[2] = 4代表后缀babab的最宽border是以4起始的后缀bab. 因为pat[4-1] != pat[2-1], bab作为babab的border无法通过a扩增为b+babab的border, 因此shift[4] = 4 - 2.\n注意到babab也有borderb, 无法扩增, 所以shift[6] = 6 - 2. 匹配时, 如果在匹配了pat[6...]后, pat[5]不匹配, 那么就可以偏移shift[6] = 4, 把pat[2]对齐过去.\n匹配时在位置j发生任何不匹配时，就将模式对齐位置向右移动shift[j + 1]。\nvoid goodSuffixPreprocessCase1(String pat) { int m = pat.length(); int[] shift = new int[m + 1]; int[] bp = new int[m + 1]; int i = m; int j = m + 1; bp[i] = j; while (i \u0026gt; 0) { while (j \u0026lt;= m \u0026amp;\u0026amp; pat.charAt(i - 1) != pat.charAt(j - 1)) { // could not extend border left forward if (shift[j] == 0) // prevent modification of shift[j] from suffix having same border shift[j] = j - i; // shift to i position j = bp[j]; // back to last border } i--; j--; // pat[i-1] == pat[j-1], extend border bp[i] = j; } } 对于case 2，部分匹配的前缀是属于整个pattern的其中一个border(不一定是最大的). 预处理阶段，就是要计算pattern的每个后缀所包含的最大border, 同时要求这个border也必须属于整个pattern的border(也就是必须是pattern的前缀)。\n因为case 1已经计算了pattern每个后缀的最大border，在这里考虑的是整个pattern, 其最大的border是b[0]. 首先把b[0]复制到shift[]的空位中(值等于0的位置); 然后对于长度比b[0]小的后缀pat[j...], 其一定不会包含后缀pat[b[0], ...], 所以需要替换为pattern下一个更小的border, j = bp[j].\ni: 0 1 2 3 4 5 6 7 p: a b b a b a b b: 5 6 4 5 6 7 7 8 s: 5 5 5 5 2 5 4 1 void goodSuffixPreprocessingCase2(String pat, int[] bp, int[] shift) { int i, j = bp[0]; for (i = 0; i \u0026lt;= m; i++) { if (shift[i] == 0) shift[i] = j; // copy widest border if (i == j) // suffix becomes shorter than bp[0] j = bp[j]; } } 预处理汇总 以上三种预处理方式汇总在一起使用\nbmPreprocess(String pat) { badCharacterPreprocess(); goodSuffixPreprocessingCase1(); goodSuffixPreprocessingCase2(); } 匹配 从右到左比较pattern和txt的字符。\n如果不匹配，pattern在BCH和GSH的偏移距离中选择较大者进行偏移对齐 如果完全匹配，pattern可以继续根据根据其最宽边框允许的数量移动。 void bmSearch(String txt, String pat) { int i = 0; // shift of the pattern with respect to text int j; int m = pat.length(); int n = txt.length(); while (i \u0026lt;= n - m) { j = m - 1; while (j \u0026gt;= 0 \u0026amp;\u0026amp; pat.charAt(j) == txt.charAt(i+j)) j--; // reduce index if match with shift i if (j \u0026lt; 0) // complete matched { // return i; match i += shift[0]; } else // pat[i] != txt[i+j], shift shift[j+1] i += Math.max(shift[j+1], j - right[txt.charAt(i+j)]); } } Horspool Algorithm BM算法使用BCH和GSH两种heuristics来决定shift距离. 不过GSH的实现非常复杂， 而且在实践中发现， 一般的字符集上的匹配性能主要依靠BCH。Horspool [Hor 1980] 提出了仅基于BCH的简化版BM方法: 不匹配时, 用当前查看的txt窗口的最右字符来确定shift距离。\n(a) Boyer-Moore\n0 1 2 3 4 5 6 7 8 9 ... a b c a b d a a c b a b c a a b b c a a b (b) Horspool\n0 1 2 3 4 5 6 7 8 9 ... a b c a b d a a c b a b c a a b b c a a b Horspool使用当前txt窗口txt[0, ... 4]的最右边字符b作为判断, 把pattern[..., m-1]最右边的b(该字符在pattern最后位置的出现不计算在内)对齐到txt[4]。\n预处理阶段会跟BM算法的BCH不一样, 对每一个字符$\\alpha \\in alphabet$, $right(pattern, \\alpha)$ 取α在pattern[:-2]最右出现位置(如果没有则-1). 不考虑pattern[-1]. 如right['text', 'x'] = 2, right['text', 't'] = 0, right['next', 't'] = -1.\nhorspoolBCH(String pat) { // Compute skip table. this.pat = pat; int m = pat.length(); int R = 256; right = new int[R]; for (int c = 0; c \u0026lt; R; c++) right[c] = -1; // -1 for chars not in pattern for (int j = 0; j \u0026lt; m - 1; j++) // rightmost position for right[pat.charAt(j)] = j; // chars in pattern } 匹配的过程和BM算法差不多。在完全匹配后或者遇到不匹配时，pattern就根据right[]数组移位\nhorspoolSearch(String txt, String pat, int[] right) { int i = 0, j; while (i \u0026lt;= n - m) { j = m - 1; while (j \u0026gt;= 0 \u0026amp;\u0026amp; pat.charAt(j) == txt.charAt(i+j)) j--; if (j \u0026lt; 0) return i; // match i += m - 1; // right most of txt window i -= right[txt.charAt(i)]; } } Sunday Algorithm BM算法利用当前位置是否匹配来判断移位. Horspool利用当前检查的文本窗口内的最右字符来判断移位. Daniel M.Sunday [Sun 90] 发现如果能利用当前文本窗口外右边的字符更好, 考虑到这些字符有可能属于下一次可能的匹配, 这种思路是可行的.\n(c) Sunday\n0 1 2 3 4 5 6 7 8 9 ... a b c a b d a a c b a b c a a b b c a a b 不匹配时, Sunay算法利用当前文本窗口txt[0, ... 4]右边的字符, 也就是txt[5] = d来判断, d不再pattern中出现过, 因此直接跳过txt[5].\n每次匹配都会从 目标字符串中 提取 待匹配字符串与 模式串 进行匹配：\n若匹配，则返回当前 idx 不匹配，则查看 待匹配字符串 的后一位字符 c： 若c存在于Pattern中，则 idx = idx + shift[c] 否则，idx = idx + len(pattern) Repeat Loop 直到 idx + len(pattern) \u0026gt; len(String) Shift偏移表 存储每一个在 模式串 中出现的字符，在 模式串 中出现的最右位置到尾部的距离 +1.\nc h e c k t h i s o u t t h i s | t h i s m i s s i s s i p p i i s s i | i s s i 最坏情况：O(nm)\n平均情况：O(n)\n预处理阶段和horspool一样, 不过right的定义要改为在整个pattern[:]上求最右出现位置.\nsundayBCH(String pat) { // Compute skip table. this.pat = pat; int m = pat.length(); int R = 256; right = new int[R]; for (int c = 0; c \u0026lt; R; c++) right[c] = -1; // -1 for chars not in pattern //*********************************************************************** for (int j = 0; j \u0026lt; m; j++) // rightmost position for the whole pattern //*********************************************************************** right[pat.charAt(j)] = j; // chars in pattern } sundaySearch(String txt, String pat, int[] right) { int i = 0, j; while (i \u0026lt;= n - m) { /** matching from right to left * ********************************* **/ j = m - 1; while (j \u0026gt;= 0 \u0026amp;\u0026amp; pat.charAt(j) == txt.charAt(i+j)) j--; if (j \u0026lt; 0) return i; // match /***********************************/ i += m; // right next to txt window if (i \u0026lt; n) i -= right[txt.charAt(i)]; } return -1; } 因为Sunday算法不限定一定要从右往左对比pattern和txt, 反方向也行, 在这里的实现选择跟随Horspool算法从右往左.\nSunday 算法通常用作一般情况下实现最简单而且平均表现最好之一的实用算法，通常表现比 Horspool、BM 都要快一点。\n可以在这里测试各种匹配算法的正确性和效率 https://leetcode-cn.com/problems/implement-strstr/\nReference Boyer, RS and Moore, JS. \u0026ldquo;A fast string searching algorithm.\u0026rdquo; Communications of the ACM 20.10 (1977): 762-772. R.N. Horspool: Practical Fast Searching in Strings. Software - Practice and Experience 10, 501-506 (1980) D.M. Sunday: A Very Fast Substring Search Algorithm. Communications of the ACM, 33, 8, 132-142 (1990) https://www.inf.hs-flensburg.de/lang/algorithmen/pattern/bmen.htm https://www.inf.hs-flensburg.de/lang/algorithmen/pattern/horsen.htm https://www.inf.hs-flensburg.de/lang/algorithmen/pattern/sundayen.htm http://www-igm.univ-mlv.fr/~lecroq/string/ ","permalink":"https://congchan.github.io/posts/%E5%AD%97%E7%AC%A6%E6%90%9C%E7%B4%A2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95-02-boyer-moorebm-horspool-sunday-algorithms/","summary":"\u003ch2 id=\"boyer-moore算法\"\u003eBoyer-Moore算法\u003c/h2\u003e\n\u003cp\u003e在从头到尾对齐txt的字符时, 每一次对齐, BM算法反方向\u003cstrong\u003e从尾到头\u003c/strong\u003e检查pattern\u003ccode\u003eP=BAABBAA\u003c/code\u003e和txt字符是否匹配。匹配过程中如果发现\u003ccode\u003etxt[i] != P[j]\u003c/code\u003e, 也就说二者子串不相等, 如\u003ccode\u003e...XAA != ...BAA\u003c/code\u003e, 且字符\u003ccode\u003etxt[i] = X\u003c/code\u003e在\u003ccode\u003eP\u003c/code\u003e中不存在时，可以把\u003ccode\u003eP\u003c/code\u003e开头对齐到\u003ccode\u003etxt[i+7]\u003c/code\u003e, 一次就跳过了\u003ccode\u003e7\u003c/code\u003e个位置. \u003cimg loading=\"lazy\" src=\"/images/bm.png\" title=\"image from: https://www.coursera.org/learn/algorithms-part2/\"\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e模式相较于文本一般较短, 所以模式中包含的字符种类相对也比较少, 那么这样的跳跃出现情况就很可观了, 可以大大加速匹配.\u003c/p\u003e\n\u003cp\u003e不过一般来说, 可能txt的某些子串会和\u003ccode\u003eP\u003c/code\u003e的其他子串重合，不失一般性我们需要像Knuth-Morris-Pratt算法一样的重启位置数组。\u003c/p\u003e\n\u003ch3 id=\"bad-character-heuristicbch\"\u003eBad Character Heuristic(BCH)\u003c/h3\u003e\n\u003cp\u003e重启点位: 预先计算各个字符\u003ccode\u003ec\u003c/code\u003e在Pattern的最rightmost的位置(若无则\u003ccode\u003e-1\u003c/code\u003e), 这些位置告诉我们如果是txt中的字符\u003ccode\u003ec\u003c/code\u003e导致不匹配, pattern可以右移的距离.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ebadCharacterPreprocess\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003epat\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Compute skip table.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003epat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003epat\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eM\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003epat\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eR\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e256\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eright\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eR\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eR\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eright\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"c1\"\u003e// -1 for chars not in pattern\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eM\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"c1\"\u003e// rightmost position for\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eright\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003epat\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003echarAt\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// chars in pattern\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e有了\u003ccode\u003eright\u003c/code\u003e数组后, 一个例子说明匹配过程:\u003c/p\u003e","title":"字符搜索匹配算法 02 - Boyer-Moore(BM), Horspool, Sunday algorithms"},{"content":"如何实现快速的幂运算？\n要求$c = a^b$, 按照朴素算法把a连乘b次的时间复杂度是$O(n)$. 而快速幂能做到$O(\\log n)$。把b转换为二进制, 二进制数第i位的权为$2^{i-1}$，就可以把二进制拆分为若干个以2为底的真数, 然后利用幂数的性质，例如用朴素算法求$a^{11}$要求乘11次. 考虑到11的二进制为1011, 如果把$a^{11}$拆分为: $$a^{11} = a^{a_0 2^0 + a_1 2^1 + a_2 2^2 + a_3 2^3} = a^1 a^2 a^8$$ 可以看到每一个因子都是上一个因子的平方，利用$a^2 a^2$求出$a^4$, 同样利用$a^4$的平方求出$a^8$, 每次计算只需要用到上一次计算出来的结果, 所以总的运算次数是4次. 任何一个数b最多能写成长度为$O(\\log b)$的二进制, 因此这个算法就是$O(\\log n)$.\n在程序设计中是根据b的二进制中是否为1来控制是否乘以上一次翻倍的积\n不断右移b, 直到b不再有1： 根据当前位的权重（当前b最后一位）是否为1来决定c是否乘以最新的a 把a平方，用于下一位计算 在Java中要考虑极端值INT_MIN\n// 递归 public double myPow(double x, int n) { if(n==0) return 1; double temp = myPow(x, n/2); if (n % 2 ==0) return temp * temp; else { if(n \u0026gt; 0) return x*temp*temp; else return (temp*temp) / x; } } // 循环 public double myPow(double x, int n) { double ans = 1; if(n \u0026lt; 0){ n = -(n+1); // 处理极端值 x = 1/x; ans *= x; } System.out.println(n); while (n \u0026gt; 0) { if ((n \u0026amp; 1) == 1) ans *= x; x *= x; n \u0026gt;\u0026gt;= 1; } return ans; } 快速幂取余 求a^b mod c. 如果b是偶数, a^b mod c = $(a^2)^{b/2} \\% c$ 如果b是奇数, a^b mod c = $((a^2)^{b/2} \\times a) \\% c$\n又因为取余有性质:a^b mod c = (a mod c)^b\n引理：(a * b) mod c = [( a mod c ) * (b mod c) ] mod c\n证明：\n设 a mod c =d，b mod c= e; 则：a=t*c + d ; b=k*c + e ; (a*b)mod c = (t*c+d)(t*c+e) = (tk c^2 + ( te+dk ) *c + d*e) mod c = de mod c 即积的取余等于取余的积的取余.\n利用快速幂的思想, 令k = (a * a) mod c，所要求的最终结果即为 k^(b/2) mod c, 这个过程可以迭代下去, 如果b是奇数, 或多出一项a mod c. 当b = 0时, 所有因子已经相乘, 迭代结束, 复杂度为O(log b)\nlong long PowerMod(int a, int b, int c) { int ans = 1; a = a % c; while(b\u0026gt;0) { if(b % 2 = = 1) ans = (ans * a) % c; b = b/2; // b\u0026gt;\u0026gt;=1; a = (a * a) % c; } return ans; } ","permalink":"https://congchan.github.io/posts/%E4%BD%8D%E6%93%8D%E4%BD%9C-%E5%BF%AB%E9%80%9F%E5%B9%82/","summary":"\u003cp\u003e如何实现快速的幂运算？\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e要求$c = a^b$, 按照朴素算法把\u003ccode\u003ea\u003c/code\u003e连乘\u003ccode\u003eb\u003c/code\u003e次的时间复杂度是$O(n)$. 而快速幂能做到$O(\\log n)$。把\u003ccode\u003eb\u003c/code\u003e转换为二进制, 二进制数第\u003ccode\u003ei\u003c/code\u003e位的权为$2^{i-1}$，就可以把二进制拆分为若干个以\u003ccode\u003e2\u003c/code\u003e为底的真数, 然后利用幂数的性质，例如用朴素算法求$a^{11}$要求乘\u003ccode\u003e11\u003c/code\u003e次. 考虑到\u003ccode\u003e11\u003c/code\u003e的二进制为\u003ccode\u003e1011\u003c/code\u003e, 如果把$a^{11}$拆分为:\n\u003c/p\u003e\n$$a^{11} = a^{a_0 2^0 + a_1 2^1 + a_2 2^2 + a_3 2^3} = a^1 a^2 a^8$$\u003cp\u003e\n可以看到每一个因子都是上一个因子的平方，利用$a^2 a^2$求出$a^4$, 同样利用$a^4$的平方求出$a^8$, 每次计算只需要用到上一次计算出来的结果, 所以总的运算次数是\u003ccode\u003e4\u003c/code\u003e次. 任何一个数\u003ccode\u003eb\u003c/code\u003e最多能写成长度为$O(\\log b)$的二进制, 因此这个算法就是$O(\\log n)$.\u003c/p\u003e\n\u003cp\u003e在程序设计中是根据\u003ccode\u003eb\u003c/code\u003e的二进制中是否为\u003ccode\u003e1\u003c/code\u003e来控制是否乘以上一次翻倍的积\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e不断右移\u003ccode\u003eb\u003c/code\u003e, 直到\u003ccode\u003eb\u003c/code\u003e不再有\u003ccode\u003e1\u003c/code\u003e：\n\u003cul\u003e\n\u003cli\u003e根据当前位的权重（当前\u003ccode\u003eb\u003c/code\u003e最后一位）是否为\u003ccode\u003e1\u003c/code\u003e来决定\u003ccode\u003ec\u003c/code\u003e是否乘以最新的\u003ccode\u003ea\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e把\u003ccode\u003ea\u003c/code\u003e平方，用于下一位计算\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在Java中要考虑极端值INT_MIN\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// 递归\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emyPow\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003etemp\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emyPow\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e%\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003etemp\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003etemp\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003etemp\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003etemp\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etemp\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003etemp\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// 循环\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emyPow\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eans\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e){\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 处理极端值\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eans\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eans\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u0026gt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eans\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"快速幂取余\"\u003e快速幂取余\u003c/h3\u003e\n\u003cp\u003e求\u003ccode\u003ea^b mod c\u003c/code\u003e.\n如果\u003ccode\u003eb\u003c/code\u003e是偶数, \u003ccode\u003ea^b mod c\u003c/code\u003e = $(a^2)^{b/2} \\% c$\n如果\u003ccode\u003eb\u003c/code\u003e是奇数, \u003ccode\u003ea^b mod c\u003c/code\u003e = $((a^2)^{b/2} \\times a) \\% c$\u003c/p\u003e","title":"位操作 - 快速幂"},{"content":"“找出只出现一次的数”， “找出唯二的只出现M次的数”， “找出缺失的数”等等这类问题，都可以利用异或操作的特性， 即一个整数和自己进行异或运算会归0的性质。\n找出缺失的数字 问题1：给定一个包含n个不同数字的数组，取自0,1,2,...,n，找到数组中缺少的数字。\n最直觉的解法是利用等差数列的性质直接数学求解。但这个方法限制于等差数列.\n问题2: 在一个长度为n的数组里的所有数字都在0 ~ n-1之间, 数组中有些数字是重复的, 找出任意一个重复的数字. 这也是\u0026laquo;剑指offer\u0026raquo;的一道题.\n但是如果利用数组大小和元素范围的特性, 就可以发现, 这里的数组的大小和数字的范围是有限定关系的. 对于第二个问题, 假如没有重复, 那么重新排列的话数组每一个位置都可以放上自己对应的数字. 对于第一个问题, 假如没有缺失, 那么除了每一个index都可以重新放上自己对应的数字外, 还会多出一个最大的数字没地方放. 这样就可以把数组包含的数字解读为index, 然后在遍历检查数组时, 同时检查以各个数字为index的其他位置的数字.\n使用这种思路可以同时解决两个问题, 这里以问题1解法为例:\npublic int missingNumber(int[] nums) { int n = nums.length; int misP = n; // points to the position where misssing. for (int i = 0; i \u0026lt; n; i++) { while (i != nums[i] \u0026amp;\u0026amp; nums[i] != misP) { int j = nums[i]; nums[i] = nums[j]; nums[j] = j; } if (nums[i] == misP) misP = i; } return misP; } 找出只出现一次的数 问题3：在一个非空整数数组，找出那个只出现了一次的元素，已知其余每个元素均出现两次。\n要达到O(n)复杂度需要利用位异或. 位异或运算能够把二进制相同的数化为0. 把数组所有的数都异或, 出现两次的数就会互相抵消为0, 剩余的就是那个只出现了一次的数:\npublic int singleNumber(int[] nums) { int output = 0; for (int i : nums) output ^= i; return output; } 问题3其实等价于问题1, 如果把问题1再加上另外一个完整连续不重复0,1,2,...,n-1数组(可以直接循环数组索引). 所以问题1也可以用同样思路解决:\npublic int missingNumber(int[] nums) { int miss = nums.length; for (int i = 0; i \u0026lt; nums.length; i++) miss ^= (nums[i] ^ i); return miss; } 找出唯一一个仅出现M次的数 但如果把问题3扩展为“其余每个元素均出现三次”， 也就是leetcode 137. Single Number II, 这样就无法直接利用异或抵消的性质了。剑指Offer的解法是用一个长度32的数组bitSum, 把原数组所有整数的二进制每一位分别累加到bitSum里面, 这样就可以通过判断bitSum哪些位不可以被3整除来找出那个数:\npublic int singleNumber(int[] nums) { int[] bitSum = new int[32]; for (int i = 0; i \u0026lt; nums.length; i++) { int bitMask = 1; for (int j = 31; j \u0026gt;= 0; j--) { int b = nums[i] \u0026amp; bitMask; if (b != 0) bitSum[j] += 1; bitMask \u0026lt;\u0026lt;= 1; } } int res = 0; for (int i = 0; i \u0026lt; 32; i++) { res \u0026lt;\u0026lt;= 1; res += bitSum[i] % 3; } return res; } 因为bitSum的长度是常数, 所以该方法复杂度还是O(N). 该方法可以进一步扩展问题为求唯一一个元素出现M次，其他所有元素出现K次的问题。\n构造状态转移表 方法来自An General Way to Handle All this sort of questions, 这个方法核心思想是建立一个记录状态的变量, 该变量代表某个数字出现一次后的状态. 目标就是使得一个数字重复出现K次后状态刚好归0.\n对于K=2, 就要使两次叠加后归0, 需要两种状态, 从信息论的角度看待, 只需要一个位(0,1)来表达，状态0对应着两种等价的情况: 一个数字完全没出现过, 或者出现了2次后一起抵消重置. 状态1对应着仅仅出现一次的情况. 在这里数字和状态概念等价，构建一个状态转移表（真值表）：\n状态 输入 输出 a c a 0 0 0 1 0 1 0 1 1 1 1 0 可以看到，不管是状态1还是0，如果输入相同数字，就会变为0；如果输入不同的数字，就会变为1. 根据表写出逻辑表达式为异或运算.\n根据真值表写出逻辑式的基本套路是: 只看输出结果为1的转移, 凡取值为1的变量写成原变量，取值为0的变量写成反变量, 得出对应的表达式, 再把所有转移方程的表达式加起来. 如输出为1的是0 \u0026amp; 1 = 1, 1 \u0026amp; 0 = 1, 表达式就是(~a \u0026amp; c) | (a \u0026amp; ~c), 这个本质上就是a ^ c\n对于K = 3, M = 1(or 2), 需要三种状态, 那么至少需要两个位(00, 01, 10)来表达. 让状态00对应\u0026quot;假\u0026quot;输出, 对应两种等价的情况: 一个数字完全没出现过, 或者出现了3次后一起抵消重置. 再定义01为出现了一次的状态, 10为出现了2次, 这两种状态都对应着\u0026quot;真\u0026quot;输出, 也就是我们想要的答案, 得出状态转移为:\n状态 输入 输出 (a, b) (c) (a,b) 0, 0\t0\t0, 0 0, 1\t0\t0, 1 1, 0\t0\t1, 0 0, 0\t1\t0, 1 0, 1\t1\t1, 0 1, 0\t1\t0, 0 得出a = (a \u0026amp; ~b \u0026amp; ~c) | (~a \u0026amp; b \u0026amp; c), b = (~a \u0026amp; b \u0026amp; ~c) | (~a \u0026amp; ~b \u0026amp; c). 只要把数组所有数按照这个逻辑分别叠加到a和b上面, 最后答案就是a | b.\npublic int singleNumber(int[] nums) { int a = 0, b = 0; for (int c : nums) { int temp = (a \u0026amp; ~b \u0026amp; ~c) | (~a \u0026amp; b \u0026amp; c); b = (~a \u0026amp; b \u0026amp; ~c) | (~a \u0026amp; ~b \u0026amp; c); a = temp; } return (a | b); } 以上只是一种通用的套路，对于每一种特定的K, M组合, 可能会有不同的特殊最优方案.\n通过不同集合收录不同数字 同上面的问题，LeetCode某大神给出一个目前为止最优的方案, 并放言\u0026quot;Challenge me\u0026quot;, 草鸡们看了瑟瑟发抖：\npublic int singleNumber(int[] nums) { int ones = 0, twos = 0; for (int c : nums) { ones = (ones ^ c) \u0026amp; ~twos; twos = (twos ^ c) \u0026amp; ~ones; } return ones; } 原理是利用两个数ones和twos作为一种概念上的集合set，通过异或操作来收录分别出现了1次和2次的数, set ^ val有两种结果:\n如果set里面没有val, 把val异或进去, 如a ^ 0 = a 如果set之前已经收录了val, 那么亦或操作就会在set中移除这个val, 如 a ^ a = 0 按照上面的定义来理解:\n(ones ^ c) \u0026amp; ~twos: 当且仅当c没有收录在twos中, 把ones收录c，否则移除c。这样的话，任何第一次出现的数都会被收入ones中, 而任何第二次出现的数会从ones中移出. So, effectively anything that appears for the first time will be in the set. Anything that appears a second time will be removed. We\u0026rsquo;ll see what happens when an element appears a third time (thats handled by the set \u0026ldquo;twos\u0026rdquo;). 紧接着, (twos ^ c) \u0026amp; ~ones用同样的逻辑更新twos. 这样意味着 twos不会收录第一次出现的数; 但对于第二次出现的数, 因为上一步已经把这个数从ones中移除, 那么这个数就会被收录进twos中, 对于第三次出现的数, 因为twos中已经收录了, 所以ones不会再收录, 而异或操作会把twos中的这个数移除. 最后的结果就是, ones仅保留出现了1次的数, twos仅保留出现了2次的数, 而那些出现了3次的数都被移除了.\n这种方法可以扩展为通用方法, 适用于任何仅存在一个只出现了M次的数, 其他数都出现了K次的数组, 如K = 4, M = 3\npublic int singleNumber(int[] nums) { int ones = 0, twos = 0, threes = 0; for (int c : nums) { ones = (ones ^ c) \u0026amp; ~twos \u0026amp; ~threes; twos = (twos ^ c) \u0026amp; ~ones \u0026amp; ~threes; threes = (threes ^ c) \u0026amp; ~twos \u0026amp; ~ones; } return threes; } public static void main(String[] args) { int[] nums = {1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4}; System.out.println(singleNumber(nums)); // 2 } 找出唯二的仅出现M次的数 LeetCode原题:给定一个整数数组nums，其中恰好有两个元素只出现一次，其余所有元素均出现两次。 找出只出现一次的那两个元素。跟前面的问题类似, 我们需要再次使用XOR来解决这个问题。通过分割数组, 把出现一次的两个数, 划分到不同的数组中, 问题就转化为寻找唯一的出现一次的数问题. 所以关键就是如何拆分数组.\n具体需要两次遍历：\n第一次遍历，对数组所有元素进行异或，获得要找的两个数字的XOR。由于两个数字是不同的，因此在XOR结果中必定有一个set bit, 即位值为'1\u0026rsquo;的位。 找出任意set bit（如最右边的）。 第二次遍历，将所有数字分成两组: 一组为具有上述set bit的数, 另一组没有。按照这种方法分组, 相同的数字一定会被分配到同一组中, 而两个只出现一次的数会分配到不同数组中。 /**代码来自: https://leetcode.com/problems/single-number-iii/discuss/68900/Accepted-C%2B%2BJava-O(n)-time-O(1)-space-Easy-Solution-with-Detail-Explanations */ public class Solution { public int[] singleNumber(int[] nums) { // Pass 1 : // Get the XOR of the two numbers we need to find int diff = 0; for (int num : nums) { diff ^= num; } // Get its last set bit diff \u0026amp;= -diff; // Pass 2 : int[] rets = {0, 0}; // this array stores the two numbers we will return for (int num : nums) { if ((num \u0026amp; diff) == 0) // the bit is not set { rets[0] ^= num; } else // the bit is set { rets[1] ^= num; } } return rets; } } ","permalink":"https://congchan.github.io/posts/%E4%BD%8D%E6%93%8D%E4%BD%9C-%E6%89%BE%E6%95%B0%E9%97%AE%E9%A2%98/","summary":"\u003cp\u003e“找出只出现一次的数”， “找出唯二的只出现M次的数”， “找出缺失的数”等等这类问题，都可以利用异或操作的特性， 即一个整数和自己进行异或运算会归0的性质。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"找出缺失的数字\"\u003e找出缺失的数字\u003c/h2\u003e\n\u003cp\u003e问题1：给定一个包含n个不同数字的数组，取自\u003ccode\u003e0,1,2,...,n\u003c/code\u003e，找到数组中缺少的数字。\u003c/p\u003e\n\u003cp\u003e最直觉的解法是利用等差数列的性质直接数学求解。但这个方法限制于等差数列.\u003c/p\u003e\n\u003cp\u003e问题2: 在一个长度为\u003ccode\u003en\u003c/code\u003e的数组里的所有数字都在\u003ccode\u003e0 ~ n-1\u003c/code\u003e之间, 数组中有些数字是重复的, 找出任意一个重复的数字. 这也是\u0026laquo;剑指offer\u0026raquo;的一道题.\u003c/p\u003e\n\u003cp\u003e但是如果利用数组大小和元素范围的特性, 就可以发现, 这里的数组的大小和数字的范围是有限定关系的. 对于第二个问题, 假如没有重复, 那么重新排列的话数组每一个位置都可以放上自己对应的数字. 对于第一个问题, 假如没有缺失, 那么除了每一个index都可以重新放上自己对应的数字外, 还会多出一个最大的数字没地方放. 这样就可以把数组包含的数字解读为index, 然后在遍历检查数组时, 同时检查以各个数字为index的其他位置的数字.\u003c/p\u003e\n\u003cp\u003e使用这种思路可以同时解决两个问题, 这里以问题1解法为例:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emissingNumber\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emisP\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// points to the position where misssing.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e!=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e!=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emisP\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e   \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enums\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emisP\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003emisP\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emisP\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"找出只出现一次的数\"\u003e找出只出现一次的数\u003c/h2\u003e\n\u003cp\u003e问题3：在一个非空整数数组，找出那个只出现了一次的元素，已知其余每个元素均出现两次。\u003c/p\u003e","title":"位操作 - 找数问题"},{"content":"求两个整数的汉明距离 hamming distance\nLeetcode 461 两个整数之间的汉明距离是该两个数之间不同的位数。 给定两个整数x和y，计算汉明距离。问题也可以理解为对于两个整数m和n, 需要改变m的二进制多少位才能得到n:\n/** Use Brian Kernighan\u0026#39;s way to count bits */ public int hammingDistance(int x, int y) { x = x ^ y; y = 0; while(x != 0){ y++; x \u0026amp;= x - 1; } return y; } public class Solution { public int hammingDistance(int x, int y) { return Integer.bitCount(x ^ y); } } 同样用到Brian Kernighan算法：\nLets say that the bit at index n is 1 and that the bits in indexes 0 up to n-1 are all 0 (we\u0026rsquo;ll use little endianess - so index 0 is 1, index 1 is 2, index 2 is 4, index 3 is 8 and so on).\nv-1 subtracts from index 0 - but it\u0026rsquo;s 0, so it converts it to 1 and subtracts from index 1 - but it\u0026rsquo;s also 0, so it converts it to 1 and subtracts from index 2 - and so on until we reach index n. Since index n is 1 it can subtract from it and turn it to 0 - and there it stops: 1101000 - 1 = 1100111\nSo, v-1 is like v except there are n 0 that became 1 and one 1 that became 0. In v \u0026amp; v - 1 all the other bits remain as is, the n zeros that where turned to ones remain 0 (because 0 \u0026amp; 1 == 0), and the one 1 that was turned to 0 turns to 0(because 1 \u0026amp; 0 == 0). So overall - only a single bit was changed in the iteration, and this change was from 1 to 0: 1101000 \u0026amp; 1100111 = 1100000\n","permalink":"https://congchan.github.io/posts/%E4%BD%8D%E6%93%8D%E4%BD%9C-%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/","summary":"\u003cp\u003e求两个整数的汉明距离 hamming distance\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e\u003ca href=\"https://leetcode.com/problems/hamming-distance/description/\"\u003eLeetcode 461\u003c/a\u003e\n两个整数之间的汉明距离是该两个数之间不同的位数。 给定两个整数x和y，计算汉明距离。问题也可以理解为对于两个整数\u003ccode\u003em\u003c/code\u003e和\u003ccode\u003en\u003c/code\u003e, 需要改变\u003ccode\u003em\u003c/code\u003e的二进制多少位才能得到\u003ccode\u003en\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e/** Use Brian Kernighan\u0026#39;s way to count bits */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ehammingDistance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e^\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e!=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e){\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eSolution\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ehammingDistance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eInteger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ebitCount\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e^\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e同样用到Brian Kernighan算法：\u003c/p\u003e","title":"位操作 - 汉明距离"},{"content":"不使用加减符号求和整数 不能使用+和-, 仅通过^和\u0026amp;操作来求和两个整数a.\n参考 每位相加可能会产生进位(carry), 所以可以把相加拆分为两部分, 如759 + 674可以拆分为不考虑进位的部分323和仅考虑进位的部分1110, 故759 + 674 = 323 + 1110 = 1433.\n二进制的加法也是从低位开始逐步往高位计算:\n进行一位二进制的加法, 也就是暂不考虑进位的位相加: 0+0=0， 0+1=1, 1+0=1， 1+1=0, 那么就是^操作. 所得的和作为新的a. 求进位: 通过a \u0026amp; b判断是否进位, 因为只有两个位均为1才会进位. 所得的进位左移一位作为新的b. 不断重复这个过程, 把低位的进位传递到高位, 累加到a中, 直到进位为0, 最后得到的a就是答案. public class Solution { public int getSum(int a, int b) { while (b != 0) { // 关键在于判断终止的时机 int c = a \u0026amp; b; //carry a ^= b; //add b = c \u0026lt;\u0026lt; 1; } return a; } } 涉及的运算就是一个多位二进制加法真值表：(对应于硬件中的全加器)\n全加器（full adder）将两个一位二进制数相加，并根据接收到的低位进位信号，输出和、进位输出。全加器的三个输入信号为两个加数A、B和低位进位Cin。全加器通常可以通过级联（cascade）的方式，构成多位（如8位、16位、32位）二进制数加法器的基本部分。全加器的输出和半加器类似，包括向高位的进位信号Cout和本位的和信号S，相加结果的总和表达为 ${\\displaystyle \\mathrm {sum} =2\\times C_{out}+S}$。\n规则是 s = (x ^ y) ^ Cin Cout = (x \u0026amp; y) | (y \u0026amp; Cin) | (x \u0026amp; Cin)\n更简单的版本:\nint getSum(int a, int b) { return b == 0 ? a : getSum(a ^ b, (a \u0026amp; b) \u0026lt;\u0026lt; 1); } 不使用缓存交换整数 利用一个整数和自己进行异或运算会归0的性质\npublic int[] exchangeAB(int[] AB) { AB[0] = AB[0] ^ AB[1]; AB[1] = AB[0] ^ AB[1]; // 只剩下AB[0] AB[0] = AB[0] ^ AB[1]; // 只剩下AB[1] return AB; } 也可以使用加减法来操作\npublic int[] exchangeAB(int[] AB) { AB[0] = AB[0] + AB[1]; AB[1] = AB[0] - AB[1]; // 只剩下AB[0] AB[0] = AB[0] - AB[1]; // 只剩下AB[1] return AB; } ","permalink":"https://congchan.github.io/posts/%E4%BD%8D%E6%93%8D%E4%BD%9C-%E4%B8%8D%E4%BD%BF%E7%94%A8%E5%8A%A0%E5%87%8F%E7%AC%A6%E5%8F%B7%E6%B1%82%E5%92%8C%E6%95%B4%E6%95%B0/","summary":"\u003ch2 id=\"不使用加减符号求和整数\"\u003e不使用加减符号求和整数\u003c/h2\u003e\n\u003cp\u003e不能使用\u003ccode\u003e+\u003c/code\u003e和\u003ccode\u003e-\u003c/code\u003e, 仅通过\u003ccode\u003e^\u003c/code\u003e和\u003ccode\u003e\u0026amp;\u003c/code\u003e操作来求和两个整数\u003ccode\u003ea\u003c/code\u003e.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e参考\u003ca href=\"https://www.cnblogs.com/kiven-code/archive/2012/09/15/2686922.html\"\u003e\u003c/a\u003e\n每位相加可能会产生进位(carry), 所以可以把相加拆分为两部分, 如\u003ccode\u003e759 + 674\u003c/code\u003e可以拆分为不考虑进位的部分\u003ccode\u003e323\u003c/code\u003e和仅考虑进位的部分\u003ccode\u003e1110\u003c/code\u003e, 故\u003ccode\u003e759 + 674 = 323 + 1110 = 1433\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e二进制的加法也是从低位开始逐步往高位计算:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e进行一位二进制的加法, 也就是暂不考虑进位的位相加: \u003ccode\u003e0+0=0， 0+1=1, 1+0=1， 1+1=0\u003c/code\u003e, 那么就是\u003ccode\u003e^\u003c/code\u003e操作. 所得的和作为新的\u003ccode\u003ea\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e求进位: 通过\u003ccode\u003ea \u0026amp; b\u003c/code\u003e判断是否进位, 因为只有两个位均为\u003ccode\u003e1\u003c/code\u003e才会进位. 所得的进位左移一位作为新的\u003ccode\u003eb\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e不断重复这个过程, 把低位的进位传递到高位, 累加到\u003ccode\u003ea\u003c/code\u003e中, 直到进位为\u003ccode\u003e0\u003c/code\u003e, 最后得到的\u003ccode\u003ea\u003c/code\u003e就是答案.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eSolution\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003egetSum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e!=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 关键在于判断终止的时机\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\t\t\t\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e//carry\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\t\t\t\u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e^=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e//add\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\t\t\t\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\t\t\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\t\t\u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e涉及的运算就是一个多位二进制加法真值表：(对应于硬件中的全加器)\u003c/p\u003e","title":"位操作 - 不使用加减符号求和整数"},{"content":"通过位移实现很多风骚的操作， 参考这个视频。\n检查一个数是否是偶数, 本质上就是取最后一位来判断, 如果是1那么就一定是奇数, 反之则为偶数:\n(x \u0026amp; 1) == 0 Check if power of two:\n(x \u0026amp; x - 1) == 0 因为如果数x是以2底的真数, 那么其二进制一定只有一个位置是1, 如0b1000, 那么x-1就会变成只有该位置是0其右边所有位变为1, 即0b0111, 也就是说这种情况下x和x-1所有位置都互异. 那么它们的位与运算就是x \u0026amp; x - 1 = 0b0000.\nx \u0026amp; x - 1的广义用途是求x二进制中1的个数, Counting bits set:\nunsigned int v; // count the number of bits set in v unsigned int c; // c accumulates the total bits set in v for (c = 0; v; c++) { v \u0026amp;= v - 1; // clear the least significant bit set } Brian Kernighan\u0026rsquo;s algorithm takes O(log N) to count set bits (1s) in an integer: each iteration sets the least significance bit that isn\u0026rsquo;t zero to zero - and only it. Since each iteration converts exactly bit from 1 to 0, it\u0026rsquo;ll take as many iterations as there are non-0 bits to convert all the bits to 0(and thus v == 0 and the loop finishes). An integer n has log(n) bits, hence the worst case is O(log(n))\n如果一个整数不为0, 那么其二进制就至少有一个1. 假设最右边一位是1, 那么减1就会把最后一位变为0, 前面所有位保持不变. 假如最后一位是0, 那么最靠右的1假设在m位置, 那么减去1, 该位置会变为0, 而其右边的所有0都会变为1, 其左边的所有位不变. v \u0026amp;= v - 1把最右的1变为0.\n获取二进制的最后一个1:\ndef last_set_bit(x): y = ~(x - 1) # = - (x - 1) - 1 = -x return x \u0026amp; y 假设最右边的1位于n, -1操作会把n右边所有0变为1, 而n位变为0. 接着~操作会把n左边所有位翻转, 而n及其右边的数会变为原来的样子, 也就是n为1, 右边全为0(或者没有右边). 最后\u0026amp;操作就只剩下n位的1和右边的0(如果有的话).\n","permalink":"https://congchan.github.io/posts/%E4%BD%8D%E6%93%8D%E4%BD%9C-%E9%A3%8E%E9%AA%9A%E7%9A%84%E8%B5%B0%E4%BD%8D%E6%93%8D%E4%BD%9C/","summary":"\u003cp\u003e通过位移实现很多风骚的操作， 参考\u003ca href=\"https://www.youtube.com/watch?v=7jkIUgLC29I\"\u003e这个视频\u003c/a\u003e。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e检查一个数是否是偶数, 本质上就是取最后一位来判断, 如果是1那么就一定是奇数, 反之则为偶数:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCheck if power of two:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e因为如果数\u003ccode\u003ex\u003c/code\u003e是以2底的真数, 那么其二进制一定只有一个位置是\u003ccode\u003e1\u003c/code\u003e, 如\u003ccode\u003e0b1000\u003c/code\u003e, 那么\u003ccode\u003ex-1\u003c/code\u003e就会变成只有该位置是\u003ccode\u003e0\u003c/code\u003e其右边所有位变为\u003ccode\u003e1\u003c/code\u003e, 即\u003ccode\u003e0b0111\u003c/code\u003e, 也就是说这种情况下\u003ccode\u003ex\u003c/code\u003e和\u003ccode\u003ex-1\u003c/code\u003e所有位置都互异. 那么它们的位与运算就是\u003ccode\u003ex \u0026amp; x - 1 = 0b0000\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ex \u0026amp; x - 1\u003c/code\u003e的广义用途是求\u003ccode\u003ex\u003c/code\u003e二进制中\u003ccode\u003e1\u003c/code\u003e的个数, \u003ca href=\"https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetKernighan\"\u003eCounting bits set\u003c/a\u003e:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eunsigned int v; // count the number of bits set in v\nunsigned int c; // c accumulates the total bits set in v\nfor (c = 0; v; c++)\n{\n  v \u0026amp;= v - 1; // clear the least significant bit set\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cblockquote\u003e\n\u003cp\u003eBrian Kernighan\u0026rsquo;s algorithm takes \u003ccode\u003eO(log N)\u003c/code\u003e to count set bits (1s) in an integer: each iteration sets the least significance bit that isn\u0026rsquo;t zero to zero - and only it. Since each iteration converts exactly bit from 1 to 0, it\u0026rsquo;ll take as many iterations as there are non-0 bits to convert all the bits to 0(and thus \u003ccode\u003ev == 0\u003c/code\u003e and the loop finishes). An integer n has \u003ccode\u003elog(n)\u003c/code\u003e bits, hence the worst case is \u003ccode\u003eO(log(n))\u003c/code\u003e\u003c/p\u003e","title":"位操作 - 风骚的走位操作"},{"content":"Knapsack背包问题 背包问题（Knapsack problem）是一种组合优化的NP完全问题。问题可以描述为：给定一组物品，每种物品都有自己的重量和价格，在限定的总重量内，我们如何选择，才能使得物品的总价格最高。问题的名称来源于如何选择最合适的物品放置于给定背包中。 也可以将背包问题描述为决定性问题，即在总重量不超过W的前提下，总价值是否能达到V。\n0-1背包 最基础的背包问题：有N件物品和一个体积为V的背包, 每种物品均只有一件, 第i件物品的大小/重量是s[i]，价值是v[i]. 求将哪些物品装入背包可使这些物品的体积总和不超过背包体积，且价值总和最大.\n对于每一个物品，只有两种结果，放入或者不放入背包，那么kn(i, j)则表示背包容量剩余j时, 前i个物品能够达到的最大值:\nkn1 = kn(i-1, j-s(i)) + v(i)表示物品i放入背包后的总价值, 为前i-1物品在第i个物品占用了背包容量s(i)后的的最优解加上第i个物品的价值v(i). kn2 = kn(i-1, j)表示物品i并没有放入背包, 等于前i-1个物品在相同背包容量的最优价值. 归纳出来的大小子问题间的关系(转移方程)为: kn(i, j) = max(kn1, kn2) = max(kn(i-1, j-s(i)) + v(i), kn(i-1, j)). 初始状态是对于不同背包剩余容量, 当没有物品可放时, 返回的最大价值一定是0. 所以背包问题, 就是二维的动态规划问题. 需要确定初始状态, 和哪些信息需要记忆.\n可以简单地用一个二维数组记忆所有kn(i, j), 但要考虑到当容量非常大, 物品非常多时, 这个二维数组是很大的, 比如当(i, j) = (2000, 2000000), 会抛出java.lang.OutOfMemoryError: Java heap space. 特别是, 当每个物品的价值也比较大时, 二维数组的j维度其实利用率很低. 所以存在很多优化的空间.\n优化的关键点在于减少记忆点. 注意到转移方程中:\nkn(i, *)只需要用到kn(i-1, *)的值, 但我们又清楚地知道，物品在这里是没有顺序的意义的，所以这里的i仅仅是表示迭代的步骤, 只是为了遍历所有物品, 至于具体的顺序是不重要的, 所以不需要记录所有i对应的kn(i, *), 仅仅记录最近一次计算值即可. 所以我们只需要至多两个数组用来记录i-1和i对应的kn值. kn(i, j)要用到kn(i-1, k), k\u0026lt;=j的值, 具体要用到哪些k是取决于i. 所以j维度的值必须都要记录下来, 以防后续需要用到. 结合起来发现只需要一个一维数组kn = new int[size + 1]即可, i对应的值可以直接在数组上更新, 不需要额外的数组记录上一次迭代的值. 在实现中, 因为kn(i, j)要用到kn(i-1, \u0026lt;=j)的值, 也就是kn[\u0026lt;j]的值不能先于kn[j]更新, 所以kn的计算要从右往左(j = size; j--). 每次决定是否加入i物品之前, 如果剩余容量j小于s[i], 那么肯定无法放入, 这个判断可以融合进j的遍历中, 因为j本身代表了剩余容量. static int[] values; static int[] sizes; public static int knapsack(int size) { int n = values.length; int[] vs = new int[size + 1]; for (int i = 0; i \u0026lt; n; i++) { // items for (int j = size; j \u0026gt;= sizes[i]; j--) { vs[j] = Math.max(vs[j - sizes[i]] + values[i], vs[j]); } } return vs[size]; } 优化以后空间复杂度由$\\theta(NS)$降到$\\theta(S)$。但时间复杂度不变.\n对于0-1背包问题，如果问题变为求恰好装满时的最大值, 参考这篇博文: 此时只有容量为0的背包可能被价值为0的物品(无物品)恰好装满，初始化合法状态kn[0] = 0, kn[j \u0026gt; 0]为负数. 反之, 如果要求的是恰好装满时的最小值，初始化为正无穷。要注意的是改变初始化以后最后一个值是恰好装满的最大值，如果不能恰好装满，那肯定是一个负数，而且对于恰好装满的的初始化情况的不要求满的最大值是0-v背包容量的最大值。即是最后一行的MAX。\n完全背包 Unbounded Knapsack: 有N种物品和一个体积为S的背包，每种物品都有无限件可用。第i件物品的体积是s[i]，价值是value[i]。求解将哪些物品装入背包可使这些物品的体积总和不超过背包体积，且价值总和最大。\n0-1背包的问题中每一种物品在背包中的数量只有0和1两种, 而完全背包问题每一种物品在背包中的数量是0个到k = S/s[i]个. 使用与0-1背包类似的定义, kn(i, j)表示背包容量剩余j时, 放入任意个前i个物品能够达到的最大值, 这样转移方程变为: kn(i, j) = max{kn(i-1, j-k*s(i)) + k*v(i)}, 0 \u0026lt;= k \u0026lt;= S/s[i]。可以直接在0-1背包的代码中增加一个循环，这样时间复杂度就增加了。对于取多少也可以利用二进制拆分，取的时候取1, 2, 4, ...。\n完全背包的算法优化 注意到完全背包本身也包含0-1背包的情况, 0-1背包是完全背包的特例. 完全背包的kn(i, j)包含了第i种物品的数量在0 - S/s[i]所有可能选择, 并取其最大值:\n若至少放一个物品i进背包, 那么在对物品i的数量进行0 - S/s[i]的遍历时, 迭代方程变为kn1 = kn(i, j-s(i)) + v(i) 若第i个物品不放入背包时, 情况和0-1背包的kn2一样, kn2 = kn(i-1, j) 所以0-1背包的迭代方程vs[j] = Math.max(vs[j - sizes[i]] + values[i], vs[j]);可以直接套用在完全背包上.\n只是kn的计算要改为从左往右(j = 0; j \u0026lt;= size; j++). 因为此时kn1用的不再是上一次迭代的kn(i-1, j-s(i)), 而是本次迭代的kn(i, j-s(i)). 即kn(i, j)要用到kn(i, \u0026lt;=j)的值, 所以kn[\u0026lt;j]的值要先于kn[j]更新.\n同样, 每次决定是否加入i物品之前, 如果剩余容量j小于s[i], 那么肯定无法放入, 这个判断可以融合进j的遍历中.\nstatic int[] values; static int[] sizes; public static int unboundedKnapsack(int size) { int n = values.length; int[] vs = new int[size + 1]; for (int i = 0; i \u0026lt; n; i++) { // items for (int j = sizes[i]; j \u0026lt;= size; j++) { vs[j] = Math.max(vs[j - sizes[i]] + values[i], vs[j]); } } return vs[size]; } 优化后的时间复杂度为O(NV).\n在数据上也可以优化：如果物品a比b价值更高, 但体积更小, 那么完全可以不考虑物品b。对于随机生成的数据，这个方法往往会大大减少搜索空间。\n多重背包 有N种物品和一个体积为V的背包。第i种物品最多有num[i]件可用，每件体积是sizes[i]，价值是value[i]。求解将哪些物品装入背包可使这些物品的体积总和不超过背包体积，且价值总和最大。\n多重背包问题可以采取基于0-1背包的算法基础上增加一层循环搜索num[i]. 但这样的时间复杂度是O(NVC).\nfor (int i = 0; i \u0026lt; n; i++) { // items for (int k = 1; k \u0026lt;= num[i]; k++) { for (int j = size; j \u0026gt;= sizes[i]; j--) { vs[j] = Math.max(vs[j - sizes[i]] + values[i], vs[j]); } } } 多重背包问题其实包含0-1背包和完全背包，可以分类处理。\n如果满足value[i]*num[i]\u0026gt;=size，这个时候就是完全背包问题, 而完全背包要比多重背包的复杂度低，是O(NV)。 如果满足num[i]=1就是0-1背包。 其他背包 混合背包: 如果将0-1、完全、多重混合起来，有的物品只可以取一次（01背包），有的物品可以取无限次（完全背包），有的物品可以取的次数有一个上限（多重背包）。需要分类求解，判断是哪一种，然后分别给出循环和循环顺序，分别调用状态转换方程。\n其他还有二维费用背包，依赖背包，分组背包\u0026hellip;\n","permalink":"https://congchan.github.io/posts/dynamic-programming-06-knapsack%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/","summary":"\u003ch2 id=\"knapsack背包问题\"\u003eKnapsack背包问题\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e背包问题（Knapsack problem）是一种组合优化的NP完全问题。问题可以描述为：给定一组物品，每种物品都有自己的重量和价格，在限定的总重量内，我们如何选择，才能使得物品的总价格最高。问题的名称来源于如何选择最合适的物品放置于给定背包中。\n也可以将背包问题描述为决定性问题，即在总重量不超过W的前提下，总价值是否能达到V。\u003c/p\u003e\u003c/blockquote\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"0-1背包\"\u003e0-1背包\u003c/h3\u003e\n\u003cp\u003e最基础的背包问题：有N件物品和一个体积为V的背包, \u003cstrong\u003e每种物品均只有一件\u003c/strong\u003e, 第i件物品的大小/重量是\u003ccode\u003es[i]\u003c/code\u003e，价值是\u003ccode\u003ev[i]\u003c/code\u003e. 求将哪些物品装入背包可使这些物品的体积总和不超过背包体积，且价值总和最大.\u003c/p\u003e\n\u003cp\u003e对于每一个物品，只有两种结果，放入或者不放入背包，那么\u003ccode\u003ekn(i, j)\u003c/code\u003e则表示背包容量剩余\u003ccode\u003ej\u003c/code\u003e时, 前\u003ccode\u003ei\u003c/code\u003e个物品能够达到的最大值:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekn1 = kn(i-1, j-s(i)) + v(i)\u003c/code\u003e表示物品\u003ccode\u003ei\u003c/code\u003e放入背包后的总价值, 为前\u003ccode\u003ei-1\u003c/code\u003e物品在第\u003ccode\u003ei\u003c/code\u003e个物品占用了背包容量\u003ccode\u003es(i)\u003c/code\u003e后的的最优解加上第\u003ccode\u003ei\u003c/code\u003e个物品的价值\u003ccode\u003ev(i)\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekn2 = kn(i-1, j)\u003c/code\u003e表示物品\u003ccode\u003ei\u003c/code\u003e并没有放入背包, 等于前\u003ccode\u003ei-1\u003c/code\u003e个物品在相同背包容量的最优价值.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e归纳出来的大小子问题间的关系(转移方程)为: \u003ccode\u003ekn(i, j) = max(kn1, kn2) = max(kn(i-1, j-s(i)) + v(i), kn(i-1, j))\u003c/code\u003e. 初始状态是对于不同背包剩余容量, 当没有物品可放时, 返回的最大价值一定是\u003ccode\u003e0\u003c/code\u003e. 所以背包问题, 就是二维的动态规划问题. 需要确定初始状态, 和哪些信息需要\u003cstrong\u003e记忆\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e可以简单地用一个二维数组记忆所有\u003ccode\u003ekn(i, j)\u003c/code\u003e, 但要考虑到当容量非常大, 物品非常多时, 这个二维数组是很大的, 比如当\u003ccode\u003e(i, j) = (2000, 2000000)\u003c/code\u003e, 会抛出\u003ccode\u003ejava.lang.OutOfMemoryError: Java heap space\u003c/code\u003e. 特别是, 当每个物品的价值也比较大时, 二维数组的\u003ccode\u003ej\u003c/code\u003e维度其实利用率很低. 所以存在很多优化的空间.\u003c/p\u003e\n\u003cp\u003e优化的关键点在于减少记忆点. 注意到转移方程中:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekn(i, *)\u003c/code\u003e只需要用到\u003ccode\u003ekn(i-1, *)\u003c/code\u003e的值, 但我们又清楚地知道，物品在这里是没有顺序的意义的，所以这里的\u003ccode\u003ei\u003c/code\u003e仅仅是表示迭代的步骤, 只是为了遍历所有物品, 至于具体的顺序是不重要的, 所以不需要记录所有\u003ccode\u003ei\u003c/code\u003e对应的\u003ccode\u003ekn(i, *)\u003c/code\u003e, 仅仅记录最近一次计算值即可. 所以我们只需要至多两个数组用来记录\u003ccode\u003ei-1\u003c/code\u003e和\u003ccode\u003ei\u003c/code\u003e对应的\u003ccode\u003ekn\u003c/code\u003e值.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekn(i, j)\u003c/code\u003e要用到\u003ccode\u003ekn(i-1, k), k\u0026lt;=j\u003c/code\u003e的值, 具体要用到哪些\u003ccode\u003ek\u003c/code\u003e是取决于\u003ccode\u003ei\u003c/code\u003e. 所以\u003ccode\u003ej\u003c/code\u003e维度的值必须都要记录下来, 以防后续需要用到.\u003c/li\u003e\n\u003cli\u003e结合起来发现只需要一个一维数组\u003ccode\u003ekn = new int[size + 1]\u003c/code\u003e即可, \u003ccode\u003ei\u003c/code\u003e对应的值可以直接在数组上更新, 不需要额外的数组记录上一次迭代的值. 在实现中, 因为\u003ccode\u003ekn(i, j)\u003c/code\u003e要用到\u003ccode\u003ekn(i-1, \u0026lt;=j)\u003c/code\u003e的值, 也就是\u003ccode\u003ekn[\u0026lt;j]\u003c/code\u003e的值不能先于\u003ccode\u003ekn[j]\u003c/code\u003e更新, 所以\u003ccode\u003ekn\u003c/code\u003e的计算要从右往左(\u003ccode\u003ej = size; j--\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e每次决定是否加入\u003ccode\u003ei\u003c/code\u003e物品之前, 如果剩余容量\u003ccode\u003ej\u003c/code\u003e小于\u003ccode\u003es[i]\u003c/code\u003e, 那么肯定无法放入, 这个判断可以融合进\u003ccode\u003ej\u003c/code\u003e的遍历中, 因为\u003ccode\u003ej\u003c/code\u003e本身代表了剩余容量.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esizes\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eknapsack\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evs\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// items\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esizes\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e--\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003evs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMath\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esizes\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e优化以后空间复杂度由$\\theta(NS)$降到$\\theta(S)$。但时间复杂度不变.\u003c/p\u003e","title":"Dynamic Programming 06 - Knapsack背包问题"},{"content":"跳台阶 跳上一个n级的台阶总共有多少种跳法，先后次序不同算不同的结果，限制条件是每次只能跳1级或者2级。\n抽象出来的模型是：给定正整数N，有多少种累加方案，不同顺序当做不同方案，限制条件可以是给定的整数$n_0, n_1, ..., n_k$作为可选累加元素.\n对于限制条件为只有两种跳法, 即1阶或者2阶的, 问题可以分解为:\n假定第一次跳的是1阶，那么就剩下n-1个台阶，剩余跳法是f(n-1); 假定第一次跳的是2阶，则剩下n-2个台阶，剩余跳法是f(n-2) 可以归纳出通用的公式: f(n) = f(n-1) + f(n-2), 只有一阶的时候f(1) = 1, 只有两阶的时候可以有f(2) = 2, 刚好就是斐波那契数列. 所以这个简单的跳台阶问题就是计算斐波那契数列的问题。\n反过来思考, 比如对于8个台阶, 有多少种回滚方案? 只有两种: 回滚1个台阶, 就到了7; 回滚2个台阶, 就到了6. 等于说: 假如有f(7)种方案跳到7, 有f(6)种方案跳到6，那么就有f(7) + f(6)种方案到达8\n从树结构来理解: 如果节点代表台阶数n对应的跳法f(n), 节点与节点间的枝代表单次可以跳的阶数, 父节点的值就是其所有子节点的值和. 对于只有两种跳法限制问题, 父节点f(n)就只有两个子节点, 分别为f(n-1)和f(n-2).\n斐波那契数列 举例：Fibonacci sequence: ${\\displaystyle 0,\\;1,\\;1,\\;2,\\;3,\\;5,\\;8,\\;13,\\;21,\\;34,\\;55,\\;89,\\;144,\\;\\ldots }$ $$F_0 = 0, F_1 = 1, F_2 = 1, F_n = F_{n-1} + F_{n-2} (n\u003e2) $$Fibonacci numbers grow almost as fast as the powers of 2.\nRecursive solution is exponential algorithm\nfib1(n): if n = 0: return 0 if n = 1: return 1 return fib1(n - 1) + fib1(n - 2) 因为每一个fib1()都会生成指数级数量的子分支计算, 所以这个算法复杂度是$O(2^n)$.\n但是注意到斐波那契数列公式是$F_n = F_{n-1} + F_{n-2}$, 也就是只要知道n前面两个值, 就能计算出$f_n$. 又因为斐波那契数列天然的是从低往高算, 那么每次迭代只需要用到前两次的值$F_{n-1}, F_{n-2}$, 计算后更新它们即可. 用这个思路来计算斐波那契数列, 复杂度就是$O(n)$.\npublic int JumpFloor(int target) { if (target \u0026lt;= 1) { return target; } int n = 2; int n0 = 1; int n1 = 1; int ways = 0; while (n \u0026lt;= target) { ways = n0 + n1; n0 = n1; n1 = ways; n++; } return ways; } 变态跳台阶 变态跳台阶就是是用来更复杂的限制条件, 比如可选单次跳阶数为[1, ... n], 也就是无限制的情况, 也可以按照上面的思路推导.\n比如从树结构的考虑, 就变成每个父节点f(n)可以有n个子节点, 就是f(n-1), f(n-2), ..., f(n-n), 所以f(n)就是所有这些子节点的和. f(n-n)也就是f(0)意味着一次跳完所有阶数n, 所以f(0) = 1. 进一步归纳, f(n-2) + ... + f(n-n) = f(n-1), 所以f(n) = f(n-1) + f(n-1), 可以用递归或者动态规划来计算.\n当然进一步归纳会发现$f(n) = 2^{n-1}$, 可以用位移来操作:\npublic int JumpFloorII(int target) { int a=1; return a \u0026lt;\u0026lt; (target - 1); } 只是要注意int是有范围的.\n大变态跳台阶 再举一个更复杂的限制条件, 可选单次跳阶数为$2^0, 2^1, ..., 2^k$, $2^k$要小于n. 那么相应的, $$f(n) = f(n - 2^0) + f(n - 2^1)... + f(n - 2^k), \\quad s.t. \\quad 2^k \u003c= n,$$ 这样就意味着对于每个f(n), 需要用到的f(k)值数量是不同的, 就不能简单地用固定数量的变量来保留较小值了.\n对于不同的f(n), 它们的很多子分支计算是共享的, 比如f(6)和f(5)都用到了f(4). 那么在递归的过程中，只要把每次计算出来的较小的f(k)储存到数组中, 后续其他f(n)要用到f(n - 2^k)时, 直接从内存中取值即可. 初始值取f(0) = f(1) = 1:\npublic int JumpFloorIII(int target) { int[] f = new int[target]; f[0] = f[1] = 1; return jump(f, target); } private static int jump(int[] f, int target) { if (f[target] == 0) { int ways = 0; for (int i = 0; (1 \u0026lt;\u0026lt; i) \u0026lt;= target; i++) { ways += jump(f, target - (1 \u0026lt;\u0026lt; i)); } f[target] = ways; } return f[target]; } 这个代码适用于n \u0026lt;= 1024的情况. 否则要改为循环。\n","permalink":"https://congchan.github.io/posts/dynamic-programming-05-%E8%B7%B3%E5%8F%B0%E9%98%B6/","summary":"\u003ch2 id=\"跳台阶\"\u003e跳台阶\u003c/h2\u003e\n\u003cp\u003e跳上一个n级的台阶总共有多少种跳法，先后次序不同算不同的结果，限制条件是每次只能跳1级或者2级。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e抽象出来的模型是：给定正整数\u003ccode\u003eN\u003c/code\u003e，有多少种累加方案，不同顺序当做不同方案，限制条件可以是给定的整数$n_0, n_1, ..., n_k$作为可选累加元素.\u003c/p\u003e\n\u003cp\u003e对于限制条件为只有两种跳法, 即1阶或者2阶的, 问题可以分解为:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e假定第一次跳的是\u003ccode\u003e1\u003c/code\u003e阶，那么就剩下\u003ccode\u003en-1\u003c/code\u003e个台阶，剩余跳法是\u003ccode\u003ef(n-1)\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e假定第一次跳的是\u003ccode\u003e2\u003c/code\u003e阶，则剩下\u003ccode\u003en-2\u003c/code\u003e个台阶，剩余跳法是\u003ccode\u003ef(n-2)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e可以归纳出通用的公式: \u003ccode\u003ef(n) = f(n-1) + f(n-2)\u003c/code\u003e, 只有一阶的时候\u003ccode\u003ef(1) = 1\u003c/code\u003e, 只有两阶的时候可以有\u003ccode\u003ef(2) = 2\u003c/code\u003e, 刚好就是斐波那契数列. 所以这个简单的跳台阶问题就是计算斐波那契数列的问题。\u003c/p\u003e\n\u003cp\u003e反过来思考, 比如对于\u003ccode\u003e8\u003c/code\u003e个台阶, 有多少种回滚方案? 只有两种: 回滚1个台阶, 就到了\u003ccode\u003e7\u003c/code\u003e; 回滚2个台阶, 就到了\u003ccode\u003e6\u003c/code\u003e. 等于说: 假如有\u003ccode\u003ef(7)\u003c/code\u003e种方案跳到\u003ccode\u003e7\u003c/code\u003e, 有\u003ccode\u003ef(6)\u003c/code\u003e种方案跳到\u003ccode\u003e6\u003c/code\u003e，那么就有\u003ccode\u003ef(7) + f(6)\u003c/code\u003e种方案到达\u003ccode\u003e8\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从树结构来理解: 如果节点代表台阶数\u003ccode\u003en\u003c/code\u003e对应的跳法\u003ccode\u003ef(n)\u003c/code\u003e, 节点与节点间的枝代表单次可以跳的阶数, 父节点的值就是其所有子节点的值和. 对于只有两种跳法限制问题, 父节点\u003ccode\u003ef(n)\u003c/code\u003e就只有两个子节点, 分别为\u003ccode\u003ef(n-1)\u003c/code\u003e和\u003ccode\u003ef(n-2)\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"斐波那契数列\"\u003e斐波那契数列\u003c/h3\u003e\n\u003cp\u003e举例：Fibonacci sequence: ${\\displaystyle 0,\\;1,\\;1,\\;2,\\;3,\\;5,\\;8,\\;13,\\;21,\\;34,\\;55,\\;89,\\;144,\\;\\ldots }$\n\u003c/p\u003e\n$$F_0 = 0, F_1 = 1, F_2 = 1, F_n = F_{n-1} + F_{n-2} (n\u003e2) $$\u003cp\u003eFibonacci numbers grow almost as fast as the powers of 2.\u003c/p\u003e","title":"Dynamic Programming 05 - 跳台阶"},{"content":"丑数 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。\n要判断一个数是不是丑数, 不断地分别除以2, 3, 5，然后检查num是否到达1:\npublic boolean isUgly(int num) { //(除数包括`4`可以让代码更简洁) for (int i=2; i\u0026lt;6 \u0026amp;\u0026amp; num\u0026gt;0; i++) while (num % i == 0) num /= i; return num == 1; } 如果要返回第n个丑数(leetcode原题), 情况就稍微复杂点. 从动态规划的角度考虑, 对于一个较大的丑数N, 必定是由某个更小的丑数M乘以2, 3, 5其中一个得来的. 所以可以从小到大不断生成丑数. 为了避免在循环中每一次计算都从头开始检查每一个数k对应的2*k, 3*k, 5*k, 需要用三个变量last2, last3, last5来分别记录最近一次用到的丑数的索引, 下一次计算时就直接从上一次停止的地方开始运行.\n/** return the nth ugly number */ public static int unglyNumber(int n) { final int INIT = 5; int[] uglys = new int[n + INIT]; for (int i = 0; i \u0026lt; 5;) { uglys[i] = ++i; } int last2, last3, last5, m2, m3, m5; last2 = last3 = last5 = 0; m2 = m3 = m5 = 1; for (int i = INIT; i \u0026lt; n; i++) { for (int j = last2 + 1; j \u0026lt; i; j++) { if (m2 \u0026lt;= uglys[i - 1] \u0026amp;\u0026amp; uglys[j] * 2 \u0026gt; uglys[i - 1]) { m2 = uglys[j] * 2; last2 = j; } } for (int j = last3 + 1; j \u0026lt; i; j++) { if (m3 \u0026lt;= uglys[i - 1] \u0026amp;\u0026amp; uglys[j] * 3 \u0026gt; uglys[i - 1]) { m3 = uglys[j] * 3; last3 = j; } } for (int j = last5 + 1; j \u0026lt; i; j++) { if (m5 \u0026lt;= uglys[i - 1] \u0026amp;\u0026amp; uglys[j] * 5 \u0026gt; uglys[i - 1]) { m5 = uglys[j] * 5; last5 = j; } } uglys[i] = Math.min(Math.min(m2, m3), m5); } return uglys[n - 1]; } 这里提供了另一个理解这个问题的思路，并由此得出了一个更快的的算法(O(n))：根据前面算法的原理，可以知道下一个丑数一定是前面某一个丑数乘以2,3,5中的一个，所以可以把问题转换为从以下三组数据中不断取最小值的问题：\n(1) 1×2, 2×2, 3×2, 4×2, 5×2, … (2) 1×3, 2×3, 3×3, 4×3, 5×3, … (3) 1×5, 2×5, 3×5, 4×5, 5×5, … 可以发现每个子序列是丑数序列本身1, 2, 3, 4, 5......分别乘以2, 3, 5。使用类似merge sort的合并方法，每次从三个数组中弹出最小的数:\n/** return the nth ugly number */ public static int getUglyNumber(int n) { if (n == 0) return 0; int[] ugly = new int[n]; ugly[0] = 1; int i2 = 0, i3 = 0, i5 = 0; int next2 = 2, next3 = 3, next5 = 5; for (int i = 1; i \u0026lt; n; i++) { ugly[i] = Math.min(next2, Math.min(next3, next5)); if (next2 == ugly[i]) next2 = ugly[++i2] * 2; if (next3 == ugly[i]) next3 = ugly[++i3] * 3; if (next5 == ugly[i]) next5 = ugly[++i5] * 5; } return ugly[n - 1]; } ","permalink":"https://congchan.github.io/posts/dynamic-programming-04-%E4%B8%91%E6%95%B0/","summary":"\u003ch2 id=\"丑数\"\u003e丑数\u003c/h2\u003e\n\u003cp\u003e把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e要判断一个数是不是丑数, 不断地分别除以\u003ccode\u003e2, 3, 5\u003c/code\u003e，然后检查\u003ccode\u003enum\u003c/code\u003e是否到达\u003ccode\u003e1\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eisUgly\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enum\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e//(除数包括`4`可以让代码更简洁)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003e6\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enum\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enum\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e%\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003enum\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e/=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enum\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e如果要返回第\u003ccode\u003en\u003c/code\u003e个丑数(\u003ca href=\"https://leetcode.com/problems/ugly-number-ii/\"\u003eleetcode原题\u003c/a\u003e), 情况就稍微复杂点. 从动态规划的角度考虑, 对于一个较大的丑数\u003ccode\u003eN\u003c/code\u003e, 必定是由某个更小的丑数\u003ccode\u003eM\u003c/code\u003e乘以\u003ccode\u003e2, 3, 5\u003c/code\u003e其中一个得来的. 所以可以从小到大不断生成丑数. 为了避免在循环中每一次计算都从头开始检查每一个数\u003ccode\u003ek\u003c/code\u003e对应的\u003ccode\u003e2*k, 3*k, 5*k\u003c/code\u003e, 需要用三个变量\u003ccode\u003elast2, last3, last5\u003c/code\u003e来分别记录最近一次用到的丑数的索引, 下一次计算时就直接从上一次停止的地方开始运行.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e/** return the nth ugly number */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eunglyNumber\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eINIT\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eINIT\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e;)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elast2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elast3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elast5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003em2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003em3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003em5\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003elast2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elast3\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elast5\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003em2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003em3\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003em5\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eINIT\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elast2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003em2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003em2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003elast2\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elast3\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003em3\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e3\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003em3\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003elast3\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elast5\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003em5\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e5\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003em5\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003elast5\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMath\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eMath\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003em2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003em3\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003em5\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euglys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ca href=\"https://www.geeksforgeeks.org/ugly-numbers/\"\u003e这里\u003c/a\u003e提供了另一个理解这个问题的思路，并由此得出了一个更快的的算法(\u003ccode\u003eO(n)\u003c/code\u003e)：根据前面算法的原理，可以知道下一个丑数一定是前面某一个丑数乘以2,3,5中的一个，所以可以把问题转换为从以下三组数据中不断取最小值的问题：\u003c/p\u003e","title":"Dynamic Programming 04 - 丑数"},{"content":"最长公共子序列 对于一个字符串, 它的子序列，就是将给字符串中任意个元素去掉之后剩余的字符串, 所以子序列不要求是连续的, 但是维持原来的顺序. 在文本相似度比较中，常用到最长公共子序列（longest common sequence）。\n同时遍历两个字符串, 如果x[i] == y[j], 则x[i]和y[j]参与了最长公共子序列z[k]的构建.\n如果用lcs[i, j]表示遍历到x[0-i]和y[0-j]时的LCS长度, 那么现在就需要判断x[i]和y[j]的关系, 分两种情况:\n如果二者相等, 那么lcs1 = lcs[i - 1, j - 1] + 1 若不相等, 那么只能在x和y中选择一个进行推进, 选择依据就是取较大值, lcs2 = max(lcs[i - 1, j], lcs[i, j - 1]) 初始状态自然是lcs[0, 0] = 0.\nstatic int[][] lcs; public static int longestCS(String x, String y) { char[] xList = x.toCharArray(); char[] yList = y.toCharArray(); for (int i = 1; i \u0026lt;= xList.length; i++) { for (int j = 1; j \u0026lt;= yList.length; j++) { if (xList[i - 1] == yList[j - 1]) { lcs[i][j] = lcs[i - 1][j - 1] + 1; } else { lcs[i][j] = Math.max(lcs[i - 1][j], lcs[i][j - 1]); } } } return lcs[x.length()][y.length()]; } 最长公共子串 最长公共子串（longest common substring）, 要求的是任意连续的子字符串。设定LCS(i, j)为包含当前字符a[i]和b[j]的最长lcs. 假如当前满足a[i] == b[j], 那么LCS(i, j) = LCS(i - 1, j - 1) + 1, 否则为0.\n比如字符串21232523311324和字符串312123223445的匹配矩阵，前者为X方向的，后者为Y方向的。例子来源于这篇文章\n0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 2 1 0 0 0 0 1 0 2 0 1 0 1 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 3 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 4 0 0 0 2 1 0 0 1 0 0 0 1 0 1 0 5 0 1 0 0 0 0 0 2 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 2 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 public int findLongest(String A, int n, String B, int m) { char[] cha = A.toCharArray(); char[] chb = B.toCharArray(); int[][] matrix = new int[n][m]; int max = 0; for (int i = 0; i \u0026lt; n; i++) for (int j = 0; j \u0026lt; m; j++) if (cha[i] == chb[j]) { int cur = (i \u0026gt; 0 \u0026amp;\u0026amp; j \u0026gt; 0) ? matrix[i - 1][j - 1] + 1 : 1; max = Math.max(max, cur); matrix[i][j] = cur; } return max; } 从另一个角度理解, 这个解法就是用一个矩阵来记录两个字符串中所有位置的两个字符之间的匹配情况，若是匹配则赋上其左上角元素的值加1，否则为0。矩阵中值最大的值，就对应着对角线最长的非0连续序列，其对应的位置就是最长匹配子串的位置，最长匹配子串的位置和长度就已经出来了。计算这个矩阵的复杂度是O(N*M).\n","permalink":"https://congchan.github.io/posts/dynamic-programming-03-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97/","summary":"\u003ch2 id=\"最长公共子序列\"\u003e最长公共子序列\u003c/h2\u003e\n\u003cp\u003e对于一个字符串, 它的子序列，就是将给字符串中任意个元素去掉之后剩余的字符串, 所以子序列不要求是连续的, 但是维持原来的顺序. 在文本相似度比较中，常用到最长公共子序列（longest common sequence）。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e同时遍历两个字符串, 如果\u003ccode\u003ex[i] == y[j]\u003c/code\u003e, 则\u003ccode\u003ex[i]\u003c/code\u003e和\u003ccode\u003ey[j]\u003c/code\u003e参与了最长公共子序列\u003ccode\u003ez[k]\u003c/code\u003e的构建.\u003c/p\u003e\n\u003cp\u003e如果用\u003ccode\u003elcs[i, j]\u003c/code\u003e表示遍历到\u003ccode\u003ex[0-i]\u003c/code\u003e和\u003ccode\u003ey[0-j]\u003c/code\u003e时的LCS长度, 那么现在就需要判断\u003ccode\u003ex[i]\u003c/code\u003e和\u003ccode\u003ey[j]\u003c/code\u003e的关系, 分两种情况:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e如果二者相等, 那么\u003ccode\u003elcs1 = lcs[i - 1, j - 1] + 1\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e若不相等, 那么只能在\u003ccode\u003ex\u003c/code\u003e和\u003ccode\u003ey\u003c/code\u003e中选择一个进行推进, 选择依据就是取较大值, \u003ccode\u003elcs2 = max(lcs[i - 1, j], lcs[i, j - 1])\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e初始状态自然是\u003ccode\u003elcs[0, 0] = 0\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[][]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elcs\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003elongestCS\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003echar\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003exList\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003etoCharArray\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003echar\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eyList\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003etoCharArray\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003exList\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eyList\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003exList\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eyList\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003elcs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e][\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elcs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e][\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"n\"\u003elcs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e][\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMath\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elcs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e][\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elcs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e][\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elcs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e][\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"最长公共子串\"\u003e最长公共子串\u003c/h3\u003e\n\u003cp\u003e最长公共子串（longest common substring）, 要求的是任意连续的子字符串。设定\u003ccode\u003eLCS(i, j)\u003c/code\u003e为包含当前字符\u003ccode\u003ea[i]\u003c/code\u003e和\u003ccode\u003eb[j]\u003c/code\u003e的最长lcs. 假如当前满足\u003ccode\u003ea[i] == b[j]\u003c/code\u003e, 那么\u003ccode\u003eLCS(i, j) = LCS(i - 1, j - 1) + 1\u003c/code\u003e, 否则为0.\u003c/p\u003e","title":"Dynamic Programming 03 - 最长公共子序列"},{"content":"最大子序列 Maximum subarray problem: In computer science, the maximum subarray problem is the task of finding the contiguous subarray within a one-dimensional array of numbers which has the largest sum. For example, for the sequence of values −2, 1, −3, 4, −1, 2, 1, −5, 4; the contiguous subarray with the largest sum is 4, −1, 2, 1, with sum 6. The problem was first posed by Ulf Grenander of Brown University in 1977, as a simplified model for maximum likelihood estimation of patterns in digitized images. A linear time algorithm was found soon afterwards by Jay Kadane of Carnegie Mellon University (Bentley 1984).\n如果我们知道在位置i结束的最大MSP[i]，那么在位置i + 1处结束的MSP[i+1]就是有两种情况，一是包含MSP[i]，即MSP[i+1] = MSP[i] + nums[i]，二是不包含, 即MSP[i+1] = nums[i], 二者取较大值。\npublic int maxSubArray(int[] nums) { int maxCur = nums[0]; // maximum value contains current int maxSoFar = maxCur; // maximum value found so far for(int i = 1; i \u0026lt; nums.length; i++) { maxCur = Math.max(nums[i], nums[i] + maxCur); maxSoFar = Math.max(maxCur, maxSoFar); } return maxSoFar; } 股票最优买卖时间点 给出一段股票价格变化序列，一次交易可以获得的最大收益。如[7,1,5,3,6,4]输出5, 在第2天买入（价格= 1）并在第5天卖出（价格= 6），利润 6-1 = 5。\n解决逻辑跟最大子序列问题一样, 使用Kadane\u0026rsquo;s Algorithm. 计算原始数组的差分, 并寻找给出最大利润的连续子序列, 如果差分小于0, 重置为0:\npublic int maxProfit(int[] prices) { int maxCur = 0; // current maximum value int maxSoFar = 0; // maximum value found so far for(int i = 1; i \u0026lt; prices.length; i++) { maxCur = Math.max(0, maxCur + prices[i] - prices[i-1]); maxSoFar = Math.max(maxCur, maxSoFar); } return maxSoFar; } 股票最优买卖时间点II 以上问题如果是无限次交易(但不允许在同一天内买卖股票):\npublic int maxProfit(int[] prices) { int profit = 0, i = 0; while (i \u0026lt; prices.length) { // find next local minimum while (i \u0026lt; prices.length-1 \u0026amp;\u0026amp; prices[i+1] \u0026lt;= prices[i]) i++; int min = prices[i++]; // need increment to avoid infinite loop for \u0026#34;[1]\u0026#34; // find next local maximum while (i \u0026lt; prices.length-1 \u0026amp;\u0026amp; prices[i+1] \u0026gt;= prices[i]) i++; profit += i \u0026lt; prices.length ? prices[i++] - min : 0; } return profit; } 如果允许T+0交易, 那么直接贪心求和所有正的差分项就好了.\n股票最优买卖时间点III 参考这个答案 以上问题如果是交易次数最多两次, 需要寻找动态规划转移方程. profit[k, i]表示第k次交易, 在i天的利润. 如果当天不交易, 那么利润不变profit[k, i] = profit[k, i - 1]. 如果当天卖出, 卖出的是第j天买入的股票(j \u0026lt; i), 那么利润就是prices[i] - prices[j] + profit[k-1, j-1] , 也就是要prices[j] - profit[k-1, j-1]最小.\npublic int maxProfit(int[] prices) { if (prices.length == 0) return 0; int[][] profit = new int[3][prices.length]; for (int k = 1; k \u0026lt;= 2; k++) { for (int i = 1; i \u0026lt; prices.length; i++) { int min = prices[0]; for (int j = 1; j \u0026lt;= i; j++) min = Math.min(min, prices[j] - profit[k-1][j-1]); profit[k][i] = Math.max(profit[k][i-1], prices[i] - min); } } return profit[2][prices.length - 1]; } 因为i从左往右, j \u0026lt; i, 所有min不需要每次都从头开始找:\npublic int maxProfit(int[] prices) { if (prices.length == 0) return 0; int[][] profit = new int[3][prices.length]; for (int k = 1; k \u0026lt;= 2; k++) { int min = prices[0]; for (int i = 1; i \u0026lt; prices.length; i++) { min = Math.min(min, prices[i] - profit[k-1][i-1]); profit[k][i] = Math.max(profit[k][i-1], prices[i] - min); } } return profit[2][prices.length - 1]; } 复杂度为O(NK). 从循环上可以看到, i只依赖于i-1, k只依赖于k-1, 因此可以压缩为一维的数组来存储, 但需要改变交换i和k的循环\npublic int maxProfit(int[] prices) { if (prices.length == 0) return 0; int[] profit = new int[3]; int[] min = new int[3]; for (int i = 1; i \u0026lt; prices.length; i++) { for (int k = 1; k \u0026lt;= 2; k++) { min[k] = Math.Min(min[k], prices[i] - profit[k-1]); profit[k] = Math.Max(profit[k], prices[i] - min[k]); } } return profit[2]; } 因为在这里k=2, 所以可以使用有限个变量来储存状态:\npublic int maxProfit(int[] prices) { int buyOne = Integer.MAX_VALUE; int SellOne = 0; int buyTwo = Integer.MAX_VALUE; int SellTwo = 0; for(int p : prices) { buyOne = Math.min(buyOne, p); SellOne = Math.max(SellOne, p - buyOne); buyTwo = Math.min(buyTwo, p - SellOne); SellTwo = Math.max(SellTwo, p - buyTwo); } return SellTwo; } ","permalink":"https://congchan.github.io/posts/dynamic-programming-02-%E6%9C%80%E5%A4%A7%E5%AD%90%E5%BA%8F%E5%88%97/","summary":"\u003ch2 id=\"最大子序列\"\u003e最大子序列\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMaximum subarray problem: In computer science, the maximum subarray problem is the task of finding the contiguous subarray within a one-dimensional array of numbers which has the largest sum. For example, for the sequence of values −2, 1, −3, 4, −1, 2, 1, −5, 4; the contiguous subarray with the largest sum is 4, −1, 2, 1, with sum 6.\nThe problem was first posed by Ulf Grenander of Brown University in 1977, as a simplified model for maximum likelihood estimation of patterns in digitized images. A linear time algorithm was found soon afterwards by Jay Kadane of Carnegie Mellon University (Bentley 1984).\u003c/p\u003e","title":"Dynamic Programming 02 - 最大子序列"},{"content":"我们已经看到了一些优雅的设计原则，例如分而治之，图探索和贪婪的选择，它们为各种重要的计算任务提供了确定的算法。这些工具的缺点是只能用于非常特定类型的问题。现在我们来谈谈算法工艺的两个大锤，即动态编程和线性编程，这两种适用性非常广泛的技术可以在更专门的方法失败时调用。可以预见的是，这种普遍性通常会带来效率上的损失。\n很多经典的方法，如 divide-and-conquer, graph exploration, and greedy等, 为各种重要的计算任务提供了确定性的解法。但是这些算法只能用于特定类型的问题。\n这里介绍两个算法大杀器: Dynamic programing 和 linear programming.\n这两种适用性非常广的算法可以在黔驴技穷时考虑调用（如 the knapsack problem, sequence alignment, and optimal binary search trees）。当然，普遍性往往会带来效率上的损失。\n动态规划 动态规划作为一种编程范例，可以从一个例子着手理解：求数列的 maximum-weighted independent sets (MIS, 最大非连续非相邻子集)和, 对于a = [1, 4, 5, 4], 其MIS为{a[1], a[3]} = 8.\n如果使用贪心法, 每次都在可选范围内取最大值, 那么就会得到{a[2], a[0]} = 6.\n如果使用分而治之法, 把数组分为两半a1 = [1, 4], a2 = [5, 4], 则分别得到MIS{a1[1]}, {a2[0]}, 合并后发现是相邻的, 与要求相悖.\n要解决这个问题，关键的步骤是找到基于子问题最优解的最优解：想办法把缩小最优解备选方案的数量，在这个较小的空间中可以直接采取暴力搜索寻找最优解。\n对于a = [1, 4, 5, 4], 假设其MIS为S, 假如从最右边的元素开始考虑, a[3] = 4只有属于S和不属于S两种情况\n若a[3] = 4属于S: 那么a[2] = 5就肯定不属于S, 则S1 = MIS([1, 4]) + MIS([4]) 若a[3]不属于S: 那么S只能存在于[1, 4, 5]中, 问题就变成S2 = MIS([1, 4, 5]) 所以归纳出S = max(S1, S2) = max(MIS([1, 4]) + MIS([4]), MIS([1, 4, 5])) = ...。 对于只剩下一个元素的去情况, MIS([4]) = max(4) = 4, 即MIS([i]) = i\n这就是一个递归的调用: 也就是从右往左, 每一个元素都要考虑一遍是否属于S, 每次会分裂出两种情况, 所以递归的复杂度是$Θ(2^n)$.\n这个算法效率不高, 需要优化. 我们考虑这个问题到底有多少不同的子问题? 因为我们是从右往左扫描每一个元素, 对于每一个元素i, 不管其属于或不属于S, 待解决的递归子问题只有一个, 就是求其左边的所有元素(前缀)的MIS, 所以理论上有$Θ(n)$个不同的子问题.\n所以虽然递归的调用是$Θ(2^n)$, 但需要解决的子问题只有$Θ(n)$, 那么就存在优化的空间. 办法就是通过记忆已经解决了的子问题的答案, 来避免重复的计算. 因为右边的元素的子问题答案需要用到其左边的子问题的答案, 所以计算时, 要从左往右计算.\n定义子问题: 用MIS(i)表示a[i]的前缀a[: i]的MIS(不包括a[i]),\nMIS(0) = 0, 因为a[0]左边没有任何元素. MIS(1) = max(a[0:1]) = a[0] 对于i \u0026gt; 1, MIS(i)只有两种情况, 取二者中较大者: 包括a[i - 1], MIS(i) = MIS(i - 2) + a[i - 1] 不包括, MIS(i) = MIS(i - 1) 从开头开始考虑(计算), 每一步i都记住对应子问题的最优解MIS(i), 计算到最后一个子问题MIS(N), 就得出了考虑了所有子问题之后的最大值\npublic static int[] forwardMIS(int[] a) { int[] mis = new int[a.length + 1]; mis[0] = 0; mis[1] = a[0]; for (int i = 2; i \u0026lt; mis.length; i++) { mis[i] = Math.max(mis[i - 1], mis[i - 2] + a[i - 1]); } return mis; } 但以上算法并没有记录MIS具体包含哪些子集，虽然可以通过修改mis数据结构来额外存储每个值对应的MIS点集, 但这样会影响效率而且浪费内存空间.\n回忆前面从右往左的分析, 每个元素都会考量是否属于MISS, 所以我们可以把forwardMIS中算好的mis数组从右往左依次判断一遍, 已决定是否把a对应位置的元素加入到S中.\npublic static ArrayList\u0026lt;Integer\u0026gt; backwardMIS(int[] a) { ArrayList\u0026lt;Integer\u0026gt; s = new ArrayList\u0026lt;\u0026gt;(); int i = mis.length - 1; while (i \u0026gt;= 2) { if (mis[i - 1] \u0026gt;= mis[i - 2] + a[i - 1]) { i--; } else { s.add(a[i - 1]); i -= 2; } } return s; } 进一步优化，我们可以用类似backward的算法一次过(O(n))计算出MIS的集合和MIS的值, 对backward算法稍作改动, 在a表末尾延申一个0元素, 然后从右往左依次判断一遍, 直接用a表自身的值来判断, 得到一个Backward算法:\nstatic int sum = 0; public static ArrayList\u0026lt;Integer\u0026gt; backwardMISI(int[] a) { ArrayList\u0026lt;Integer\u0026gt; s = new ArrayList\u0026lt;\u0026gt;(); int i = a.length - 2; // assume a = [a, 0] while (i \u0026gt;= 0) { int x = get(a, i); if (get(a, i + 1) \u0026gt;= get(a, i + 2) + x) { i--; } else { s.add(x); sum += x; i -= 2; } } return s; } private static int get(int[] a, int index) { if (index \u0026gt; a.length -1) { return 0; } else { return a[index]; } } 总结动态规划的解法：\n定义合适的子问题集合: 这些子问题应该尽可能小，数量尽可能少。因为即使在最好的情况下，也要花费 constant time 来解决每个子问题，因此子问题的数量和大小就是整个算法运行时间的下限。 归纳转移方程：系统地解决从最小的子问题开始的所有子问题后，如何转向越来越大的子问题。 通过记忆减少重复的递归调用计算: 要求前面子问题的解决方案能够用来快速计算当前子问题。 参考资料 https://people.eecs.berkeley.edu/~vazirani/algorithms/chap6.pdf http://cs.yazd.ac.ir/farshi/Teaching/DP3901/ ","permalink":"https://congchan.github.io/posts/dynamic-programming-01-%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","summary":"\u003cp\u003e我们已经看到了一些优雅的设计原则，例如分而治之，图探索和贪婪的选择，它们为各种重要的计算任务提供了确定的算法。这些工具的缺点是只能用于非常特定类型的问题。现在我们来谈谈算法工艺的两个大锤，即动态编程和线性编程，这两种适用性非常广泛的技术可以在更专门的方法失败时调用。可以预见的是，这种普遍性通常会带来效率上的损失。\u003c/p\u003e\n\u003cp\u003e很多经典的方法，如 divide-and-conquer, graph exploration, and greedy等, 为各种重要的计算任务提供了确定性的解法。但是这些算法只能用于特定类型的问题。\u003c/p\u003e\n\u003cp\u003e这里介绍两个算法大杀器: \u003cstrong\u003eDynamic programing\u003c/strong\u003e 和 \u003cstrong\u003elinear programming\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e这两种适用性非常广的算法可以在黔驴技穷时考虑调用（如 the knapsack problem, sequence alignment, and optimal binary search trees）。当然，普遍性往往会带来效率上的损失。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"动态规划\"\u003e动态规划\u003c/h2\u003e\n\u003cp\u003e动态规划作为一种编程范例，可以从一个例子着手理解：求数列的 maximum-weighted independent sets (MIS, 最大非连续非相邻子集)和, 对于\u003ccode\u003ea = [1, 4, 5, 4]\u003c/code\u003e, 其MIS为\u003ccode\u003e{a[1], a[3]} = 8\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e如果使用贪心法, 每次都在可选范围内取最大值, 那么就会得到\u003ccode\u003e{a[2], a[0]} = 6\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e如果使用分而治之法, 把数组分为两半\u003ccode\u003ea1 = [1, 4], a2 = [5, 4]\u003c/code\u003e, 则分别得到MIS\u003ccode\u003e{a1[1]}, {a2[0]}\u003c/code\u003e, 合并后发现是相邻的, 与要求相悖.\u003c/p\u003e\n\u003cp\u003e要解决这个问题，关键的步骤是找到\u003cstrong\u003e基于子问题最优解的最优解\u003c/strong\u003e：想办法把缩小最优解备选方案的数量，在这个较小的空间中可以直接采取暴力搜索寻找最优解。\u003c/p\u003e\n\u003cp\u003e对于\u003ccode\u003ea = [1, 4, 5, 4]\u003c/code\u003e, 假设其MIS为\u003ccode\u003eS\u003c/code\u003e, 假如从最右边的元素开始考虑, \u003ccode\u003ea[3] = 4\u003c/code\u003e只有属于\u003ccode\u003eS\u003c/code\u003e和不属于\u003ccode\u003eS\u003c/code\u003e两种情况\u003c/p\u003e","title":"Dynamic Programming 01 - 理解动态规划"},{"content":"一些常规的操作， 参考这个视频。\n基本位操作 把某一位变为1：\ndef set_bit(x, position): mask = 1 \u0026lt;\u0026lt; position return x | mask bin(set_bit(0b110, 0b101)) 输出0b100110. 因为x = 0b110 = 6, 翻转第五位，就用position = 0b101 = 5， 得到mask = 0b00100000, 用|把第五位变为1.\n清除某一位（1变0)：\ndef clear_bit(x, position): mask = 1 \u0026lt;\u0026lt; position return x \u0026amp; ~mask 通过XOR^和1来翻转某一位：\ndef flip_bit(x, position): mask = 1 \u0026lt;\u0026lt; position return x ^ mask 通过\u0026amp;1可以作为取位操作, 来判断某一位是否是1:\ndef is_bit_set(x, position): shifted = x \u0026gt;\u0026gt; position return shifted \u0026amp; 1 0b1100110 \u0026gt;\u0026gt; 0b101 = 0b11, 0b11 \u0026amp; 0b01 = 1\n根据参数state来控制修改某一位, 如果参数是1那么就是set, 如果是0那么就是clear:\ndef modify_bit(x, position, state): mask = 1 \u0026lt;\u0026lt; position return (x \u0026amp; ~mask) | (-state \u0026amp; mask) 如果state = 0b1, -state = 0b11111111 如果state = 0b0, -state = 0b0\n","permalink":"https://congchan.github.io/posts/%E4%BD%8D%E6%93%8D%E4%BD%9C-%E5%9F%BA%E7%A1%80%E7%9A%84%E4%BD%8D%E8%BF%90%E7%AE%97/","summary":"\u003cp\u003e一些常规的操作， 参考\u003ca href=\"https://www.youtube.com/watch?v=7jkIUgLC29I\"\u003e这个视频\u003c/a\u003e。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"基本位操作\"\u003e基本位操作\u003c/h2\u003e\n\u003cp\u003e把某一位变为\u003ccode\u003e1\u003c/code\u003e：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eset_bit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emask\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ebin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eset_bit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mb\"\u003e0b110\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mb\"\u003e0b101\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e输出\u003ccode\u003e0b100110\u003c/code\u003e. 因为\u003ccode\u003ex = 0b110 = 6\u003c/code\u003e, 翻转第五位，就用\u003ccode\u003eposition = 0b101 = 5\u003c/code\u003e， 得到\u003ccode\u003emask = 0b00100000\u003c/code\u003e, 用\u003ccode\u003e|\u003c/code\u003e把第五位变为\u003ccode\u003e1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e清除某一位（\u003ccode\u003e1\u003c/code\u003e变\u003ccode\u003e0\u003c/code\u003e)：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eclear_bit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emask\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e \u003cspan class=\"o\"\u003e~\u003c/span\u003e\u003cspan class=\"n\"\u003emask\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e通过XOR\u003ccode\u003e^\u003c/code\u003e和\u003ccode\u003e1\u003c/code\u003e来翻转某一位：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eflip_bit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emask\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e^\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e通过\u003ccode\u003e\u0026amp;1\u003c/code\u003e可以作为取位操作, 来判断某一位是否是\u003ccode\u003e1\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eis_bit_set\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eshifted\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eshifted\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003e0b1100110 \u0026gt;\u0026gt; 0b101\u003c/code\u003e = \u003ccode\u003e0b11\u003c/code\u003e, \u003ccode\u003e0b11 \u0026amp; 0b01\u003c/code\u003e = \u003ccode\u003e1\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e根据参数\u003ccode\u003estate\u003c/code\u003e来控制修改某一位, 如果参数是\u003ccode\u003e1\u003c/code\u003e那么就是set, 如果是\u003ccode\u003e0\u003c/code\u003e那么就是clear:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emodify_bit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emask\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003eposition\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e \u003cspan class=\"o\"\u003e~\u003c/span\u003e\u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003estate\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e如果\u003ccode\u003estate = 0b1\u003c/code\u003e, \u003ccode\u003e-state = 0b11111111\u003c/code\u003e\n如果\u003ccode\u003estate = 0b0\u003c/code\u003e, \u003ccode\u003e-state = 0b0\u003c/code\u003e\u003c/p\u003e","title":"位操作 - 基础的位运算"},{"content":"在很多语言中，字符char类型是八位, 那么可能取值有256种(-128 ~ -1, 0 ~ 127). 但是用二进制表示为0000 0000 ~ 1111 1111, 无符号整数的全部位都表示数值，而有符号数的最高位是符号位（0表示正数，1表示负数），所以实际表达数值的只剩下n-1位。这样理论上char的取值应该是1111 1111 = -127到0111 1111 = 127. 而-128 = 1 1000 0000需要9位来表达, 所以char是如何仅仅通过八位表达-128?\n首先, 因为计算机只能做加法, 所以减法操作要转化为加法, 尝试将符号位参与运算, 1-1就转化为1 + (-1), 用二进制表达为0000 0001 + 1000 0001 = -2, 很明显是错的. 如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的. 这也就是为何计算机内部不使用原码表示一个数.\n为了避免这种错误, 引入反码(正数的反码是其本身, 负数的反码是符号位不变, 其余位取反), 用-1的原码1000 0001的反码1111 1110来表达-1, 这样1 + (-1) = [0000 0001]反 + [1111 1110]反 = [1111 1111]反, 转为原码1000 0000 = -0. 发现用反码计算减法, 结果的真值部分是正确的.\n二进制补码 为了彻底解决负数表达中的-0 = 0, 1000 0000 = 0000 0000问题, 引入补码(正数的补码是其本身，负数的补码为其反码加1). 补码是计算机中最常用的有符号数的表达形式。补码定义为最高位权重为负的二进制码。 $$ B2T_4(0001) = -0 + 0 + 0 + 1 = 1 $$ $$ B2T_4(1111) = -8 + 4 + 2 + 1 = -1 $$ $$ B2T_4(0101) = -0 + 4 + 0 + 1 = 5 $$ $$ B2T_4(1011) = -8 + 0 + 2 + 1 = -5 $$0 1 1 1 1 1 1 1 = 127 0 0 0 0 0 0 1 0 = 2 0 0 0 0 0 0 0 1 = 1 0 0 0 0 0 0 0 0 = 0 1 1 1 1 1 1 1 1 = −128 + 127 = -1 1 1 1 1 1 1 1 0 = −2 1 0 0 0 0 0 0 1 = −128 + 1 = -127 1 0 0 0 0 0 0 0 = −128\n这样减法操作转化为加补码1+(-1) = [0000 0001]补 + [1111 1111]补 = [1 0000 0000]补, char定义为8位, 故第九位舍弃, 得到[0000 0000]补, 转换为原码为[1000 0000]原 = -0.\n所以, -0 = [1000 0000]原的补码为1 0000 0000 = 0000 0000 = 0, 0 = 0000 0000的补码为1000 0000 = -0. 鉴于0的非负非正特性, 仅仅使用0000 0000来表达0和其补码就足够了. 这样1000 0000就可以挪作他用,用来表达-128. 之所以用来表达-128是有其合理性的. 因为-1 + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补, -128 = 1 1000 0000的补码也刚好是1 1000 0000, 放进char的八位空间中, 需要把第九位截断, 得到的刚好是1000 0000.\n可以推理出补码的取值范围，最小值为[10...0], $TMin_w = -2^{w-1}$, 最大值为[01...1], $TMax_w = 2^{w-1} - 1$. 使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127]. 同理16位short中-32768~32767，-32768原码为17位，丢弃最高位剩下的16和-0的原码相同。\n可以观察到, abs(TMin) = TMax + 1 = TMin, 也就是最小值的绝对值还是最小值: -(INT_MIN) = - 0[1000 0000]补 = 1[1000 0000]补 = [1000 0000]补 = -128 = [0111 1111]补 + 1.\n二进制补码运算是加法逆运算。补码系统的最大优点是可以在加法或减法处理中，不需因为数字的正负而使用不同的计算方式。只要一种加法电路就可以处理各种有号数加法，而且减法可以用一个数加上另一个数的补码来表示，因此只要有加法电路及补码电路即可完成各种有号数加法及减法。\nBitwise operators Bitwise NOT - ~, or complement: 0 become 1, 1 become 0.\nThe bitwise complement is equal to the two\u0026rsquo;s complement of the value minus one. ~x = -x − 1\n因为~x + x = b[1...1] = -1, 通过~x + 1得到一个数的负数, 得到的负数也就是该二进制数字的补码. 比如0 = b[0000], ~0 = b[1111],b[1111] + b[0001] = b[0000] = 0, 又比如TMin = b[1000], ~TMin = b[0111], ~TMin + 1 = b[0111] + b[0001] = b[1000] = TMin, 也就是-TMin = TMin.\nBitwise AND - \u0026amp;: 1 \u0026amp; 1 = 1, 1 \u0026amp; 0 = 0, 0 \u0026amp; 0 = 0.\nperforms the logical AND operation on each pair of the corresponding bits, which is equivalent to multiplying them. Thus, if both bits in the compared position are 1, the bit in the resulting binary representation is 1 (1 × 1 = 1); otherwise, the result is 0 (1 × 0 = 0 and 0 × 0 = 0).\nBitwise OR - |: 1 | 1 = 1, 1 | 0 = 1, 0 | 0 = 0\ntakes two bit patterns of equal length and performs the logical inclusive OR operation on each pair of corresponding bits. The result in each position is 0 if both bits are 0, while otherwise the result is 1.\nBitwise XOR - ^: 1 ^ 1 = 0, 1 ^ 0 = 1, 0 ^ 0 = 0\ntakes two bit patterns of equal length and performs the logical exclusive OR operation on each pair of corresponding bits. The result in each position is 1 if only the first bit is 1 or only the second bit is 1, but will be 0 if both are 0 or both are 1. In this we perform the comparison of two bits, being 1 if the two bits are different, and 0 if they are the same.\nBit shifts Java中有三种移位运算符.\n\u0026lt;\u0026lt;: 左移运算符，num \u0026lt;\u0026lt; n, 把num的二进制左移n位, 右边的空位用0补上, 每一次移位相当于num * 2(除非overflow) \u0026gt;\u0026gt;: 右移运算符，num \u0026gt;\u0026gt; n, 右移n, 每次移位相当于num / 2 如果是无符号数值，也就是\u0026gt;\u0026gt;\u0026gt;，左边空缺用0补上, 如果是有符号数值，则用数字的符号位填补最左边的n位 如果是正数, 则右移后在最左边补n个0 如果原先是负数, 则右移后在最左边补n个1 ","permalink":"https://congchan.github.io/posts/%E4%BD%8D%E6%93%8D%E4%BD%9C-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%93%8D%E4%BD%9C%E7%AC%A6/","summary":"\u003cp\u003e在很多语言中，字符\u003ccode\u003echar\u003c/code\u003e类型是八位, 那么可能取值有256种(\u003ccode\u003e-128 ~ -1, 0 ~ 127\u003c/code\u003e). 但是用二进制表示为\u003ccode\u003e0000 0000 ~ 1111 1111\u003c/code\u003e, 无符号整数的全部位都表示数值，而有符号数的最高位是符号位（0表示正数，1表示负数），所以实际表达数值的只剩下\u003ccode\u003en-1\u003c/code\u003e位。这样理论上\u003ccode\u003echar\u003c/code\u003e的取值应该是\u003ccode\u003e1111 1111 = -127\u003c/code\u003e到\u003ccode\u003e0111 1111 = 127\u003c/code\u003e. 而\u003ccode\u003e-128 = 1 1000 0000\u003c/code\u003e需要9位来表达, 所以\u003ccode\u003echar\u003c/code\u003e是如何仅仅通过八位表达\u003ccode\u003e-128\u003c/code\u003e?\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e首先, 因为计算机只能做加法, 所以减法操作要转化为加法, 尝试将符号位参与运算, \u003ccode\u003e1-1\u003c/code\u003e就转化为\u003ccode\u003e1 + (-1)\u003c/code\u003e, 用二进制表达为\u003ccode\u003e0000 0001 + 1000 0001 = -2\u003c/code\u003e, 很明显是错的. 如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的. 这也就是为何计算机内部不使用原码表示一个数.\u003c/p\u003e\n\u003cp\u003e为了避免这种错误, 引入\u003cstrong\u003e反码\u003c/strong\u003e(正数的反码是其本身, 负数的反码是符号位不变, 其余位取反), 用\u003ccode\u003e-1\u003c/code\u003e的原码\u003ccode\u003e1000 0001\u003c/code\u003e的反码\u003ccode\u003e1111 1110\u003c/code\u003e来表达\u003ccode\u003e-1\u003c/code\u003e, 这样\u003ccode\u003e1 + (-1) = [0000 0001]反 + [1111 1110]反 = [1111 1111]反\u003c/code\u003e, 转为原码\u003ccode\u003e1000 0000 = -0\u003c/code\u003e. 发现用反码计算减法, 结果的真值部分是正确的.\u003c/p\u003e","title":"位操作 - 二进制操作符"},{"content":"Memory Bit. 0 or 1. Byte. 8 bits. Megabyte (MB). 1 million or $2^{20}$ bytes. Gigabyte (GB). 1 billion or $2^{30}$ bytes. 64-bit machine. We assume a 64-bit machine with 8 byte pointers (References). ・Can address more memory. ・Pointers use more space (some JVMs \u0026ldquo;compress\u0026rdquo; ordinary object pointers to 4 bytes to avoid this cost).\nTypical memory usage for primitive types and arrays primitive types (bytes): boolean 1 byte 1 char 2 int 4 float 4 long 8 double 8\nfor one-dimensional arrays (bytes): char[] 2N + 24 int[] 4N + 24 double[] 8N + 24\nTypical memory usage for objects in Java Object overhead. 16 bytes. Reference. 8 bytes. Padding. Each object uses a multiple of 8 bytes. ","permalink":"https://congchan.github.io/posts/algorithms-03-memory-%E5%86%85%E5%AD%98/","summary":"\u003ch2 id=\"memory\"\u003eMemory\u003c/h2\u003e\n\u003cp\u003eBit. 0 or 1.\nByte. 8 bits.\nMegabyte (MB). 1 million or $2^{20}$ bytes.\nGigabyte (GB). 1 billion or $2^{30}$ bytes.\n64-bit machine. We assume a 64-bit machine with 8 byte pointers (References).\n・Can address more memory.\n・Pointers use more space (some JVMs \u0026ldquo;compress\u0026rdquo; ordinary object pointers to 4 bytes to avoid this cost).\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"typical-memory-usage-for-primitive-types-and-arrays\"\u003eTypical memory usage for primitive types and arrays\u003c/h3\u003e\n\u003cp\u003eprimitive types (bytes):\n\u003ccode\u003eboolean\u003c/code\u003e 1\n\u003ccode\u003ebyte\u003c/code\u003e 1\n\u003ccode\u003echar\u003c/code\u003e 2\n\u003ccode\u003eint\u003c/code\u003e 4\n\u003ccode\u003efloat\u003c/code\u003e 4\n\u003ccode\u003elong\u003c/code\u003e 8\n\u003ccode\u003edouble\u003c/code\u003e 8\u003c/p\u003e","title":"Algorithms 03 - Memory 内存"},{"content":"假如有两种交税方式：\n每天付 3 金币 每次付的金币呈指数级增长，但通知付款频率呈指数级下降 第1天：付 1 第2天：付 2 (累计 3) 第4天：付 4 (累积 7) 第8天：付 8 (累积 15) 哪种付的钱比较少？ 第二种比较划算，本质上等同于每天付 2，就是amortized constant。\nA more rigorous examination of amortized analysis is done here, in three steps:\nPick a cost model (like in regular runtime analysis) Compute the average cost of the i\u0026rsquo;th operation Show that this average (amortized) cost is bounded by a constant. 类似的应用在Array list 扩容中提到的 geometric resizing 方法(实际也是Python list 使用的方法)有体现, 所以使用一个因数来扩容数组, 可以让 ArrayList 的 add操作变为 amortized constant time.\n总结\nAmortized analysis provides a way to prove the average cost of operations. If we chose $a_i$ such that $\\Phi_i$ is never negative and $a_i$ is constant for all $i$, then the amortized cost is an upper bound on the true cost. \u0026ndash; from: https://joshhug.gitbooks.io/ ","permalink":"https://congchan.github.io/posts/algorithms-02-amortized-analysis-%E5%B9%B3%E6%91%8A%E5%88%86%E6%9E%90/","summary":"\u003cp\u003e假如有两种交税方式：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e每天付 3 金币\u003c/li\u003e\n\u003cli\u003e每次付的金币呈指数级增长，但通知付款频率呈指数级下降\n\u003cul\u003e\n\u003cli\u003e第1天：付 1\u003c/li\u003e\n\u003cli\u003e第2天：付 2 (累计 3)\u003c/li\u003e\n\u003cli\u003e第4天：付 4 (累积 7)\u003c/li\u003e\n\u003cli\u003e第8天：付 8 (累积 15)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e哪种付的钱比较少？\n第二种比较划算，本质上等同于每天付 2，就是\u003cstrong\u003eamortized constant\u003c/strong\u003e。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eA more rigorous examination of amortized analysis is done here, in three steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ePick a cost model (like in regular runtime analysis)\u003c/li\u003e\n\u003cli\u003eCompute the average cost of the i\u0026rsquo;th operation\u003c/li\u003e\n\u003cli\u003eShow that this average (amortized) cost is bounded by a constant.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e类似的应用在\u003ca href=\"/NOTE-CS61B-data-structures-07-java-array-based-list#%E6%95%B0%E7%BB%84%E6%89%A9%E5%AE%B9\"\u003eArray list 扩容\u003c/a\u003e中提到的 geometric resizing 方法(实际也是Python list 使用的方法)有体现, 所以使用一个因数来扩容数组, 可以让 ArrayList 的 \u003ccode\u003eadd\u003c/code\u003e操作变为 amortized constant time.\u003c/p\u003e","title":"Algorithms 02 - Amortized Analysis 平摊分析"},{"content":"Resource and Reference:\nCS61B Berkeley - Josh Hug Algorithms Princeton - ROBERT SEDGEWICK, KEVIN WAYNE 效率来源两个方面:\n编程成本: 开发程序需要多长时间？代码是否容易阅读，修改和维护（大部分成本来自维护和可扩展性）？ 运行成本: 程序需要多长时间运行 (Time complexity)？ 需要多少内存 (Space complexity)？ Asymptotic Analysis Care about what happens for very large N (asymptotic behavior). We want to consider what types of algorithms would best handle scalability - Algorithms that scale well have better asymptotic runtime behavior.\nSimplification Summary\nOnly consider the worst case. Pick a representative operation (aka: cost model) Ignore lower order terms Ignore multiplicative constants. Simplified Analysis Process\nChoose cost model (representative operation) Figure out the order of growth for the count of representative operation by either: Making an exact count, and discarding unnecessary pieces Only consider the worst case. Ignore lower order terms Ignore constants. Or, using intuition/inspection to determine orders of growth. Big Theta Formalizing Order of Growth: Suppose a function $R(N)$ with order of growth $f(N)$, this is represented as $R(N) \\in \\Theta(f(N))$ in notation. Means that there exists positive constants $k_1, k_2$ such that: $$k_1 ⋅ f(N) ≤ R(N) ≤ k_​2 ⋅ f(N),$$ for all values of $N$ greater than some $N_0$(a very large N).\nProcedure:\nGiven a piece of code, express its runtime as a function $R(N)$ $N$ is some property of the input of the function. Oftentimes, $N$ represents the size of the input Rather than finding $R(N)$ exactly, instead care about the order of growth of $R(N)$. One approach (not universal): Choose a representative operation Let $C(N)$ = count of how many times that operation occurs, as a function of $N$. Determine order of growth $C(N) \\in \\Theta(f(N))$ Often (but not always) consider the worst case count. If operation takes constant time, then $R(N) \\in \\Theta(f(N))$ 在 Big Theta 的范畴内，对于涉及 logarithm 的情况，底数并不重要，任何底数都是等价的： Binary search: $\\Theta(\\log N)$ 直接忽略底数符号。 Selection sort: $\\Theta(N^2)$ Merge two sorted array (Merge Sort): $\\Theta(N)$\n用 merge sort 加速 selection sort - 把 selection sort 递归地平分, 总共能分解出$\\log_2N$个 merge sorts, 伪代码:\nIf the list is size 1: return else: Mergesort the left half Mergesort the right half Merge the results Total runtime is $≈Nk$, where $k = \\log_2(N)$ is the number of levels, overall runtime is $\\Theta(N \\log N)$. $N^2$ vs. $N \\log N$ is an enormous difference. Going from $N\\log N$ to $N$ is nice, but not a radical change.\nUseful math: $1 + 2 + 3 + ... + N = N * (N + 1) / 2 = \\Theta(N^2)$ $1 + 2 + 4 + ... + N = 2N - 1 = \\Theta(N)$\nTo estimate a discrete sum, replace the sum with an integral, and use calculus: $1 + 2 + 3 + ... + N = \\sum_{i=1}^{N} i \\sim \\int_{x=1}^N x dx \\sim \\frac{1}{2}(N^2)$\n$1^k + 2^k + ... + N^k = \\sum_{i=1}^{N} i^k \\sim \\int_{x=1}^N x^k dx \\sim \\frac{1}{k+1}(N^{k+1})$\n$1 + 1/2 + 1/3 + … + 1/N = \\sum_{i=1}^{N} i^{-1} \\sim \\int_{x=1}^N x^{-1} dx \\sim \\ln N$\n3-sum triple loop, $\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\sum_{k=1}^{N} 1 \\sim \\int_{x=1}^N\\int_{y=x}^N\\int_{z=y}^N dz dy dx \\sim \\frac{1}{6}N^3$\nBig O Big Theta expresses the exact order of as a function of the input size. However, if the runtime depends on more than just the size of the input, then we must qualify our statements into different cases before using Big Theta.\nBig O: $R(N) \\in O(f(N))$, means that there exists positive constants $k_2$, such that: $R(N) \\leq k_2 \\cdot f(N)$ for all values of $N$ greater than some $N_0$(a very large $N$). This is a looser condition than Big Theta since Big O does not care about the lower bound, thus it is less informative than Big Theta.\nTo summarize the usefulness of Big O:\nIt allows us to make simple statements without case qualifications, in cases where the runtime is different for different inputs. Sometimes, for particularly tricky problems, we (the computer science community) don\u0026rsquo;t know the exact runtime, so we may only state an upper bound. It\u0026rsquo;s a lot easier to write proofs for Big O than Big Theta, like we saw in finding the runtime of mergesort in the previous chapter. This is beyond the scope of this course. 类似的也可以定义一个下限概念 - Big Omega ($\\Omega$)， 一般用于表明一个问题的难度有多大。\nSummary Big O is an upper bound (\u0026ldquo;less than or equals\u0026rdquo;) Big Omega is a lower bound (\u0026ldquo;greater than or equals\u0026rdquo;) Big Theta is both an upper and lower bound (\u0026ldquo;equals\u0026rdquo;) Big O does NOT mean \u0026ldquo;worst case\u0026rdquo;. We can still describe worst cases using Big Theta Big Omega does NOT mean \u0026ldquo;best case\u0026rdquo;. We can still describe best cases using Big Theta Big O is sometimes colloquially used in cases where Big Theta would provide a more precise statement \u0026ndash; from: https://joshhug.gitbooks.io/ ","permalink":"https://congchan.github.io/posts/algorithms-01-asymptotic-analysis-%E6%B8%90%E8%BF%9B%E5%88%86%E6%9E%90/","summary":"\u003cp\u003eResource and Reference:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCS61B Berkeley - Josh Hug\u003c/li\u003e\n\u003cli\u003eAlgorithms Princeton - ROBERT SEDGEWICK, KEVIN WAYNE\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e效率来源两个方面:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e编程成本: 开发程序需要多长时间？代码是否容易阅读，修改和维护（大部分成本来自维护和可扩展性）？\u003c/li\u003e\n\u003cli\u003e运行成本: 程序需要多长时间运行 (Time complexity)？ 需要多少内存 (Space complexity)？\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"asymptotic-analysis\"\u003eAsymptotic Analysis\u003c/h2\u003e\n\u003cp\u003eCare about what happens for very large \u003ccode\u003eN\u003c/code\u003e (asymptotic behavior). We want to consider what types of algorithms would best handle scalability - Algorithms that scale well have better asymptotic runtime behavior.\u003c/p\u003e\n\u003cp\u003eSimplification Summary\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eOnly consider the worst case.\u003c/li\u003e\n\u003cli\u003ePick a representative operation (aka: cost model)\u003c/li\u003e\n\u003cli\u003eIgnore lower order terms\u003c/li\u003e\n\u003cli\u003eIgnore multiplicative constants.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSimplified Analysis Process\u003c/p\u003e","title":"Algorithms 01 - Asymptotic Analysis 渐进分析"},{"content":"Stanford CS106B Programming Abstractions 和 CS106A 的学习笔记. 课程作业(cs106b spring 2017)实现代码见 https://github.com/ShootingSpace/cs106b-programming-abstraction\nTopics: A: Intro (by Java) B: Recursion, algorithms analysis (sort/search/hash), dynamic data structures (lists, trees, heaps), data abstraction (stacks, queues, maps), implementation strategies/tradeoffs\nPurposes become acquainted with the C++ programming language learn more advanced programming techniques explore classic data structures and algorithms and apply these tools to solving complex problems Reference Text Book: Data Structures \u0026amp; Algorithm Analysis in C++, 4th ed, by Mark A. Weiss Text Book: Programming Abstractions in C++ 1st Edition by Eric Roberts Text Book: Algorithms, 4th Edition Blog: Red Blob Games, Amit’s A* Pages Coding style Why writing clean, well-structured code\nThe messy code trap harder to build and debug harder to clean up Decomposition Decompose the problem, not the programs, program is written from the already decomposed framework. Logical and readable Methods should be short and to the point, Strive to design methods that are general enough for a variety of situations and achieve specifics trough use of parameters. Avoid redundants methods. Avoid repeated lines or methods. Readable code: Writing readable code not only help future readers but also help avoid your own bugs: Because bugs are codes that fail to expresses idea in mind. Reader can see the algorithmic idea when sweeping the code. Works correctly in all situations: Using a listing of specific test cases to exercise the program on.\nThe overall approach is straight-forward, data structure is cleanly organized, tasks are nicely decomposed, algorithms are clear and easy to follow, comments are helpful, layout is consistent.\nHow to write clean, well-structured code? Choosing good names for variables Name reflect what it stores, normally nouns; In Java, conventionly, begin variables with the first word lowercase, and upper case later words bestScore Widely used idiomatic one-letter names: i, j, k for int loop counters; x, y, z for coordinates. Choosing good names for methods Name reflect the action they perform, verbs normally; The prefixes get and set have a typical role: A get method gets a piece of information from an object; set methods are used to pass a value in to an object Returning a boolean are ofter named starting with is or has. Using whitespace to separate logical parts: Put in blank lines to separate the code into its natural sub sections that accomplis logical sub-parts of the whole algoriithm. Each little section might have a comment to describe what it accomplishes. Use Indentation to show hierarchy structure Comments Attributions: consider as an important tennet of academic integrity. Comments Examples of information you might include in comments:\nGeneral overview. What are the goals and requirements of this program? this function? The overview comment should also contain author and version information: who worked on this file and when. Data structures. How is the data stored? How is it ordered, searched, accessed? Design decisions. Why was a particular data structure or algorithm chosen? What other strategies were tried and rejected? Error handling. How are error conditions handled? What assumptions are made? What happens if those assumptions are violated? Nitty-gritty code details. Comments are invaluable for explaining the inner workings of particularly complicated (often labeled \u0026ldquo;clever\u0026rdquo;) paths of the code. Planning for the future. How might one make modifications or extensions later? And more\u0026hellip; (This list is by no means exhaustive) ADT An abstract data type is a set of objects together with a set of operations. Abstract data types are mathematical abstractions; nowhere in an ADT’s definition is there any mention of how the set of operations is implemented. Objects such as lists, sets, and graphs, along with their operations, can be viewed as ADTs. Also there are search tree, set, hash table, priority queue.\nClient uses class as abstraction Invokes public operations only Internal implementation not relevant! Client can\u0026rsquo;t and shouldn\u0026rsquo;t muck with internals: Class data should private Imagine a \u0026ldquo;wall\u0026rdquo; between client and implementor Wall prevents either from getting involved in other\u0026rsquo;s business Interface is the \u0026ldquo;chink\u0026rdquo; in the wall Conduit allows controlled access between the two Consider Lexicon Abstraction is a word list, operations to verify word/prefix How does it store list? using array? vector? set? does it matter to client? Why ADTs?\nAbstraction: Client insulated from details, works at higher-level Encapsulation: Internals private to ADT, not accessible by client Independence: Separate tasks for each side (once agreed on interface) Flexibility: ADT implementation can be changed without affecting client The C++ language includes, in its library, an implementation of common data structures. This part of the language is popularly known as the Standard Template Library (STL). In general, these data structures are called collections or containers.\nIterators In the STL, a position is represented by a nested type, iterator.\nGetting an Iterator\niterator begin( ) returns an appropriate iterator representing the first item in the container. iterator end( ) returns an appropriate iterator representing the endmarker in the container (i.e., the position after the last item in the container). Iterator Methods * itr++ and ++itr advances the iterator itr to the next location. Both the prefix and postfix forms are allowable. * itr returns a reference to the object stored at iterator itr’s location. The reference returned may or may not be modifiable (we discuss these details shortly). * itr1==itr2 / itr1!=itr2, returns true if iterators itr1 and itr2 refer to the same / different location and false otherwise.\nContainer Operations that require Iterators. The three most popular methods that require iterators are those that add or remove from the list (either a vector or list) at a specified position:\niterator insert( iterator pos, const Object \u0026amp; x ): adds x into the list, prior to the position given by the iterator pos. This is a constant-time operation for list, but not for vector. The return value is an iterator representing the position of the inserted item. iterator erase( iterator pos ): removes the object at the position given by the iterator. This is a constant-time operation for list, but not for vector. The return value is the position of the element that followed pos prior to the call. This operation invalidates pos, which is now stale, since the container item it was viewing has been removed. iterator erase( iterator start, iterator end ): removes all items beginning at position start, up to, but not including end. Observe that the entire list can be erased by the call c.erase( c.begin( ), c.end( ) ) Range for loop: C++11 also allows the use of the reserved word auto to signify that the compiler will automatically infer the appropriate type, for simple data type: for( auto x : squares ) cout\u0026lt;\u0026lt; x; for complicate data type like map: Each element of the container is a map\u0026lt;K, V\u0026gt;::value_type, which is a typedef for std::pair\u0026lt;const K, V\u0026gt;. Consequently, you\u0026rsquo;d write this as for (auto\u0026amp; kv : myMap) { std::cout \u0026lt;\u0026lt; kv.first \u0026lt;\u0026lt; \u0026#34; has value \u0026#34; \u0026lt;\u0026lt; kv.second \u0026lt;\u0026lt; std::endl; } Recursion Helper Function\nNo clear definition of helper function How to utilize helper function to help constructing recursion algarithm: construct a same-name recursive function with extra parameters to pass in. In some other cases, decomposition with several step into a function is itself a helper function, which help to make the main function simple and clean. Exhaustive recursion Permutations/subsets are about choice\nBoth have deep/wide tree of recursive calls Depth represents total number of decisions made Width of branching represents number of available options per decision Explores every possible option at every decision point, typically very expensive, N! permutations, 2N subsets Recursive Backtracking Partial exploration of exhaustive space. In the case that if we are interested in finding any solution, whichever one that works out first is fine. If we eventually reach our goal from here, we have no need to consider the paths not taken. However, if this choice didn\u0026rsquo;t work out and eventually leads to nothing but dead ends; when we backtrack to this decision point, we try one of the other alternatives.\nThe back track based on the stacks of recursion, if a stack return false (or fail result), we back to previous stack and try another way(un-making choice). Need something return(normally bool) to step out of the entire recursion once any one solution found. One great tip for writing a backtracking function is to abstract away the details of managing the configuration (what choices are available, making a choice, checking for success, etc.) into other helper functions so that the body of the recursion itself is as clean as can be. This helps to make sure you have the heart of the algorithm correct and allows the other pieces to be developed, test, and debugged independently. Pointer lvalue: In C++, any expression that refers to an internal memory location capable of storing data is called an lvalue (pronounced “ell-value”). x = 1.0;\nDeclaring pointer variables\nint main() { -------------------------------------------------- // Declaration, in the stack // Not yet initialized! int num; int *p, *q; // If cout \u0026lt;\u0026lt; num \u0026lt;\u0026lt; p \u0026lt;\u0026lt; q \u0026lt;\u0026lt; endl; // There will be junk number, junk address. // If now *p=10, it may blow up, because what *p point to is an address points to somewhere around that could be invalid. --------------------------------------------------- // new operator allocate memory from the heap, returns address p = new int; // P -----\u0026gt; [ int ] （heep 1000） *p = 10; // P -----\u0026gt; [ 10 ] （heep 1000） q = new int; // P -----\u0026gt; [ int ] （heep 1004） *q = *p; // q -----\u0026gt; [ 10 ] （heep 1004） q = p; // q -----\u0026gt; [ 10 ] （heep 1000） // [ 10 ] （heep 1004） became orphan, and could not be reclaim back --------------------------------------------------- delete p; // [ 10 ] （heep 1000）memory was reclaimed and free, // and available for others as [ ]（heep 1000）, // but p still hold the address delete q; // bad idea, [ 10 ]（heep 1000） already been reclaimed! q = NULL; // NULL is zero pointer, means the pointer does not hold any address, // used as sentinel value, sometimes better than delete. // Accessing \u0026#34;deleted\u0026#34; memory has unpredictable consequences --------------------------------------------------- // int *p declaration reserves only a single word, which is large enough to hold a machine address. // ≠ // int *p = NULL declare pointer p as nullptr --------------------------------------------------- (*newOne).name = name // \u0026#34;.\u0026#34; \u0026gt; \u0026#34;*\u0026#34; newOne-\u0026gt;name = name Use of pointer Big program that contains a certain amout of classes and objects that are share some relationship. Instead of copying data from each other, using pointer to point to specific data is better:\nSaves space by not repeating the same information. If some objects gets new information to update, change in one place only! Dynamic allocation Request memory: To acquire new memory when you need it and to free it explicitly when it is no longer needed. Acquiring new storage when the program is running. While the program is running, you can reserve part of the unallocated memory, leaving the rest for subsequent allocations. The pool of unallocated memory available to a program is called the heap. int *p = new int; //new operator to allocate memory from the heap In its simplest form, the new operator takes a type and allocates space for a variable of that type located in the heap. The call to new operator will return the address of a storage location in the heap that has been set aside to hold an integer.\nFree occupied memory: Delete which takes a pointer previously allocated by new and returns the memory associated with that pointer to the heap.\nTree Node, tree, subtree, parent, child, root, edge, leaf For any node ni, the depth of ni is the length of the unique path from the root to ni. The height of ni is the length of the longest path from ni to a leaf Rules for all trees Recursive branching structure Single root node Every node reachable from root by unique path Binary tree Each node has at most 2 children.\nBinary search tree\nAll nodes in left subtree are less than root, all nodes in right subtree are greater. Arranged for efficient search/insert. It is the basis for the implementation of two library collections classes, set and map. Most operations\u0026rsquo; average running time is O(log N). Operating on trees Many tree algorithms are recursive\nHandle current node, recur on subtrees Base case is empty tree (NULL) Tree traversals to visit all nodes, order of traversal:\nPre: cur, left, right In: left, cur, right Post: left, right, cur Others: level-by-level, reverse orders, etc Balanced Search Trees Binary search tree have poor worst-case performance.\nTo make costs are guaranteed to be logarithmic, no matter what sequence of keys is used to construct them, the ideal is to keep binary search trees perfectly balanced. Unfortunately, maintaining perfect balance for dynamic insertions is too expensive. So consider data structure that slightly relaxes the perfect balance requirement to provide guaranteed logarithmic performance not just for the insert and search operations, but also for all of the ordered operations (except range search).\nAVL tree Adelson-Velskii and Landis tree is a binary search tree with a balance condition.\nTrack balance factor for each node: Height of right subtree - height of left subtree information is kept for each node (in the node structure) For every node in the tree, the height of the left and right subtrees can differ by at most 1 (Balance factor = 0 or 1). When balance factor hits 2, restructure Rotation moves nodes from heavy to light side Local rearrangement around specific node When finished, node has 0 balance factor Single rotation: one time rotation between new insert node and its parent node Double rotation: two single rotation of the new insert node 2-3 trees Allow the nodes in the tree to hold more than one key: 3-nodes, which hold three links and two keys.\nA 2-3 search tree is a tree that is either empty or\nA 2-node, with one key (and associated value) and two links, a left link to a 2-3 search tree with smaller keys, and a right link to a 2-3 search tree with larger keys A 3-node, with two keys (and associated values) and three links, a left link to a 2-3 search tree with smaller keys, a middle link to a 2-3 search tree with keys between the node’s keys, and a right link to a 2-3 search tree with larger keys A perfectly balanced 2-3 search tree is one whose null links are all the same distance from the root. The concept guarantee that search and insert operations in a 2-3 tree with N keys are to visit at most lg N nodes.\nBut its dicrect implementation is inconvenient: Not only is there a substantial amount of code involved, but the overhead incurred could make the algorithms slower than standard BST search and insert. Consider a simple representation known as a red-black BST that leads to a natural implementation. Binary Heap A heap is a binary tree that is completely filled, with the possible exception of the bottom level, which is filled from left to right. Such a tree is known as a complete binary tree.\nA heap data structure consist of an array (of Comparable objects) and an integer representing the current heap size. For any element in array position i, the left child is in position 2i, the right child is in the cell after the left child [2i + 1], and the parent is in position [i/2]. Heap-Order Property: For every node X, the key in the parent of X is smaller than (or equal to) the key in X. So to make find minimum operation quick. Basic Heap Operation\ninsert: To insert an element X into the heap, create a hole in the next available location. Then Percolate up - swap X with its parent index (i/2) so long as X has a higher priority than its parent. Continue this process until X has no more lower priority parent. //Percolate up int hole = ++size; binaryQueue[0]=std::move(*newOne); for ( ; (priority \u0026lt; binaryQueue[hole/2].priority || (priority == binaryQueue[hole/2].priority \u0026amp;\u0026amp; name \u0026lt; binaryQueue[hole/2].name) ); hole/=2) { binaryQueue[hole] = std::move(binaryQueue[hole/2]); } binaryQueue[hole] = std::move(binaryQueue[0]); deleteMin: When the minimum is removed, a hole is created at the root. Move the last element X in the heap to place in the root hole. Then Percolate down - swapp X with its more urgent-priority child [index (i2 or i2+1)] so long as it has a lower priority than its child. Repeat this step until X has no more higher priority child. //Percolate down int child; for (; hole*2\u0026lt;=size;hole=child) { child = hole*2; if ( child!=size \u0026amp;\u0026amp; (binaryQueue[child+1].priority\u0026lt;binaryQueue[child].priority || (binaryQueue[child+1].priority==binaryQueue[child].priority \u0026amp;\u0026amp; binaryQueue[child+1].name\u0026lt;binaryQueue[child].name)) ) ++child; if ( binaryQueue[child].priority\u0026lt;priority_tobePerD || (binaryQueue[child].priority==priority_tobePerD \u0026amp;\u0026amp; binaryQueue[child].name\u0026lt;name_tobePerD) ) { binaryQueue[hole] = std::move(binaryQueue[child]); } else break; } Use integer division to avoid even odd index. Priority Queues A priority queue is a data structure that allows at least the following two operations: insert, and deleteMin, which finds, returns, and removes the minimum element in the priority queue.\nAlgorithm Analysis Space/time, big-O, scalability\nBig-O Computational complexity: The relationship between N and the performance of an algorithm as N becomes large Big-O notation: to denote the computational complexity of algorithms. Standard simplifications of big-O Eliminate any term whose contribution to the total ceases to be significant as N becomes large. Eliminate any constant factors. Worst-case versus average-case complexity Average-case performance often reflects typical behavior, while worst-case performance represents a guarantee for performance on any possible input. Predicting computational complexity from code structure Constant time: Code whose execution time does not depend on the problem size is said to run in constant time, which is expressed in big-O notation as O(1). Linear time: function that are executed exactly n times, once for each cycle of the for loop, O(N) Quadratic time: Algorithms like selection sort that exhibit O(N2) performance are said to run in quadratic tim For many programs, you can determine the computational complexity simply by finding the piece of the code that is executed most often and determining how many times it runs as a function of N Space/time In general, the most important measure of performance is execution time. It also possible to apply complexity analysis to the amount of memory space required. Nowadays the memory is cheap, but it still matters when designing extreamly big programs, or APPs on small memory device, such as phones and wearable devices. Sorting There are lots of different sorting algoritms, from the simple to very complex. Some optimized for certain situations (lots of duplicates, almost sorted, etc.). So why do we need multiple algorithms?\nSelection sort Select smallest and swap to front/backend\nvoid SelectionSort(Vector\u0026lt;int\u0026gt; \u0026amp;arr) { for (int i = 0; i \u0026lt; arr.size()-1; i++) { int minIndex = i; for (int j = i+1; j \u0026lt; arr.size(); j++) { if (arr[j] \u0026lt; arr[minIndex]) minIndex = j; } Swap(arr[i], arr[minIndex]); } } Count work inside loops:\nFirst iteration does N-1 compares, second does N-2, and so on. One swap per iteration O(N2) Insertion sort As sorting hand of just-dealt cards, each subsequent element inserted into proper place\nStart with first element (already sorted) Insert next element relative to first Repeat for third, fourth, etc. Slide elements over to make space during insert void InsertionSort(Vector\u0026lt;int\u0026gt; \u0026amp;v) { for (int i = 1; i \u0026lt; v.size(); i++) { int cur = v[i]; // slide cur down into position to left for (int j=i-1; j \u0026gt;= 0 \u0026amp;\u0026amp; v[j] \u0026gt; cur; j--) v[j+1] = v[j]; v[j+1] = cur; } } Because of the nested loops, each of which can take N iterations, insertion sort is O(N2).\nHeapsort Priority queues can be used to sort in O(N log N) time. The algorithm based on this idea is known as heapsort.\nThe building of the heap, uses less than 2N comparisons. In the second phase, the ith deleteMax uses at most less than 2\\*log (N − i + 1) comparisons, for a total of at most 2N log N − O(N) comparisons (assuming N ≥ 2). Consequently, in the worst case, at most 2N log N − O(N) comparisons are used by heapsort.\nMerge sort Inspiration: Algorithm like selection sort is quadratic growth (O(N2)). Double input -\u0026gt; 4X time, halve input -\u0026gt; 1/4 time. Can recursion save the day? If there are two sorted halves, how to produce sorted full result?\nDivide and conquer algorithm\nDivide input in half Recursively sort each half Merge two halves together \u0026ldquo;Easy-split hard-join\u0026rdquo;\nNo complex decision about which goes where, just divide in middle Merge step preserves ordering from each half Merge depends on the fact that the first element in the complete ordering must be either the first element in v1 or the first element in v2, whichever is smaller.\nvoid MergeSort(Vector\u0026lt;int\u0026gt; \u0026amp;v) { if (v.size() \u0026gt; 1) { int n1 = v.size()/2; int n2 = v.size() - n1; Vector\u0026lt;int\u0026gt; left = Copy(v, 0, n1); Vector\u0026lt;int\u0026gt; right = Copy(v, n1, n2); MergeSort(left); MergeSort(right); v.clear(); Merge(v, left, right); } } void Merge(Vector\u0026lt;int\u0026gt; \u0026amp;v,Vector\u0026lt;int\u0026gt; \u0026amp;left,Vector\u0026lt;int\u0026gt; \u0026amp;right) { int l=0, r=0; while(l\u0026lt;left.size() \u0026amp;\u0026amp; r\u0026lt;right.size()) { if (left[l]\u0026lt;right[r]) v.add(left[l++]); else v.add(right[r++]); } while(l\u0026lt;left.size()) v.add(left[l++]); while(r\u0026lt;right.size()) v.add(right[r++]); } The time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge, which is linear. T(N) = N + 2T(N/2). log N levels * N per level= O(NlogN). Mergesort uses the lowest number of comparisons of all the popular sorting algorithms.\nTheoretical result show that no general sort algorithm could be better than NlogN.\nBut there is still better in practice:\nThe running time of mergesort, when compared with other O(N log N) alternatives, depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array). These costs are language dependent. In Java, when performing a generic sort (using a Comparator), an element comparison can be expensive, but moving elements is cheap (because they are reference assignments, rather than copies of large objects). In C++, in a generic sort, copying objects can be expensive if the objects are large, while comparing objects often is relatively cheap because of the ability of the compiler to aggressively perform inline optimization. Quicksort Most sorting programs in use today are based on an algorithm called Quicksort, which employs a Divide and conquer strategy as merge sort, but instead take a different approach to divide up input vector into low half and high half. Quicksort uses a few more comparisons, in exchange for significantly fewer data movements. The reason that quicksort is faster is that the partitioning step can actually be performed in place and very efficiently.\n\u0026ldquo;Hard-split easy-join\u0026rdquo;, Each element examined and placed in correct half, so that join step become trivial.\nChoose an element (pivot) to serve as the boundary between the small and large elements. Partitioning: Rearrange the elements in the vector so that all elements to the left of the boundary are less than the pivot and all elements to the right are greater than or possibly equal to the pivot. Sort the elements in each of the partial vectors. void Quicksort(Vector\u0026lt;int\u0026gt; \u0026amp;v, int start, int stop) { if (stop \u0026gt; start) { int pivot = Partition(v, start, stop); Quicksort(v, start, pivot-1); Quicksort(v, pivot+1, stop); } } The running time of quicksort is equal to the running time of the two recursive calls plus the linear time spent in the partition (the pivot selection takes only constant time). T(N) = T(i) + T(N − i − 1) + cN, where i = |S1| is the number of elements in S1.\nThere are thre cases\nIdeal 50/50 split: The pivot is in the middle, T(N) = cN + 2T(N/2) =\u0026gt; O(NlogN) Average bad 90/10 split: N per level, but more levels, solve N*(9/10)k = 1, still k = O(NlogN) Worst N-1/1 split: The pivot is the smallest element, all the time. Then i = 0, T(N) = T(N − 1) + cN, N \u0026gt; 1. With N levels! O(N2) In a vector with randomly chosen elements, Quicksort tends to perform well, with an average-case complexity of O(N log N). In the worst case — which paradoxically consists of a vector that is already sorted — the performance degenerates to O(N2). Despite this inferior behavior in the worst case, Quicksort is so much faster in practice than most other algorithms that it has become the standard.\nQuicksort strategy Picking the pivot Picking a good pivot improves performance, but also costs some time. If the algorithm spends more time choosing the pivot than it gets back from making a good choice, you will end up slowing down the implementation rather than speeding it up.\nThe popular, uninformed choice is to use the first element as the pivot. This is acceptable if the input is random, but if the input is presorted or in reverse order, then the pivot provides a poor partition. A safe approach is to choose the pivot element randomly. On the other hand, random number generation is generally an expensive commodity and does not reduce the average running time of the rest of the algorithm at all. A good estimate can be obtained by picking three elements randomly and using the median of these three as pivot. The randomness turns out not to help much, so the common course is to use as pivot the median of the left, right, and center elements. Quicksort partitioning strategy A known method that is very easy to do it wrong or inefficiently.\nGeneral process: The first step is to get the pivot element out of the way by swapping it with the last element. Two pointers, i point to the first element and j to the next-to-last element. What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part. “Small” and “large” are relative to the pivot. While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot. We move j left, skipping over elements that are larger than the pivot. When i and j have stopped, i is pointing at a large element and j is pointing at a small element. If i is to the left of j (not yet cross), those elements are swapped. Repeat the process until i and j cross The final is to swap the pivot element with present i element One important detail we must consider is how to handle elements that are equal to the pivot? Suppose there are 10,000,000 elements, of which 500,000 are identical (or, more likely, complex elements whose sort keys are identical). To get an idea of what might be good, we consider the case where all the elements in the array are identical. If neither i nor j stops, and code is present to prevent them from running off the end of the array, no swaps will be performed. Although this seems good, a correct implementation would then swap the pivot into the last spot that i touched, which would be the next-to last position (or last, depending on the exact implementation). This would create very uneven subarrays. If all the elements are identical, the running time is O(N2). If both i and j stop, there will be many swaps between identical elements. The partition creates two nearly equal subarrays. The total running time would then be O(N log N). Thus it is better to do the unnecessary swaps and create even subarrays than to risk wildly uneven subarrays. Small arrays For very small arrays (N ≤ 20), quicksort does not perform as well as insertion sort. Furthermore, because quicksort is recursive, these cases will occur frequently. A common solution is not to use quicksort recursively for small arrays, but instead use a sorting algorithm that is efficient for small arrays, such as insertion sort. A good cutoff range is N = 10, although any cutoff between 5 and 20 is likely to produce similar results. This also saves nasty degenerate cases, such as taking the median of three elements when there are only one or two. Design Strategy When an algorithm is given, the actual data structures need not be specified. It is up to the programmer to choose the appropriate data structure in order to make the running time as small as possible. There are many to be considered: algorithms, data structure, space-time tradeoff, code complexity.\nDynamic Programming To solve optimization problems in which we make a set of choices in order to arrive at an optimal solution. As we make each choice, subproblems of the same form often arise. Dynamic programming is effective when a given subproblem may arise from more than one partial set of choices; the key technique is to store the solution to each such subproblem in case it should reappear. Unlike divide-and-conquer algorithms which partition the problem into disjoint subproblems, dynamic programming applies when the subproblems overlap.\n“Programming” in this context refers to a tabular method. When should look for a dynamic-programming solution to a problem? Optimal substructure: a problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems. Overlapping subproblems: When a recursive algorithm revisits the same problem repeatedly, we say that the optimization problem has overlapping subproblems. In contrast, a problem for which a divide-andconquer approach is suitable usually generates brand-new problems at each step of the recursion. General setps of Dynamic Programming Characterize the structure of an optimal solution. Recursively define the value of an optimal solution. Compute the value of an optimal solution, typically in a bottom-up fashion. Construct an optimal solution from computed information. Greedy Algorithms Greedy algorithms work in phases. In each phase, a decision is made in a locally optimal manner, without regard for future consequences. When the algorithm terminates, we hope that the local optimum is equal to the global optimum. If this is the case, then the algorithm is correct; otherwise, the algorithm has produced a suboptimal solution.\nHuffman Codes\nA Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The reason that this is a greedy algorithm is that at each stage we perform a merge without regard to global considerations. We merely select the two smallest trees. If we maintain the trees in a priority queue, ordered by weight, then the running time is O(C logC), since there will be one buildHeap, 2C − 2 deleteMins, and C − 2 inserts. A simple implementation of the priority queue, using a list, would give an O(C2) algorithm. The choice of priority queue implementation depends on how large C is. In the typical case of an ASCII character set, C is small enough that the quadratic running time is acceptable. Divide and Conquer Traditionally, routines in which the text contains at least two recursive calls and subproblems be disjoint (that is, essentially nonoverlapping) are called divide-and-conquer algorithms.\nDivide: Smaller problems are solved recursively (except, of course, base cases). Conquer: The solution to the original problem is then formed from the solutions to the subproblems. We have already seen several divide-and-conquer algorithms: mergesort and quicksort, which have O(N log N) worst-case and averagecase bounds, respectively. Backtracking Algorithms See Recursive Backtracking In some cases, the savings over a brute-force exhaustive search can be significant. The elimination of a large group of possibilities in one step is known as pruning.\nHow to evaluate/compare alternatives Often interested in execution performance: Time spent and memory used Should also consider ease of developing, verifying, maintaining code Text editor case study Buffer requirements\nSequence of characters + cursor position Operations to match commands above What to consider?\nImplementation choices performance implications Buffer class interface\nclass Buffer { public: Buffer(); ~Buffer(); void moveCursorForward(); void moveCursorBackward(); void moveCursorToStart(); void moveCursorToEnd(); void insertCharacter(char ch); void deleteCharacter(); void display(); private: // TBD! }; Buffer layered on Vector\nNeed character data + cursor Chars in Vector\u0026lt;char\u0026gt; Represent cursor as integer index Minor detail \u0026ndash; is index before/after cursor? Buffer contains: AB|CDE // for Buffer class private: Vector\u0026lt;char\u0026gt; chars; int cursor; Performance insertCharacter() and deleteCharacter() is linear, other operation is just O(1) Space used ~1 byte per char Buffer layered on Stack\nInspiration: add/remove at end of vector is fast If chars next to cursor were at end… Build on top of stack? Another layered abstraction! How is cursor represented? Buffer contains:AB|CDE There is no explicit cursor representation, instead using two stack to represent a whole data structure being seperated by the implicit cursor. // for Buffer class private: Stack\u0026lt;char\u0026gt; before, after; Performance moveCursorToStart(), moveCursorToEnd() operation is linear, other operation is just O(1) Space used ~2 byte per char Buffer as double linked list\nInspiration: contiguous memory is constraining Connect chars without locality Add tail pointer to get direct access to last cell Add prev link to speed up moving backwards Buffer contains:AB|CDE // for Buffer class private: struct cellT { char ch; cellT *prev, *next; }; cellT *head, *tail, *cursor; Cursor design To cell before or after? 5 letters, 6 cursor positions… Add \u0026ldquo;dummy cell\u0026rdquo; to front of list Performance destruction is linear, other operation is just O(1) Space used ~9 byte per char Compare implementations\nOperation Vector Stack Single linked list Double linked list Buffer() O(1) O(1) O(1) O(1) ~Buffer() O(1) O(1) O(N) O(N) moveCursorForward() O(1) O(1) O(1) O(1) moveCursorBackward() O(1) O(1) O(N) O(1) moveCursorToStart() O(1) O(N) O(1) O(1) moveCursorToEnd() O(1) O(N) O(N) O(1) insertCharacter() O(N) O(1) O(1) O(1) deleteCharacter() O(N) O(1) O(1) O(1) Space used 1N 2N 5N 9N Space-time tradeoff Doubly-linked list is O(1) on all six operations But, each char uses 1 byte + 8 bytes of pointers =\u0026gt; 89% overhead! Compromise: chunklist Array and linked list hybrid Shares overhead cost among several chars Chunksize can be tuned as appropriate Cost shows up in code complexity Cursor must traverse both within and across chunks Splitting/merging chunks on insert/deletes Map Map is super-useful, support any kind of dictionary, lookup table, index, database, etc. Map stores key-value pairs, support fast access via key, operations to optimize: add, getValue How to make it work efficiently?\nImplement Map as Vector Layer on Vector, provides convenience with low overhead Define pair struct, to olds key and value together, Vector\u0026lt;pair\u0026gt; Vector sorted or unsorted? If sorted, sorted by what? Sorting: Provides fast lookup, but still slow to insert (because of shuffling) How to implement getValue, add? Does a linked list help? Easy to insert, once at a position But hard to find position to insert\u0026hellip; Implementing Map as tree Implementatation Each Map entry adds node to tree, node contains: string key, client-type value, pointers to left/right subtrees Tree organized for binary search, Key is used as search field getValue: Searches tree, comparing keys, find existing match or error add: Searches tree, comparing keys, overwrites existing or adds new node Private members for Map template \u0026lt;typename ValType\u0026gt; class Map { public: // as before private: struct node { string key; ValType value; node *left, *right; }; node *root; node *treeSearch(node * t, string key); void treeEnter(node *\u0026amp;t, string key, ValType val); DISALLOW_COPYING(Map) }; Evaluate Map as tree Space used: Overhead of two pointers per entry (typically 8 bytes total) Runtime performance: Add/getValue take time proportional to tree height(expected to be O(logN)) Degenerate trees The insert order is \u0026ldquo;sorted\u0026rdquo;: 2 8 14 15 18 20 21, totally unbalanced with height = 7 The insert order is \u0026ldquo;alternately sorted\u0026rdquo;: 21 2 20 8 14 15 18 or 2 8 21 20 18 14 15 Association: What is the relationship between worst-case inputs for tree insertion and Quicksort? What to do about it: AVL tree Compare Map implementations Operation Vector BST Sorted Vector getValue O(N) O(lgN) O(lgN) add O(N) O(lgN) O(N) Space used N 9N N Hashing Hash table ADT Hash table data structure: A list of keys and TableSize Hash function: A mapping that map each key into some number in the range 0 to TableSize-1 and distributes the keys evenly among the appropriate cell Hashing The major problems are choosing a function, deciding what to do when two keys hash to the same value (this is known as a collision), and deciding on the table size Rehashing If the table gets too full, the running time for the operations will start taking too long, and insertions might fail for open addressing hashing with quadratic resolution. A solution is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table. The Big-Five In C++11, classes come with five special functions that are already written for you. These are the destructor, copy constructor, move constructor, copy assignment operator, and move assignment operator. Collectively these are the big-five.\nDestructor The destructor is called whenever an object goes out of scope or is subjected to a delete. Typically, the only responsibility of the destructor is to free up any resources that were acquired during the use of the object. This includes calling delete for any corresponding news, closing any files that were opened, and so on. The default simply applies the destructor on each data member.\nConstructor A constructor is a method that describes how an instance of the class is constructed. If no constructor is explicitly defined, one that initializes the data members using language defaults is automatically generated.\nCopy Constructor and Move Constructor\nCopy Assignment and Move Assignment (operator=) By Defaults, if a class consists of data members that are exclusively primitive types and objects for which the defaults make sense, the class defaults will usually make sense. The main problem occurs in a class that contains a data member that is a pointer.\nThe default destructor does nothing to data members that are pointers (for good reason—recall that we must delete ourselves). Furthermore, the copy constructor and copy assignment operator both copy the value of the pointer rather than the objects being pointed at. Thus, we will have two class instances that contain pointers that point to the same object. This is a so-called shallow copy (contrast to deep copy). To avoid shallow copy, ban the copy funtionality by calling DISALLOW_COPYING(ClassType). As a result, when a class contains pointers as data members, and deep semantics are important, we typically must implement the destructor, copy assignment, and copy constructors ourselves.\nExplicit constructor: All one-parameter constructors should be made explicit to avoid behind-the-scenes type conversions. Otherwise, there are somewhat lenient rules that will allow type conversions without explicit casting operations. Usually, this is unwanted behavior that destroys strong typing and can lead to hard-to-find bugs. The use of explicit means that a one-parameter constructor cannot be used to generate an implicit temporary\nclass IntCell { public: explicit IntCell( int initialValue = 0 ) : storedValue{ initialValue } { } int read( ) const { return storedValue; } private: int storedValue; }; IntCell obj; // obj is an IntCell obj = 37; // Should not compile: type mismatch Since IntCell constructor is declared explicit, the compiler will correctly complain that there is a type mismatch\nTemplate Type-independent When we write C++ code for a type-independent algorithm or data structure, we would prefer to write the code once rather than recode it for each different type\nFunction template A function template is not an actual function, but instead is a pattern for what could become a function. An expansion for each new type generates additional code; this is known as code bloat when it occurs in large projects. Class template template \u0026lt;typename Object\u0026gt; class MemoryCell { public: explicit MemoryCell( const Object \u0026amp; initialValue = Object{ } ) : storedValue{ initialValue } { } private: Object storedValue; }; MemoryCell is not a class, it is only a class template. It will be a class if specify the Object type. MemoryCell\u0026lt;int\u0026gt; and MemoryCell\u0026lt;string\u0026gt; are the actual classes.\nGraph Algorithms Definitions: vertices, edges, arcs, directed arcs = digraphs, weight/cost, path, length, acyclic(no cycles)\nTopological Sort A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from vi to vj, then vj appears after vi in the ordering. A topological ordering is not possible if the graph has a cycle To find a topological ordering, define the indegree of a vertex v as the number of edges (u, v), then use a queue or stack to keep the present 0 indegree vertexes. At each stage, as long as the queue is not empty, dequeue a 0 indegree vertexes in the queue, enqueue each new generated 0 indegree vertexes into the queue. Shortest-Path Algorithms Breadth-first search\nExplores equally in all directions To find unweighted shortest paths Operates by processing vertices in layers: The vertices closest to the start are evaluated first, and the most distant vertices are evaluated last. Dijkstra\u0026rsquo;s Algorithm\nAlso called Uniform Cost Search, cost matters Instead of exploring all possible paths equally, it favors lower cost paths. Dijkstra’s algorithm proceeds in stages. At each stage, while there are still vertices waiting to be known: Selects a vertex v, which has the smallest dv among all the unknown vertices, and declares v as known stage. For each of v\u0026rsquo;s neighbors, w, if the new path\u0026rsquo;s cost from v to w is better than previous dw, dw will be updated. But w will not be marked as known, unless at next while-loop stage, dw happens to be the smalles. The above steps could be implemented via a priority queue. A proof by contradiction will show that this algorithm always works as long as no edge has a negative cost. If the graph is sparse, with |E| =θ(|V|), this algorithm is too slow. In this case, the distances would need to be kept in a priority queue. Selection of the vertex v is a deleteMin operation. The update of w’s distance can be implemented two ways. One way treats the update as a decreaseKey operation. An alternate method is to insert w and the new value dw into the priority queue every time w’s distance changes. Greedy Best First Search(Heuristic search)\nWith Breadth First Search and Dijkstra’s Algorithm, the frontier expands in all directions. This is a reasonable choice if you’re trying to find a path to all locations or to many locations. However, a common case is to find a path to only one location. A modification of Dijkstra’s Algorithm, optimized for a single destination. It prioritizes paths that seem to be leading closer to the goal. To make the frontier expand towards the goal more than it expands in other directions. First, define a heuristic function that tells us how close we are to the goal, design a heuristic for each type of graph def heuristic(a, b): # Manhattan distance on a square grid return abs(a.x - b.x) + abs(a.y - b.y) Use the estimated distance to the goal for the priority queue ordering. The location closest to the goal will be explored first. This algorithm runs faster when there aren’t a lot of obstacles, but the paths aren’t as good(not always the shortest). A* Algorithm\nDijkstra’s Algorithm works well to find the shortest path, but it wastes time exploring in directions that aren’t promising. Greedy Best First Search explores in promising directions but it may not find the shortest path. The A* algorithm uses both the actual distance from the start and the estimated distance to the goal. Compare the algorithms: Dijkstra’s Algorithm calculates the distance from the start point. Greedy Best-First Search estimates the distance to the goal point. A* is using the sum of those two distances. So A* is the best of both worlds. As long as the heuristic does not overestimate distances, A* does not use the heuristic to come up with an approximate answer. It finds an optimal path, like Dijkstra’s Algorithm does. A* uses the heuristic to reorder the nodes so that it’s more likely that the goal node will be encountered sooner. Conclusion: Which algorithm should you use for finding paths on a map?\nIf you want to find paths from or to all all locations, use Breadth First Search or Dijkstra’s Algorithm. Use Breadth First Search if movement costs are all the same; use Dijkstra’s Algorithm if movement costs vary. If you want to find paths to one location, use Greedy Best First Search or A*. Prefer A* in most cases. When you’re tempted to use Greedy Best First Search, consider using A* with an “inadmissible” heuristic. If you want the optimal paths, Breadth First Search and Dijkstra’s Algorithm are guaranteed to find the shortest path given the input graph. Greedy Best First Search is not. A* is guaranteed to find the shortest path if the heuristic is never larger than the true distance. (As the heuristic becomes smaller, A* turns into Dijkstra’s Algorithm. As the heuristic becomes larger, A* turns into Greedy Best First Search.) Advanced Data Structures Red-Black Trees Red-black tree leads to a natural implementation of the insertion algorithm for 2-3 trees\nRBT definition\nRed-black tree means encoding 2-3 trees in this way: red links, which bind together two 2-nodes to represent 3-nodes, and black links, which bind together the 2-3 tree. An equivalent definition is to define red-black BSTs as BSTs having red and black links and satisfying the following three restrictions: Red links lean left. No node has two red links connected to it. The tree has perfect black balance : every path from the root to a null link has the same number of black links. A 1-1 correspondence: If we draw the red links horizontally in a red-black BST, all of the null links are the same distance from the root, and if we then collapse together the nodes connected by red links, the result is a 2-3 tree. RBT implementaion\nColor representation: Each node is pointed to by precisely one link from its parent, Encode the color of links in nodes, by adding a boolean instance variable color to our Node data type, which is true if the link from the parent is red and false if it is black. By convention, null links are black. For clarity, define constants RED and BLACK for use in setting and testing this variable. Rotation To correct right-leaning red links or two red links in a row conditions. takes a link to a red-black BST as argument and, assuming that link to be to a Node h whose right link is red, makes the necessary adjustments and returns a link to a node that is the root of a red-black BST for the same set of keys whose left link is red. Actually it is switching from having the smaller of the two keys at the root to having the larger of the two keys at the root. Flipping colors to split a 4-node In addition to flipping the colors of the children from red to black, we also flip the color of the parent from black to red. Keeping the root black. Insertion Maintain the 1-1 correspondence between 2-3 trees and red-black BSTs during insertion by judicious use of three simple operations: left rotate, right rotate, and color flip. If the right child is red and the left child is black, rotate left. If both the left child and its left child are red, rotate right. If both children are red, flip colors. Deletion Assignments Name Hash Game of Life Serafini Recursion Boggle! Patient Queue Huffman Encoding Trailblazer ","permalink":"https://congchan.github.io/posts/stanford-cs106a/b-programming-intro-%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E8%AF%BE/","summary":"\u003cp\u003e\u003ca href=\"https://see.stanford.edu/Course/CS106B\"\u003eStanford CS106B Programming Abstractions\u003c/a\u003e 和 \u003ca href=\"https://see.stanford.edu/Course/CS106A\"\u003eCS106A\u003c/a\u003e 的学习笔记. 课程作业(cs106b spring 2017)实现代码见 \u003ca href=\"https://github.com/ShootingSpace/cs106b-programming-abstraction\"\u003ehttps://github.com/ShootingSpace/cs106b-programming-abstraction\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"topics\"\u003eTopics:\u003c/h2\u003e\n\u003cp\u003eA: Intro (by Java)\nB: Recursion, algorithms analysis (sort/search/hash), dynamic data structures (lists, trees, heaps), data abstraction (stacks, queues, maps), implementation strategies/tradeoffs\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"purposes\"\u003ePurposes\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003ebecome acquainted with the C++ programming language\u003c/li\u003e\n\u003cli\u003elearn more advanced programming techniques\u003c/li\u003e\n\u003cli\u003eexplore classic data structures and algorithms\u003c/li\u003e\n\u003cli\u003eand apply these tools to solving complex problems\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"reference\"\u003eReference\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eText Book: \u003ca href=\"https://www.pearson.com/us/higher-education/program/Weiss-Data-Structures-and-Algorithm-Analysis-in-C-4th-Edition/PGM148299.html\"\u003eData Structures \u0026amp; Algorithm Analysis in C++, 4th ed, by Mark A. Weiss\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eText Book: \u003ca href=\"https://www.pearson.com/us/higher-education/program/Roberts-Programming-Abstractions-in-C/PGM80147.html\"\u003eProgramming Abstractions in C++ 1st Edition by Eric Roberts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eText Book: \u003ca href=\"http://algs4.cs.princeton.edu/home/\"\u003eAlgorithms, 4th Edition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eBlog: \u003ca href=\"http://www.redblobgames.com/pathfinding/a-star/introduction.html\"\u003eRed Blob Games\u003c/a\u003e, \u003ca href=\"http://theory.stanford.edu/~amitp/GameProgramming/\"\u003eAmit’s A* Pages\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2 id=\"coding-style\"\u003eCoding style\u003c/h2\u003e\n\u003cp\u003eWhy writing clean, well-structured code\u003c/p\u003e","title":"Stanford CS106A/B Programming Intro 斯坦福大学编程入门课"},{"content":"Hash Tables Save items in a key-indexed table. Index is a function of the key - Hash function, method for computing array index from key.\n要实现哈希表, 需要解决几个问题:\n如何定义/计算哈希函数。 相等判定：如何检查两个键是否相等。 冲突解决：寻找能够处理哈希到同一索引的两个密钥的算法和数据结构。 时空权衡设计问题:\n如果没有空间限制, 那么可以使用非常简单的哈希函数, 极端情况就是给每一种键分配一个索引。 如果没有时间限制, 那么对于键冲突问题可以使用简单的顺序搜索。 而现实中, 哈希表就是解决同时存在空间和时间限制的问题。 哈希函数 最理想的目标, 生成均匀分布的索引, 这样计算高效.\n比如电话号码, 使用前三个数字作为索引是一种比较草稿的设计, 因为前三个数字一般代表区号, 且区号都是有限的, 这样同一个区的号码都会挤在同一个索引位置. 较好的方式是使用后三位数字. 身份证号同理.\n在实际设计过程中, 不同的数据类型适用不同的方法.\n所有Java类都继承了int hashCode()方法. 该方法的基本要求是: If x.equals(y), then (x.hashCode() == y.hashCode()) 最好(但不是必须的)能够满足: If !x.equals(y), then (x.hashCode() != y.hashCode()) 默认的实现方式是利用内存位置.\nJava integers, booleans, and doubles:\npublic final class Integer { private final int value; ... public int hashCode() { return value; } } public final class Boolean { private final boolean value; ... public int hashCode() { if (value) return 1231; else return 1237; } } // convert to IEEE 64-bit representation; // xor most significant 32-bits // with least significant 32-bits public final class Double { private final double value; ... public int hashCode() { long bits = doubleToLongBits(value); return (int) (bits ^ (bits \u0026gt;\u0026gt;\u0026gt; 32)); } } strings\npublic final class String { private int hash = 0; private final char[] s; ... public int hashCode() { int h = hash; if (h != 0) return h; for (int i = 0; i \u0026lt; length(); i++) hash = s[i] + (31 * h); hash = h; return h; } } Horner\u0026rsquo;s method to hash string of length L: L multiplies/adds $$h = s[0] \\cdot 31^{L-1} + ... + s[L-3] \\cdot 31^2 + s[L–2] \\cdot 31^1 + s[L–1]$$String s = \u0026quot;call\u0026quot;; int code = s.hashCode();, $$code = 99 \\cdot 31^3 + 97 \\cdot 31^2 + 108 \\cdot 31^1 + 108 = 3045982$$\u0026ldquo;Standard\u0026rdquo; recipe for user-defined types. ・Combine each significant field using the 31x + y rule. ・If field is a primitive type, use wrapper type hashCode(). ・If field is null, return 0. ・If field is a reference type, use hashCode(). ・If field is an array, apply to each entry, or use Arrays.deepHashCode().\nBasic rule. Need to use the whole key to compute hash code;\nModular hashing Hash code. An int between $-2^{31}$ and $2^{31} - 1$. Hash function. An int between 0 and M - 1 (for use as array index, typically M is a prime or power of 2) A buggy version: 1-in-a-billion bug\nprivate int hash(Key key) { return Math.abs(key.hashCode()) % M; } hashCode() of \u0026quot;polygenelubricants\u0026quot; is $-2^{31}$\nA correct version\nprivate int hash(Key key) { return (key.hashCode() \u0026amp; 0x7fffffff) % M; } 键索引冲突 Collision. Two distinct keys hashing to same index.\nSeparate chaining symbol table Use an array of M \u0026lt; N linked lists. (H. P. Luhn, IBM 1953) public class SeparateChainingHashST\u0026lt;Key, Value\u0026gt; { private int M = 97; // number of chains private Node[] st = new Node[M]; // array of chains private static class Node { private Object key; // no generic array creation private Object val; private Node next; public Node(Key key, Value val, Node next) { this.key = key; this.val = val; this.next = next; } } private int hash(Key key) { return (key.hashCode() \u0026amp; 0x7fffffff) % M; } public Value get(Key key) { int i = hash(key); for (Node x = st[i]; x != null; x = x.next) if (key.equals(x.key)) return (Value) x.val; return null; } public void put(Key key, Value val) { int i = hash(key); for (Node x = st[i]; x != null; x = x.next) if (key.equals(x.key)) { x.val = val; return; } st[i] = new Node(key, val, st[i]); //new key put ahead } } Proposition. Under uniform hashing assumption, prob. that the number of keys in a list is within a constant factor of N / M is extremely close to 1. Consequence. Number of probes for search/insert is proportional to N / M\nIf M too large ⇒ too many empty chains. If M too small ⇒ chains too long. Typical choice: M ~ N / 5 ⇒ constant-time ops. Linear Probing Open addressing. (Amdahl-Boehme-Rocherster-Samuel, IBM 1953) When a new key collides, find next empty slot, and put it there.\nHash. Map key to integer i between 0 and M-1. Insert. Put at table index i if free; if not try i+1, i+2, etc Search. Search table index i; if occupied but no match, try i+1, i+2, etc.\npublic class LinearProbingHashST\u0026lt;Key, Value\u0026gt; { private int M = 30001; private Value[] vals = (Value[]) new Object[M]; private Key[] keys = (Key[]) new Object[M]; private int hash(Key key) { /* as before */ } public Value get(Key key) { for (int i = hash(key); keys[i] != null; i = (i+1) % M) if (key.equals(keys[i])) return vals[i]; return null; } public void put(Key key, Value val) { int i; for (i = hash(key); keys[i] != null; i = (i+1) % M) if (keys[i].equals(key)) break; keys[i] = key; vals[i] = val; } } Knuth\u0026rsquo;s parking problem 提供了一个理解 linear probing的模型: Cars arrive at one-way street with M parking spaces. Each desires a random space i : if space i is taken, try i + 1, i + 2, etc. So what is mean displacement of a car?\nHalf-full. With M / 2 cars, mean displacement is ~ 3 / 2. Full. With M cars, mean displacement is ~ sqrt(π M / 8)\nProposition. Under uniform hashing assumption, the average # of probes in a linear probing hash table of size M that contains N = α M keys is: search hit ~ (1 + 1 / (1 - α)) / 2, search miss / insert $1/2 (1 + 1 / (1 - α)^ 2)$\nTypical choice: α = N / M ~ ½. So that # probes for search hit is about 3/2, # probes for search miss is about 5/2.\n不同哈希表实现比较 Separate chaining. ・Easier to implement delete. ・Performance degrades gracefully. ・Clustering less sensitive to poorly-designed hash function.\nLinear probing. ・Less wasted space. ・Better cache performance.\n其他变种 Two-probe hashing. (separate-chaining variant) ・Hash to two positions, insert key in shorter of the two chains. ・Reduces expected length of the longest chain to log log N\nDouble hashing. (linear-probing variant) ・Use linear probing, but skip a variable amount, not just 1 each time. ・Effectively eliminates clustering. ・Can allow table to become nearly full. ・More difficult to implement delete.\nCuckoo hashing. (linear-probing variant) ・Hash key to two positions; insert key into either position; if occupied, reinsert displaced key into its alternative position (and recur). ・Constant worst case time for search.\n哈希表和平衡二叉树的比较 Hash tables. ・Simpler to code. ・No effective alternative for unordered keys. ・Faster for simple keys (a few arithmetic ops versus log N compares). ・Better system support in Java for strings (e.g., cached hash code).\nBalanced search trees. ・Stronger performance guarantee. ・Support for ordered ST operations. ・Easier to implement compareTo() correctly than equals() and hashCode().\nJava system includes both. ・Red-black BSTs: java.util.TreeMap, java.util.TreeSet. ・Hash tables: java.util.HashMap, java.util.IdentityHashMap.\n有关哈希表的攻击 均匀哈希假设在实践中是否重要？ 恶意攻击者学习你的哈希函数（例如，通过阅读Java API）并导致单个插槽大量堆积，从而使性能停滞不前\n案例:\nBro服务器：使用比拨号调制解调器更少的带宽，将精心选择的数据包发送到DOS服务器。 Perl 5.8.0：将精心挑选的字符串插入关联数组中。 Linux 2.4.20内核：使用精心选择的名称保存文件。 单向哈希函数, 使得找到一个键对应的哈希值（或两个哈希到相同值的键）变得困难. 如已知是不安全的MD4, MD5, SHA-0和SHA-1. 其他的还有SHA-2, WHIRLPOOL, RIPEMD-160, \u0026hellip;.\n/* prints bytes as hex string */ String password = args[0]; MessageDigest sha1 = MessageDigest.getInstance(\u0026#34;SHA1\u0026#34;); byte[] bytes = sha1.digest(password); 这种哈希函数对于符号表而言有点过于昂贵了\nBit Map Bit-map用一个bit位来标记某个元素对应的Value， 而Key即是该元素。由于采用了Bit为单位来存储数据，因此在存储空间方面，可以大大节省。\n假设我们要对0-7内的5个元素4,7,2,5,3排序（假设这些元素没有重复）。那么我们就可以采用Bit-map的方法来达到排序的目的。要表示8个数，我们就只需要8个Bit（1Bytes），\n首先我们开辟1Byte的空间，将这些空间的所有Bit位都置为0，0 0 0 0 0 0 0 0. 然后遍历这5个元素，首先第一个元素是4，那么就把4对应的位置设为1, p+(i/8)|(0x01\u0026lt;\u0026lt;(i%8)), 这里默认为Big-ending, 0 0 0 0 1 0 0 0. 然后再处理第二个元素7，将第八位置为1,，接着再处理第三个元素，一直到最后处理完所有的元素，将相应的位置为1，这时候的内存的Bit位的状态0 0 1 1 1 1 0 1 遍历一遍Bit区域，把1的索引依次输出（2，3，4，5，7），这样就达到了排序的目的。 算法的关键是如何确定十进制的数映射到二进制bit位的map图。算法占用很少内存，比如N=10000000；只需占用内存为N/8=1250000Byte=1.25M。缺点是不能有重复数据。\nMap映射表 假设需要排序或者查找的总数N=10000000，那么我们需要申请内存空间的大小为int a[1 + N/32]，其中：a[0]在内存中占32位, 可以对应十进制数0-31，依次类推： bitmap表为：\na[0]---------\u0026gt;0-31 a[1]---------\u0026gt;32-63 a[2]---------\u0026gt;64-95 a[3]---------\u0026gt;96-127 .......... 十进制数需要转换为对应的bit位\n位移转换 将十进制数转换为对应的bit位, 申请一个int一维数组，作为32列的二维数组，\nint a[0] |0000000000000000000000000000000000000| int a[1] |0000000000000000000000000000000000000| ……………… int a[N] |0000000000000000000000000000000000000| 例如十进制0，对应在a[0]第一位： 00000000000000000000000000000001\n求十进制0-N对应在数组a的索引：十进制0-31，对应a[0]，先由十进制数n转换为与32的余可转化为对应在数组a中的索引0。比如n=24,那么 n/32=0，则24对应a[0]。又比如n=60, 那么n/32=1，则60对应a[1]。 求0-N对应0-31中的数：十进制0-31就对应0-31，而32-63则对应也是0-31，即给定一个数n可以通过模32求得对应0-31中的数。 利用移位0-31使得对应32bit位为1. 找到对应0-31的数为M, 左移M位：即2 ^ M, 置1. Bloom Filter 为了降低键值冲突的概率，Bloom Filter使用了多个哈希函数：创建一个m位BitSet，先将所有位初始化为0，然后选择k个不同的哈希函数。第i个哈希函数对字符串str哈希的结果记为h(i, str)，且h(i, str)的范围是0到m-1 。\n对于字符串str，分别计算h(1, str), h(2, str), ... h(k, str), 以这些哈希值作为索引, 将BitSet的对应位置的位设为1, 这样就把str映射到BitSet的k个二进制位了.\n如果要检查某string是否已经被记录在BitSet中, 只需要计算其哈希值数组, 并检查BitSet上对应位置的值是否为1, 若对应位置中有任何一个不是1, 那么该字符串一定没有被记录过, 若全部对应位置都为1, 那么按照false positive认为该字符串已经被记录过了(但不是100%肯定).\n删除操作会影响到其他字符串。如果需要删除字符串的功能，使用Counting bloomfilter(CBF)，这是一种Bloom Filter的变体，CBF将Bloom Filter每一个Bit改为一个计数器，这样就可以实现删除字符串的功能了。\nBloom Filter跟单哈希函数Bit-Map不同之处在于：Bloom Filter使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。\n","permalink":"https://congchan.github.io/posts/java-hash-table/","summary":"\u003ch2 id=\"hash-tables\"\u003eHash Tables\u003c/h2\u003e\n\u003cp\u003eSave items in a key-indexed table. Index is a function of the key - \u003cstrong\u003eHash function\u003c/strong\u003e, method for computing array index from key.\u003c/p\u003e\n\u003cp\u003e要实现哈希表, 需要解决几个问题:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e如何定义/计算哈希函数。\u003c/li\u003e\n\u003cli\u003e相等判定：如何检查两个键是否相等。\u003c/li\u003e\n\u003cli\u003e冲突解决：寻找能够处理哈希到同一索引的两个密钥的算法和数据结构。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e时空权衡设计问题:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e如果没有空间限制, 那么可以使用非常简单的哈希函数, 极端情况就是给每一种键分配一个索引。\u003c/li\u003e\n\u003cli\u003e如果没有时间限制, 那么对于键冲突问题可以使用简单的顺序搜索。\u003c/li\u003e\n\u003cli\u003e而现实中, 哈希表就是解决同时存在空间和时间限制的问题。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"哈希函数\"\u003e哈希函数\u003c/h3\u003e\n\u003cp\u003e最理想的目标, 生成均匀分布的索引, 这样计算高效.\u003c/p\u003e\n\u003cp\u003e比如电话号码, 使用前三个数字作为索引是一种比较草稿的设计, 因为前三个数字一般代表区号, 且区号都是有限的, 这样同一个区的号码都会挤在同一个索引位置. 较好的方式是使用后三位数字. 身份证号同理.\u003c/p\u003e\n\u003cp\u003e在实际设计过程中, 不同的数据类型适用不同的方法.\u003c/p\u003e\n\u003cp\u003e所有Java类都继承了\u003ccode\u003eint hashCode()\u003c/code\u003e方法. 该方法的基本要求是:\n\u003ccode\u003eIf x.equals(y), then (x.hashCode() == y.hashCode())\u003c/code\u003e\n最好(但不是必须的)能够满足:\n\u003ccode\u003eIf !x.equals(y), then (x.hashCode() != y.hashCode())\u003c/code\u003e\n默认的实现方式是利用内存位置.\u003c/p\u003e\n\u003cp\u003eJava integers, booleans, and doubles:\u003c/p\u003e","title":"Java Hash Table"},{"content":"HashMap HashMap 是基于哈希表的 Map 接口的非同步实现。此实现提供所有可选的映射操作，并允许使用 null 值和 null 键。此类不保证映射的顺序，特别是它不保证该顺序不是stable的。\nHashMap 底层就是一个数组结构，数组中的每一项又是一个链表。 当新建一个 HashMap 的时候，就会初始化一个数组table = new Entry[capacity];, 每个 Entry 是一个 key-value 对，有一个指向下一个元素的引用，这就构成了链表。\npublic HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); // Find a power of 2 \u0026gt;= initialCapacity int capacity = 1; while (capacity \u0026lt; initialCapacity) capacity \u0026lt;\u0026lt;= 1; this.loadFactor = loadFactor; threshold = (int)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; useAltHashing = sun.misc.VM.isBooted() \u0026amp;\u0026amp; (capacity \u0026gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); init(); } static class Entry\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final K key; V value; Entry\u0026lt;K,V\u0026gt; next; final int hash; …… } HashMap 底层数组的长度总是 2 的 n 次方，这是 HashMap 在速度上的优化, 所以有这段代码保证初始化时 HashMap 的容量总是 2 的 n 次方，即底层数组的长度总是为 2 的 n 次方。\n// Find a power of 2 \u0026gt;= initialCapacity int capacity = 1; while (capacity \u0026lt; initialCapacity) capacity \u0026lt;\u0026lt;= 1; 插入/更新 如果 key 存在了，那么新的 value 会代替旧的 value，并且如果 key 存在的情况下，该方法返回的是旧的 value，如果 key 不存在，那么返回 null。\n/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with \u0026lt;tt\u0026gt;key\u0026lt;/tt\u0026gt;, or * \u0026lt;tt\u0026gt;null\u0026lt;/tt\u0026gt; if there was no mapping for \u0026lt;tt\u0026gt;key\u0026lt;/tt\u0026gt;. * (A \u0026lt;tt\u0026gt;null\u0026lt;/tt\u0026gt; return can also indicate that the map * previously associated \u0026lt;tt\u0026gt;null\u0026lt;/tt\u0026gt; with \u0026lt;tt\u0026gt;key\u0026lt;/tt\u0026gt;.) */ public V put(K key, V value) { //其允许存放null的key和null的value，当其key为null时，调用putForNullKey方法，放入到table[0]的这个位置 if (key == null) return putForNullKey(value); //通过调用hash方法对key进行哈希，得到哈希之后的数值。该方法实现可以通过看源码，其目的是为了尽可能的让键值对可以分不到不同的桶中 int hash = hash(key); //根据上一步骤中求出的hash得到在数组中是索引i int i = indexFor(hash, table.length); //如果i处的Entry不为null，则通过其next指针不断遍历e元素的下一个元素。 for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } 往 HashMap 中 put 元素的时候，先根据 key 的 hashCode 重新计算 hash 值，根据 hash 值得到这个元素在数组中的位置（即下标），如果数组该位置上已经存放有其他元素了，那么在这个位置上的元素将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。如果数组该位置上没有元素，就直接将该元素放到此数组中的该位置上。\naddEntry(hash, key, value, i)方法根据计算出的 hash 值，将 key-value 对放在数组 table 的 i 索引处。首先要判断容量如果超过阈值threshold, 并且索引位置不为空, 就要先扩容：\n/** * Adds a new entry with the specified key, value and hash code to * the specified bucket. It is the responsibility of this * method to resize the table if appropriate. * * Subclass overrides this to alter the behavior of put method. */ void addEntry(int hash, K key, V value, int bucketIndex) { if ((size \u0026gt;= threshold) \u0026amp;\u0026amp; (null != table[bucketIndex])) { resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } void createEntry(int hash, K key, V value, int bucketIndex) { // 获取指定 bucketIndex 索引处的 Entry Entry\u0026lt;K,V\u0026gt; e = table[bucketIndex]; // 将新创建的 Entry 放入 bucketIndex 索引处，并让新的 Entry 指向原来的 Entry e table[bucketIndex] = new Entry\u0026lt;\u0026gt;(hash, key, value, e); size++; } hash(int h)方法根据 key 的 hashCode 重新计算一次散列。此算法加入了高位计算，防止低位不变，高位变化时，造成的 hash 冲突。\nfinal int hash(Object k) { int h = 0; if (useAltHashing) { if (k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h = hashSeed; } //得到k的hashcode值 h ^= k.hashCode(); //进行计算 h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } 高效计算索引 我们希望 HashMap 里面元素位置尽量的分布均匀，尽量使得每个位置上的元素数量只有一个，那么当用 hash 算法求得这个位置的时候，马上就可以知道对应位置的元素，而不用再去遍历链表，这样就大大优化了查询的效率。\n对于任意给定的对象，只要它的 hashCode() 返回值相同，那么程序调用 hash(int h) 方法所计算得到的 hash 码值总是相同的。我们首先想到的就是把 hash 值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，“模”运算的消耗还是比较大的，在 HashMap 中是这样做的：调用 indexFor(int h, int length) 方法来计算该对象应该保存在 table 数组的哪个索引处。\n/** * Returns index for hash code h. */ static int indexFor(int h, int length) { return h \u0026amp; (length-1); } 当 length 总是 2 的 n 次方时，h \u0026amp; (length-1)运算等价于对 length 取模，也就是 h % length，但是位运算比 % 更高效。\nhash table.length-1 8 \u0026amp; (15-1)： 0100 \u0026amp; 1110 = 0100 9 \u0026amp; (15-1)： 0101 \u0026amp; 1110 = 0100 8 \u0026amp; (16-1)： 0100 \u0026amp; 1111 = 0100 9 \u0026amp; (16-1)： 0101 \u0026amp; 1111 = 0101 15不是以2为底的真数, 15-1 = 0b1110 最后一位永远是 0, 任何与之\u0026amp;的hash值最后一位也永远是0, 这就导致 0001，0011，0101，1001，1011，0111，1101 这几个位置永远都没机会存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！\n而如果length是以2底的真数, 其二进制高位一定是1, 且只有这一个1, 如0b1000, 那么length-1就会变成只有该位置是0其右边低位所有位变为1, 即0b0111. 这使得在低位上与操作时，得到的和原 hash 的低位相同. 加之 hash(int h)方法对 key 的 hashCode 的进一步优化，加入了高位计算，就使得只有完全相同的 hash 值的两个key才会被放到数组中的同一个位置上形成链表。所以如果数组长度为 2 的 n 次幂的时候，不同的 key 算得得 index 相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，查询的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。\n另一种计算Hash的方法: 通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。\n//jdk1.8 \u0026amp; jdk1.7 static final int hash(Object key) { int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h \u0026gt;\u0026gt;\u0026gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 一旦出现链表过长，严重影响HashMap的性能，所以JDK1.8版本对数据结构做了进一步的优化，引入了红黑树。当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法\n读取 从 HashMap 中 get 元素时，首先计算 key 的 hashCode，找到数组中对应位置的某一元素，然后通过 key 的 equals 方法在对应位置的链表中找到需要的元素。\n/** * Returns the value to which the specified key is mapped, * or {@code null} if this map contains no mapping for the key. * * \u0026lt;p\u0026gt;More formally, if this map contains a mapping from a key * {@code k} to a value {@code v} such that {@code (key==null ? k==null : * key.equals(k))}, then this method returns {@code v}; otherwise * it returns {@code null}. (There can be at most one such mapping.) * * \u0026lt;p\u0026gt;A return value of {@code null} does not \u0026lt;i\u0026gt;necessarily\u0026lt;/i\u0026gt; * indicate that the map contains no mapping for the key; it\u0026#39;s also * possible that the map explicitly maps the key to {@code null}. * The {@link #containsKey containsKey} operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */ public V get(Object key) { if (key == null) return getForNullKey(); Entry\u0026lt;K,V\u0026gt; entry = getEntry(key); return null == entry ? null : entry.getValue(); } final Entry\u0026lt;K,V\u0026gt; getEntry(Object key) { int hash = (key == null) ? 0 : hash(key); for (Entry\u0026lt;K,V\u0026gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return e; } return null; } resize rehash 当 HashMap 中的元素越来越多的时候，hash 冲突的几率也就越来越高，因为数组的长度是固定的。所以为了提高查询的效率，就要对 HashMap 的数组进行扩容。\nHashMap 数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是 resize。\n当 HashMap 中的元素个数超过数组大小 * loadFactor时，就会进行数组扩容，loadFactor的默认值为 0.75，这是一个折中的取值。也就是说，默认情况下，数组大小为 16，那么当 HashMap 中元素个数超过 16*0.75=12 的时候，就把数组的大小扩大一倍, 扩展为 2*16=32，然后重新计算每个元素在数组中的位置. 所以如果我们已经预知 HashMap 中元素的个数，那么预设元素的个数能够有效的提高 HashMap 的性能。\nJDK1.7的HashMap在实现resize()时，新table[]的列表采用LIFO方式，即队头插入。这样做的目的是避免尾部遍历。\nvoid resize(int newCapacity) { //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; } Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值 } // 原Entry数组的元素拷贝到新的Entry数组里 void transfer(Entry[] newTable) { Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j \u0026lt; src.length; j++) { //遍历旧的Entry数组 Entry\u0026lt;K,V\u0026gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) { src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do { Entry\u0026lt;K,V\u0026gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1], 变为头节点 newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 } while (e != null); } } } newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话）\nJDK1.8优化了重新计算hash这一步。因为使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1位. 因此，在扩容HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看原来的hash值在新增的那个bit位是1还是0就好了，是0的话索引没变，是1的话索引变成原索引+oldCap. 这样省去了重新计算hash值的时间，而且由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。\n// JDK1.8 final Node\u0026lt;K,V\u0026gt;[] resize() { Node\u0026lt;K,V\u0026gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap \u0026gt; 0) { // 超过最大值就不再扩充了 if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u0026lt;\u0026lt; 1; // double threshold } else if (oldThr \u0026gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中 for (int j = 0; j \u0026lt; oldCap; ++j) { Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash \u0026amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); else { // preserve order Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; Node\u0026lt;K,V\u0026gt; next; do { next = e.next; // 原索引 oldCap - 1: 0 1111 oldCap : 1 0000 判断 key的 hash值的那一位是否为1分为两类 if ((e.hash \u0026amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引+oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 扩容的线程不安全 扩容时存在条件竞争，如果两个线程都发现HashMap需要调整大小，它们会同时尝试调整大小。在调整的过程中，为了避免尾部遍历(tail traversing)而采用队头插入的方式，会让原先的链表顺序会反转。如果在多线程环境中发生条件竞争，会导致死循环。因此在并发环境下，使用CurrentHashMap来替代HashMap\nvoid transfer(Entry[] newTable) { ... do { Entry\u0026lt;K,V\u0026gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1], 变为头节点 newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 } while (e != null); } 如果当 thread1 运行到 int i = indexFor(e.hash, newCapacity);时e = key(3), next = key(7), 而thread2 已经执行完毕transfer(), 此时的状态是. 此时实际上已经完成了transfer()了, key(3)和key(7)顺序反转了, key(7).next = key(3). 但thread1还没跑完, 此时Thread1的 e = key(3)，而next = key(7)，现在thread1继续运行, 1, e.next = newTable[i]; 此时就是把key(3).next指向key(7), 环形链接出现 2, newTalbe[i] = e;, 把key(3)置于表头 .\nJDK1.8的优化，通过增加tail指针，既避免了死循环问题（让数据直接插入到队尾），又避免了尾部遍历。\n// http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/f4129fcfacdc/src/share/classes/java/util/HashMap.java final Node\u0026lt;K,V\u0026gt;[] resize() { Node\u0026lt;K,V\u0026gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap \u0026gt; 0) { // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u0026lt;\u0026lt; 1; // double threshold } else if (oldThr \u0026gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中 for (int j = 0; j \u0026lt; oldCap; ++j) { Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash \u0026amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); else { // preserve order // // 新扩容部分，标识为hi，原来old的部分标识为lo Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; Node\u0026lt;K,V\u0026gt; next; do { next = e.next; // 原索引 if ((e.hash \u0026amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引+oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } HashMap性能参数 HashMap 包含如下几个构造函数：\nHashMap()：构建一个初始容量为 16，负载因子为 0.75 的 HashMap。 HashMap(int initialCapacity)：构建一个初始容量为 initialCapacity，负载因子为 0.75 的 HashMap。 HashMap(int initialCapacity, float loadFactor)：以指定初始容量、指定的负载因子创建一个 HashMap。 负载因子 loadFactor 衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小。对于使用链表法的数组来说，查找一个元素的平均时间是 O(1+a)，因此如果负载因子越大，对空间的利用更充分，然而后果是查找效率的降低；如果负载因子太小，那么数组的数据将过于稀疏，对空间造成严重浪费。\nHashMap 的实现中，通过 threshold 字段来判断 HashMap 的最大容量：threshold = (int) (capacity * loadFactor);\nthreshold 就是在给定 loadFactor 和 capacity 下允许的最大元素数目，超过这个数目就重新 resize，以降低实际的负载因子。默认的的负载因子 0.75 是对空间和时间效率的一个平衡选择。当容量超出此最大容量时， resize 后的 HashMap 容量是翻倍.\n迭代中的线程不安全 如果在使用迭代器的过程中有其他线程修改了 HashMap，那么将抛出ConcurrentModificationException，这就是所谓 fail-fast 策略。Fail-fast 机制是 java 集合(Collection)中的一种错误机制。当多个线程对同一个集合的内容进行操作时，就可能会产生 fail-fast 事件。\n这一策略的实现是通过 modCount变量，对 HashMap 内容（当然不仅仅是 HashMap 才会有，其他例如 ArrayList 也会）的修改都将增加这个值（在很多操作中都有 modCount++ 这句），那么在迭代器初始化过程中会将这个值赋给迭代器的 expectedModCount。\nHashIterator() { expectedModCount = modCount; if (size \u0026gt; 0) { // advance to first entry Entry[] t = table; while (index \u0026lt; t.length \u0026amp;\u0026amp; (next = t[index++]) == null) ; } } 在迭代过程中，判断 modCount 跟 expectedModCount 是否相等，如果不相等就表示已经有其他线程修改了 Map：modCount 声明为 volatile，保证线程之间修改的可见性。\nfinal Entry\u0026lt;K,V\u0026gt; nextEntry() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } fail-fast 机制是一种错误检测机制, 它只能被用来检测错误，因为 JDK 并不保证 fail-fast 机制一定会发生。若在多线程环境下使用 fail-fast 机制的集合，建议使用java.util.concurrent包下的类。\nHashMap 的遍历方式: 使用entrySet()的遍历效率较高\nMap map = new HashMap(); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) { Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object val = entry.getValue(); } 什么wrapper类适合作为键 String, Interger. 因为不可变性质, String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。\n不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能\n可以使用自定义的对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。\nHashMap和HashTable的比较 和Hash Table的假设一样，假定哈希函数将元素适当地分布在各区之间，可为基本操作（get 和 put）提供稳定的性能。\n跟 Hash Table 相比， Hash Map的作者多了 Doug Lea。他写了util.concurrent包。著有并发编程Concurrent Programming in Java: Design Principles and Patterns 一书。\nHashtable是java一开始发布时就提供的键值映射的数据结构，但是现在Hashtable基本上已经被弃用了。而产生于JDK1.2的HashMap已经成为应用最为广泛的一种数据类型了。造成这样的原因是因为Hashtable是线程安全的（同步的），效率比较低。\n继承的父类不一样 HashMap是继承自AbstractMap类，而HashTable是继承自Dictionary类(Dictionary类已经被废弃)。不过它们都实现了同时实现了map、Cloneable、Serializable这三个接口\n对外提供的接口不同 HashMap比Hashtable少了elments()和contains()两个方法。HashMap 有containsKEY()和containsValue(), 事实上，contansValue()就是调用了contains()方法。\n对Null的支持不同 Hashtable既不支持Null key也不支持Null value。HashMap中，null可以作为键，这样的键只有一个；值可以为null且不做数量限制。当get()方法返回null时，可能是 HashMap 中没有该键，也可能使该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键，而应该用containsKey()方法来判断。\n线程安全性不同 Hashtable是线程安全的，它的每个方法中都加入了Synchronize方法。在多线程并发的环境下，可以直接使用Hashtable，不需要自己为它的方法实现同步. Hashmap 不是同步的(不是线程安全)，如果多个线程同时访问一个 HashMap，而其中至少一个线程从结构上（指添加或者删除一个或多个映射关系的任何操作）修改了，则必须保持外部同步，以防止对映射进行意外的非同步访问。\n由于Hashtable是线程安全的也是synchronized，所以在单线程环境下它比HashMap要慢。如果不需要同步，只需要单一线程，那么使用HashMap性能要好过Hashtable。\n初始容量和扩容不同 Hashtable默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap默认的初始化大小为16。之后每次扩充，容量变为原来的2n。\n创建时，如果给定了容量初始值，那么Hashtable会直接使用你给定的大小，而HashMap会将其扩充为2的幂次方大小。也就是说Hashtable会尽量使用素数、奇数。而HashMap则总是使用2的幂作为哈希表的大小。\n之所以会有这样的不同，是因为Hashtable和HashMap设计时的侧重点不同。Hashtable的侧重点是哈希的结果更加均匀，使得哈希冲突减少。当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀。而HashMap则更加关注hash的计算效率问题。在取模计算时，如果模数是2的幂，那么我们可以直接使用位运算来得到结果，效率要大大高于做除法。HashMap为了加快hash的速度，将哈希表的大小固定为了2的幂。当然这引入了哈希分布不均匀的问题，所以HashMap为解决这问题，又对hash算法做了一些改动。这从而导致了Hashtable和HashMap的计算hash值的方法不同\n计算hash的方法不同 Hashtable直接使用对象的hashCode。hashCode是JDK根据对象的地址或者字符串或者数字算出来的int类型的数值。然后再使用取余数获得最终的位置。\nint hash = key.hashCode(); int index = (hash \u0026amp; 0x7FFFFFFF) % tab.length; Hashtable在计算元素的位置时需要进行一次除法运算，而除法运算是比较耗时的。\nHashMap为了提高计算效率，将哈希表的大小固定为了2的幂，这样在取模预算时，不需要做除法，只需要做位运算。位运算比除法的效率要高很多。这样效率虽然提高了，但是hash冲突却也增加了。因为它得出的hash值的低位相同的概率比较高，为了解决这个问题，HashMap重新根据hashcode计算hash值后，又对hash值做了一些运算来打散数据。使得取得的位置更加分散，从而减少了hash冲突。当然了，为了高效，HashMap只做了一些简单的位处理。从而不至于把使用2 的幂次方带来的效率提升给抵消掉。\nstatic final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 参考资料 ","permalink":"https://congchan.github.io/posts/java-hashmap/","summary":"\u003ch2 id=\"hashmap\"\u003eHashMap\u003c/h2\u003e\n\u003cp\u003eHashMap 是基于哈希表的 Map 接口的非同步实现。此实现提供所有可选的映射操作，并允许使用 null 值和 null 键。此类不保证映射的顺序，特别是它不保证该顺序不是stable的。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eHashMap 底层就是一个数组结构，数组中的每一项又是一个链表。\u003cimg loading=\"lazy\" src=\"/images/hash_map.png\" title=\"image from: https://algs4.cs.princeton.edu/\"\u003e\n当新建一个 HashMap 的时候，就会初始化一个数组\u003ccode\u003etable = new Entry[capacity];\u003c/code\u003e, 每个 \u003ccode\u003eEntry\u003c/code\u003e 是一个 key-value 对，有一个指向下一个元素的引用，这就构成了链表。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eHashMap\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003einitialCapacity\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003efloat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eloadFactor\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einitialCapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eIllegalArgumentException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;Illegal initial capacity: \u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                                           \u003c/span\u003e\u003cspan class=\"n\"\u003einitialCapacity\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einitialCapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMAXIMUM_CAPACITY\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003einitialCapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMAXIMUM_CAPACITY\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eloadFactor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e||\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eFloat\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eisNaN\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eloadFactor\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eIllegalArgumentException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;Illegal load factor: \u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                                           \u003c/span\u003e\u003cspan class=\"n\"\u003eloadFactor\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Find a power of 2 \u0026gt;= initialCapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003einitialCapacity\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ecapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u0026lt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eloadFactor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eloadFactor\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"n\"\u003eMath\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eloadFactor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMAXIMUM_CAPACITY\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003etable\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ecapacity\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003euseAltHashing\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esun\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emisc\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eVM\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eisBooted\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eHolder\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eALTERNATIVE_HASHING_THRESHOLD\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enext\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ehash\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"err\"\u003e……\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHashMap 底层数组的长度总是 2 的 n 次方，这是 HashMap 在速度上的优化, 所以有这段代码保证初始化时 HashMap 的容量总是 2 的 n 次方，即底层数组的长度总是为 2 的 n 次方。\u003c/p\u003e","title":"Java HashMap"},{"content":"LinkedHashMap HashMap 是无序的，HashMap 在 put 的时候是根据 key 的 hashcode 进行 hash 然后放入对应的地方。所以在按照一定顺序 put 进 HashMap 中，然后遍历出 HashMap 的顺序跟 put 的顺序不同. JAVA 在 JDK1.4 以后提供了 LinkedHashMap 来实现有序的 HashMap！\nLinkedHashMap 是 HashMap 的一个子类，它保留插入的顺序，如果需要输出的顺序和输入时的相同，那么就选用 LinkedHashMap。LinkedHashMap 是 Map 接口的哈希表和链表数组实现，具有可预知的迭代顺序。此实现提供所有可选的映射操作，并允许使用 null 值和 null 键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。\npublic class LinkedHashMap\u0026lt;K,V\u0026gt; extends HashMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt; {} LinkedHashMap 采用的 hash 算法和 HashMap 相同，但是它重新定义了数组中保存的元素 Entry，该 Entry 除了保存当前对象的引用外，还保存了其上一个元素 before 和下一个元素 after 的引用，从而在哈希表的基础上又构成了双向链表。迭代顺序可以是插入顺序或者是访问顺序。\n/** * The iteration ordering method for this linked hash map: \u0026lt;tt\u0026gt;true\u0026lt;/tt\u0026gt; * for access-order, \u0026lt;tt\u0026gt;false\u0026lt;/tt\u0026gt; for insertion-order. * 如果为true，则按照访问顺序；如果为false，则按照插入顺序。 */ private final boolean accessOrder; /** * 双向链表的表头元素。 */ private transient Entry\u0026lt;K,V\u0026gt; header; /** * LinkedHashMap的Entry元素。 * 继承HashMap的Entry元素，又保存了其上一个元素before和下一个元素after的引用。 */ private static class Entry\u0026lt;K,V\u0026gt; extends HashMap.Entry\u0026lt;K,V\u0026gt; { Entry\u0026lt;K,V\u0026gt; before, after; …… } 根据链表中元素的顺序可以分为：按插入顺序的链表，和按访问顺序(调用 get 方法)的链表。默认是按插入顺序排序，如果指定按访问顺序排序，那么调用get方法后，会将这次访问的元素移至链表尾部，不断访问可以形成按访问顺序排序的链表。\n注意，此实现不是同步的。如果多个线程同时访问链接的哈希映射，而其中至少一个线程从结构上修改了该映射，则它必须保持外部同步。\n初始化 在 LinkedHashMap 的构造方法中，实际调用了父类 HashMap 的相关构造方法来构造一个底层存放的 table 数组，但额外可以增加 accessOrder 这个参数，如果不设置，默认为 false，代表按照插入顺序进行迭代；显式设置为 true，代表以访问顺序进行迭代。\npublic LinkedHashMap(int initialCapacity, float loadFactor,boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder; } LinkedHashMap 重写了 init() 方法，在调用父类的构造方法完成构造后，进一步实现了对其元素 Entry 的初始化操作。\n/** * Called by superclass constructors and pseudoconstructors (clone, * readObject) before any entries are inserted into the map. Initializes * the chain. */ @Override void init() { header = new Entry\u0026lt;\u0026gt;(-1, null, null, null); header.before = header.after = header; } 存储 LinkedHashMap 并未重写父类 HashMap 的 put 方法，而是重写了父类 HashMap 的 put 方法调用的子方法void recordAccess(HashMap m)，void addEntry(int hash, K key, V value, int bucketIndex) 和void createEntry(int hash, K key, V value, int bucketIndex)，提供了自己特有的双向链接列表的实现。\npublic V put(K key, V value) { if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } 重写方法\nvoid recordAccess(HashMap\u0026lt;K,V\u0026gt; m) { LinkedHashMap\u0026lt;K,V\u0026gt; lm = (LinkedHashMap\u0026lt;K,V\u0026gt;)m; if (lm.accessOrder) { lm.modCount++; remove(); addBefore(lm.header); } } void addEntry(int hash, K key, V value, int bucketIndex) { // 调用create方法，将新元素以双向链表的的形式加入到映射中。 createEntry(hash, key, value, bucketIndex); // 删除最近最少使用元素的策略定义 Entry\u0026lt;K,V\u0026gt; eldest = header.after; if (removeEldestEntry(eldest)) { removeEntryForKey(eldest.key); } else { if (size \u0026gt;= threshold) resize(2 * table.length); } } void createEntry(int hash, K key, V value, int bucketIndex) { HashMap.Entry\u0026lt;K,V\u0026gt; old = table[bucketIndex]; Entry\u0026lt;K,V\u0026gt; e = new Entry\u0026lt;K,V\u0026gt;(hash, key, value, old); table[bucketIndex] = e; // 调用元素的addBrefore方法，将元素加入到哈希、双向链接列表。 e.addBefore(header); size++; } private void addBefore(Entry\u0026lt;K,V\u0026gt; existingEntry) { after = existingEntry; before = existingEntry.before; before.after = this; after.before = this; } 读取 LinkedHashMap 重写了父类 HashMap 的 get 方法，实际在调用父类 getEntry() 方法取得查找的元素后，再判断当排序模式 accessOrder 为 true 时，记录访问顺序，将最新访问的元素添加到双向链表的表头，并从原来的位置删除。由于的链表的增加、删除操作是常量级的，故并不会带来性能的损失。\npublic V get(Object key) { // 调用父类HashMap的getEntry()方法，取得要查找的元素。 Entry\u0026lt;K,V\u0026gt; e = (Entry\u0026lt;K,V\u0026gt;)getEntry(key); if (e == null) return null; // 记录访问顺序。 e.recordAccess(this); return e.value; } void recordAccess(HashMap\u0026lt;K,V\u0026gt; m) { LinkedHashMap\u0026lt;K,V\u0026gt; lm = (LinkedHashMap\u0026lt;K,V\u0026gt;)m; // 如果定义了LinkedHashMap的迭代顺序为访问顺序， // 则删除以前位置上的元素，并将最新访问的元素添加到链表表头。 if (lm.accessOrder) { lm.modCount++; remove(); addBefore(lm.header); } } /** * Removes this entry from the linked list. */ private void remove() { before.after = after; after.before = before; } /**clear链表，设置header为初始状态*/ public void clear() { super.clear(); header.before = header.after = header; } 排序模式 LinkedHashMap 定义了排序模式 accessOrder，该属性为 boolean 型变量， 对于访问顺序，为 true； 对于插入顺序，则为 false(默认值)。\n这些构造方法都会默认指定排序模式为插入顺序。如果你想构造一个 LinkedHashMap，并打算按从近期访问最少到近期访问最多的顺序（即访问顺序）来保存元素，那么请使用下面的构造方法构造 LinkedHashMap：public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder)\n该哈希映射的迭代顺序就是最后访问其条目的顺序，这种映射很适合构建 LRU 缓存。LinkedHashMap 提供了 removeEldestEntry(Map.Entry\u0026lt;K,V\u0026gt; eldest) 方法。该方法可以提供在每次添加新条目时移除最旧条目的实现程序，默认返回 false，这样，此映射的行为将类似于正常映射，即永远不能移除最旧的元素。\n","permalink":"https://congchan.github.io/posts/java-linkedhashmap/","summary":"\u003ch2 id=\"linkedhashmap\"\u003eLinkedHashMap\u003c/h2\u003e\n\u003cp\u003eHashMap 是无序的，HashMap 在 put 的时候是根据 key 的 hashcode 进行 hash 然后放入对应的地方。所以在按照一定顺序 put 进 HashMap 中，然后遍历出 HashMap 的顺序跟 put 的顺序不同.\nJAVA 在 JDK1.4 以后提供了 LinkedHashMap 来实现有序的 HashMap！\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003eLinkedHashMap 是 HashMap 的一个子类，它保留插入的顺序，如果需要输出的顺序和输入时的相同，那么就选用 LinkedHashMap。LinkedHashMap 是 Map 接口的哈希表和链表数组实现，具有可预知的迭代顺序。此实现提供所有可选的映射操作，并允许使用 null 值和 null 键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eLinkedHashMap\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eHashMap\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMap\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eLinkedHashMap 采用的 hash 算法和 HashMap 相同，但是它重新定义了数组中保存的元素 Entry，该 Entry 除了保存当前对象的引用外，还保存了其上一个元素 before 和下一个元素 after 的引用，从而在哈希表的基础上又构成了双向链表。迭代顺序可以是插入顺序或者是访问顺序。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e* The iteration ordering method for this linked hash map: \u0026lt;tt\u0026gt;true\u0026lt;/tt\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e* for access-order, \u0026lt;tt\u0026gt;false\u0026lt;/tt\u0026gt; for insertion-order.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e* 如果为true，则按照访问顺序；如果为false，则按照插入顺序。\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e*/\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eaccessOrder\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e* 双向链表的表头元素。\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003etransient\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eheader\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e* LinkedHashMap的Entry元素。\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e* 继承HashMap的Entry元素，又保存了其上一个元素before和下一个元素after的引用。\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eHashMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ebefore\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eafter\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"err\"\u003e……\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e根据链表中元素的顺序可以分为：按插入顺序的链表，和按访问顺序(调用 get 方法)的链表。默认是按插入顺序排序，如果指定按访问顺序排序，那么调用get方法后，会将这次访问的元素移至链表尾部，不断访问可以形成按访问顺序排序的链表。\u003c/p\u003e","title":"Java LinkedHashMap"},{"content":"LRU缓存 用缓存来存放之前读取过的数据，这样，再次读取的时候，可以直接在缓存里面取，而不用再重新查找一遍，这样系统的反应能力会有很大提高。但是，当我们读取的个数特别大的时候，我们不可能把所有已经读取的数据都放在缓存里，毕竟内存大小是一定的，所以一般把最近常读取的放在缓存里。\nLRU(Least Recently Used)缓存利用了这样的一种思想, 把最新读取的数据放在最前面，缓存中存储的是读取最频繁的数据，以能够提高系统的性能。\nLinkedHashMap实现LRU LinkedHashMap支持按照访问顺序的存储，也就是说，最近读取的会放在最前面，最不常读取的会放在最后。其次，LinkedHashMap 有一个方法用于判断是否需要移除最不常读取的数，原始方法默认不需要移除，所以，LRU 需要 override 这个方法，使得当缓存里存放的数据个数超过规定个数后，就把最不常用的移除掉。\nimport java.util.LinkedHashMap; import java.util.Collection; import java.util.Map; import java.util.ArrayList; /** * An LRU cache, based on \u0026lt;code\u0026gt;LinkedHashMap\u0026lt;/code\u0026gt;. * * \u0026lt;p\u0026gt; * This cache has a fixed maximum number of elements (\u0026lt;code\u0026gt;cacheSize\u0026lt;/code\u0026gt;). * If the cache is full and another entry is added, the LRU (least recently * used) entry is dropped. * * \u0026lt;p\u0026gt; * This class is thread-safe. All methods of this class are synchronized. * * \u0026lt;p\u0026gt; * Author: Christian d\u0026#39;Heureuse, Inventec Informatik AG, Zurich, Switzerland\u0026lt;br\u0026gt; * Multi-licensed: EPL / LGPL / GPL / AL / BSD. */ public class LRUCache\u0026lt;K, V\u0026gt; { private static final float hashTableLoadFactor = 0.75f; private LinkedHashMap\u0026lt;K, V\u0026gt; map; private int cacheSize; /** * Creates a new LRU cache. 在该方法中，new LinkedHashMap\u0026lt;K,V\u0026gt;(hashTableCapacity, * hashTableLoadFactor, true)中，true代表使用访问顺序 * * @param cacheSize * the maximum number of entries that will be kept in this cache. */ public LRUCache(int cacheSize) { this.cacheSize = cacheSize; int hashTableCapacity = (int) Math .ceil(cacheSize / hashTableLoadFactor) + 1; map = new LinkedHashMap\u0026lt;K, V\u0026gt;(hashTableCapacity, hashTableLoadFactor, true) { // (an anonymous inner class) private static final long serialVersionUID = 1; @Override protected boolean removeEldestEntry(Map.Entry\u0026lt;K, V\u0026gt; eldest) { return size() \u0026gt; LRUCache.this.cacheSize; } }; } /** * Retrieves an entry from the cache.\u0026lt;br\u0026gt; * The retrieved entry becomes the MRU (most recently used) entry. * * @param key * the key whose associated value is to be returned. * @return the value associated to this key, or null if no value with this * key exists in the cache. */ public synchronized V get(K key) { return map.get(key); } /** * Adds an entry to this cache. The new entry becomes the MRU (most recently * used) entry. If an entry with the specified key already exists in the * cache, it is replaced by the new entry. If the cache is full, the LRU * (least recently used) entry is removed from the cache. * * @param key * the key with which the specified value is to be associated. * @param value * a value to be associated with the specified key. */ public synchronized void put(K key, V value) { map.put(key, value); } /** * Clears the cache. */ public synchronized void clear() { map.clear(); } /** * Returns the number of used entries in the cache. * * @return the number of entries currently in the cache. */ public synchronized int usedEntries() { return map.size(); } /** * Returns a \u0026lt;code\u0026gt;Collection\u0026lt;/code\u0026gt; that contains a copy of all cache * entries. * * @return a \u0026lt;code\u0026gt;Collection\u0026lt;/code\u0026gt; with a copy of the cache content. */ public synchronized Collection\u0026lt;Map.Entry\u0026lt;K, V\u0026gt;\u0026gt; getAll() { return new ArrayList\u0026lt;Map.Entry\u0026lt;K, V\u0026gt;\u0026gt;(map.entrySet()); } // Test routine for the LRUCache class. public static void main(String[] args) { LRUCache\u0026lt;String, String\u0026gt; c = new LRUCache\u0026lt;String, String\u0026gt;(3); c.put(\u0026#34;1\u0026#34;, \u0026#34;one\u0026#34;); // 1 c.put(\u0026#34;2\u0026#34;, \u0026#34;two\u0026#34;); // 2 1 c.put(\u0026#34;3\u0026#34;, \u0026#34;three\u0026#34;); // 3 2 1 c.put(\u0026#34;4\u0026#34;, \u0026#34;four\u0026#34;); // 4 3 2 if (c.get(\u0026#34;2\u0026#34;) == null) throw new Error(); // 2 4 3 c.put(\u0026#34;5\u0026#34;, \u0026#34;five\u0026#34;); // 5 2 4 c.put(\u0026#34;4\u0026#34;, \u0026#34;second four\u0026#34;); // 4 5 2 // Verify cache content. if (c.usedEntries() != 3) throw new Error(); if (!c.get(\u0026#34;4\u0026#34;).equals(\u0026#34;second four\u0026#34;)) throw new Error(); if (!c.get(\u0026#34;5\u0026#34;).equals(\u0026#34;five\u0026#34;)) throw new Error(); if (!c.get(\u0026#34;2\u0026#34;).equals(\u0026#34;two\u0026#34;)) throw new Error(); // List cache content. for (Map.Entry\u0026lt;String, String\u0026gt; e : c.getAll()) System.out.println(e.getKey() + \u0026#34; : \u0026#34; + e.getValue()); } } ","permalink":"https://congchan.github.io/posts/java-linkedhashmap%E5%92%8Clrucache/","summary":"\u003ch2 id=\"lru缓存\"\u003eLRU缓存\u003c/h2\u003e\n\u003cp\u003e用缓存来存放之前读取过的数据，这样，再次读取的时候，可以直接在缓存里面取，而不用再重新查找一遍，这样系统的反应能力会有很大提高。但是，当我们读取的个数特别大的时候，我们不可能把所有已经读取的数据都放在缓存里，毕竟内存大小是一定的，所以一般把最近常读取的放在缓存里。\u003c/p\u003e\n\u003cp\u003eLRU(Least Recently Used)缓存利用了这样的一种思想, 把最新读取的数据放在最前面，缓存中存储的是读取最频繁的数据，以能够提高系统的性能。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"linkedhashmap实现lru\"\u003eLinkedHashMap实现LRU\u003c/h3\u003e\n\u003cp\u003eLinkedHashMap支持按照访问顺序的存储，也就是说，最近读取的会放在最前面，最不常读取的会放在最后。其次，LinkedHashMap 有一个方法用于判断是否需要移除最不常读取的数，原始方法默认不需要移除，所以，LRU 需要 override 这个方法，使得当缓存里存放的数据个数超过规定个数后，就把最不常用的移除掉。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nn\"\u003ejava.util.LinkedHashMap\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nn\"\u003ejava.util.Collection\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nn\"\u003ejava.util.Map\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nn\"\u003ejava.util.ArrayList\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * An LRU cache, based on \u0026lt;code\u0026gt;LinkedHashMap\u0026lt;/code\u0026gt;.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e *\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * \u0026lt;p\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * This cache has a fixed maximum number of elements (\u0026lt;code\u0026gt;cacheSize\u0026lt;/code\u0026gt;).\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * If the cache is full and another entry is added, the LRU (least recently\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * used) entry is dropped.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e *\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * \u0026lt;p\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * This class is thread-safe. All methods of this class are synchronized.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e *\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * \u0026lt;p\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * Author: Christian d\u0026#39;Heureuse, Inventec Informatik AG, Zurich, Switzerland\u0026lt;br\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * Multi-licensed: EPL / LGPL / GPL / AL / BSD.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eLRUCache\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003efloat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ehashTableLoadFactor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003e75f\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eLinkedHashMap\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecacheSize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * Creates a new LRU cache. 在该方法中，new LinkedHashMap\u0026lt;K,V\u0026gt;(hashTableCapacity,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * hashTableLoadFactor, true)中，true代表使用访问顺序\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * @param cacheSize\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *            the maximum number of entries that will be kept in this cache.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eLRUCache\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecacheSize\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ecacheSize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ecacheSize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ehashTableCapacity\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMath\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eceil\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecacheSize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ehashTableLoadFactor\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eLinkedHashMap\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ehashTableCapacity\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ehashTableLoadFactor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"c1\"\u003e// (an anonymous inner class)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003elong\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eserialVersionUID\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"kd\"\u003eprotected\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eremoveEldestEntry\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eeldest\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e                \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eLRUCache\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ecacheSize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e};\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * Retrieves an entry from the cache.\u0026lt;br\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * The retrieved entry becomes the MRU (most recently used) entry.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * @param key\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *            the key whose associated value is to be returned.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * @return the value associated to this key, or null if no value with this\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *         key exists in the cache.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003esynchronized\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * Adds an entry to this cache. The new entry becomes the MRU (most recently\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * used) entry. If an entry with the specified key already exists in the\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * cache, it is replaced by the new entry. If the cache is full, the LRU\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * (least recently used) entry is removed from the cache.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * @param key\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *            the key with which the specified value is to be associated.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * @param value\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *            a value to be associated with the specified key.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003esynchronized\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * Clears the cache.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003esynchronized\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eclear\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eclear\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * Returns the number of used entries in the cache.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * @return the number of entries currently in the cache.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003esynchronized\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eusedEntries\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * Returns a \u0026lt;code\u0026gt;Collection\u0026lt;/code\u0026gt; that contains a copy of all cache\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * entries.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     *\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     * @return a \u0026lt;code\u0026gt;Collection\u0026lt;/code\u0026gt; with a copy of the cache content.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e     */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003esynchronized\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eCollection\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003egetAll\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eArrayList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eentrySet\u003c/span\u003e\u003cspan class=\"p\"\u003e());\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Test routine for the LRUCache class.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eLRUCache\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eLRUCache\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;one\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 1\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;2\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;two\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 2 1\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;3\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;three\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 3 2 1\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;4\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;four\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 4 3 2\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;2\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enull\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eError\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 2 4 3\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;5\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;five\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 5 2 4\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;4\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;second four\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 4 5 2\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"c1\"\u003e// Verify cache content.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eusedEntries\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e!=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eError\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;4\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"na\"\u003eequals\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;second four\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eError\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;5\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"na\"\u003eequals\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;five\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eError\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;2\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"na\"\u003eequals\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;two\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eError\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"c1\"\u003e// List cache content.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eEntry\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003egetAll\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003egetKey\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34; : \u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003egetValue\u003c/span\u003e\u003cspan class=\"p\"\u003e());\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Java LinkedHashMap和LRUcache"},{"content":"在Java中，JVM负责内存动态分配和垃圾回收的问题。Java的对象必须要有引用才能被使用，也就是说如果要操作对象，必须通过引用来进行。如果一个对象唯一的引用变量死了（随着堆栈块一起解散），对象就会被认定为可被垃圾回收（Garbage Collection）的。没有被引用的对象，是没有存在意义的，因为没有人知道它的地址，无法调用它，它的存在只会浪费空间。\n目前内存的动态分配和内存回收技术已经相当成熟，但还是需要了解GC和内存分配，这样当需要排查各种内存溢出、内存泄漏问题时，当GC成为系统达到更高并发量的瓶颈时，需要对这些自动化的技术实施必要的监控和调节。\n判断对象是否可回收 最简单的方法是通过引用计数（reference counting）来判断一个对象是否可以被回收。不失一般性，如果一个对象没有任何引用与之关联，那么这个对象就被判定为可被回收的对象了。这种方式实现简单，而且效率较高，但是它无法解决对象间循环引用的问题，因此在Java中并没有采用这种方式（Python采用的是引用计数法）。\n主流的商用程序语言（Java，C#，Lisp）的主流实现中，使用可达性分析（reachability analysis）来判定对象是否存活。\n使用一些列GC Root对象作为起始点，从这些节点开始往下沿着引用链搜索，如果GC Root到某个对象无法通过任何引用链项链，则该对象会被标记一次, 并且进行一次筛选, 筛选的条件是此对象是否有必要执行finalize()方法. 当对象没有覆盖finalize()方法, 或者该方法已经被JVM调用过, JVM都会视之为没有必要执行finalize()。 如果该对象被判定为有必要执行finalize()方法, 那么这个对象会被放置在F-Queue队列中, 并在稍后由一个由JVM自动建立的, 低优先级的Finalizer线程去执行. 执行只是触发该方法, 但不会等待它结束, 因为可能会有执行缓慢或者死循环等特殊情况 稍后GC会动F-Queue里的对象进行第二次标记, 如果对象要在finalize()中避免被消灭, 只需要重新与引用链上的任何一个对象建立关联即可(finalize()的优先级较低), 这样第二次标记时它将不会被考虑. 否则就只能被回收. 要注意, 任何一个对象的finalize()只会被系统自动调用一次, 下次再GC时不会再执行, 也就是只有一次自救机会.\n在Java中，可作为GC Roots的对象包括：\n虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中native方法引用的对象 引用 引用分为强引用，软引用，弱引用，虚引用。根据不同引用，有不同的GC回收策略。\n强引用：类似Object obj = new Object();这种, 垃圾回收器永远不会回收他们\n软引用：非必须引用，内存溢出之前进行回收. 如果这次回收后内存还不足,才会抛出内存溢出异常\nObject obj = new Object(); SoftReference\u0026lt;Object\u0026gt; sf = new SoftReference\u0026lt;Object\u0026gt;(obj); obj = null; sf.get();//有时候会返回null 软引用主要用户实现类似缓存的功能，在内存足够的情况下直接通过软引用取值，无需从繁忙的真实来源查询数据，提升速度；当内存不足时，自动删除这部分缓存数据，从真正的来源查询这些数据。\n弱引用：非必须引用, 强度比软引用更弱. 当GC工作时, 无论当前内存是否足够, 都会回收掉只有弱引用关联的对象.\nObject obj = new Object(); WeakReference\u0026lt;Object\u0026gt; wf = new WeakReference\u0026lt;Object\u0026gt;(obj); obj = null; wf.get();//有时候会返回null wf.isEnQueued();//返回是否被垃圾回收器标记为即将回收的垃圾 虚引用：幽灵引用， 最弱的引用关系。无法通过虚引用取到实例，为一个对象设置虚引用关联的唯一目的是在这个对象被收集器回收时收到一个系统通知。\nObject obj = new Object(); PhantomReference\u0026lt;Object\u0026gt; pf = new PhantomReference\u0026lt;Object\u0026gt;(obj); obj=null; pf.get();//永远返回null pf.isEnQueued();//返回是否从内存中已经删除 GC收集算法 标记／清除算法 Mask-Sweep 分为两个阶段：标记阶段和清除阶段。标记阶段的任务是标记出所有需要被回收的对象，清除阶段就是回收被标记的对象所占用的空间。\n缺点：效率低；释放空间不连续容易导致内存碎片；会停止整个程序运行；\n标记-整理算法 Mark-Compact 该算法标记阶段和Mark-Sweep一样，但是在完成标记之后，它不是直接清理可回收对象，而是将存活对象都向一端移动，然后清理掉端边界以外的内存。\n复制算法 将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用的内存空间一次清理掉，这样一来就不容易出现内存碎片的问题。这种算法对内存空间的使用做了一半的牺牲，效率跟存活对象的数目多少有很大的关系，如果存活对象很多，那么Copying算法的效率将会大大降低。\n现在的商业虚拟机，基于IBM的研究假设：新生代中的对象98%是朝生暮死的，所以并不需要按照1:1划分内存，而是按照一定的比例把内存空间划分为一块较大的Eden空间（80%）和两块较小的Survivor空间（10%）， 每次使用Eden和其中一个Survivor。所有对象创建在新生代的Eden区，当Eden区满后触发新生代的Minor GC，将Eden区和非空闲Survivor区存活的对象复制到另外一个空闲的Survivor区中，然后再清理掉原先的空间。这样保证其中一个Survivor区是空的，新生代Minor GC就是在两个Survivor区之间相互复制存活对象，直到Survivor区满为止。\n分代收集算法 Generational Collection 是目前大部分JVM的垃圾收集器采用的算法。根据对象存活的生命周期将内存划分为若干个不同的区域。一般情况下将堆区划分为老年代（Tenured Generation）和新生代（Young Generation）。老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象死亡，那么就可以根据不同代的特点采取最适合的收集算法。\n目前大部分垃圾收集器对于新生代都采取Copying算法，因为新生代中每次垃圾回收都要回收大部分对象，也就是说需要复制的操作次数较少。\n老年代的特点是每次回收都只回收少量对象，一般使用的是Mark-Compact或者Mask-Sweep算法。当新生代Survivor区也满了之后就通过Minor GC将对象复制到老年代。老年代也满了的话，就将触发Full GC，针对整个堆（包括新生代、老年代、持久代）进行垃圾回收。\n堆区之外还有一个永久代（Permanet Generation），它用来存储class类、常量、方法描述等。对永久代的回收主要回收两部分内容：废弃常量和无用的类。持久代如果满了，将触发Full GC。\n垃圾收集器 垃圾收集算法是内存回收的理论基础，而垃圾收集器就是内存回收的具体实现。JDK 7的HotSpot虚拟机提供多种垃圾收集器，可以需求组合出各个年代使用的收集器。\nSerial/Serial Old：最基本最古老的收集器，它是一个单线程收集器，并且在它进行垃圾收集时，必须暂停所有用户线程。Serial收集器是针对新生代的收集器，采用的是Copying算法，Serial Old收集器是针对老年代的收集器，采用的是Mark-Compact算法。它的优点是实现简单高效，但是缺点是会给用户带来停顿。 ParNew收集器是Serial收集器的多线程版本，使用多个线程进行垃圾收集。 Parallel Scavenge收集器是一个新生代的多线程收集器（并行收集器），它在回收期间不需要暂停其他用户线程，其采用的是Copying算法，该收集器与前两个收集器有所不同，它主要是为了达到一个可控的吞吐量。 Parallel Old是Parallel Scavenge收集器的老年代版本（并行收集器），使用多线程和Mark-Compact算法。 Current Mark Sweep（CMS）收集器是一种以获取最短回收停顿时间为目标的收集器，它是一种并发收集器，采用的是Mark-Sweep算法。 G1收集器是当今收集器技术发展最前沿的成果，它是一款面向服务端应用的收集器，它能充分利用多CPU、多核环境。因此它是一款并行与并发收集器，并且它能建立可预测的停顿时间模型。 内存分配 对象的内存分配，总的来说就是在堆上分配，对象主要分配在新生代的Eden Space和From Space，少数情况下会直接分配在老年代。\n对象优先在Eden分配。\n如果新生代的Eden Space和From Space的空间不足，则会发起一次Minor GC。 在GC的过程中，会将Eden Space和From Space中的存活对象移动到To Space，然后将Eden Space和From Space进行清理。 如果在清理的过程中，To Space无法足够来存储某个对象，就会将该对象移动到老年代中。 如果进行了GC之后，Eden Space和From Space能够容纳该对象就放在Eden Space和From Space。下次GC时会将存活对象复制到From Space，如此反复循环。 一般来说，大对象会被直接分配到老年代，所谓的大对象是指需要大量连续存储空间的对象，最常见的一种大对象就是大数组，比如：byte[] data = new byte[4*1024*1024];这种一般会直接在老年代分配存储空间。\n长期存活的对象进入老年代。虚拟机给每个对象定义了一个对象年龄计数器，如对象在Survivor区躲过一次GC的话，其对象年龄便会加1，默认情况下，如果对象年龄达到15岁，就会移动到老年代中。阈值可以通过-XX : MaxTenuringThreshold设置.\n如果在Survivor空间中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就直接进入老年代，不必等到MaxTenuringThreshold\n当然分配的规则并不是百分之百固定的，这要取决于当前使用的是哪种垃圾收集器组合和JVM的相关参数。\n参考资料 深入理解Java虚拟机 Java垃圾回收机制 ","permalink":"https://congchan.github.io/posts/java-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/","summary":"\u003cp\u003e在Java中，JVM负责内存动态分配和垃圾回收的问题。Java的对象必须要有引用才能被使用，也就是说如果要操作对象，必须通过引用来进行。如果一个对象唯一的引用变量死了（随着堆栈块一起解散），对象就会被认定为可被垃圾回收（Garbage Collection）的。没有被引用的对象，是没有存在意义的，因为没有人知道它的地址，无法调用它，它的存在只会浪费空间。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e目前内存的动态分配和内存回收技术已经相当成熟，但还是需要了解GC和内存分配，这样当需要排查各种内存溢出、内存泄漏问题时，当GC成为系统达到更高并发量的瓶颈时，需要对这些自动化的技术实施必要的监控和调节。\u003cimg loading=\"lazy\" src=\"/images/jvm.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"判断对象是否可回收\"\u003e判断对象是否可回收\u003c/h3\u003e\n\u003cp\u003e最简单的方法是通过引用计数（reference counting）来判断一个对象是否可以被回收。不失一般性，如果一个对象没有任何引用与之关联，那么这个对象就被判定为可被回收的对象了。这种方式实现简单，而且效率较高，但是它无法解决对象间循环引用的问题，因此在Java中并没有采用这种方式（Python采用的是引用计数法）。\u003c/p\u003e\n\u003cp\u003e主流的商用程序语言（Java，C#，Lisp）的主流实现中，使用可达性分析（reachability analysis）来判定对象是否存活。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e使用一些列GC Root对象作为起始点，从这些节点开始往下沿着引用链搜索，如果GC Root到某个对象无法通过任何引用链项链，则该对象会被标记一次, 并且进行一次筛选, 筛选的条件是此对象是否有必要执行\u003ccode\u003efinalize()\u003c/code\u003e方法.\u003c/li\u003e\n\u003cli\u003e当对象没有覆盖\u003ccode\u003efinalize()\u003c/code\u003e方法, 或者该方法已经被JVM调用过, JVM都会视之为没有必要执行\u003ccode\u003efinalize()\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e如果该对象被判定为有必要执行\u003ccode\u003efinalize()\u003c/code\u003e方法, 那么这个对象会被放置在F-Queue队列中, 并在稍后由一个由JVM自动建立的, 低优先级的Finalizer线程去执行. 执行只是触发该方法, 但不会等待它结束, 因为可能会有执行缓慢或者死循环等特殊情况\u003c/li\u003e\n\u003cli\u003e稍后GC会动F-Queue里的对象进行第二次标记, 如果对象要在\u003ccode\u003efinalize()\u003c/code\u003e中避免被消灭, 只需要重新与引用链上的任何一个对象建立关联即可(\u003ccode\u003efinalize()\u003c/code\u003e的优先级较低), 这样第二次标记时它将不会被考虑. 否则就只能被回收.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e要注意, 任何一个对象的\u003ccode\u003efinalize()\u003c/code\u003e只会被系统自动调用一次, 下次再GC时不会再执行, 也就是只有一次自救机会.\u003c/p\u003e\n\u003cp\u003e在Java中，可作为GC Roots的对象包括：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e虚拟机栈（栈帧中的本地变量表）中引用的对象。\u003c/li\u003e\n\u003cli\u003e方法区中类静态属性引用的对象\u003c/li\u003e\n\u003cli\u003e方法区中常量引用的对象\u003c/li\u003e\n\u003cli\u003e本地方法栈中native方法引用的对象\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"引用\"\u003e引用\u003c/h3\u003e\n\u003cp\u003e引用分为强引用，软引用，弱引用，虚引用。根据不同引用，有不同的GC回收策略。\u003c/p\u003e\n\u003cp\u003e强引用：类似\u003ccode\u003eObject obj = new Object();\u003c/code\u003e这种, 垃圾回收器永远不会回收他们\u003c/p\u003e\n\u003cp\u003e软引用：非必须引用，内存溢出之前进行回收. 如果这次回收后内存还不足,才会抛出内存溢出异常\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eObject\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eobj\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eObject\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eSoftReference\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eObject\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esf\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eSoftReference\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eObject\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eobj\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eobj\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enull\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003esf\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"c1\"\u003e//有时候会返回null\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e软引用主要用户实现类似缓存的功能，在内存足够的情况下直接通过软引用取值，无需从繁忙的真实来源查询数据，提升速度；当内存不足时，自动删除这部分缓存数据，从真正的来源查询这些数据。\u003c/p\u003e\n\u003cp\u003e弱引用：非必须引用, 强度比软引用更弱. 当GC工作时, 无论当前内存是否足够, 都会回收掉只有弱引用关联的对象.\u003c/p\u003e","title":"Java 垃圾回收机制"},{"content":"Exception-handling 假如调用了一个不是自己写的方法, 该方法执行某些有风险的任务(当然,自己写的也可能会有风险),可能会在运行期间出状况,那么就必须认识到该方法是有风险的, 要写出可以在发生状况时做出应对的代码.\n当程序出现错误时，假如继续运行下去已经没有意义（或者根本不可能继续），那么我们就想要中断正常的控制流程 - throws an exception。\nThrowing Exceptions 比如当想从某ArrayMap中提取某个不存在的键值时, java自动抛出一个implicit exception\n$ java ExceptionDemo Exception in thread \u0026#34;main\u0026#34; java.lang.ArrayIndexOutOfBoundsException: -1 at ArrayMap.get(ArrayMap.java:38) at ExceptionDemo.main(ExceptionDemo.java:6) 如果想让自己的程序抛出更详细的信息, 可以在程序中加入explicit exception\npublic V get(K key) { intlocation = findKey(key); if(location \u0026lt; 0) { throw newIllegalArgumentException(\u0026#34;Key \u0026#34; + key + \u0026#34; does not exist in map.\u0026#34;\\); } return values[findKey(key)]; } $java ExceptionDemo Exception in thread \u0026#34;main\u0026#34; java.lang.IllegalArgumentException: Key yolp does not exist in map. at ArrayMap.get(ArrayMap.java:40) at ExceptionDemo.main(ExceptionDemo.java:6) Catch Exceptions 单纯 throw exception 会导致代码崩溃。但是通过 try - catch “捕捉”异常(RuntimeException 是 Java object), 可以防止程序崩溃。\n比如通过捕捉异常, 来引入修正措施: 下面这个代码通过\n/** 当狗在生气时, 如果尝试拍拍它，会抛出一个 RuntimeException， 捕捉到 exception 后, 用香蕉来抚慰它. */ Dog d = new Dog(\u0026#34;Lucy\u0026#34;, \u0026#34;Retriever\u0026#34;, 80); d.becomeAngry(); try { // 把有风险的代码块放在try块 d.receivePat(); } catch (Exception e) { System.out.println( \u0026#34;Tried to pat: \u0026#34; + e); d.eatTreat(\u0026#34;banana\u0026#34;); } d.receivePat(); System.out.println(d); $ java ExceptionDemo Tried to pat: java.lang.RuntimeException: grrr... snarl snarl Lucy is a displeased Retriever weighing 80.0 standard lb units. $ java ExceptionDemo Tried to pat: java.lang.RuntimeException: grrr... snarl snarl Lucy munches the banana Lucy enjoys the pat. Lucy is a happy Retriever weighing 80.0 standard lb units. Exception 继承自 Throwable, 异常也是多态的。\npublic class Throwable extends Object implements Serializable The Throwable class is the superclass of all errors and exceptions in the Java language. Direct Known Subclasses: Error, Exception\n如果使用if else来管理异常会让代码变得很乱而难以阅读. 而使用try catch可以为每种类型的 exception 提供不同的应对。使代码像清晰的记述文般铺展开来: 首先，尝试执行所需的操作。然后，捕捉任何错误。\npublic class Laundry { public void doLaundry() throws PantsException, LingerieException {} } public class Foo { public void go() { Laundry laundry = new Laundry(); try { laundry.doLaundry(); } catch (PantsException pex) { // doSomething } catch (LingerieException lex) { //doSomething } } } 这种清晰度使代码的长期维护变得更容易。 以异常的父类来声明会抛出的异常, 这样就不必声明每个子类异常了, 在catch块中只需要捕捉异常的父类.\npublic void doLaundry() throws ClothingException {} try { laundry.doLaundry(); } catch (ClothingException cex) { } 但如果需要对每个不同的子类异常采取不同的措施, 那么还是需要声明各个子类异常, 并分别捕捉.\ntry { laundry.doLaundry(); } catch (LingerieException lex) { //doSomething } catch (ClothingException cex) { //doSomething } catch (Exception ex) { //doSomething } 此时多个catch块要从小排到大(在继承树上看, 就是先捕捉子类), 不能把父类异常放在子类上面,否则无法通过编译. 因为JVM只会从上开始往下找第一个符合范围的catch块, 如果第一个catch就是catch (Exception ex), 那么剩下的捕捉都没用了.\nFinally 在try-catch后面, 使用finally指示无论如何都要执行的部分(不管有没有异常).\ntry { turnOvenOn(); x.bake(); } catch (BakingException ex) { ex.printStackTrace(); } finally { // 不管怎样最后都要关火! turnOvenOff(); } 如果try块失败了, 抛出异常, 流程马上转移到catch块. 当catch块完成时, 继续执行finally块. 当finally块完成时, 继续执行其他代码 如果try块成功, 流程会跳过catch块并移动到finally. finally块完成时, 继续执行其他代码 即使try或者catch块中有return指令，finally还是会执行！， 流程会先暂时保存return的值，去执行finally, 然后再return. 但是, 如果finally中也有return指令，那么会直接执行该return指令，结束整个流程，这意味着try/catch中的return得不到执行, 且它的值会丢失. 如果不想处理异常 有些情况下, 比如某个异常是在你的程序调用的其他程序中抛出的, 你可以选择不处理这个异常, 把它duck掉, 让那些调用你方法的程序来处理这个异常. 也就说你的程序只是把异常转个手, 给个出路.\n// 只有抛出, 没有try/catch 异常 public void foo() throws ReallyBadException {} 方法抛出异常时, 方法会从栈上立即被取出, 而异常会再度丢给栈上的方法, 也就说下一个调用方, 这种过程可以一直循环下去. 但ducking只是在击鼓传花, 最后总得有个方法接盘, 如果连main()也duck掉, 那么就是Uncaught Exceptions Stack Trace, 异常到达堆栈底部后仍未被捕获，JVM崩溃，Java 打印出堆栈的跟踪:\njava.lang.RuntimeException in thread “main”: at ArrayRingBuffer.peek:63 at GuitarString.sample:48 at GuitarHeroLite.java:110 程序猿可以据此追踪错误路径。\nChecked vs Unchecked Exceptions 有时候，某些抛出的 exception 无法通过编译，可以理解为这些异常在编译器看来是非常恶心的存在，需要程序猿必须给这些 exception 提供明确的应对处理方案 - 这种叫 checked exception （\u0026ldquo;must be checked\u0026rdquo;）。这种异常是以人无法预测或防止的方法出现的执行期失败状况, 比如我们无法保证文件一直都在, 无法保证服务器不会死机.\npublic class Eagle { public static void gulgate() { if (today == “Thursday”) { throw new IOException(\u0026#34;hi\u0026#34;); } } } $ javac Eagle Eagle.java:4: error: unreported exception IOException; must be caught or declared to be thrown throw new IOException(\u0026#34;hi\u0026#34;); } ^ 很明显，Java对此IOException并不满意, 因为IOExceptions是 checked exception, 而这里没有提供应对处理方案。但假如换做RuntimeException就可以编译通过 (虽然在 runtime 时会崩溃). InterruptException也是要检查的异常. 大多数check exception都有修正的可能性。例如遇到FileNotFound，可以考虑要求用户重新指定他们想要的文件 (可能是因为错误输入导致的)。\nErrors 和 Runtime Exceptions, 以及它们的子类都是unchecked. 大部分RuntimeException都是因为程序逻辑的问题. 虽然check exception是人力无法保证, 但我们可以确保程序的逻辑不出错, 例如对只有长度N的数组中取第N+1个元素, 这种是逻辑错误了, 不存在什么补救, 而是应该在写代码时就要避免。try/catch是用来处理真正的异常, 而不是程序的逻辑错误.\nJava在尽最大努力确保每个程序运行时不会崩溃，所以它不会允许程序留下任何明明可以应对修正却没有被明确地修正的错误。\n两种方法来处理 checked error:\nCatch public static void gulgate() { try { if (today == “Thursday”) { throw new IOException(\u0026#34;hi\u0026#34;); } } catch (Exception e) { System.out.println(\u0026#34;psych!\u0026#34;); } } 假如能够应对，尽量用 catch 锁定异常防止其逃逸。 Specify: 如果实在不想在该方法中处理这种异常，可以将责任推迟到别的地方。我们可以指定该方法是危险的 public static void gulgate() throws IOException { ... throw new IOException(\u0026#34;hi\u0026#34;); ... } 然后任何其他调用`gulgate()`的方法也变成危险的了, 它们也需要被处理(同样使用两种方法之一) ```java // catch public static void main(String[] args) { try { gulgate(); } catch(IOException e) { System.out.println(\u0026quot;Averted!\u0026quot;); } } // 或 specify public static void main(String[] args) throws IOException { gulgate(); } ``` 需要明确异常处理责任人。同时确保调用者知道该方法是危险的！ 异常处理规则 catch和finally不能没有try try块和catch块之间不能有其他代码 try一定要有catch或finally 只带有finally而没有catch的try必须要声明异常, 也就是明确抛出void go() throws FooException {try {} finally {} } ","permalink":"https://congchan.github.io/posts/java-exceptions/","summary":"\u003ch2 id=\"exception-handling\"\u003eException-handling\u003c/h2\u003e\n\u003cp\u003e假如调用了一个不是自己写的方法, 该方法执行某些有风险的任务(当然,自己写的也可能会有风险),可能会在运行期间出状况,那么就必须认识到该方法是有风险的, 要写出可以在发生状况时做出应对的代码.\u003c/p\u003e\n\u003cp\u003e当程序出现错误时，假如继续运行下去已经没有意义（或者根本不可能继续），那么我们就想要中断正常的控制流程 - throws an exception。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"throwing-exceptions\"\u003eThrowing Exceptions\u003c/h3\u003e\n\u003cp\u003e比如当想从某\u003ccode\u003eArrayMap\u003c/code\u003e中提取某个不存在的键值时, java自动抛出一个\u003ccode\u003eimplicit exception\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003e$\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ejava\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eExceptionDemo\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eException\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ein\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ethread\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;main\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ejava\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elang\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eArrayIndexOutOfBoundsException\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eArrayMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eArrayMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ejava\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"n\"\u003e38\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eExceptionDemo\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eExceptionDemo\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ejava\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"n\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e如果想让自己的程序抛出更详细的信息, 可以在程序中加入\u003ccode\u003eexplicit exception\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eintlocation\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003efindKey\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elocation\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ethrow\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enewIllegalArgumentException\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;Key \u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34; does not exist in map.\u0026#34;\u003c/span\u003e\u003cspan class=\"err\"\u003e\\\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003efindKey\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003e$java\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eExceptionDemo\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eException\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ein\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ethread\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;main\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ejava\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elang\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eIllegalArgumentException\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eKey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eyolp\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003edoes\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enot\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eexist\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ein\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eArrayMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eArrayMap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ejava\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"n\"\u003e40\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eat\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eExceptionDemo\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eExceptionDemo\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ejava\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"n\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"catch-exceptions\"\u003eCatch Exceptions\u003c/h3\u003e\n\u003cp\u003e单纯 throw exception 会导致代码崩溃。但是通过 \u003ccode\u003etry - catch\u003c/code\u003e “捕捉”异常(\u003ccode\u003eRuntimeException\u003c/code\u003e 是 Java object), 可以防止程序崩溃。\u003c/p\u003e","title":"Java Exceptions"},{"content":" An immutable data type is a data type whose instances cannot change in any observable way after instantiation.\n比如String是immutable, Array是mutable.\nImmutable 的类型: String, Integer, Double, Color, Vector, Transaction, Point2D. Mutable: StringBuilder, Stack, Counter, Java array.\npublic final class Vector { private final int N; private final double[] data; public Vector(double[] data) { this.N = data.length; this.data = new double[N]; for (int i = 0; i \u0026lt; N; i++) { // defensive copy of mutable instance variables this.data[i] = data[i]; } } ... } 如何防止变量在第一次赋值后被更改：\n可以使用final: 在 class constructor 里面, 或者变量初始化时, 给变量赋值一次, 之后就无法再被赋值了. final的变量代表不能改变它的值, final的方法代表不能被覆盖, final的类代表不能被继承. 要保证immutable不一定要使用final, 有时候也可以用private. Immutable data types 因为属性不能改变, 优点是：\n可以防止bugs, 并使debugging更容易 可以信赖对象具有某种行为/特质 更安全（出现攻击性代码时） 简化并发 可以放心地用作优先队列和符号表的键 缺点是需要更改属性，复制等操作时，需要给每个数据类型创建一个新对象\n注意：\n将一个引用声明为final并不会保证引用指向的对象是immutable. public final ArrayDeque\u0026lt;String\u0026gt;() deque = new ArrayDeque\u0026lt;String\u0026gt;();变量deque是final的, 仅意味着不能重新被赋值, 但其指向的数组队列对象自身还是可变的. 使用Reflection API，甚至可能对private变量进行更改 Classes should be immutable unless there\u0026rsquo;s a very good reason to make them mutable\u0026hellip;. If a class cannot be made immutable, you should still limit its mutability as much as possible. \u0026ndash; Effective Java, by Joshua Bloch\n","permalink":"https://congchan.github.io/posts/java-immutability/","summary":"\u003cblockquote\u003e\n\u003cp\u003eAn immutable data type is a data type whose instances cannot change in any observable way after instantiation.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e比如\u003ccode\u003eString\u003c/code\u003e是immutable, \u003ccode\u003eArray\u003c/code\u003e是mutable.\u003c/p\u003e\n\u003cp\u003eImmutable 的类型: String, Integer, Double, Color, Vector, Transaction, Point2D.\nMutable: StringBuilder, Stack, Counter, Java array.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eVector\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eN\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003efinal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eVector\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eN\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003edata\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eN\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eN\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"c1\"\u003e// defensive copy of mutable instance variables\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003edata\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e如何防止变量在第一次赋值后被更改：\u003c/p\u003e","title":"Java Immutability"},{"content":"认识栈和堆的概念， 有助于了解变量的有效范围（scope），对象的建立，内存管理，线程（thread）和异常处理。\n对象生存在可垃圾回收的堆（heap）上面，方法调用（方法的参数，局部变量的引用）的生存空间（stack）。\n当JVM启动时，从底层操作系统取得一块内存， 以此区段来执行Java程序。\n实例变量和局部变量 实例变量声明在类方法之外：实例变量保存在所属的对象中，位于堆上。如果实例变量是个对对象的引用，则引用和对象都在堆上。\npublic class Duck { int size; Mouth mouth; } 当新建一个Duck()时, Java必须在堆上帮它找位置, 保证空间足够存放该对象所有实例变量.\n对于primitive数据类型的实例变量, Java会根据类型的大小为它们留下堆空间, int 32, long 64 .... 如果实例变量是个对象, 也就是Duck对象带有引用变量mouth, Java会在堆上留给Duck的专属空间只包含该引用变量, 不包含引用的那个对象. 被引用的对象, 只有实际实例化后, 才会在堆上占有一席之地mouth = new Mouth(), 该空间并不属于Duck. 局部变量声明在方法或方法的参数上：它们是暂时的，且生命周期只限于被放在栈上的这段时间（也就是方法调用至执行完毕位止）。所有局部变量存在栈上相对应的堆栈块中，故又称为栈变量。引用对象的局部变量(只是对对象的引用)和primitive数据类型变量也都放在栈上。\npublic void foo(int x) { int i = x + 3; } 不管是实例变量还是局部变量， 它们指向的对象本身在堆上。\n栈 程序对方法的调用，以带有方法的状态的堆栈块的形式，不断堆在栈上面，当前被执行的方法处于栈顶。执行完毕的方法会出栈。递归调用太深层，会导致栈空间耗尽。\n如果方法中有局部变量引用对象，那么该局部变量也是存在于栈中. 但被引用的对象还是运行在堆上面.\n堆 变量的生命周期\n局部变量只会存活在声明该变量的方法中, 其余方法无法接触到 实例变量的寿命与所属对象相同. 如果所属对象还活着, 则实例变量也会活着. public class Life { int size; // size 在类中一直可以用 public void setSize(int s) { size = s; } // 但 s 仅限于setSize中, 且在方法结束后消失 } 所以这里存在生命周期life和范围scope的概念差别\nLife：只要变量的堆栈块还存在于堆栈上， 局部变量就还活着 Scope：局部变量的活动范围仅限于声明它的方法之内。此方法如果调用别的方法时，该变量还活着，但不在目前活跃的方法范围内。得等到其他方法执行完毕返回时，才回到该局部变量所属方法的范围。 引用变量只能在处于它的范围内才能被引用。即使引用变量暂时不在活跃范围内，只要引用变量还活着，被引用的对象也就活着。如果一个对象唯一的引用变量死了（随着堆栈块一起解散），对象就会被认定为可被垃圾回收（Garbage Collection）的。没有被引用的对象，是没有存在意义的，因为没有人知道它的地址，无法调用它，它的存在只会浪费空间。\n程序内存不足时，GC就会去把可GC的对象回收，释放内存，具体怎么回收，回收多少，有很多不同算法。如果回收完全，内存还是不够用，那就会变成真正的内存不足。\n三种方法释放对象的引用：\n引用永久性地离开它的范围void go() { Life z = new Life(); }, z会在方法结束时消失. 引用被赋值到其他对象: Life z = new Life(); z = new Life();, 第一个对象失去了引用 直接null引用: z = null, null代表空字节组合, 但实际上是什么只有JVM知道 ","permalink":"https://congchan.github.io/posts/java-%E5%A0%86%E6%A0%88/","summary":"\u003cp\u003e认识栈和堆的概念， 有助于了解变量的有效范围（scope），对象的建立，内存管理，线程（thread）和异常处理。\u003c/p\u003e\n\u003cp\u003e对象生存在可垃圾回收的堆（heap）上面，方法调用（方法的参数，局部变量的引用）的生存空间（stack）。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e当JVM启动时，从底层操作系统取得一块内存， 以此区段来执行Java程序。\u003c/p\u003e\n\u003ch2 id=\"实例变量和局部变量\"\u003e实例变量和局部变量\u003c/h2\u003e\n\u003cp\u003e实例变量声明在类方法之外：实例变量保存在所属的对象中，位于堆上。如果实例变量是个对对象的引用，则引用和对象都在堆上。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDuck\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eMouth\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emouth\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e当新建一个\u003ccode\u003eDuck()\u003c/code\u003e时, Java必须在堆上帮它找位置, 保证空间足够存放该对象所有\u003cstrong\u003e实例变量\u003c/strong\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对于primitive数据类型的实例变量, Java会根据类型的大小为它们留下堆空间, \u003ccode\u003eint 32, long 64 ...\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e如果实例变量是个对象, 也就是\u003ccode\u003eDuck\u003c/code\u003e对象带有引用变量\u003ccode\u003emouth\u003c/code\u003e, Java会在堆上留给\u003ccode\u003eDuck\u003c/code\u003e的专属空间只包含该引用变量, 不包含引用的那个对象.\n\u003cul\u003e\n\u003cli\u003e被引用的对象, 只有实际实例化后, 才会在堆上占有一席之地\u003ccode\u003emouth = new Mouth()\u003c/code\u003e, 该空间并不属于\u003ccode\u003eDuck\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e局部变量声明在方法或方法的参数上：它们是暂时的，且生命周期只限于被放在栈上的这段时间（也就是方法调用至执行完毕位止）。所有局部变量存在栈上相对应的堆栈块中，故又称为栈变量。引用对象的局部变量(只是对对象的引用)和primitive数据类型变量也都放在栈上。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003efoo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e不管是实例变量还是局部变量， 它们指向的对象本身在堆上。\u003c/p\u003e\n\u003ch2 id=\"栈\"\u003e栈\u003c/h2\u003e\n\u003cp\u003e程序对方法的调用，以带有方法的状态的堆栈块的形式，不断堆在栈上面，当前被执行的方法处于栈顶。执行完毕的方法会出栈。递归调用太深层，会导致栈空间耗尽。\u003c/p\u003e\n\u003cp\u003e如果方法中有局部变量引用对象，那么该局部变量也是存在于栈中. 但被引用的对象还是运行在堆上面.\u003c/p\u003e\n\u003ch2 id=\"堆\"\u003e堆\u003c/h2\u003e\n\u003cp\u003e变量的生命周期\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e局部变量只会存活在声明该变量的方法中, 其余方法无法接触到\u003c/li\u003e\n\u003cli\u003e实例变量的寿命与所属对象相同. 如果所属对象还活着, 则实例变量也会活着.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eLife\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// size 在类中一直可以用\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003esetSize\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 但 s 仅限于setSize中, 且在方法结束后消失\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e所以这里存在生命周期life和范围scope的概念差别\u003c/p\u003e","title":"Java 堆栈"},{"content":"Encapsulation 封装是面向对象编程的基本原则之一，也是程序员处理复杂性一个方法。管理复杂性是编写大型程序时必须面对的主要挑战之一。\n对抗复杂性的一些工具包括:\nHierarchical abstraction: 创建一个个具有明确的 abstraction barriers 的抽象层 Abstraction Barriers：使用private, 保证对象内部不能被查看, 确保底层的复杂性不会暴露给外部世界。 “Design for change” (D. Parnas) Organize program around objects. Let objects decide how things are done. Hide information others don’t need. 大概的想法都是 - 程序应该被构建成模块化，可互换的片段，可以在不破坏系统的情况下进行交换。\n封装就是构建在这种对外部隐藏信息的概念上。以细胞为类比：细胞内部可能非常复杂，由染色体，线粒体，核糖体等组成，但它完全被封装在一个单一模块中 - 抽象了内部的复杂性。\nModule: A set of methods that work together as a whole to perform some task or set of related tasks. Encapsulated: A module is said to be encapsulated if its implementation is completely hidden, and it can be accessed only through a documented interface.\nPackages 同样功能的类可能有多种版本, 或者不同类刚好命名相同。通过 packages 来为每个 classes 提供唯一的标识名称，如java.util.\nA package is a namespace that organizes classes and interfaces.\n在IntelliJ的操作：\n创建 package： 1, File → New Package 2, 选择 package name (i.e. “ug.joshh.animal”)\n给 Package 添加(新) Java 文件： 1, 右键 package name 2, New → Java Class 3, 命名 class, 然后 IntelliJ 会自动把文件放进正确的路径, 并添加 package declaration.\n移动其他.java文件到 Package 1, 在文件顶部声明 package [packagename] 2, 将文件存储在（移动到）与 package name 对应的文件夹中：如ug.joshh.animal 对应ug/joshh/animal文件路径.\n注意, 不存在sub-package这种概念, 即ug.joshh.Animal和ug.joshh.Plant是完全不同的.\nPackage好处: Organizing, making things package private 坏处: Specific\nDefault packages 没有在文件顶部明确指明 package name 的Java类默认属于 default package 的一部分。\n一般而言, Java文件应该以明确的 package 声明开头以避免将文件留在 default package 中（除非它是一个非常小的示例程序）。因为来自 default package 的代码无法 import，并且可能会意外地在 default package 下创建相同名称的类。\nJAR 一般情况下，程序会包含多个.class文件。如果想共享此程序，可以把压缩成一个.jar文件，此.jar文件将包含程序所有.class文件以及其他附加信息。JAR文件就像zip文件一样, 可以将文件解压缩回.java文件。JAR文件并不会加密保护代码.\nCreating a JAR File (IntelliJ)\nGo to File → Project Structure → Artifacts → JAR → “From modules with dependencies” Click OK a couple of times Click Build → Build Artifacts (this will create a JAR file in a folder called “Artifacts”) Distribute this JAR file to other Java programmers, who can now import it into IntelliJ (or otherwise) 权限控制 cs61b Josh Hug: Private\nOnly code from the given class can access private members.\nPackage Private\nThe default access given to Java members if there is no explicit modifier written. Classes that belong in the same package can access, but not subclasses!\nProtected\nClasses within the same package and subclasses can access these members, but the rest of the world (e.g. classes external to the package or non-subclasses) cannot! Subtypes might need it, but subtype clients will not.\nPublic\nOpen and promised to the world, once deployed, the public members’ signatures should not change. 就像承诺和合同，尽量不要更改，以便用户始终可以（用已有的代码）访问。如果开发者要舍弃某一个Public，一般标识为deprecated.\n细节:\nAccess is Based Only on Static Types 接口的方法默认是public的 ","permalink":"https://congchan.github.io/posts/java-%E5%B0%81%E8%A3%85-%E5%8C%85-jar-%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/","summary":"\u003ch2 id=\"encapsulation\"\u003eEncapsulation\u003c/h2\u003e\n\u003cp\u003e封装是面向对象编程的基本原则之一，也是程序员处理复杂性一个方法。管理复杂性是编写大型程序时必须面对的主要挑战之一。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e对抗复杂性的一些工具包括:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHierarchical abstraction: 创建一个个具有明确的 abstraction barriers 的抽象层\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAbstraction Barriers\u003c/strong\u003e：使用\u003ccode\u003eprivate\u003c/code\u003e, 保证对象内部不能被查看, 确保底层的复杂性不会暴露给外部世界。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e“Design for change” (D. Parnas)\n\u003cul\u003e\n\u003cli\u003eOrganize program around objects.\u003c/li\u003e\n\u003cli\u003eLet objects decide how things are done.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHide information\u003c/strong\u003e others don’t need.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e大概的想法都是 - 程序应该被构建成模块化，可互换的片段，可以在不破坏系统的情况下进行交换。\u003c/p\u003e\n\u003cp\u003e封装就是构建在这种对外部隐藏信息的概念上。以细胞为类比：细胞内部可能非常复杂，由染色体，线粒体，核糖体等组成，但它完全被封装在一个单一模块中 - 抽象了内部的复杂性。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eModule: A set of methods that work together as a whole to perform some task or set of related tasks.\nEncapsulated: A module is said to be encapsulated if its implementation is completely hidden, and it can be accessed only through a documented interface.\u003c/p\u003e","title":"Java 封装, 包, JAR, 权限控制"},{"content":"Abstract Data Types (ADTS) ADTS 是由其行为属性定义的抽象类型, 跟如何实现无关.\n堆栈 Stacks 和队列 Queues 是两种类似的线性集合。堆栈是后进先出的ADT：元素总是从数据结构的一端添加或删除。队列是先进先出的ADT. 二者都支持以下操作: push(): 加入 peek(): 返回下一个 poll(): 返回下一个并删除\nJava的Deque(double ended queue, “deck”) 接口融合了堆栈和队列, 支持两端的元素插入和移除. ArrayDeque和LinkedListDeque都是实现deque这个接口，deque只是罗列了一些 methods，也即是一种合约，保证会实现的行为。而这些方法的具体实现则是由ArrayDeque和LinkedListDeque完成。从概念上讲，deque就是一种抽象的数据类型，只说会有什么行为，但不体现这些行为的具体实现方式，所以是抽象的。\n优先级队列 priority queue 的每个元素都有一个与之关联的优先级，以决定从队列中元素操作的顺序。\n三种方式实现Stack的push(Item x):\n继承模式：extends LinkedList\u0026lt;Item\u0026gt;以使用其方法: public class ExtensionStack\u0026lt;Item\u0026gt; extends LinkedList\u0026lt;Item\u0026gt; { public void push(Item x) { add(x); } } 委托模式Delegation， 生成Linked List并调用其方法来达到目的 public class DelegationStack\u0026lt;Item\u0026gt; { private LinkedList\u0026lt;Item\u0026gt; L = new LinkedList\u0026lt;Item\u0026gt;(); public void push(Item x) { L.add(x); } } 类似委托模式, 只是这里可以利用任何实现了List接口的类, 如Linked List, ArrayList, 等等 public class StackAdapter\u0026lt;Item\u0026gt; { private List L; public StackAdapter(List\u0026lt;Item\u0026gt; worker) { L = worker; } public void push(Item x) { L.add(x); } } Delegation vs Extension: Extension 一般是基于对父类有比较清楚的了解认知下才会使用。此外，扩展基本上等于在说明正在扩展的类与被扩展类是相似的。如果两个类无法看做是同属的, 那么就用委托模式。\nViews: 通过视图进行的更改会影响底层对象。\n/** Create an ArrayList. */ List\u0026lt;String\u0026gt; L = new ArrayList\u0026lt;\u0026gt;(); /** Add some items. */ L.add(“at”); L.add(“ax”); … List\u0026lt;String\u0026gt; SL = l.subList(1, 4); /** Mutate that thing. */ SL.set(0, “jug”); API\u0026rsquo;s An API(Application Programming Interface) of an ADT is the list of constructors and methods and a short description of each.\nAPI 包括语法规范和语义规范\n编译器确认语法符合要求 测试以帮助确认语义描述是否正确 Java Libraries Java有一些内置的抽象数据类型，打包在Java库中。 三个最重要的ADTs来自java.util库：\nList 列表：一个有序的元素集合，如ArrayList Set 集合：元素严格唯一（不重复）的(无序)集合，如HashSet Map 映射：A collection of Key - value 映射, key是唯一的。通过key访问value，如HashMap。 /** takes in a String inputFileName and puts every word from the input file into a list*/ public static List\u0026lt;String\u0026gt; getWords(String inputFileName) { List\u0026lt;String\u0026gt; lst = new ArrayList\u0026lt;String\u0026gt;(); In in = new In(); while (!in.isEmpty()) { lst.add(in.readString()); } return lst; } /** takes in a List\u0026lt;String\u0026gt; and counts how many unique words there are in the file.*/ public static int countUniqueWords(List\u0026lt;String\u0026gt; words) { Set\u0026lt;String\u0026gt; ss = new HashSet\u0026lt;\u0026gt;(); for (String s : words) { ss.add(s); } return ss.size(); } /** takes in a List\u0026lt;String\u0026gt; targets and a List\u0026lt;String\u0026gt; words, and finds the number of times each target word appears in the word list.*/ public static Map\u0026lt;String, Integer\u0026gt; collectWordCount(List\u0026lt;String\u0026gt; words) { Map\u0026lt;String, Integer\u0026gt; counts = new HashMap\u0026lt;String, Integer\u0026gt;(); for (String t: target) { counts.put(s, 0); } for (String s: words) { if (counts.containsKey(s)) { counts.put(word, counts.get(s)+1); } } return counts; } 通过设置环境变量（如CLASSPATH = ）让Java编译器/解释器知道去哪里找 libraries。\nCLASSPATH：Linux or MacOS, paths are separated by :. In Windows, paths are separated by ;.\n/home/--/--/javalib/*, 在.class和.jar文件内查找依赖包，用于指定绝对路径。有同名时，会根据环境变量的先后顺序去排序靠前的。 ./指当前目录，../指上一层目录，用于指定相对路径。 也可以指定classpath, 这样系统的CLASSPATH会被忽略: javac -cp ./:/home/stuff/:../ Foo.java, 当有重名时, 选择顺序就是指明的路径顺序（当前目录-stuff目录-上一层目录） IntelliJ会忽略CLASSPATH，它会自动调用-cp, 变量是基于当前项目指定的 libraries.\n/** 查看 IntelliJ 使用的 classpath*/ import java.net.URL; import java.net.URLClassLoader; public static void main(String[] args) { ClassLoader cl = ClassLoader.getSystemClassLoader(); URL[] urls = ((URLClassLoader)cl).getURLs(); for(URL url: urls){ System.out.println(url.getFile()); } } Build Systems：可以简单地将文件放入适当的位置，然后通过 Maven, Ant 和 Gradle 等工具使用 Build Systems 来自动设置项目, 省去了手动加载一长串 libraries.\n","permalink":"https://congchan.github.io/posts/java-%E6%8A%BD%E8%B1%A1%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","summary":"\u003ch2 id=\"abstract-data-types-adts\"\u003eAbstract Data Types (ADTS)\u003c/h2\u003e\n\u003cp\u003eADTS 是由其行为属性定义的抽象类型, 跟如何实现无关.\u003c/p\u003e\n\u003cp\u003e堆栈 Stacks 和队列 Queues 是两种类似的线性集合。堆栈是后进先出的ADT：元素总是从数据结构的一端添加或删除。队列是先进先出的ADT. 二者都支持以下操作:\n\u003ccode\u003epush()\u003c/code\u003e: 加入\n\u003ccode\u003epeek()\u003c/code\u003e: 返回下一个\n\u003ccode\u003epoll()\u003c/code\u003e: 返回下一个并删除\u003c/p\u003e\n\u003cp\u003eJava的\u003ccode\u003eDeque\u003c/code\u003e(double ended queue, “deck”) 接口融合了堆栈和队列, 支持两端的元素插入和移除. \u003ccode\u003eArrayDeque\u003c/code\u003e和\u003ccode\u003eLinkedListDeque\u003c/code\u003e都是实现\u003ccode\u003edeque\u003c/code\u003e这个接口，\u003ccode\u003edeque\u003c/code\u003e只是罗列了一些 methods，也即是一种合约，保证会实现的行为。而这些方法的具体实现则是由\u003ccode\u003eArrayDeque\u003c/code\u003e和\u003ccode\u003eLinkedListDeque\u003c/code\u003e完成。从概念上讲，\u003ccode\u003edeque\u003c/code\u003e就是一种抽象的数据类型，只说会有什么行为，但不体现这些行为的具体实现方式，所以是抽象的。\u003c/p\u003e\n\u003cp\u003e优先级队列 priority queue 的每个元素都有一个与之关联的优先级，以决定从队列中元素操作的顺序。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e三种方式实现\u003ccode\u003eStack\u003c/code\u003e的\u003ccode\u003epush(Item x)\u003c/code\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e继承模式：\u003ccode\u003eextends LinkedList\u0026lt;Item\u0026gt;\u003c/code\u003e以使用其方法:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eExtensionStack\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eLinkedList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"2\"\u003e\n\u003cli\u003e委托模式\u003cstrong\u003eDelegation\u003c/strong\u003e， 生成\u003ccode\u003eLinked List\u003c/code\u003e并调用其方法来达到目的\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDelegationStack\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eLinkedList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eLinkedList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003e类似委托模式, 只是这里可以利用任何实现了\u003ccode\u003eList\u003c/code\u003e接口的类, 如\u003ccode\u003eLinked List, ArrayList\u003c/code\u003e, 等等\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eStackAdapter\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eList\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eStackAdapter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eworker\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eworker\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eDelegation vs Extension: Extension 一般是基于对父类有比较清楚的了解认知下才会使用。此外，扩展基本上等于在说明正在扩展的类与被扩展类是相似的。如果两个类无法看做是同属的, 那么就用委托模式。\u003c/p\u003e","title":"Java 抽象数据类型"},{"content":"Java格式化指令\u0026quot; 跟在%后面的都是格式化指令. % [argument number] [flags] [width] [.precision] type\n[]内都是选择性的参数, 且必须按照顺序.\nargument number 当要格式化的参数超过一个, 这里指定是哪一个 flags 特定类型的特定选项, 如数字是否要加逗号或正负号 width 最小的字符数. 但不是总数, 输出可以超过此宽度, 若不足则会主动补零 .precision 精确度 type 类型标识 d decimal, 十进制整数 f 浮点数 x hexadecimal, 16进制 c character 如format(\u0026quot;%, 6.1f\u0026quot;, 33.000);\n如果有多个待输出的参数, 可以把新的参数加到后面, 并对应两个不同的格式化设定, 也就是两个%格式指令 String s = String.format(\u0026quot;%,.2f out of %,d\u0026quot;, 999, 1000)\n可变参数列表 可以看到格式化的参数似乎可以不断添加，如果用重载来实现会显得不现实。为了应对格式化的API，Java支持可变参数列表 varable argument list (vararg).\n日期 日期格式化的类型是用t开头的两个字符表示\n%tc 完整的日期与时间 Sun Nov 03 14:52:41 2018 %tr 只有时间 03:01:47 PM 周月日 Sunday, November 03, 通过组合而来\nDate today = new Date(); String.format(\u0026#39;%tA, %tB %td\u0026#39;, today, today, today); 如果不想重复给参数\nString.format(\u0026#39;%tA, %\u0026lt;tB %\u0026lt;td\u0026#39;, today); \u0026lt;符号指示格式化程序重复利用参数\n但除了要去的当前日期用到Date之外, 其余的时间功能几乎都在Calendar上面. 因Calendar是抽象类, 所以不能取得它的实例, 但可调用它的静态方法, 对Calendar静态方法的调用以取得一个具体子类的实例, Calendar cal = Calendar.getInstance();\n","permalink":"https://congchan.github.io/posts/java-%E6%A0%BC%E5%BC%8F/","summary":"\u003ch2 id=\"java格式化指令\"\u003eJava格式化指令\u0026quot;\u003c/h2\u003e\n\u003cp\u003e跟在\u003ccode\u003e%\u003c/code\u003e后面的都是格式化指令.\n\u003ccode\u003e% [argument number] [flags] [width] [.precision] type\u003c/code\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e\u003ccode\u003e[]\u003c/code\u003e内都是选择性的参数, 且必须按照顺序.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eargument number\u003c/code\u003e 当要格式化的参数超过一个, 这里指定是哪一个\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflags\u003c/code\u003e 特定类型的特定选项, 如数字是否要加逗号或正负号\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ewidth\u003c/code\u003e 最小的字符数. 但不是总数, 输出可以超过此宽度, 若不足则会主动补零\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e.precision\u003c/code\u003e 精确度\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etype\u003c/code\u003e 类型标识\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ed\u003c/code\u003e decimal, 十进制整数\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ef\u003c/code\u003e 浮点数\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ex\u003c/code\u003e hexadecimal, 16进制\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ec\u003c/code\u003e character\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如\u003ccode\u003eformat(\u0026quot;%, 6.1f\u0026quot;, 33.000);\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e如果有多个待输出的参数, 可以把新的参数加到后面, 并对应两个不同的格式化设定, 也就是两个\u003ccode\u003e%\u003c/code\u003e格式指令 \u003ccode\u003eString s = String.format(\u0026quot;%,.2f out of %,d\u0026quot;, 999, 1000)\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"可变参数列表\"\u003e可变参数列表\u003c/h2\u003e\n\u003cp\u003e可以看到格式化的参数似乎可以不断添加，如果用重载来实现会显得不现实。为了应对格式化的API，Java支持可变参数列表 varable argument list (vararg).\u003c/p\u003e\n\u003ch2 id=\"日期\"\u003e日期\u003c/h2\u003e\n\u003cp\u003e日期格式化的类型是用\u003ccode\u003et\u003c/code\u003e开头的两个字符表示\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e%tc\u003c/code\u003e 完整的日期与时间 \u003ccode\u003eSun Nov 03 14:52:41 2018\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e%tr\u003c/code\u003e 只有时间 \u003ccode\u003e03:01:47 PM\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e周月日\n\u003ccode\u003eSunday, November 03\u003c/code\u003e, 通过组合而来\u003c/p\u003e","title":"Java 格式"},{"content":"对物品排序首先需要比较各个物品的大小, 这个大小的定义既可以是按照\u0026quot;自然顺序\u0026quot;, 也可以是其他指定的特殊规则.\nComparable Java的对象不能直接使用\u0026gt;, \u0026lt;, =进行比较. 在Python或C++中，当应用于不同对象类型时，比较运算符可以重新定义，但Java不支持。但可以借用接口继承，Java提供了一个Comparable接口，包含一个compareTo方法, 以保证任何实现该接口的类可以和其他同类做比较：\n/** Return negative if this \u0026lt; o. Return 0 if this equals o. Return positive if this \u0026gt; o. */ public interface Comparable\u0026lt;T\u0026gt; { public int compareTo(T obj); } 当有class需要与其他class比较时, 就实现这个接口:\npublic class Dog implements Comparable\u0026lt;Dog\u0026gt; { ... public int compareTo(Dog uddaDog) { return this.size - uddaDog.size; } } Comparable定义了类用于比较的自然顺序（Natural order）, 返回的是三种结果负整数, 0, 正整数, 分别对应小于, 等于和大于. Insertion.sort()不需要知道要排序的数组类型, 因为它直接调用数组成员自带的compareTo方法. Java的Integer, Double, String, Date, File数据类型都扩展了Comparable接口。\nA comparable object is capable of comparing itself with another object\nComparator 如果我们想用灵活的不同方式对类进行比较比较呢？比如对音乐库里的歌曲根据艺术家、歌名等排序，二者都是String, 但一个类里面的Comparable只能有一个, 所以仅仅靠Comparable接口不够. 在Python可以使用HOF，编写新的比较函数，然后直接以参数形式传递该函数。\nJava的方案是使用Comparator接口：\npublic interface Comparator\u0026lt;T\u0026gt; { int compare(T o1, T o2); } 比如Java系统自带的sort:\n/** ・Create Comparator object. ・Pass as second argument to Arrays.sort(). */ String[] a; ... Arrays.sort(a); // uses natural order ... // uses alternate order defined by Comparator\u0026lt;String\u0026gt; object Arrays.sort(a, String.CASE_INSENSITIVE_ORDER); insertion sort的解决思路类似: ・Use Object instead of Comparable. ・Pass Comparator to sort() and less().\npublic static void sort(Object[] a, Comparator comparator) { int N = a.length; for (int i = 0; i \u0026lt; N; i++) for (int j = i; j \u0026gt; 0 \u0026amp;\u0026amp; less(comparator, a[j], a[j-1]); j--) exch(a, j, j-1); } private static boolean less(Comparator c, Object v, Object w) { return c.compare(v, w) \u0026lt; 0; } private static void exch(Object[] a, int i, int j) { Object swap = a[i]; a[i] = a[j]; a[j] = swap; } 需要自定义时, 根据需要在class内部编写实现Comparator接口的(嵌套)类, 并实现compare方法:\npublic class Student { public static final Comparator\u0026lt;Student\u0026gt; BY_NAME = new ByName(); public static final Comparator\u0026lt;Student\u0026gt; BY_SECTION = new BySection(); private final String name; private final int section; ... private static class ByName implements Comparator\u0026lt;Student\u0026gt; { // 直接利用 String 已经定义好的 compareTo public int compare(Student v, Student w) { return v.name.compareTo(w.name); } } private static class BySection implements Comparator\u0026lt;Student\u0026gt; { public int compare(Student v, Student w) { return v.section - w.section; } } } 在其他函数中调用时\nStudent s1; Student s2; ... if (Student.BY_NAME.compare(s1, s2) \u0026gt; 0) { ... } ... Arrays.sort(a, Student.BY_NAME); Arrays.sort(a, Student.BY_SECTION); 同理若需要增加其他判断标准，就创建新的实现Comparator的 class.\nComparator是可以将两个对象进行比较的第三方对象。由于只有一个compareTo的空间，如果想要支持不同方式进行比较，则要使用不同的Comparator。\nA Comparator is its own definition of how to compare two objects, and can be used to compare objects in a way that might not align with the natural ordering.\n","permalink":"https://congchan.github.io/posts/java-%E6%AF%94%E8%BE%83%E5%AF%B9%E8%B1%A1%E5%A4%A7%E5%B0%8F/","summary":"\u003cp\u003e对物品排序首先需要比较各个物品的大小, 这个大小的定义既可以是按照\u0026quot;自然顺序\u0026quot;, 也可以是其他指定的特殊规则.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"comparable\"\u003eComparable\u003c/h3\u003e\n\u003cp\u003eJava的对象不能直接使用\u003ccode\u003e\u0026gt;, \u0026lt;, =\u003c/code\u003e进行比较. 在Python或C++中，当应用于不同对象类型时，比较运算符可以重新定义，但Java不支持。但可以借用接口继承，Java提供了一个\u003ccode\u003eComparable\u003c/code\u003e接口，包含一个\u003ccode\u003ecompareTo\u003c/code\u003e方法, 以保证任何实现该接口的类可以和其他同类做比较：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e/** Return negative if this \u0026lt; o.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e    Return 0 if this equals o.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e    Return positive if this \u0026gt; o.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e*/\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003einterface\u003c/span\u003e \u003cspan class=\"nc\"\u003eComparable\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eT\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ecompareTo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eT\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eobj\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e当有class需要与其他class比较时, 就实现这个接口:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDog\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eComparable\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ecompareTo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euddaDog\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003esize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euddaDog\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003eComparable\u003c/code\u003e定义了类用于比较的自然顺序（Natural order）, 返回的是三种结果\u003ccode\u003e负整数, 0, 正整数\u003c/code\u003e, 分别对应小于, 等于和大于. \u003ccode\u003eInsertion.sort()\u003c/code\u003e不需要知道要排序的数组类型, 因为它直接调用数组成员自带的\u003ccode\u003ecompareTo\u003c/code\u003e方法. Java的\u003ccode\u003eInteger, Double, String, Date, File\u003c/code\u003e数据类型都扩展了\u003ccode\u003eComparable\u003c/code\u003e接口。\u003c/p\u003e","title":"Java 比较对象大小"},{"content":"泛型 泛型意味着更好的类型安全性。主要目的是支持类型安全性的集合，让问题尽可能在编译阶段就能捉到。\n泛型定义在类声明中 public class ArrayLiat\u0026lt;E\u0026gt; extends AbstractLìst\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt; { public boolean add (E o); } E代表用来创建赋予初始ArrayList的类型\nArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;; 编译器会自动把E看做String.\n泛型方法 使用未定义在类声明的类型参数: 在返回类型之前指定泛型\nmaxKey: 返回给定ArrayMap中所有keys的最大值（仅在key可以比较的情况下）。假如这样写public static K maxKey(Map61B\u0026lt;K, V\u0026gt; map) { ... }会报错. 要将方法声明为泛型，必须在返回类型前面指定正式的类型参数\npublic static \u0026lt;K extends Comparable\u0026lt;K\u0026gt;, V\u0026gt; K maxKey(Map61B\u0026lt;K, V\u0026gt; map) { List\u0026lt;K\u0026gt; keylist = map.keys(); K largest = map.get(0); for (K k: keylist) { if (k.compareTo(largest)) { largest = k; } } return largest; } K extends Comparable\u0026lt;K\u0026gt; 保证了keys必须实现Comparable接口（也是一个generic接口）, 并可以与其他K进行比较。\n这里没有使用implement, 而是用extends, 这里跟多态不同. K extends Comparable\u0026lt;K\u0026gt;是type upper bounding, 意味着k必须是一种Comparable, 但不需要具备Comparable的所有方法行为.\n在inheritance的含义中，extends指为子类提供超类的能力. 在泛型范畴内, extends只是陈述一个事实：该类是其扩展的类的一个子类, 是加了约束, 而不是赋予能力.\n泛型与多态 如果使用多态类定义下面的方法是没有问题的\npublic void takeAnimals(ArrayList\u0026lt;Animal\u0026gt; animals) { for (Animal a : animals) a.eat(); } public void go() { ArrayList\u0026lt;Animal\u0026gt; animals = new ArrayList\u0026lt;\u0026gt;(); animals.add(new Dog()); animals.add(new Cat()); takeAnimals(animals); // 可以编译 ArrayList\u0026lt;Dog\u0026gt; dogs = new ArrayList\u0026lt;\u0026gt;(); dogs.add(new Dog()); dogs.add(new Dog()); takeAnimals(dogs); // ! 无法编译 } 那么在实际运行时, 如果声明为ArrayList\u0026lt;Animal\u0026gt;, 则不管传入的animals包含的是Dog还是Cat, 这个方法调用的都是Animal的eat(), 多态支持这种操作. 但如果声明为ArrayList\u0026lt;Dog\u0026gt;就不行, 静态类型检查不通过.\n那么Java为何不允许这种情况编译呢? 反过来想, 如果可以会怎样? 假如方法换为这种\npublic void takeAnimals(ArrayList\u0026lt;Animal\u0026gt; animals) { animals.add(new Cat()); } 可以看到会有很大问题. 在方法内部看来, 把Cat加到ArrayList\u0026lt;Animal\u0026gt;中是完全合法的, 但是对于从外部传入的参数ArrayList\u0026lt;Dog\u0026gt;来说, 就不合理了. 所以需要保证这种情况无法通过编译.\n但如果是把Dog[] dogs中的元素改为Cat, 却可以通过编译.\npublic void takeAnimals(ArrayList[] animals) { animals[0] = new Cat(); } 但在运行时, JVM会指出错误. 因为数组的类型是在runtime期间检查的.\n要想在使用多态的情况下, 让方法自动调用子类型参数的方法, 就要使用万用字符(wildcard)\npublic void takeAnimals(ArrayList\u0026lt;? extends Animal\u0026gt; animals) { for (Animal a : animals) a.eat(); } 使用万用字符, 编译器会组织任何可能破坏参数所指向集合的行为, 比如加入元素animals.add(new Cat());是无法编译通过的.\n使用泛型, 也可以实现上面的方法, 就是让泛型继承父类\npublic \u0026lt;T extends Animal\u0026gt; void takeAnimals(ArrayList\u0026lt;T\u0026gt; list); 这意味着T可以是任何一种Animal, 任何被声明为Animal或其子类的ArrayList都是合法的.\n这两种方法等价, 如果需要传入多个参数, 那么只声明一次会更有效率\npublic \u0026lt;T extends Animal\u0026gt; void takeThing(ArrayList\u0026lt;T\u0026gt; one, ArrayList\u0026lt;T\u0026gt; two); public void takeAnimals(ArrayList\u0026lt;? extends Animal\u0026gt; one, ArrayList\u0026lt;? extends Animal\u0026gt; two); 融合两种方法的声明\npublic static \u0026lt;T extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; void sort(List\u0026lt;T\u0026gt; list); 这意味着sort支持任何一种实现了以T的父类为泛型的Comparable的类型.\nAutoboxing 在Java中调用包含 Generics 的class时，需要提供确切的类型参数。对于每一种 primitive type (byte, short, int, long, float, double, boolean, char)，必须要用其对应的 reference type (Byte, Short, Integer, Long, Float, Double, Boolean, Character) - 也即是 wrapper classes 作为泛型的实际类型参数。虽然声明函数和变量时必须要用 wraper classes，但在实际的数值传递中，对于 primitives 类型的数据，并不需要显式地转换为 reference types。\n因为 Java 有 Autoboxing，可以隐式地在 wrapper/primitives 类型间转换. Java会自动 “box” 和 “unbox” primitive type 和其对应的 reference type 之间的值。也就是说，如果Java期望的是 wrapper classes （如Integer），假如即使接收到的是 int 这样的基本类型，Java也会“autoboxing”这种整数。\npublic static void blah(Integer x) { System.out.println(x); } int x = 20; blah(x); // 实际上会转换为 blah(new Integer(20)) 反过来就是unboxing。\nAutoboxing/Unboxing 注意事项:\n不适用于 array 数组 有性能负担 Wrapper types 比 primitive types 占用更多内存: 在大多数现代的系统里，对象的引用地址占用64位，还需要额外的64位开销用于存储动态类型等信息。 更多信息参考 Memory usage of Java objects: general guide 或 Memory Usage Estimation in Java. 类型转换的静态方法:\nInteger.parseInt(\u0026quot;2\u0026quot;), Double.parseDouble(\u0026quot;135.26\u0026quot;), new Boolean(\u0026quot;true\u0026quot;).booleanValue(), 取String, 返回对应的primitive类型值. 将 primitive 主数据类型值转换为String double d = 22.2; String DoubleString = \u0026quot;\u0026quot; + d;, +操作数是Java中唯一有重载过的运算符 String s = Double.toString(d); Widening Java会根据需要在 primitive types 之间自动扩展.\npublic static void blahDouble(double x) { System.out.println(“double: “ + x); } int x = 20; blahDouble(x); //等同于 blahDouble((double) x) 但如果想从一个 wider type 转换为 narrower type，则必须手动 cast. 有关 widening 的更多详细信息，包括哪些类型比其他类型更 wider ，参阅官方的Java文档。\n","permalink":"https://congchan.github.io/posts/java-%E6%B3%9B%E5%9E%8B/","summary":"\u003ch2 id=\"泛型\"\u003e泛型\u003c/h2\u003e\n\u003cp\u003e泛型意味着更好的类型安全性。主要目的是支持类型安全性的集合，让问题尽可能在编译阶段就能捉到。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"泛型定义在类声明中\"\u003e泛型定义在类声明中\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eArrayLiat\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eE\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eAbstractLìst\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eE\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eE\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eadd\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eE\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eo\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eE代表用来创建赋予初始ArrayList的类型\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eArrayList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elist\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eArrayList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e编译器会自动把\u003ccode\u003eE\u003c/code\u003e看做\u003ccode\u003eString\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"泛型方法\"\u003e泛型方法\u003c/h3\u003e\n\u003cp\u003e使用未定义在类声明的类型参数: 在返回类型之前指定泛型\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003emaxKey\u003c/code\u003e: 返回给定\u003ccode\u003eArrayMap\u003c/code\u003e中所有keys的最大值（仅在key可以比较的情况下）。假如这样写\u003ccode\u003epublic static K maxKey(Map61B\u0026lt;K, V\u0026gt; map) { ... }\u003c/code\u003e会报错. 要将方法声明为泛型，\u003cstrong\u003e必须在返回类型前面指定正式的类型参数\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eComparable\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emaxKey\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eMap61B\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekeylist\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ekeys\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elargest\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekeylist\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ecompareTo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elargest\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003elargest\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elargest\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003eK extends Comparable\u0026lt;K\u0026gt;\u003c/code\u003e 保证了keys必须实现\u003ccode\u003eComparable\u003c/code\u003e接口（也是一个generic接口）, 并可以与其他\u003ccode\u003eK\u003c/code\u003e进行比较。\u003c/p\u003e","title":"Java 泛型"},{"content":"Java提供了 foreach (enhanced for) 的循环简写语法:\nArrayMap\u0026lt;String, Integer\u0026gt; am = new ArrayMap\u0026lt;String, Integer\u0026gt;(); for (String s : am) { System.out.println(s); } 实现的关键原理是使用Iterable接口使一个类变成可迭代的: 该接口包含一个iterator()方法用于返回一个Iterator对象。Iterator接口定义Iterator对象和hasNext(), next()方法来进行实际的迭代操作。\npublic class ArrayMap\u0026lt;K, V\u0026gt; implements Map61B\u0026lt;K, V\u0026gt;, Iterable\u0026lt;K\u0026gt; { private K[] keys; private V[] values; int size; public ArrayMap() { keys = (K[]) new Object[100]; values = (V[]) new Object[100]; size = 0; } @Override public Iterator\u0026lt;T\u0026gt; iterator() { return new KeyIterator(); } public class KeyIterator implements Iterator\u0026lt;K\u0026gt; { private int ptr; public KeyIterator() { ptr = 0; } public boolean hasNext() { return (ptr != size); } public K next() { K returnItem = keys[ptr]; ptr = ptr + 1; return returnItem; } } } 不同的数据结构，Iterator有不同的实现方式.\nKeyIterator即使是private也可以编译, 因为iterator()在这里是public的:\nimport java.util.Iterator; public class Demo{ public static void main(String[] args) { ArrayMap\u0026lt;String, Integer\u0026gt; am = new ArrayMap\u0026lt;String, Integer\u0026gt;(); Iterator\u0026lt;String\u0026gt; it = am.iterator(); for (String s : am) { ... } }1 } 除了用嵌套类来自定义实现Iterator, 也可以利用数据结构本身的特性. 比如ArrayMap里面刚好包含一个可迭代的数据结构List keys\npublic Iterator\u0026lt;T\u0026gt; iterator() { List\u0026lt;K\u0026gt; keylist = keys(); return keylist.Iterator(); } 注意要点\nhasNext()的判断依据是当前状态下能返回至少一个成员, 不要混淆为下一次能否返回: 因为迭代时过程中, 每次调用next()之前, java 都会先调用hasNext(). 实现方法时, 要保证第一次next()返回的是第一个成员. ","permalink":"https://congchan.github.io/posts/java-%E8%BF%AD%E4%BB%A3-iteration/","summary":"\u003cp\u003eJava提供了 foreach (enhanced for) 的循环简写语法:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eArrayMap\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eInteger\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eam\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eArrayMap\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eInteger\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eam\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e实现的关键原理是使用\u003ccode\u003eIterable\u003c/code\u003e接口使一个类变成可迭代的: 该接口包含一个\u003ccode\u003eiterator()\u003c/code\u003e方法用于返回一个\u003ccode\u003eIterator\u003c/code\u003e对象。\u003ccode\u003eIterator\u003c/code\u003e接口定义\u003ccode\u003eIterator\u003c/code\u003e对象和\u003ccode\u003ehasNext(), next()\u003c/code\u003e方法来进行实际的迭代操作。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eArrayMap\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eMap61B\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eIterable\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eArrayMap\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eObject\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e100\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eV\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eObject\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e100\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e     \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eIterator\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eT\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eiterator\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eKeyIterator\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eKeyIterator\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eIterator\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eptr\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eKeyIterator\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eptr\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eboolean\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ehasNext\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eptr\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e!=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003enext\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003eK\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ereturnItem\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eptr\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003eptr\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eptr\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ereturnItem\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e不同的数据结构，\u003ccode\u003eIterator\u003c/code\u003e有不同的实现方式.\u003c/p\u003e","title":"Java 迭代 Iteration"},{"content":"Higher Order Functions A higher order function is a function that treats other functions as data.\n在 Java 7 及之前的版本, memory boxes (variables) 不能包含指向 functions 的 pointers, 也就是无法给 functions 指定 types. 所以不能像Python一样直接把 function 作为参数传递到另一个 function 中。只能借用 interface：\npublic interface IntUnaryFunction { int apply(int x); } // 定义一个方法 public class TenX implements IntUnaryFunction { /* Returns ten times the argument. */ public int apply(int x) { return 10 * x; } } // 高阶方法 public static int do_twice(IntUnaryFunction f, int x) { return f.apply(f.apply(x)); } // 调用高阶方法 System.out.println(do_twice(new TenX(), 2)); 在JAVA 8中，提供了很多函数式的接口。Java 8 引入 java.util.Function\u0026lt;T, R\u0026gt;接口, 可以接受存储一个函数，\u0026lt;T, R\u0026gt;对应该函数的参数和返回对象\nCall Back Java接口提供了回调(call back)的能力:\n有时一个函数需要调用另一个尚未写好的 helper function, 这时这个 helper function 就是 call back。比如“排序函数”需要“比较函数”的帮助。 不同语言对于回调有不同的处理方式 Python, Perl, ML, Javascript 用函数传递 - first-class functions, Higher Order Functions Java 选择把函数包含在一个接口中 C: function pointers. C++: class-type functors. C#: delegates. 比如Java的 Insertion Sort 可以排序任何类型的数据Insertion.sort(a);, a可以是Double, String, java.io.File数组. 在这里Callback就是对一个可执行代码的引用:\n・Client passes array of objects to sort() function. ・The sort() function calls back object\u0026rsquo;s compareTo() method as needed.\n","permalink":"https://congchan.github.io/posts/java-%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0%E5%92%8C%E5%9B%9E%E8%B0%83/","summary":"\u003ch2 id=\"higher-order-functions\"\u003eHigher Order Functions\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA higher order function is a function that treats other functions as data.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e在 Java 7 及之前的版本, memory boxes (variables) 不能包含指向 functions 的 pointers, 也就是无法给 functions 指定 types. 所以不能像Python一样直接把 function 作为参数传递到另一个 function 中。只能借用 interface：\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003einterface\u003c/span\u003e \u003cspan class=\"nc\"\u003eIntUnaryFunction\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eapply\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// 定义一个方法\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTenX\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eIntUnaryFunction\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/* Returns ten times the argument. */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eapply\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e10\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e// 高阶方法\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003edo_twice\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eIntUnaryFunction\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eapply\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eapply\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e));\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e// 调用高阶方法\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edo_twice\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eTenX\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e));\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e在JAVA 8中，提供了很多函数式的接口。Java 8 引入\u003ccode\u003e java.util.Function\u0026lt;T, R\u0026gt;\u003c/code\u003e接口, 可以接受存储一个函数，\u003ccode\u003e\u0026lt;T, R\u0026gt;\u003c/code\u003e对应该函数的参数和返回对象\u003c/p\u003e","title":"Java 高阶函数和回调"},{"content":"主要介绍：\nHashcode（哈希码）与 equals（判断相等）的关系 Hashcode 方法的底层实现原理 开发中需要掌握的原则和方法 HashSet, HashMap, HashTable HashSet底层是调用HashMap. HashMap 使用hashCode和equals来进行对象比较。\n拿HashSet和add()举例(其余的数据结构,和 remove, contains等方法类似): 假设HashSet里面已经有了obj1, 那么当调用HashSet.add(obj2)时:\nif (obj1 == obj2), 那么没有必要调用 hashCode(), 已经有了这个对象, 没必要添加了 else, if hashCode 不同，那么可以直接添加了, 没必要进一步调用 obj1.equals(obj2) 来判断对象是否相等 else hashCode 相同，那么需要进一步调用obj1.equals(obj2) 下面这段代码虽然 HashSet 只存了 a 对象，但当检查是否包含 b 对象时，返回true。\nHashSet\u0026lt;String\u0026gt; wordSet = new HashSet\u0026lt;String\u0026gt;(); String a = \u0026#34;hello\u0026#34;; String b = \u0026#34;hello\u0026#34;; wordSet.add(a); return wordSet.contains(b); // return true 根据Javadoc for Set.\nadds the specified element e to this set if the set contains no element e2 such that (e==null ? e2==null : e.equals(e2)).\n根据Javadoc for String.equals\nCompares this string to the specified object. The result is true if and only if the argument is not null and is a String object that represents the same sequence of characters as this object.\nJava的set是使用它包含的元素（对象）的 equals()来比较 b 和 a 的。这里 String 类的equals()method 是比较字符串值是否相等(准确的说，是先检查是不是引用同一个对象，再看是不是同一个类，再比较值)，而不是引用的对象是否一样，故b.equals(a)是 true。\n同样的，remove 和 add 也会先进行类似检查。\n问题是，为何 hashCode 不同，就没有进一步调用equals()的必要呢？因为有一个前提是代码遵守The hashCode contract。\nHashcode and equals 在Java中，每个对象都有一个hashCode，它有时容易被人遗忘或误用。有以下三点需要注意，避免掉入常见的陷阱。\n根据 The hashCode contract:\nObjects that are equal must have the same hash code within a running process.\n除了字面意思，也有其他隐含的意思: 不相等的对象的hashcode也可能一样; 具有相同 hash code 的对象不一定相等.\nYou must override hashCode() in every class that overrides equals(). Failure to do so will result in a violation of the general contract for Object.hashCode(), which will prevent your class from functioning properly in conjunction with all hash-based collections, including HashMap, HashSet, and Hashtable. \u0026mdash; Effective Java, by Joshua Bloch\n根据这个contract，可以延伸出以下实践原则：\n一、 每当你 override equals 时，也要 override hashCode 假如你需要使用不一样的equals判断标准，那么就需要重写equals。但假如仅仅重写equals，而不重写hashcode()，就可能会违背 The hashCode contract。\n为什么？因为 hashCode method 需要同时适配真正使用到的 equals method 的判断标准。通过重写equals，我们重新声明了一种判断对象是否相等的标准，但原始的 hashCode method还是会将所有对象视为不同的对象。所以如果没有不重写hashcode，那么根据@Override equals 判断为相同的对象将拥有不同的hashcode（可能）。这样，即使已经有了这个object，在HashMap上调用 contains() 也会返回false。\n例子：在Java的创建街道street这个类，在判断两条街道是否相同时，我们有自定义的规则 - 只要是在同一个城市，有同样的街道名，那么两个street就相等，即使他们是存放在不同内存位置的两个对象（Java 的 Object 原生的equals是根据引用的对象内存地址来比较判断的）。\npublic class Street { private String name; private String city; // ... @Override public boolean equals(Object obj) { if (!(obj instanceof Street)) return false; if (obj == this) return true; Street rhs = (Street) obj; return new EqualsBuilder(). // if deriving: appendSuper(super.equals(obj)). append(name, rhs.name). append(age, rhs.city). isEquals(); } @Override public int hashCode() { return new HashCodeBuilder(17, 31). // two randomly chosen prime numbers // if deriving: appendSuper(super.hashCode()). append(name). append(city). toHashCode(); } } 如果没有重写hashCode()， 那么两个名字和所在城市一样的，但引用不同地址的street就会按照默认的 hashcode() 返回不一样的code，但是根据重写的equals(), 他们是一样的, 这样就违背了 hashCode contract。\n为了安全起见，让Eclipse IDE 生成 equals 和 hashCode 函数：Source \u0026gt; Generate hashCode() and equals()... 为了提醒自己, 还可以配置Eclipse以检测是否有违反此规则的情况，并为仅重写了equals但没重写hashCode的情况显示错误：Preferences \u0026gt; Java \u0026gt; Compiler \u0026gt; Errors/Warnings, then use the quick filter to search for “hashcode” HashCode collisions HashCode collisions 指两个不同的对象具有相同的hashcode这种情况, 这不是什么严重的问题. 只是会导致更多的搜索步骤，太多collisions就可能会降低系统性能\n但是，如果将HashCode错误地用作对象的唯一句柄，例如将其用作Map中的key，那么有时会得到错误的对象。虽然collisions一般很少见，但却是不可避免的。例如，字符串“Aa”和“BB”产生相同的hashCode：2112. 因此衍生出第二个原则\n二、永远不要把hashcode当做key来使用\nJava中有4,294,967,296个（232)可能的int值）。既然拥有40亿个插槽，collisions似乎几乎不可能对吧？\n但事实上，也不是那么不可能。试想，一个房间里有23名随机人员。你如何估计里面有两个人生日一样的概率？很低？因为一年有365天？事实上，概率约为50％！这种现象被称为生日问题(悖论)。\n如果一个房间里有23个或23个以上的人，那么至少有两个人的生日相同的概率要大于50%。\n问题的本质是\u0026quot;23人之中两两之间存在生日相同的概率\u0026quot;\u0026quot;,而不是\u0026quot;其他22人与其中一个人的生日相同的概率\u0026quot;.\n类比到hashcode里，这意味着有77,163个不同的对象，collisions概率是50%（假设有一个理想的hashCode函数，将对象均匀分布在所有可用的buckets中）。\nHashCodes 会变 HashCode 不保证在不同的执行过程中总能返回相同的code。根据JavaDoc：Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application.\n这种情况并不常见，实际上，库中的某些类甚至指定了用于计算hashcode的精确公式（例如String）。对于这些类，hashcode总是相同的。但是，尽管大多数的hashCode方法提供了稳定的值，但我们不能依赖它。正如这篇文章所指出的那样，Java库实际上在不同的进程中返回不同的hashCode值，这往往会让人们感到困惑。 Google的Protocol Buffers就是一个例子。 因此，您不应该在分布式应用程序中使用hash code。即使两者相等，远程对象的 hash code 也可能与本地的不同。\n三、不要在分布式应用程序中使用 hashCode 此外，要意识到，hashCode函数的实现可能会随着版本的更改而改变。因此我们的代码最好不依赖任何特定的hash code 值。例如，你不应该使用hash code来保持某种状态，不然下次运行时，“相同”对象的hash code可能会不同。\n所以最好的建议可能是：除非自己创建了基于 hashcode 算法，否则根本就不要使用 hashCode 呵呵……\n总结 在依赖于 HashSet, HashMap, HashTable \u0026hellip; 等数据结构的程序中： 3. 仅重写 equals()，会导致业务出错 4. 仅重写 hashcode(), 在比较两个对象时不会强制Java忽略内存地址 3. 如果不涉及对象比较(比如仅仅是iteration), 那么不需要hashCode and/or equals\n参考： https://eclipsesource.com/blogs/2012/09/04/the-3-things-you-should-know-about-hashcode/ https://stackoverflow.com/questions/27581/what-issues-should-be-considered-when-overriding-equals-and-hashcode-in-java\n","permalink":"https://congchan.github.io/posts/java-hash-@override-equals-hashcode/","summary":"\u003cp\u003e主要介绍：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHashcode（哈希码）与 equals（判断相等）的关系\u003c/li\u003e\n\u003cli\u003eHashcode 方法的底层实现原理\u003c/li\u003e\n\u003cli\u003e开发中需要掌握的原则和方法\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"hashset-hashmap-hashtable\"\u003eHashSet, HashMap, HashTable\u003c/h2\u003e\n\u003cp\u003eHashSet底层是调用HashMap. HashMap 使用hashCode和equals来进行对象比较。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e拿HashSet和add()举例(其余的数据结构,和 remove, contains等方法类似): 假设HashSet里面已经有了obj1, 那么当调用HashSet.add(obj2)时:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eif (obj1 == obj2), 那么没有必要调用 hashCode(), 已经有了这个对象, 没必要添加了\u003c/li\u003e\n\u003cli\u003eelse, if hashCode 不同，那么可以直接添加了, 没必要进一步调用 obj1.equals(obj2) 来判断对象是否相等\u003c/li\u003e\n\u003cli\u003eelse hashCode 相同，那么需要进一步调用obj1.equals(obj2)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下面这段代码虽然 HashSet 只存了 a 对象，但当检查是否包含 b 对象时，返回true。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eHashSet\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ewordSet\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eHashSet\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;hello\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;hello\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003ewordSet\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ewordSet\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003econtains\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// return true\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e根据\u003ca href=\"https://docs.oracle.com/javase/6/docs/api/java/util/Set.html#add%28E%29\"\u003eJavadoc for Set\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eadds the specified element e to this set if the set contains no element e2 such that (e==null ? e2==null : e.equals(e2)).\u003c/p\u003e","title":"Java Hash @Override equals() hashcode()"},{"content":"多线程 线程是独立的执行空间，Java语言内置多线程功能，用类Thread来表达。每个Java应用程序会启动一个主线程 \u0026ndash; 将main()放在自己的执行空间的最开始处. JVM会负责主线程的启动(以及其他比如GC的系统线程). 程序员负责启动自己的建立的线程.\n启动新线程\n建立Runnable对象作为线程任务Runnable job = new MyRunnable(), Runnable接口只有一个方法run() 建立Thread对象并赋值Runnable Thread thread1 = new Thread(job); Thread thread2 = new Thread(job) 启动thread.start(); 另一种创建线程的方法是用Thread的子类覆盖掉run(), 构造新线程Thread t = new Thread();. 从OO的角度看待, 此时Thread 与线程任务是不同概念的. 让子类继承Thread的目的通常是需要更特殊的Thread, 需要特殊的行为, 如果没有这种需求, 就没必要继承Thread.\n线程调度器 多线程间的切换由调度器scheduler来管理, 线程有多种状态:\n执行中, sleep(2000): 睡眠2000ms, 时间到之前不会被执行, 但时间到了并不保证一定会被执行. 可能会抛出InterruptedException, 所以对它的调用要包含在try/catch中. locked. 线程的run完成执行后, 将无法重新启动.\n调度器在不同的JVM有不同的做法. 测试多线程时需要在不同机器上测试.\n并发 Concurrency并发环境中, 为了避免冲突, 需要上锁, 使用synchronized来修饰方法使之每次只能被单一线程读写. 同步化是有代价的, 查询钥匙有性能上的损耗, 同步化也会强制线程排队执行, 还可能出现死锁.\n死锁 因为两个线程互相持有对方正在等待的东西, 导致没有一方可以脱离等待. 数据库有事务回滚机制来复原死锁的事务, 但Java没有处理死锁的机制.\nVolatile Java为了提高程序运行效率, 编译器自动会优化, 把经常被访问的变量混存起来, 程序在读取这个变量时有可能会直接从缓存(例如寄存器)中读取这个值, 而不会去内存中读取. 但在多线程环境中, 变量的值可能因为别的线程而改变了, 而该缓存的值不会相应改变, 从而造成应用程序读取的值和实际的变量值不一致.\n使用volatile修饰被不同线程访问和修改的变量, 使得其每次被用到时, 都是直接从对应的内存中提取, 而不会利用缓存了.\npublic class MyThread implements Runnable { private volatile Boolean flag; public void stop() { flag = false; } public void run() { while(flag) ; } } 如果flag没有被声明为volatile, 那么当这个县城的run()判断flag时, 使用的有可能是缓存中的值, 此时就不能及时地获取其他线程对flag所做的操作了.\n但volatile不能保证操作的原子性, 因此一般情况下不能代替sychronized. 此外, 使用volatile会阻止编译器对代码的优化, 因此会降低\n","permalink":"https://congchan.github.io/posts/java-%E5%A4%9A%E7%BA%BF%E7%A8%8B/","summary":"\u003ch2 id=\"多线程\"\u003e多线程\u003c/h2\u003e\n\u003cp\u003e线程是独立的执行空间，Java语言内置多线程功能，用类\u003ccode\u003eThread\u003c/code\u003e来表达。每个Java应用程序会启动一个主线程 \u0026ndash; 将\u003ccode\u003emain()\u003c/code\u003e放在自己的执行空间的最开始处. JVM会负责主线程的启动(以及其他比如GC的系统线程). 程序员负责启动自己的建立的线程.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e启动新线程\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e建立\u003ccode\u003eRunnable\u003c/code\u003e对象作为线程任务\u003ccode\u003eRunnable job = new MyRunnable()\u003c/code\u003e, \u003ccode\u003eRunnable\u003c/code\u003e接口只有一个方法\u003ccode\u003erun()\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e建立Thread对象并赋值Runnable \u003ccode\u003eThread thread1 = new Thread(job); Thread thread2 = new Thread(job)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e启动\u003ccode\u003ethread.start();\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e另一种创建线程的方法是用Thread的子类覆盖掉\u003ccode\u003erun()\u003c/code\u003e, 构造新线程\u003ccode\u003eThread t = new Thread();\u003c/code\u003e. 从OO的角度看待, 此时Thread 与线程任务是不同概念的. 让子类继承Thread的目的通常是需要更特殊的Thread, 需要特殊的行为, 如果没有这种需求, 就没必要继承Thread.\u003c/p\u003e\n\u003ch3 id=\"线程调度器\"\u003e线程调度器\u003c/h3\u003e\n\u003cp\u003e多线程间的切换由调度器scheduler来管理, 线程有多种状态:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e执行中,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esleep(2000)\u003c/code\u003e: 睡眠2000ms, 时间到之前不会被执行, 但时间到了并不保证一定会被执行. 可能会抛出InterruptedException, 所以对它的调用要包含在try/catch中.\u003c/li\u003e\n\u003cli\u003elocked.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e线程的\u003ccode\u003erun\u003c/code\u003e完成执行后, 将无法重新启动.\u003c/p\u003e\n\u003cp\u003e调度器在不同的JVM有不同的做法. 测试多线程时需要在不同机器上测试.\u003c/p\u003e\n\u003ch3 id=\"并发\"\u003e并发\u003c/h3\u003e\n\u003cp\u003eConcurrency并发环境中, 为了避免冲突, 需要上锁, 使用\u003ccode\u003esynchronized\u003c/code\u003e来修饰方法使之每次只能被单一线程读写. 同步化是有代价的, 查询钥匙有性能上的损耗, 同步化也会强制线程排队执行, 还可能出现死锁.\u003c/p\u003e\n\u003ch3 id=\"死锁\"\u003e死锁\u003c/h3\u003e\n\u003cp\u003e因为两个线程互相持有对方正在等待的东西, 导致没有一方可以脱离等待. 数据库有事务回滚机制来复原死锁的事务, 但Java没有处理死锁的机制.\u003c/p\u003e","title":"Java 多线程"},{"content":"Socket 网络上的两个程序通过一个双向的通讯连接实现数据的交换，这个双向链路的一端称为一个Socket，确切的说Socket是个代表两台机器之间网络连接的对象。Socket是TCP/IP协议的一个十分流行的编程界面，一个Socket由一个IP地址和一个端口号唯一确定。 Socket socket = new Socket(\u0026quot;192.168.2.1\u0026quot;, 5000), 第一个参数是IP地址, 第二个参数是端口. Socket连接的建立代表两台机器之间存有对方的信息, 包括网络地址和TCP端口号.\n但是，Socket所支持的协议种类也不光TCP/IP一种，因此两者之间是没有必然联系的。在Java环境下，Socket编程主要是指基于TCP/IP协议的网络编程。\n使用BufferedReader从Socket上读取数据：\n建立对服务器的Socket连接Socket socket = new Socket(\u0026quot;192.168.2.1\u0026quot;, 5000) 建立连接到Socket上低层输入串流的InputStreamReader InputStreamReader stream = new InputStreamReader(socket.getInputStream());, 作为从低层和高层串流间的桥梁, 把来自服务器的字节转换为字符 建立缓冲区BufferedReader来读取 BufferedReader reader = new BufferedReader(stream); String msg = reader.readLine(); 用PrintWriter写数据到Socket上:\n建立对服务器的Socket连接 建立链接到Socket的PrintWriter PrintWriter writer = new PrintWriter(socket.getOutputStream()), 作为字符数据和字节间的转换桥梁, 可以衔接Strings和Socket两端 写入数据 writer.println(\u0026#34;You have a message\u0026#34;); TCP/IP TCP/IP协议集包括应用层, 传输层，网络层，网络访问层。\n应用层协议主要包括如下几个：FTP、TELNET、DNS、SMTP、NFS、HTTP。\nFTP(File Transfer Protocol）是文件传输协议，一般上传下载用FTP服务，数据端口是20H，控制端口是21H。 Telnet服务是用户远程登录服务，使用23H端口，使用明码传送，保密性差、简单方便。 DNS(Domain Name Service）是域名解析服务，提供域名到IP地址之间的转换，使用端口53。 SMTP(Simple Mail Transfer Protocol）是简单邮件传输协议，用来控制信件的发送、中转，使用端口25。 NFS（Network File System）是网络文件系统，用于网络中不同主机间的文件共享。 HTTP(Hypertext Transfer Protocol）是超文本传输协议，用于实现互联网中的WWW服务，使用端口80。 网络接口层:\nInternet协议(IP): 数据链路层是负责接收IP数据包并通过网络发送，或者从网络上接收物理帧，抽出IP数据包，交给IP层。 正向地址解析协议(ARP): 通过已知的IP，寻找对应主机的MAC地址。 反向地址解析协议(RARP): 通过MAC地址确定IP地址。比如无盘工作站还有DHCP服务。 TCP和UDP TCP 协议和 UDP 协议都属于TCP/IP协议簇。也叫端到端传输协议，因为他们将数据从一个应用程序传输到另一个应用程序，而 IP 协议只是将数据从一个主机传输到另一个主机。\nTCP是面向连接的通信协议，通过三次握手建立连接，通讯完成时要拆除连接，由于TCP是面向连接的所以只能用于端到端的通讯。TCP提供的是一种可靠的数据流服务，采用“带重传的肯定确认”技术来实现传输的可靠性。TCP还采用一种称为“滑动窗口”的方式进行流量控制，所谓窗口实际表示接收能力，用以限制发送方的发送速度。\nUDP是面向无连接的通讯协议，UDP数据包括目的端口号和源端口号信息，\nUDP通讯时不需要接收方确认，属于不可靠的传输，可能会出现丢包现象，实际应用中要求程序员编程验证。 不保证可靠交付，不保证顺序，因此主机不需要维持复杂的链接状态表 由于传输数据不建立连接，因此也就不需要维护连接状态，包括收发状态等，因此一台服务机可同时向多个客户机传输相同的消息。 应用：\nTCP在网络通信上有极强的生命力，例如远程连接（Telnet）和文件传输（FTP）都需要不定长度的数据被可靠地传输。但是可靠的传输是要付出代价的，对数据内容正确性的检验必然占用计算机的处理时间和网络的带宽，因此TCP传输的效率不如UDP高。 UDP操作简单，而且仅需要较少的监护，因此通常用于局域网高可靠性的分散系统中client/server应用程序。例如视频会议系统，并不要求音频视频数据绝对的正确，只要保证连贯性就可以了，这种情况下显然使用UDP会更合理一些。 TCP连接与断开 最初两端的TCP进程都处于CLOSED关闭状态，客户A主动打开连接，而服务器B被动打开连接, 过程类似于想给你发数据可以吗？ - 可以，现在发？ - 对，请准备接收”。过程是三次握手：\n起初两端都处于CLOSED关闭状态，Client将标志位SYN置为1，随机产生一个值seq=x，并将该数据包发送给Server，Client进入SYN-SENT状态，等待Server确认； Server收到数据包后由标志位SYN=1得知Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=x+1，随机产生一个值seq=y，并将该数据包发送给Client以确认连接请求，Server进入SYN-RCVD状态，此时操作系统为该TCP连接分配TCP缓存和变量； Client收到确认后，检查ack是否为x+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=y+1，并且此时操作系统为该TCP连接分配TCP缓存和变量，并将该数据包发送给Server，Server检查ack是否为y+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client和Server就可以开始传输数据。 之所以不可以仅靠二次握手, 主要为了防止已失效的连接请求报文段突然又传送到了B(因为延误等原因)，因而产生错误, 故A还要发送一次确认.\n断开连接需要四次挥手:\nA的应用进程先向其TCP发出连接释放报文段（FIN=1，序号seq=u），并停止再发送数据，主动关闭TCP连接，进入FIN-WAIT-1（终止等待1）状态，提出停止TCP连接的请求, 等待B的确认。 B收到连接释放报文段后即发出确认报文段，（ACK=1，确认号ack=u+1，序号seq=v），确认来路方向上的TCP连接将关闭, B进入CLOSE-WAIT（关闭等待）状态，此时的TCP处于半关闭状态，A到B的连接释放。A收到B的确认后，进入FIN-WAIT-2（终止等待2）状态，等待B发出的连接释放报文段。 B已经没有要向A发出的数据，B再提出反方向的关闭请求, B发出连接释放报文段（FIN=1，ACK=1，序号seq=w，确认号ack=u+1），B进入LAST-ACK（最后确认）状态，等待A的确认。 A收到B的连接释放报文段后，对此发出确认报文段（ACK=1，seq=u+1，ack=w+1），A进入TIME-WAIT（时间等待）状态。此时TCP未释放掉，需要经过时间等待计时器设置的时间2 Maximum Segment Lifetime (MSL)后，A才进入CLOSED状态。 总的流程图 其中三个比较重要的状态\nSYN_RECV ：服务端收到建立连接的SYN没有收到ACK包的时候处在SYN_RECV状态。有两个相关系统配置. 这些处在SYNC_RECV的TCP连接称为半连接，并存储在内核的半连接队列中，在内核收到对端发送的ack包时会查找半连接队列，并将符合的requst_sock信息存储到完成三次握手的连接的队列中，然后删除此半连接。大量SYNC_RECV的TCP连接会导致半连接队列溢出，这样后续的连接建立请求会被内核直接丢弃，这就是SYN Flood攻击。 net.ipv4.tcp_synack_retries ：INTEGER 默认值是5. 对于远端的连接请求SYN，内核会发送SYN + ACK数据报，以确认收到上一个 SYN连接请求包。这是所谓的三次握手(threeway handshake)机制的第二个步骤。这里决定内核在放弃连接之前所送出的 SYN+ACK 数目。不应该大于255，默认值是5，对应于180秒左右时间。通常我们不对这个值进行修改，因为我们希望TCP连接不要因为偶尔的丢包而无法建立。 net.ipv4.tcp_syncookies 一般服务器都会设置 net.ipv4.tcp_syncookies=1来防止SYN Flood攻击。假设一个用户向服务器发送了SYN报文后突然死机或掉线，那么服务器在发出SYN+ACK应答报文后是无法收到客户端的ACK报文的（第三次握手无法完成），这种情况下服务器端一般会重试（再次发送SYN+ACK给客户端）并等待一段时间后丢弃这个未完成的连接，这段时间的长度我们称为SYN Timeout，一般来说这个时间是分钟的数量级（大约为30秒-2分钟）。 CLOSE_WAIT: 被动关闭的server收到FIN后，但未发出ACK的TCP状态是CLOSE_WAIT。出现这种状况一般都是由于server端代码的问题，如果你的服务器上出现大量CLOSE_WAIT，应该要考虑检查代码。CLOSE_WAIT状态什么时候终结， 取决于应用程序什么时候来close socket, 从理论上来讲，只要被动关闭端不断电，进程不退出， 那么CLOSE_WAIT状态就会一直持续下去。因此理论上CLOSE_WAIT的最大时间可以达到无限长。 TIME_WAIT: 发起socket主动关闭的一方 socket将进入TIME_WAIT状态。TIME_WAIT状态将持续2个MSL(Max Segment Lifetime),在Windows下默认为4分钟，即240秒。TIME_WAIT状态下的socket不能被回收使用. 具体现象是对于一个处理大量短连接的服务器,如果是由服务器主动关闭客户端的连接，将导致服务器端存在大量的处于TIME_WAIT状态的socket， 甚至比处于Established状态下的socket多的多,严重影响服务器的处理能力，甚至耗尽可用的socket，停止服务。TIME_WAIT是TCP协议用以保证被重新分配的socket不会受到之前残留的延迟重发报文影响的机制,是必要的逻辑保证。 端口 面向连接服务TCP协议和无连接服务UDP协议使用16bits端口号来表示和区别网络中的不同应用程序，网络层协议IP使用特定的协议号（TCP 6，UDP 17）来表示和区别传输层协议。任何TCP/IP实现所提供的服务都是1-1023之间的端口号，这些端口号由IANA分配管理, 作为保留端口供特定服务使用。其中，低于255的端口号保留用于公共应用；255到1023的端口号分配给各个公司，用于特殊应用；对于高于1023的端口号，称为临时端口号，IANA未做规定。不同程序无法共享一个端口, 因此新程序只能使用空闲端口.\n常用的保留TCP端口号有： HTTP 80，FTP 20/21，Telnet 23，SMTP 25，DNS 53等。 常用的保留UDP端口号有： DNS 53，BootP 67（server）/ 68（client），TFTP 69，SNMP 161等。\n每个TCP报文头部都包含源端口号source port和目的端口号destination port，用于标识和区分源端设备和目的端设备的应用进程。在TCP/IP协议栈中，源端口号和目的端口号分别与源IP地址和目的IP地址组成套接字，唯一的确定一条TCP连接。 相对于TCP报文，UDP报文只有少量的字段：源端口号、目的端口号、长度、校验和等，各个字段功能和TCP报文相应字段一样。\n在linux一般使用netstat来查看系统端口使用情况。netstat命令的功能是显示网络连接、路由表和网络接口信息，可以让用户得知目前都有哪些网络连接正在运作。 比如查看 TCP 22 端口有两种方法： 第一种查看方法\n[root@Demon proc]# netstat -ntlp | grep 22 tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1960/sshd tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 2263/mysqld tcp 0 0 :::22 :::* LISTEN 1960/sshd 第二种查看方法\n[root@Demon proc]# lsof -i tcp:22 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME sshd 1960 root 3u IPv4 14435 0t0 TCP *:ssh (LISTEN) sshd 1960 root 4u IPv6 14441 0t0 TCP *:ssh (LISTEN) -i 显示所有网络接口的信息。\nSocket通讯的过程 Server端Listen(监听)某个端口是否有连接请求，Client端向Server 端发出Connect(连接)请求，Server端向Client端发回Accept（接受）消息。一个连接就建立起来了。Server端和Client 端都可以通过Send，Write等方法与对方通信。\n对于一个功能齐全的Socket，都要包含以下基本结构，其工作过程包含以下四个基本的步骤：\n（1） 创建Socket；\n（2） 打开连接到Socket的输入/出流；\n（3） 按照一定的协议对Socket进行读/写操作；\n（4） 关闭Socket.（在实际应用中，并未使用到显示的close，虽然很多文章都推荐如此，不过在我的程序中，可能因为程序本身比较简单，要求不高，所以并未造成什么影响。）\n","permalink":"https://congchan.github.io/posts/java-%E5%A5%97%E6%8E%A5%E5%AD%97socket/","summary":"\u003ch2 id=\"socket\"\u003eSocket\u003c/h2\u003e\n\u003cp\u003e网络上的两个程序通过一个双向的通讯连接实现数据的交换，这个双向链路的一端称为一个Socket，确切的说Socket是个代表两台机器之间网络连接的对象。Socket是TCP/IP协议的一个十分流行的编程界面，一个Socket由一个IP地址和一个端口号唯一确定。\n\u003ccode\u003eSocket socket = new Socket(\u0026quot;192.168.2.1\u0026quot;, 5000)\u003c/code\u003e, 第一个参数是IP地址, 第二个参数是端口. Socket连接的建立代表两台机器之间存有对方的信息, 包括网络地址和TCP端口号.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e但是，Socket所支持的协议种类也不光TCP/IP一种，因此两者之间是没有必然联系的。在Java环境下，Socket编程主要是指基于TCP/IP协议的网络编程。\u003c/p\u003e\n\u003cp\u003e使用BufferedReader从Socket上读取数据：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e建立对服务器的Socket连接\u003ccode\u003eSocket socket = new Socket(\u0026quot;192.168.2.1\u0026quot;, 5000)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e建立连接到Socket上低层输入串流的InputStreamReader \u003ccode\u003eInputStreamReader stream = new InputStreamReader(socket.getInputStream());\u003c/code\u003e, 作为从低层和高层串流间的桥梁, 把来自服务器的字节转换为字符\u003c/li\u003e\n\u003cli\u003e建立缓冲区BufferedReader来读取\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eBufferedReader\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ereader\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eBufferedReader\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estream\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emsg\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ereader\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ereadLine\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e用PrintWriter写数据到Socket上:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e建立对服务器的Socket连接\u003c/li\u003e\n\u003cli\u003e建立链接到Socket的PrintWriter \u003ccode\u003ePrintWriter writer = new PrintWriter(socket.getOutputStream())\u003c/code\u003e, 作为字符数据和字节间的转换桥梁, 可以衔接Strings和Socket两端\u003c/li\u003e\n\u003cli\u003e写入数据\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ewriter\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;You have a message\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"tcpip\"\u003eTCP/IP\u003c/h3\u003e\n\u003cp\u003eTCP/IP协议集包括\u003cstrong\u003e应用层, 传输层，网络层，网络访问层\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e应用层协议主要包括如下几个：FTP、TELNET、DNS、SMTP、NFS、HTTP。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFTP(File Transfer Protocol）是文件传输协议，一般上传下载用FTP服务，数据端口是20H，控制端口是21H。\u003c/li\u003e\n\u003cli\u003eTelnet服务是用户远程登录服务，使用23H端口，使用明码传送，保密性差、简单方便。\u003c/li\u003e\n\u003cli\u003eDNS(Domain Name Service）是域名解析服务，提供域名到IP地址之间的转换，使用端口53。\u003c/li\u003e\n\u003cli\u003eSMTP(Simple Mail Transfer Protocol）是简单邮件传输协议，用来控制信件的发送、中转，使用端口25。\u003c/li\u003e\n\u003cli\u003eNFS（Network File System）是网络文件系统，用于网络中不同主机间的文件共享。\u003c/li\u003e\n\u003cli\u003eHTTP(Hypertext Transfer Protocol）是超文本传输协议，用于实现互联网中的WWW服务，使用端口80。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e网络接口层:\u003c/p\u003e","title":"Java 套接字Socket"},{"content":"抽象类 有些情况下，有些父类在实际应用中只有被继承的和产生多态的意义，而没有实例化的意义（比如抽象的Animal, Canine等，实例化这些抽象概念没有实际意义），所以不希望这种父类会被初始化。通过标记类为抽象类，编译器就知道该类不能创建实例(不能作为new实例, 但可以用于声明类型).\nabstract class Canine extends Animal { ... } Canine c = new Dog; // Canine c = new Canine(); 无法编译 反之，不抽象的类就是具体类.\n有抽象类自然就有抽象方法，抽象的方法没有实体，所有就不会含有具体的实现public abstract void doSomething();. 只有抽象类才能拥有抽象方法. 在抽象父类中定义可继承的抽象方法, 可以定义出一组子类共同的协议, 这样能够保证多态. 但因为抽象方法只是为了标记处多态而存在, 它们没有具体的内容, 这样在继承树结构下的第一个具体类就必须要实现所有的抽象方法. 当然, 树结构中的抽象类也可以提前把抽象方法实现了(default方法).\n抽象类和接口的比较 接口是抽象类的极端形式，接口是完全抽象的，不包含实现（新的特性支持default method）。\n有很多情景需要某个类继承多种父类。比如，因为原来的业务需求比较单一，只需要Animal - Canine - Dog这种类结构就满足需求了, 此时Dog只是Animal的子类. 但后来有了新的功能需求, 上线了宠物功能, 理论上可以为每一种具体的属于宠物的子类添加宠物功能, 这就涉及大量的人工和bug. 但假如额外设计一种Pet类, 那么Pet和Animal会有交叉重叠, 如果让宠物子类同时继承两种超类, 那就是多重继承. 因为多重继承会有致命方块的问题. 所以Java不支持这种方式.\n而接口这个概念, 就是用于解决这个问题的:\npublic interface Pet { public abstract void beFriendly(); public abstract void play(); } // 对于属于宠物的子类，让其实现接口`Pet`. public class Dog extends Canine implements Pet {} 基本上，接口能做的抽象类都可以做。 但接口的最大意义就是其无比的适用性, 若以接口取代具体的子类或抽象类作为参数或返回类型, 那么就可以传入任何有实现该接口的东西. 有了接口,\n类就可以继承超过一个以上的来源: 可以继承某个父类, 并且实现其他接口. 接口自身可以extends多个其他接口. 一个类, 可以实现多个接口. class Dog extends Animal implements Pet, Saveable, paintable {} 一个接口, 可以给不同的类使用, 因此就可以为不同的需求组合出不同的继承层次. 等于说有了接口, 那么不管一个类是什么类, 只要它实现了一个接口, 那么就知道它一定会履行对应的行为合约. 允许不同继承树的类实现共同的接口对Java API来说是非常重要的, 比如要将对象状态保存起来, 只要去实现Serializable接口即可; 打算让对象以单独的线程来执行, 就实现Runnable.\n要注意，接口能扩展extends多个接口，不能实现implement任何接口.\n实际应用中，抽象类通常用于部分地实现接口，在接口和实际的类中间扮演一个中间概念。\npublic interface Car { void move(Speed x); void stop(); } public abstract class DeluxeModel implements Car { public double x; ... public void move(Speed x) { ... } public abstract void autoPilot(); } // 实现时, 要 override 所有抽象方法 public class TeslaX extends DeluxeModel { public void stop() { ... } public void autoPilot() { ... } } 若不确定用哪种, 就优先考虑接口，以降低复杂性。 \u0026ndash; https://docs.oracle.com/javase/tutorial/java/IandI/abstract.html\nAbstract classes 介于接口和 classes 之间。\n方法可以是public或private, 也支持protected和package private. 支持任何类型的变量 无法实例化 除非指定为abstract，否则方法默认是具体的实现 每个类只能实现一个 Abstract classes 抽象类不需要实现其继承的接口所有抽象方法 Interface:\n除非指定access modifier, 否则所有的方法默认都是public （Java 9 支持 private） 可以提供变量, 但都是public static final, 也即没有实例变量 无法实例化 除非指定为default，否则所有方法均为抽象的 一个类可以实现多个接口 根据协议承诺, 实现类必须实现其继承的接口的所有抽象方法; 否则要声明为抽象类. 如何判断应该设计类，子类，抽象类，还是接口呢？\n如果新的类无法对其他的类通过IS-A测试时，就设计不继承其他类的类 只有在需要某类的特殊化版本时，以覆盖或增加新的方法来继承现有的类 当需要定义一群子类的模板，又不想让程序员初始化此模板时，设计出抽象的类给他们用 如果想要定义类可以扮演的角色，使用接口 ","permalink":"https://congchan.github.io/posts/java-%E6%8A%BD%E8%B1%A1%E7%B1%BB/","summary":"\u003ch2 id=\"抽象类\"\u003e抽象类\u003c/h2\u003e\n\u003cp\u003e有些情况下，有些父类在实际应用中只有被继承的和产生多态的意义，而没有实例化的意义（比如抽象的\u003ccode\u003eAnimal\u003c/code\u003e, \u003ccode\u003eCanine\u003c/code\u003e等，实例化这些抽象概念没有实际意义），所以不希望这种父类会被初始化。通过标记类为抽象类，编译器就知道该类不能创建实例(不能作为new实例, 但可以用于声明类型).\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003eabstract\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eCanine\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eAnimal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eCanine\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ec\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e// Canine c = new Canine(); 无法编译\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c!-- more --\u003e\n\u003cp\u003e反之，不抽象的类就是具体类.\u003c/p\u003e\n\u003cp\u003e有抽象类自然就有抽象方法，抽象的方法没有实体，所有就不会含有具体的实现\u003ccode\u003epublic abstract void doSomething();\u003c/code\u003e. 只有抽象类才能拥有抽象方法. 在抽象父类中定义可继承的抽象方法, 可以定义出一组子类共同的协议, 这样能够保证多态. 但因为抽象方法只是为了标记处多态而存在, 它们没有具体的内容, 这样在继承树结构下的第一个具体类就必须要实现所有的抽象方法. 当然, 树结构中的抽象类也可以提前把抽象方法实现了(default方法).\u003c/p\u003e\n\u003ch3 id=\"抽象类和接口的比较\"\u003e抽象类和接口的比较\u003c/h3\u003e\n\u003cp\u003e接口是抽象类的极端形式，接口是完全抽象的，不包含实现（新的特性支持default method）。\u003c/p\u003e\n\u003cp\u003e有很多情景需要某个类继承多种父类。比如，因为原来的业务需求比较单一，只需要\u003ccode\u003eAnimal - Canine - Dog\u003c/code\u003e这种类结构就满足需求了, 此时\u003ccode\u003eDog\u003c/code\u003e只是\u003ccode\u003eAnimal\u003c/code\u003e的子类. 但后来有了新的功能需求, 上线了宠物功能, 理论上可以为每一种具体的属于宠物的子类添加宠物功能, 这就涉及大量的人工和bug. 但假如额外设计一种\u003ccode\u003ePet\u003c/code\u003e类, 那么\u003ccode\u003ePet\u003c/code\u003e和\u003ccode\u003eAnimal\u003c/code\u003e会有交叉重叠, 如果让宠物子类同时继承两种超类, 那就是\u003cstrong\u003e多重继承\u003c/strong\u003e. 因为多重继承会有\u003cstrong\u003e致命方块\u003c/strong\u003e的问题. 所以Java不支持这种方式.\u003c/p\u003e\n\u003cp\u003e而接口这个概念, 就是用于解决这个问题的:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003einterface\u003c/span\u003e \u003cspan class=\"nc\"\u003ePet\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eabstract\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ebeFriendly\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eabstract\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eplay\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e// 对于属于宠物的子类，让其实现接口`Pet`.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDog\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eCanine\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ePet\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e基本上，接口能做的抽象类都可以做。 但接口的最大意义就是其无比的适用性, 若以接口取代具体的子类或抽象类作为参数或返回类型, 那么就可以传入任何有实现该接口的东西. 有了接口,\u003c/p\u003e","title":"Java 抽象类"},{"content":"类的继承扩展 定义class之间的层次关系.\n假设要构建一个RotatingSLList，它具有与SLList相同的功能，如addFirst, size等，但是需要额外的rotateRight操作将最后一项放到列表的前面，因为继承允许子类重用已经定义的类中的代码。所以让RotatingSLList类从SLList继承部分代码:\npublic class RotatingSLList\u0026lt;Item\u0026gt; extends SLList\u0026lt;Item\u0026gt; {} RotatingSLList\u0026ldquo;是一种\u0026rdquo;SLList, extends可以让我们继承SLList的原始功能，并能修改或添加其他功能。\n/** The rotateRight method takes in an existing list, and rotates every element one spot to the right, moving the last item to the front of the list. For example, input [5, 9, 15, 22] should return [22, 5, 9, 15]. */ public void rotateRight() { Item x = removeLast(); addFirst(x); } 通过extends, 子类继承父类的所有成员，成员包括：\n所有实例和静态变量 所有方法 所有嵌套类 但注意，构造函数不继承，并且私有成员不能被子类直接访问。\n从属和拥有的区别：subclass 和 superclass 是上下级从属分类，而不是拥有与被拥有的关系，不要跟 nested class 混淆。\nInterface Methods: Default methods and abstract methods in interfaces are inherited like instance methods. However, when the supertypes of a class or interface provide multiple default methods with the same signature, the Java compiler follows inheritance rules to resolve the name conflict. \u0026ndash; https://docs.oracle.com/javase/tutorial/java/IandI/index.html\nOverriding 上面的例子使用父类的removeLast()把最后的元素直接丢弃了，但假如有一个子类VengefulSLList想保留被丢弃的元素呢?\n子类可以覆盖 override父类的方法。\n区分 Override 与 重载 overloaded：Override 的方法 signature 相同；overloaded的方法同名但不同signature。\n子类可以override父类的removeLast, 通过添加一个实例变量来追踪所有已删除的元素.\npublic class VengefulSLList\u0026lt;Item\u0026gt; extends SLList\u0026lt;Item\u0026gt; { SLList\u0026lt;Item\u0026gt; deletedItems; public VengefulSLList() { deleteItems = new SLList\u0026lt;Item\u0026gt;(); } @Override public Item removeLast() { Item x = super.removeLast(); deletedItems.addLast(x); return x; } /** Prints deleted items. */ public void printLostItems() { deletedItems.print(); } } 继承的好处是代码得以重复利用. 坏处是因为“Is a”的关系, debugging的路径会很烦人。\n但即使不包含这个@Override，我们仍然override了这个方法。所以从技术上来说，@Override并不是必须的。但是，它可以作为一个保障, 提醒编译器我们打算override此方法, 如果过程中出现问题, 编译器可以提醒。假设当我们想 override addLast，却不小心写成addLsat。此时如果不包含@Override，那么可能无法发现错误。如果有了@Override，编译器就会提示我们修复错误。\nConstructors Are Not Inherited Java要求所有子类的构造函数必须先调用其某一超类的构造函数。 \u0026ndash; https://docs.oracle.com/javase/tutorial/java/IandI/super.html\n因为逻辑上，如果作为基础的超类没有构建，那么子类的构建的无从谈起。完整的子类构造函数应该是：\npublic VengefulSLList() { super(); // 第一行 deletedItems = new SLList\u0026lt;Item\u0026gt;(); } 之前的例子没有super();也可以通过编译，是因为Java会自动为我们调用超类的无参数构造函数。\n具体分情况考虑：\n编译器会自动为任何没有构造函数的类提供一个无参数的默认构造函数：这个默认构造函数将调用其超类的（accessible）无参构造函数。 如果子类构造函数没有指定要调用哪个超类构造函数：则编译器将自动调用超类的可访问的无参数构造函数 public class Base { } public class Derived extends Base { } 如果其超类有有参数构造函数，但没有无参数构造函数，那么编译出错： public class Base { public Base(String s) { } } 此时要在子类构造函数第一行添加super(s) 如果超类的无参数构造函数是不可访问的，那么编译出错： public class Base { private Base() { } } 如果没有显式的超类，那么就调用隐式的超类Object的无参构造函数。 Constructor Chaining：当子类构造函数调用其父类的构造函数时（无论是显式还是隐式调用），可以认为有一链式的连续调用构造函数，一直到Object的构造函数\n同样， 可以通过super.someMethod()在子类中调用父类的方法\nInheritance Cheatsheet VengefulSLList extends SLList means VengefulSLList \u0026ldquo;is-an\u0026rdquo; SLList, and inherits all of SLList\u0026rsquo;s members: 总结 Inheritance 的一些要点:\n当子类VengefulSLList extends 超类SLList时, 意味着VengefulSLList也\u0026quot;是\u0026quot;SLList, 并继承SLList的所有成员: Variables, methods, nested classes 除了 constructors: Subclass constructors 必须先调用 superclass constructor; 通过 super 调用 overridden superclass methods 和 constructors. 调用 overridden methods 遵循两个规则:\n编译器只允许与 static type 符合的行为. 对于 overridden methods, 调用是基于 dynamic type 可以使用 casting 来规避 compiler type checking. （子类型）多态 Subtype Polymorphism\n多态（polymorphism），是指相同的消息给予不同的对象会引发不同的动作。\n动态多态（dynamic polymorphism）：通过类继承机制和虚函数机制生效于运行期。可以优雅地处理异质对象集合，只要其共同的基类定义了虚函数的接口。 在面向对象程序设计中，多态一般是指子类型多态（Subtype polymorphism）或包含多态（inclusion polymorphism）。一般是通过某种可代换性（ substitutability）与另一个数据类型（超类型，supertype）相关的数据类型，这意味着为在这个超类型的元素上运算而写计算机程序也可以在子类型的元素上运算。 静态多态（static polymorphism）：模板也允许将不同的特殊行为和单个泛化记号相关联，由于这种关联处理于编译期而非运行期，因此被称为“静态”。可以用来实现类型安全、运行高效的同质对象集合操作。C++ STL不采用动态多态来实现就是个例子。 非参数化多态或译作特设多态（Ad-hoc polymorphism）： 函数重载（Function Overloading） 运算符重载（Operator Overloading） 带变量的宏多态（macro polymorphism） 参数化多态（Parametric polymorphism）：把类型作为参数的多态。在面向对象程序设计中，这被称作泛型编程。 子类型反映了类型（即面向对象的接口）之间的关系；而继承反映了一类对象可以从另一类对象创造出来，是语言特性的实现。因此，子类型也称接口继承；继承称作实现继承。\n多态允许引用和对象类型不同, 如引用类型可以是实际对象类型的父类: Animal myDog = new Dog();. 任何通过IS-A测试的类型，任何extends过声明引用变量类型的对象都可以被赋值给该引用变量。\n多态有很多应用场景，比如可以容纳不同子类型的数组：\nAnimal[] animals = new Animal[3]; animals[0] = new Dog(); animals[1] = new Cat(); animals[2] = new Wolf(); 参数和返回类型也可以多态. 这样通过多态, 可以编写自适应任何新类型子类的程序.\nThe Object Class Java中的每个类都是 Object class的后代，或者扩展了Object类。即使在类中没有显式的extends仍然隐式地继承了Object。也就是所有 classes 都继承了 Object. 既然Object是所有类的超类, 那为何不适用它来构造万用数据结构(或者方法)呢? 事实上的确很多ArrayList方法都用到Object超级类型.\nObject类提供的方法, 都是任何对象都需要用到的方法:\n// -- https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html String toString() boolean equals(Object obj) Class\u0026lt;?\u0026gt; getClass() int hashCode() protected Object clone() protected void finalize() void notify() void notifyAll() void wait() void wait(long timeout) void wait(long timeout, int nanos) ==检查两个变量是否引用同一个对象（检查内存地址位）; .equals()默认是等同于==, 但不同的类可能会通过 override 重写它的功能(如Array.equals()就是重写为检查数组内容是否相同). 当override .equals()时，注意：\n必须体现 equivalence relation reflexive: x.equals(x) is true symmetric: x.equals(y) IFF y.equals(x) transitive: x.equals(y) and y.equals(z) implies x.equals(z) 要 override 原本的.equals()，必须接收一个 Object 参数 必须 consistent：假如x.equals(y), 那么只要x和y保持不变, 那么x继续等于y. null永远非真: x.equals(null) 一定是false Interfaces don’t extend Object. \u0026ndash; http://docs.oracle.com/javase/specs/jls/se7/html/jls-9.html#jls-9.2\n但是, 使用Object多态是有代价的:\nArrayList\u0026lt;Object\u0026gt; dogs = new ArrayList\u0026lt;Object\u0026gt;(); Dog a = new Dog(); dogs.add(a); // Dog b = dogs.get(0); 无法编译 无法编译是因为, dogs.get()返回的是Object类型, 因此编译器无法保证返回的是Dog.\n根本原因是因为类型检查。\n类型检查 Type Checking\npublic class VengefulSLList\u0026lt;Item\u0026gt; extends SLList\u0026lt;Item\u0026gt; { ... } public static void main(String[] args) { VengefulSLList\u0026lt;Integer\u0026gt; vsl = new VengefulSLList\u0026lt;Integer\u0026gt;(9); SLList\u0026lt;Integer\u0026gt; sl = vsl; // 超类包含子类 //sl dynamic type is VengefulSLList sl.addLast(50); sl.removeLast(); // 根据 dynamic type 选择 VengefulSLList 的 removeLast sl.printLostItems(); //编译不过, 因为编译时检查的是 static type VengefulSLList\u0026lt;Integer\u0026gt; vsl2 = sl; // 编译不过, 子类无法包含超类 } Expressions 是 compile-time types (static), 使用new的表达式具有指定的 compile-time types:\nSLList\u0026lt;Integer\u0026gt; sl = new VengefulSLList\u0026lt;Integer\u0026gt;();, 表达式右边 compile-time types 是VengefulSLList。编译器检查并保证VengefulSLList一定也是SLList，因此允许此赋值. VengefulSLList\u0026lt;Integer\u0026gt; vsl = new SLList\u0026lt;Integer\u0026gt;();, 表达式右边 compile-time types 是SLList。因为SLList并不一定是VengefulSLList，故编译报错. Static type checking 好处: Checks for errors early , reads more like a story\n坏处就是不够灵活。\nMethod calls have compile-time types equal to their declared type.\npublic static Dog maxDog(Dog d1, Dog d2) { ... } Poodle frank = new Poodle(\u0026#34;Frank\u0026#34;, 5); Poodle frankJr = new Poodle(\u0026#34;Frank Jr.\u0026#34;, 15); Dog largerDog = maxDog(frank, frankJr); // 编译不过! RHS compile-time type is Dog Poodle largerPoodle = maxDog(frank, frankJr); 编译器报错, maxDog返回的是Dog, 虽然此时我们都知道这里的\u0026quot;狗\u0026quot;肯定是指贵宾犬, 但编译器无法确认Dog一定是largerPoodle. 有没有办法让编译器认可这种关系呢? 有！\nCasting 通过 casting, 可以告诉编译器一个表达式有某个特定的 compile-time types.\nPoodle largerPoodle = (Poodle) maxDog(frank, frankJr); 编译通过, 右边 compile-time type 转换为 Poodle.\nCaution: Casting is a powerful but dangerous tool. Essentially, casting is telling the compiler not to do its type-checking duties - telling it to trust you and act the way you want it to.\n如果程序猿也无法确认类型, 可以使用instanceof来检查\nif (o instanceof Poodle) Poodle largerPoodle = (Poodle) maxDog(frank, frankJr); ","permalink":"https://congchan.github.io/posts/java-%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF%E6%89%A9%E5%B1%95-extends/","summary":"\u003ch2 id=\"类的继承扩展\"\u003e类的继承扩展\u003c/h2\u003e\n\u003cp\u003e定义class之间的层次关系.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e假设要构建一个\u003ccode\u003eRotatingSLList\u003c/code\u003e，它具有与\u003ccode\u003eSLList\u003c/code\u003e相同的功能，如\u003ccode\u003eaddFirst, size\u003c/code\u003e等，但是需要额外的\u003ccode\u003erotateRight\u003c/code\u003e操作将最后一项放到列表的前面，因为继承允许子类重用已经定义的类中的代码。所以让\u003ccode\u003eRotatingSLList\u003c/code\u003e类从\u003ccode\u003eSLList\u003c/code\u003e继承部分代码:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eRotatingSLList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eSLList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003eRotatingSLList\u003c/code\u003e\u0026ldquo;是一种\u0026rdquo;\u003ccode\u003eSLList\u003c/code\u003e, \u003ccode\u003eextends\u003c/code\u003e可以让我们继承\u003ccode\u003eSLList\u003c/code\u003e的原始功能，并能修改或添加其他功能。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e/** The rotateRight method  takes in an existing list,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e    and rotates every element one spot to the right,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e    moving the last item to the front of the list.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e    For example, input [5, 9, 15, 22] should return [22, 5, 9, 15].\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e*/\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003erotateRight\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eItem\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eremoveLast\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eaddFirst\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e通过\u003ccode\u003eextends\u003c/code\u003e, 子类\u003cstrong\u003e继承\u003c/strong\u003e父类的所有成员，成员包括：\u003c/p\u003e","title":"Java 类的继承扩展 Extends"},{"content":"除了单纯提供声明之外，Java 8 也允许接口提供具体的实现方法。\n缺省方法 从 Java 8开始支持 Default method。\n我们可以在List中列出已实现的method。这些方法就是 default method，定义了List hypernyms的一些默认行为：default public void method() { ... }.\n我们可以自由调用interface中定义的方法，而不用操心具体的实现。Default method 适用于实现接口的任何类型的对象！子类可以直接调用，而不必重新实现 default method。\n// List default public void print() { ... } 不过，我们仍然可以override default method，在子类中重新定义该方法。这样，只要我们在LinkedLList上调用print()，它就会调用子类override的方案，而不是父类的。\n// LinkedList @Override public void print() { ... } Dynamic type Java是通过一个叫“dynamic method selection”的特性，来确定要调用 default method 还是已经被子类override的method。\n当实例声明List\u0026lt;String\u0026gt; l = new LinkedList\u0026lt;String\u0026gt;();, 则指明l是 static 类型的 List。由 new 生成的 object 是LinkedList类型，也从属于 List 类型。但是，因为这个对象本身是使用 LinkedList 构造函数实例化的，所以我们称之为 dynamic type。\nDynamic type 的名称源于: 当l被重新分配指向另一种类型的对象时，比如说一个 ArrayList 对象，l的动态类型现在就变为 ArrayList. 因为它根据当前引用的对象的类型而改变, 所以是动态的。\nStatic vs. Dynamic Type:\nJava 每个变量都有一个static type （compile-time type），这是变量声明时指定的类型，在编译时会检查。 每个变量也有一个 Dynamic Type（run-time type），此类型在变量实例化（new）时指定，并在运行时检查。等同于地址指向的对象的类型。 当Java运行一个被overriden的方法时，会根据该实例的dynamic type 匹配对应的 method。\n注意，如果是overload:\npublic static void peek(List\u0026lt;String\u0026gt; list) { ... } public static void peek(LinkedList\u0026lt;String\u0026gt; list) { ... } 对于上面的实例化的l, 当Java检查要调用哪个方法时，它会检查 static type (此时是List)并使用相同类型的参数调用该方法，也就是使用List作为签名的那个方法。\nImplementation inheritance 也有一些缺点：\n人会犯错。我们有可能忘了自己曾经override过一个方法。 如果两个接口给出冲突的 default method，则可能很难解决冲突。 无形中鼓励代码复杂化。 Breaks encapsulation! ","permalink":"https://congchan.github.io/posts/java-13-%E6%8E%A5%E5%8F%A3%E9%BB%98%E8%AE%A4%E6%96%B9%E6%B3%95/","summary":"\u003cp\u003e除了单纯提供声明之外，Java 8 也允许接口提供具体的实现方法。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"缺省方法\"\u003e缺省方法\u003c/h3\u003e\n\u003cp\u003e从 Java 8开始支持 Default method。\u003c/p\u003e\n\u003cp\u003e我们可以在List中列出已实现的method。这些方法就是 default method，定义了List hypernyms的一些默认行为：\u003ccode\u003edefault public void method() { ... }\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e我们可以自由调用interface中定义的方法，而不用操心具体的实现。Default method 适用于实现接口的任何类型的对象！子类可以直接调用，而不必重新实现 default method。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// List\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"k\"\u003edefault\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e不过，我们仍然可以override default method，在子类中重新定义该方法。这样，只要我们在LinkedLList上调用\u003ccode\u003eprint()\u003c/code\u003e，它就会调用子类override的方案，而不是父类的。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// LinkedList\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"nd\"\u003e@Override\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"dynamic-type\"\u003eDynamic type\u003c/h3\u003e\n\u003cp\u003eJava是通过一个叫“dynamic method selection”的特性，来确定要调用 default method 还是已经被子类override的method。\u003c/p\u003e\n\u003cp\u003e当实例声明\u003ccode\u003eList\u0026lt;String\u0026gt; l = new LinkedList\u0026lt;String\u0026gt;();\u003c/code\u003e, 则指明\u003ccode\u003el\u003c/code\u003e是 static 类型的 \u003ccode\u003eList\u003c/code\u003e。由 new 生成的 object 是LinkedList类型，也从属于 List 类型。但是，因为这个对象本身是使用 LinkedList 构造函数实例化的，所以我们称之为 dynamic type。\u003c/p\u003e","title":"Java 13 | 接口默认方法"},{"content":"子类在什么情况下需要多个父类？\n比如，因为原来的业务需求比较单一，只需要Animal - Canine - Dog这种类结构就满足需求了, 此时Dog只是Animal的子类. 但后来有了新的功能需求, 上线了宠物功能, 理论上可以为每一种具体的属于宠物的子类添加宠物功能, 这就涉及大量的人工和bug. 但假如额外设计一种Pet类, 那么Pet和Animal会有交叉重叠, 如果让宠物子类同时继承两种超类, 那就是多重继承. 因为多重继承会有致命方块的问题, 不同父类对同一个方法的可能有不同的实现方式, 这会导致冲突. 所以Java不支持这种方式.\n接口 而接口这个概念, 就可以用于解决这个问题的. 类不需要继承多个父类, 只需要实现一个或多个接口指定的所有方法/行为的关系.\npublic interface Pet { public abstract void beFriendly(); public abstract void play(); } // 对于属于宠物的子类，让其实现接口`Pet`. public class Dog extends Canine implements Pet {} 接口作为参数 我们前面创建的 LinkedList and ArrayList 其实很相似 - 所有的method都一样.\n如果我们需要写一个需要用到数组的类比如WordUtils class,\npublic class WordUtils { /** Returns the length of the longest word. */ public static String longest(LinkedList\u0026lt;String\u0026gt; list) { ... return list.get(maxDex); } } 我们如何让longest方法也适配ArrayList？简单的方法及时写两个同名不同参数的methods。即所谓method overloading。 public static String longest(LinkedList\u0026lt;String\u0026gt; list) public static String longest(ArrayList\u0026lt;String\u0026gt; list)\n但 overload 有几个缺点:\n重复冗余，写两个几乎相同的代码块。 产生更多需要维护的代码，那意味着如果你想对的方法做一个小优化或debug，你需要在对应每种list的方法中改变它。 如果我们想要适配更多的列表类型，不得不复制每个新列表类的方法。 为避免以上问题，我们一般希望能尽量把两个功能近似的方法合并，但要保证其足够广泛的适用场景。\n定义通用列表接口 interface List。然后把LinkedList和ArrayList实现List。\npublic interface List\u0026lt;Item\u0026gt; { public void addFirst(Item x); ... } 这里的 List 是接口。本质上是一个指定必须能够做什么的合约，但不提供具体实现。\npublic class ArrayList\u0026lt;Item\u0026gt; implements List\u0026lt;Item\u0026gt;{ // 具体的执行 public void addFirst(Item x) { insert(x, 0); } } public class LinkedList\u0026lt;Item\u0026gt; implements List\u0026lt;Item\u0026gt;{ // 具体的执行 } ArrayList\u0026lt;Item\u0026gt; implements List\u0026lt;Item\u0026gt;类似签合约 - ArrayList保证实现List接口列出的所有属性（变量）和方法。\n这样就可以同时适配多种list：\npublic class WordUtils { /** Returns the length of the longest word. */ public static String longest(List\u0026lt;String\u0026gt; list) { ... return list.get(maxDex); } public static void main(String[] args) { ArrayList\u0026lt;String\u0026gt; someList = new ArrayList\u0026lt;\u0026gt;(); //or LinkedList\u0026lt;String\u0026gt; someList = new LinkedList\u0026lt;\u0026gt;(); ... System.out.println(longest(someList)); } } 接口列出所有方法的声明，就像‘合约’，但没有具体的实现. 根据‘合约’，由子类来实现且必须实现（override）每一个method，否则无法通过编译. 继承关系可以延续多代。例如，B可以继承A，C可以继承B.\nGRoE 根据Java的Golden Rule of Equals，每一个赋值a = b，本质上是把b中的bits拷贝到a中，着要求b和a的类型相同。\n同理, 假设public static String longest(List\u0026lt;String\u0026gt; list)既接受List, 也接受ArrayList和LinkedList，但是由于ArrayList和List是不同的类，那怎么遵守GRoE呢？\n因为ArrayList与List有着上下位包含的关系，这意味着ArrayList应该能够赋值给List的内存位中.\npublic static void main(String[] args) { List\u0026lt;String\u0026gt; someList = new SLList\u0026lt;String\u0026gt;(); someList.addFirst(\u0026#34;elk\u0026#34;); } 这段代码运行时，会创建SLList并将其地址存储在someList变量中。\n","permalink":"https://congchan.github.io/posts/java-12-%E6%8E%A5%E5%8F%A3-interface/","summary":"\u003cp\u003e子类在什么情况下需要多个父类？\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e比如，因为原来的业务需求比较单一，只需要\u003ccode\u003eAnimal - Canine - Dog\u003c/code\u003e这种类结构就满足需求了, 此时\u003ccode\u003eDog\u003c/code\u003e只是\u003ccode\u003eAnimal\u003c/code\u003e的子类. 但后来有了新的功能需求, 上线了宠物功能, 理论上可以为每一种具体的属于宠物的子类添加宠物功能, 这就涉及大量的人工和bug. 但假如额外设计一种\u003ccode\u003ePet\u003c/code\u003e类, 那么\u003ccode\u003ePet\u003c/code\u003e和\u003ccode\u003eAnimal\u003c/code\u003e会有交叉重叠, 如果让宠物子类同时继承两种超类, 那就是\u003cstrong\u003e多重继承\u003c/strong\u003e. 因为多重继承会有\u003cstrong\u003e致命方块\u003c/strong\u003e的问题, 不同父类对同一个方法的可能有不同的实现方式, 这会导致冲突. 所以Java不支持这种方式.\u003c/p\u003e\n\u003ch2 id=\"接口\"\u003e接口\u003c/h2\u003e\n\u003cp\u003e而接口这个概念, 就可以用于解决这个问题的. 类不需要继承多个父类, 只需要实现一个或多个接口指定的所有方法/行为的关系.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003einterface\u003c/span\u003e \u003cspan class=\"nc\"\u003ePet\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eabstract\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ebeFriendly\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eabstract\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eplay\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e// 对于属于宠物的子类，让其实现接口`Pet`.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDog\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eCanine\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eimplements\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ePet\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"接口作为参数\"\u003e接口作为参数\u003c/h3\u003e\n\u003cp\u003e我们前面创建的 \u003ccode\u003eLinkedList\u003c/code\u003e and \u003ccode\u003eArrayList\u003c/code\u003e 其实很相似 - 所有的method都一样.\u003c/p\u003e\n\u003cp\u003e如果我们需要写一个需要用到数组的类比如\u003ccode\u003eWordUtils\u003c/code\u003e class,\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eWordUtils\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"cm\"\u003e/** Returns the length of the longest word. */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003elongest\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eLinkedList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emaxDex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e我们如何让\u003ccode\u003elongest\u003c/code\u003e方法也适配\u003ccode\u003eArrayList\u003c/code\u003e？简单的方法及时写两个同名不同参数的methods。即所谓\u003ccode\u003emethod overloading\u003c/code\u003e。\n\u003ccode\u003epublic static String longest(LinkedList\u0026lt;String\u0026gt; list)\u003c/code\u003e\n\u003ccode\u003epublic static String longest(ArrayList\u0026lt;String\u0026gt; list)\u003c/code\u003e\u003c/p\u003e","title":"Java 12 | 接口 Interface"},{"content":"FBI WARNING 这不是python入门\n函数 Fundamentally, the qualities of good functions all reinforce the idea that functions are abstractions.\n函数作为一种机制, 提供了用于抽象数值运算的模式, 使其独立于所涉及的特定值。\n文档 code is written only once, but often read many times.\ndocstring\ndef pressure(v, t, n): \u0026#34;\u0026#34;\u0026#34;Compute the pressure in pascals of an ideal gas. Applies the ideal gas law: http://en.wikipedia.org/wiki/Ideal_gas_law v -- volume of gas, in cubic meters t -- absolute temperature in degrees kelvin n -- particles of gas \u0026#34;\u0026#34;\u0026#34; \u0026gt;\u0026gt;\u0026gt; help(pressure) Python docstring guidelines\n高阶函数 Functions that manipulate functions are called higher-order functions.\n高阶函数进一步扩展一般函数，能表达通用的, 独立于其调用的特定函数的计算方案。\nFunctions as Arguments def summation(n, term): total, k = 0, 1 while k \u0026lt;= n: total, k = total + term(k), k + 1 return total def cube(x): return x*x*x def sum_cubes(n): return summation(n, cube) Nested Definitions One negative consequence of this approach is that the global frame becomes cluttered with names of small functions, which must all be unique. Another problem is that we are constrained by particular function signatures.\n当同一环境下，当出现需要相似功能但与已有函数的参数不同时，此时嵌套函数可以方便我们定义函数.\ndef improve(update, close, guess=1): while not close(guess): guess = update(guess) return guess 这里的update只接受一个参数, 假如我们刚好有需要两个参数的\ndef sqrt_update(x, a): \u0026#34;\u0026#34;\u0026#34;square root\u0026#34;\u0026#34;\u0026#34; return average(x, a/x) 这个函数就无法传入improve中.\n嵌套函数, 让sqrt_update传入参数保持一个, 同时额外能够获取其本地环境frame的其他参数\ndef sqrt(a): def sqrt_update(x): return average(x, a/x) def sqrt_close(x): return approx_eq(x * x, a) return improve(sqrt_update, sqrt_close) local def statements only affect the current local frame. lexical scoping: sharing names among nested definitions\nFunctions as Returned Values def compose1(f, g): def h(x): return f(g(x)) return h 比如TensorFlow中常用的\ndef model_fn_builder(...): \u0026#34;\u0026#34;\u0026#34;Returns `model_fn` closure.\u0026#34;\u0026#34;\u0026#34; def model_fn(features, labels, mode, params): # pylint: disable=unused-argument \u0026#34;\u0026#34;\u0026#34;The actual `model_fn`.\u0026#34;\u0026#34;\u0026#34; ... return ... return model_fn Currying 一种变换方式, 使用高阶函数将一个带有多个参数的函数转换为一个函数链，每个函数都接受一个参数。\ndef curried_pow(x): def h(y): return pow(x, y) return h \u0026gt;\u0026gt;\u0026gt; curried_pow(2)(3) 8 Lambda Expressions 利用lambda表达式动态创建函数, 省去命名\nlambda x : f(g(x)) \u0026#34;A function that takes x and returns f(g(x))\u0026#34; lambda 来由\nIt may seem perverse to use lambda to introduce a procedure/function. The notation goes back to Alonzo Church, who in the 1930\u0026rsquo;s started with a \u0026ldquo;hat\u0026rdquo; symbol; he wrote the square function as \u0026ldquo;ŷ . y × y\u0026rdquo;. But frustrated typographers moved the hat to the left of the parameter and changed it to a capital lambda: \u0026ldquo;Λy . y × y\u0026rdquo;; from there the capital lambda was changed to lowercase, and now we see \u0026ldquo;λy . y × y\u0026rdquo; in math books and (lambda (y) (* y y)) in Lisp. —Peter Norvig (norvig.com/lispy2.html)\nFunction Decorators 装饰器也是一种高阶函数,\ndef trace(fn): def wrapped(x): print(\u0026#39;-\u0026gt; \u0026#39;, fn, \u0026#39;(\u0026#39;, x, \u0026#39;)\u0026#39;) return fn(x) return wrapped @trace def triple(x): return 3 * x 以上等价于\ntriple = trace(triple) \u0026gt;\u0026gt;\u0026gt; triple(12) -\u0026gt; \u0026lt;function triple at 0x102a39848\u0026gt; ( 12 ) 36 ","permalink":"https://congchan.github.io/posts/python%E4%B9%8B%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7/","summary":"\u003cp\u003eFBI WARNING 这不是python入门\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"函数\"\u003e函数\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFundamentally, the qualities of good functions all reinforce the idea that functions are \u003cstrong\u003eabstractions\u003c/strong\u003e.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e函数作为一种机制, 提供了用于抽象数值运算的模式, 使其独立于所涉及的特定值。\u003c/p\u003e\n\u003ch3 id=\"文档\"\u003e文档\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003ecode is written only once, but often read many times.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003edocstring\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003epressure\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ev\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Compute the pressure in pascals of an ideal gas.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e  Applies the ideal gas law: http://en.wikipedia.org/wiki/Ideal_gas_law\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e  v -- volume of gas, in cubic meters\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e  t -- absolute temperature in degrees kelvin\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e  n -- particles of gas\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e  \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026gt;\u0026gt;\u0026gt; help(pressure)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ca href=\"http://www.python.org/dev/peps/pep-0257/\"\u003ePython docstring guidelines\u003c/a\u003e\u003c/p\u003e","title":"Python之奇技淫巧"},{"content":"测试 如何知道自己的程序是否真的在工作？在现实世界中，程序员相信他们的代码，因为代码通过了他们自己编写的测试。常用的测试有 Ad Hoc Testing， Unit test 和 Integration Testing。\nAd Hoc Testing，是指没有计划和记录的软件测试，除非发现缺陷，不然一般只运行一次。\nUnit test 程序可分解为单元（或程序中可测试的最小部分），Unit test 严格测试代码的每个单元，最终确保项目正确运行。 Unit test 好处：\nUnit test 保证良好的代码结构（每个 method “只打一份工”），帮助我们较好地解析任务， 允许我们考虑每个方法的所有边界情况，并单独测试它们。 让我们每次只专注于一个单元，进行测试，debug，对准确度有信心后，再进行下一个单元的开发。相比于一次性写完所有代码，再测试debug，Unit test 减少了 debugging 时间。 坏处：\n测试也要花时间 测试本身也是有可能出错的，测试可能不全面，不规范，或者有bug 有些单元是依赖于其他单元的 Unit testing 无法保证各个模块的交互，无法保证整个系统作为一个整体是否正常工作。 JUnit JUnit是一个给Java做测试的框架，由Erich Gamma（Design Patterns）和Kent Beck（eXtreme Programming）编写。 JUnit使用Java的 reflection 功能（Java程序可以检查自己的代码）和注释。 JUnit允许我们：\n定义并执行测试和测试套件 使用测试作为规范的有效手段 使用测试来支持重构 将修改的代码集成到构建中 JUnit可用于多个IDE，例如BlueJ，JBuilder和Eclipse在一定程度上具有JUnit集成。 import org.junit.Test; import static org.junit.Assert.*; @Test public void testMethod() { assertEquals(\u0026lt;expected\u0026gt;, \u0026lt;actual\u0026gt;); } assertEquals测试一个变量的实际值是否等于它的期望值。 JUnit test 各个测试方法，必须是非静态的（JUnit的设计人员设计规定的）。\nJUnit的术语\nTest runner：测试运行器， 运行测试和报告结果的软件。实现方式：集成到IDE中，独立GUI，命令行等 Test suite：测试套件是一组测试用例。 Test case：测试用例用于测试单个方法对特定输入集的响应。 Unit test：单元测试的单元，是代码中我们能够相对合理地测试的最小的元素，通常是单个类。 常用的JUnit接口和方法 @Before: Creates a test fixture by creating and initialising objects and values.\n@After: Releases any system resources used by the test fixture. Java usually does this for free, but files, network connections etc. might not get tidied up automatically.\n@Test：tests cases.\nstatic void assertTrue(boolean test), static void assertTrue(String message, boolean test), static void assertFalse(boolean test), static void assertFalse(String message, boolean test)\nIntegration Testing 鉴于 Unit testing 无法保证，有交互的多个模块，作为一个整体是否正常工作。 我们可能需要 integration testing，把各个模块合并，作为一个组合，进行测试（也可以把 Unit test 组合起来变成 integration testing）。\nIntegration testing 一般都比较麻烦，也不容易自动化，而且一般是在比较高的抽象层进行测试，可能会漏掉微小的错误。\n当把所有模块都作为一个整体，也就是整个系统作为测试对象时，就是 system testing。\nTest driven development TDD开发步骤：\n明确一项新功能需求。 为该功能编写 Unit test。 运行测试，按理应该无法通过测试（因为还没写功能程序）。 编写通过实现该功能的代码，通过测试。 可选：重构代码，使其更快，更整洁等等。 ","permalink":"https://congchan.github.io/posts/java-11-%E6%B5%8B%E8%AF%95-testing/","summary":"\u003ch2 id=\"测试\"\u003e测试\u003c/h2\u003e\n\u003cp\u003e如何知道自己的程序是否真的在工作？在现实世界中，程序员相信他们的代码，因为代码通过了他们自己编写的测试。常用的测试有 Ad Hoc Testing， Unit test 和 Integration Testing。\u003c/p\u003e\n\u003cp\u003eAd Hoc Testing，是指没有计划和记录的软件测试，除非发现缺陷，不然一般只运行一次。\u003c/p\u003e\n\u003ch3 id=\"unit-test\"\u003eUnit test\u003c/h3\u003e\n\u003cp\u003e程序可分解为单元（或程序中可测试的最小部分），Unit test 严格测试代码的每个单元，最终确保项目正确运行。\nUnit test 好处：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUnit test 保证良好的代码结构（每个 method “只打一份工”），帮助我们较好地解析任务，\u003c/li\u003e\n\u003cli\u003e允许我们考虑每个方法的所有边界情况，并单独测试它们。\u003c/li\u003e\n\u003cli\u003e让我们每次只专注于一个单元，进行测试，debug，对准确度有信心后，再进行下一个单元的开发。相比于一次性写完所有代码，再测试debug，Unit test 减少了 debugging 时间。\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e坏处：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e测试也要花时间\u003c/li\u003e\n\u003cli\u003e测试本身也是有可能出错的，测试可能不全面，不规范，或者有bug\u003c/li\u003e\n\u003cli\u003e有些单元是依赖于其他单元的\u003c/li\u003e\n\u003cli\u003eUnit testing 无法保证各个模块的交互，无法保证整个系统作为一个整体是否正常工作。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"junit\"\u003eJUnit\u003c/h3\u003e\n\u003cp\u003eJUnit是一个给Java做测试的框架，由Erich Gamma（Design Patterns）和Kent Beck（eXtreme Programming）编写。\nJUnit使用Java的 reflection 功能（Java程序可以检查自己的代码）和注释。\nJUnit允许我们：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e定义并执行测试和测试套件\u003c/li\u003e\n\u003cli\u003e使用测试作为规范的有效手段\u003c/li\u003e\n\u003cli\u003e使用测试来支持重构\u003c/li\u003e\n\u003cli\u003e将修改的代码集成到构建中\nJUnit可用于多个IDE，例如BlueJ，JBuilder和Eclipse在一定程度上具有JUnit集成。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nn\"\u003eorg.junit.Test\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kn\"\u003eimport static\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nn\"\u003eorg.junit.Assert.*\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"nd\"\u003e@Test\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003etestMethod\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eassertEquals\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eexpected\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eactual\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003eassertEquals\u003c/code\u003e测试一个变量的实际值是否等于它的期望值。\nJUnit test 各个测试方法，必须是非静态的（JUnit的设计人员设计规定的）。\u003c/p\u003e","title":"Java 11 | 测试 Testing"},{"content":"Java 提供了 ArrayList, ArrayDeque 和 LinkedList 几个API.\n队列 queue, 通俗的含义, 就是不能插队, 只能在末尾插入.\n双端队列 Double Ended Queue (Deque) 是具有动态大小的序列容器，可以在两端（前端或后端）扩展或收缩 \u0026ndash;http://www.cplusplus.com/reference/deque/deque/\nCS61b的project 1a需要实现两种双端队列（array based 和 linkedklist based）.\n不同的API, 在考虑什么时候应该用哪个时, 我们需要考虑它们的性能差异:\n搜索/定位：与LinkedList相比，ArrayList搜索更快。 ArrayList的get(int index)性能是O(1)的，而LinkedList的性能是O(n)。因为ArrayList基于array数据结构，可以直接用 array index 定位元素。 删除/插入：LinkedList 操作性能是O(1)，而ArrayList的性能从O(n)（删除/插入第一个元素）到O(n)（最后一个元素）都有可能。因为LinkedList的每个元素都包含两个指向其相邻前后元素的指针（地址），因此仅需要改变，被删节点的prev和next指针位置。而在ArrayList中，需要移动剩余元素，来重新填充array空间。 内存开销：LinkedList的每个元素都有更多的内存开销(额外的指针), 而ArrayLists没有这个开销。但是，ArrayLists需要占用初始容量。一般ArrayList的默认初始容量非常小（Java 1.4 - 1.8使用10）。但是，往ArrayLists添加元素时， 它可能会适当地增大容量，所以如果添加了很多元素，则必须不断调整数组的大小，那样也可能会导致元素频繁挪动位置。 综上所述：\n如果在应用中需要频繁插入和删除，那么选择LinkedList。 假如一开始，就知道后面要添加大量元素，那就使用较高的初始容量来构造ArrayList。 大部分用例中, 相比LinkedList, 人们更偏爱ArrayList以及ArrayDeque。如果你不确定应该选哪个, 那么就直接考虑ArrayList吧(参考 https://stackoverflow.com/questions/322715/when-to-use-linkedlist-over-arraylist). ","permalink":"https://congchan.github.io/posts/java-10-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-linkedlist-%E8%BF%98%E6%98%AF-arraylist/","summary":"\u003cp\u003eJava 提供了 \u003ccode\u003eArrayList\u003c/code\u003e, \u003ccode\u003eArrayDeque\u003c/code\u003e 和 \u003ccode\u003eLinkedList\u003c/code\u003e 几个API.\u003c/p\u003e\n\u003cp\u003e队列 queue, 通俗的含义, 就是不能插队, 只能在末尾插入.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e双端队列 Double Ended Queue (Deque) 是具有动态大小的序列容器，可以在两端（前端或后端）扩展或收缩\n\u0026ndash;\u003ca href=\"http://www.cplusplus.com/reference/deque/deque/\"\u003ehttp://www.cplusplus.com/reference/deque/deque/\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eCS61b的\u003ca href=\"http://sp18.datastructur.es/materials/proj/proj1a/proj1a\"\u003eproject 1a\u003c/a\u003e需要实现两种双端队列（array based 和 linkedklist based）.\u003c/p\u003e\n\u003cp\u003e不同的API, 在考虑什么时候应该用哪个时, 我们需要考虑它们的性能差异:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e搜索/定位\u003c/strong\u003e：与\u003ccode\u003eLinkedList\u003c/code\u003e相比，\u003ccode\u003eArrayList\u003c/code\u003e搜索更快。 \u003ccode\u003eArrayList\u003c/code\u003e的\u003ccode\u003eget(int index)\u003c/code\u003e性能是\u003ccode\u003eO(1)\u003c/code\u003e的，而LinkedList的性能是\u003ccode\u003eO(n)\u003c/code\u003e。因为\u003ccode\u003eArrayList\u003c/code\u003e基于\u003ccode\u003earray\u003c/code\u003e数据结构，可以直接用 array index 定位元素。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e删除/插入\u003c/strong\u003e：\u003ccode\u003eLinkedList\u003c/code\u003e 操作性能是\u003ccode\u003eO(1)\u003c/code\u003e，而\u003ccode\u003eArrayList\u003c/code\u003e的性能从\u003ccode\u003eO(n)\u003c/code\u003e（删除/插入第一个元素）到\u003ccode\u003eO(n)\u003c/code\u003e（最后一个元素）都有可能。因为\u003ccode\u003eLinkedList\u003c/code\u003e的每个元素都包含两个指向其相邻前后元素的指针（地址），因此仅需要改变，被删节点的\u003ccode\u003eprev\u003c/code\u003e和\u003ccode\u003enext\u003c/code\u003e指针位置。而在\u003ccode\u003eArrayList\u003c/code\u003e中，需要移动剩余元素，来重新填充\u003ccode\u003earray\u003c/code\u003e空间。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e内存开销\u003c/strong\u003e：\u003ccode\u003eLinkedList\u003c/code\u003e的每个元素都有更多的内存开销(额外的指针), 而\u003ccode\u003eArrayLists\u003c/code\u003e没有这个开销。但是，\u003ccode\u003eArrayLists\u003c/code\u003e需要占用初始容量。一般\u003ccode\u003eArrayList\u003c/code\u003e的默认初始容量非常小（Java 1.4 - 1.8使用10）。但是，往\u003ccode\u003eArrayLists\u003c/code\u003e添加元素时， 它可能会适当地增大容量，所以如果添加了很多元素，则必须不断调整数组的大小，那样也可能会导致元素频繁挪动位置。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e综上所述：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e如果在应用中需要\u003cstrong\u003e频繁插入和删除\u003c/strong\u003e，那么选择\u003ccode\u003eLinkedList\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e假如一开始，就知道后面要添加大量元素，那就使用较高的初始容量来构造\u003ccode\u003eArrayList\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e大部分用例中, 相比LinkedList, 人们更偏爱ArrayList以及ArrayDeque。如果你不确定应该选哪个, 那么就直接考虑ArrayList吧(参考 \u003ca href=\"https://stackoverflow.com/questions/322715/when-to-use-linkedlist-over-arraylist%29\"\u003ehttps://stackoverflow.com/questions/322715/when-to-use-linkedlist-over-arraylist)\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e","title":"Java 10 | 数据结构 - LinkedList 还是 ArrayList"},{"content":"双向链表 Doubly Linked List\n前面介绍过的单向链表有几个缺点. 第一个就是它的addLast操作非常慢。单向链表只有一个变量保存列表头的地址, 以及每个节点对后面节点的单向引用(链接). 对于很长的列表，addLast方法必须遍历整个列表, 直到找到列表末尾才能执行插入操作.\n最直观的优化方案就是加个\u0026rsquo;车尾\u0026rsquo; 这样我们就可以直接通过last.next引用末尾位置.\n不过另一个问题并没有解决, 就是删除列表最后一项removeLast这个操作还是很慢。因为在目前的结构设计下, 我们需要先找到倒数第二项，然后将其下一个指针设置为null。而要找到倒数第二节点, 我们就得先找到倒数第三个节点\u0026hellip;\u0026hellip; 以此类推。也就是说，对于删除末尾的操作，还是要几乎遍历整个列表。\n反方向的链接 基于前面单向链表构建双向链表, 一个比较有效的方法是额外为每个节点添加一个指向前面节点的链接 - 指针.\npublic class OneNode { public OneNode prev; //指向前 public int item; public OneNode next; //指向后 } 增加这些额外的指针会导致额外的代码复杂度, 以及额外的内存开销, 这就是追求时间效率的代价.\nSentinel 与尾节点 双向链表的一个设计初衷，就是为了解决单向链表针对列表末尾位置的操作效率不高的问题，除了sentinel和反方向的链接还不够，我们还需要一个节点（指针）能够直接帮我们定位到列表末端。可以考虑添加一个的尾节点last 这样的列表就可以支持O(1)复杂度的addLast,getLast 和 removeLast操作了。\n循环双端队列 Circular double ended queue\n上面的尾节点设计虽然没什么错误，但有点瑕疵：最后一个尾节点指针有时指向前哨节点，有时指向一个真正的节点。更好的方法是使双向链表首尾相连, 构成一个循环，即前后节点共享唯一的一个前哨节点。 这样的设计相对更整洁，更美观(主观上的), sentinel的prev就指向列表最后一个节点, sentinel的next指向列表第一个节点.\npublic class LinkedListDeque\u0026lt;GType\u0026gt; { private class OneNode { public OneNode prev; public GType item; public OneNode next; public OneNode(OneNode p, GType i, OneNode n) { prev = p; item = i; next = n; } } } Sentinel\u0026rsquo;s forward link always points to the last element. Sentinel\u0026rsquo;s backward link always points to the first element.\n然后修改构造函数:\n/** Creates an empty deque. */ public LinkedListDeque(){ sentinel = new OneNode(null,null, null); sentinel.prev = sentinel; sentinel.next = sentinel; size = 0; } /** Creates a deque with x */ public LinkedListDeque(GType x){ sentinel = new OneNode(null, null, null); sentinel.next = new OneNode(sentinel, x, sentinel); sentinel.prev = sentinel.next; size = 1; } 如果初始化的是空列表, 其实就是一个自己指向自己的sentinel节点. 如果是非空列表, 那么sentinel节点和真实的节点就构成了一个最简单的二元循环体.\n针对列表末尾位置的操作 双端链表结构优雅，虽然某些操作如addFirst等编码复杂度会提高, 但不影响速度. 更重要的是, 相比单向链表, 它反而使得addLast, moveLast等方法的代码实现变得简单了, 而且还进一步提升了运行速度(从O(n)到O(c)).\n/** Adds an item to the back of the Deque - O(c) */ public void addLast(GType x){ OneNode oldBackNode = sentinel.prev; OneNode newNode = new OneNode(oldBackNode, x, sentinel); sentinel.prev = newNode; oldBackNode.next = newNode; size += 1; } /** Removes and returns the item at the front of the Deque. * If no such item exists, returns null.O(c). */ public GType removeFirst(){ if (isEmpty()){ return null; } OneNode oldFrontNode = sentinel.next; sentinel.next = oldFrontNode.next; oldFrontNode.next.prev = sentinel; size -= 1; return oldFrontNode.item; } ","permalink":"https://congchan.github.io/posts/java-09-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8-doubly-linked-list/","summary":"\u003ch2 id=\"双向链表\"\u003e双向链表\u003c/h2\u003e\n\u003cp\u003eDoubly Linked List\u003c/p\u003e\n\u003cp\u003e前面介绍过的单向链表有几个缺点. 第一个就是它的\u003ccode\u003eaddLast\u003c/code\u003e操作非常慢。单向链表只有一个变量保存列表头的地址, 以及每个节点对后面节点的单向引用(链接). 对于很长的列表，\u003ccode\u003eaddLast\u003c/code\u003e方法必须遍历整个列表, 直到找到列表末尾才能执行插入操作.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e最直观的优化方案就是加个\u0026rsquo;车尾\u0026rsquo;\u003cimg loading=\"lazy\" src=\"/images/sllist_last_pointer.png\" title=\"image from: https://joshhug.gitbooks.io/\"\u003e 这样我们就可以直接通过\u003ccode\u003elast.next\u003c/code\u003e引用末尾位置.\u003c/p\u003e\n\u003cp\u003e不过另一个问题并没有解决, 就是删除列表最后一项\u003ccode\u003eremoveLast\u003c/code\u003e这个操作还是很慢。因为在目前的结构设计下, 我们需要先找到倒数第二项，然后将其下一个指针设置为\u003ccode\u003enull\u003c/code\u003e。而要找到倒数第二节点, 我们就得先找到倒数第三个节点\u0026hellip;\u0026hellip; 以此类推。也就是说，对于删除末尾的操作，还是要几乎遍历整个列表。\u003c/p\u003e\n\u003ch3 id=\"反方向的链接\"\u003e反方向的链接\u003c/h3\u003e\n\u003cp\u003e基于前面单向链表构建双向链表, 一个比较有效的方法是额外为每个节点添加一个指向前面节点的链接 - 指针.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eprev\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e//指向前\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eitem\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enext\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e//指向后\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e增加这些额外的指针会导致额外的代码复杂度, 以及额外的内存开销, 这就是追求时间效率的代价.\u003c/p\u003e\n\u003ch3 id=\"sentinel-与尾节点\"\u003eSentinel 与尾节点\u003c/h3\u003e\n\u003cp\u003e双向链表的一个设计初衷，就是为了解决单向链表针对列表末尾位置的操作效率不高的问题，除了sentinel和反方向的链接还不够，我们还需要一个节点（指针）能够直接帮我们定位到列表末端。可以考虑添加一个的尾节点\u003ccode\u003elast\u003c/code\u003e\u003cimg loading=\"lazy\" src=\"https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist_basic_size_0.png\" title=\"image from: https://joshhug.gitbooks.io/\"\u003e \u003cimg loading=\"lazy\" src=\"https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist_basic_size_2.png\" title=\"image from: https://joshhug.gitbooks.io/\"\u003e 这样的列表就可以支持\u003ccode\u003eO(1)\u003c/code\u003e复杂度的\u003ccode\u003eaddLast\u003c/code\u003e,\u003ccode\u003egetLast\u003c/code\u003e 和 \u003ccode\u003eremoveLast\u003c/code\u003e操作了。\u003c/p\u003e\n\u003ch2 id=\"循环双端队列\"\u003e循环双端队列\u003c/h2\u003e\n\u003cp\u003eCircular double ended queue\u003c/p\u003e\n\u003cp\u003e上面的尾节点设计虽然没什么错误，但有点瑕疵：最后一个尾节点指针有时指向前哨节点，有时指向一个真正的节点。更好的方法是使双向链表首尾相连, 构成一个循环，即前后节点共享唯一的一个前哨节点。\u003cimg loading=\"lazy\" src=\"https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist_circular_sentinel_size_0.png\" title=\"image from: https://joshhug.gitbooks.io/\"\u003e \u003cimg alt=\"fig source https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist_circular_sentinel_size_2.png\" loading=\"lazy\" src=\"https://joshhug.gitbooks.io/hug61b/content/chap2/fig23/dllist_circular_sentinel_size_2.png\" title=\"image from: https://joshhug.gitbooks.io/\"\u003e\n这样的设计相对更整洁，更美观(主观上的), sentinel的\u003ccode\u003eprev\u003c/code\u003e就指向列表最后一个节点, sentinel的\u003ccode\u003enext\u003c/code\u003e指向列表第一个节点.\u003c/p\u003e","title":"Java 09 | 数据结构 - 双向链表 Doubly Linked List"},{"content":"链表 Linked List\n前面有介绍以array为基础搭建的列表，支持自动扩容, 各种插入，删除速度都很快. 这里再介绍另一种方案, 链表, 也可以实现列表自动扩容.\n带链接的节点 链表的核心组成是带链接的节点, 每个节点就像火车车厢, 有钩子连接下一节车厢.\n以int节点为例:\npublic class IntNode { public int item; public IntNode next; public IntNode(int i, IntNode n) { item = i; next = n; } } next就是这个链接, 每一个节点就是其上一个节点的next.\n使用嵌套类 这个节点作为一个相对独立的数据结构, 我们更希望让他单独作为一个类来维护. 再另外创建一个名为LinkedList的class与用户进行交互. 这样还有另一个好处就是提供一个命名为LinkedList的类给用户交互，用户更直观地知道自己是在调用链表。如果直接与node类交互，用户可能会困扰. 但同时考虑到这个node类只有LinkedList会调用，所以我们可以把node类嵌套进LinkedList中。\npublic class LinkedList\u0026lt;XXX\u0026gt; { private class OneNode { public XXX item; public OneNode next; public OneNode(XXX i, OneNode n) { item = i; next = n; } } private OneNode first; private int size; public LinkedList(XXX x) { first = new OneNode(x, null); size = 1; } //下面是各种方法... } 以上定义使用了泛型。声明OneNode实例first为私有变量, 是为了防止用户错误地摆弄链接指向，private和public的使用参考.\n补充必要的实例方法 插入的操作核心是改变链接指向， 比如原来是A-\u0026gt;B-\u0026gt;D, 要插入C, 则把C.next指向D,然后把B.next改为指向C, 变为A-\u0026gt;B-\u0026gt;C-\u0026gt;D\npublic class LinkedList\u0026lt;XXX\u0026gt; { private class OneNode { ... } private OneNode first; private int size; public LinkedList(XXX x) { ... } /** 在列表开头插入 x. */ public void addFirst(XXX x) { first = new OneNode(x, first); size += 1; } /** 返回列表第一个元素. */ public XXX getFirst() { return first.item; } /** 在列表末尾插入 x. */ public void addLast(XXX x) { size += 1; OneNode p = first; /* 把 p 当做指针顺藤摸瓜一直挪到列表末尾. */ while (p.next != null) { p = p.next; } p.next = new OneNode(x, null); } /** 删除列表末尾的元素. */ public void removeLast(){ //自行补充... } public int size() { return size; } } 可以看到，如果用户不小心把某节点x指回自己x.next=x,那就会进入死循环，所以我们需要把OnoNode实例first声明为私有变量已提供必要的保护。\n超载 Overloading\n如果想初始化一个空列表, 可以:\n/** 构造一个空列表. */ public LinkedList() { fist = null; size = 0; } 即使原来已经有一个带参数x的构造器了, 这里再加一个同名构造器也没问题. 因为Java允许有不同参数的方法重名, 即超载 overloading.\n程序不变条件 Invariants\n上面超载了一个初始化空列表的构造器, 加入初始化一个空列表，然后直接调用addLast，程序会报错, 因为null没有next.\n有几种修改方法, 比如用if else这种加特例的方法. 这个方案虽然可以能解决问题，但是应尽量避免加入特例代码。毕竟有特例就意味着增加了复杂度和额外的代码特例记忆需求, 而人记忆是有限的.\n一个更简洁（尽管不太显而易见）的解决方案是修改数据结构本身，让所有LinkedList，维护起来都没有差别，即使是空的。如果把列表比做拉货的火车，那么货物就是列表承载的数据。一列火车如果只有车厢而没有车头（或者车尾）的话是没有意义的，因为没有动力。所以不管火车有没有拉货，有车厢还是没车厢，要称之为火车我们至少需要一个火车头，通过创建一个特殊节点 - 前哨节点 sentinel。前哨节点将保存一个值，具体数值我们不关心，它只是作为火车头，不装货。 所以我们要修改LinkedList为：\n/* 第一个元素 （假如有的话）就是 sentinel.next. */ public class LinkedList\u0026lt;XXX\u0026gt; { private class OneNode { //... } private OneNode sentinel; private int size; /** 构造一个空列表. */ public LinkedList() { sentinel = new OneNode(null, null); size = 0; } /** 构造一个初始元素为x的列表. */ public LinkedList(XXX x) { sentinel = new OneNode(null, null); sentinel.next = new OneNode(x, null); size = 1; } } 对于像LinkedList这样简单的数据结构来说，特例不多，我们也许可以hold住, 一旦后续遇到像树tree等更复杂的数据结构，控制特例数量就显得极为重要了。所以现在就要培养自己的这方面的习惯，保持程序不变条件成立。所谓 invariants 就是指数据结构任何情况下都是不会出错（除非程序有bug）.\n具有前哨节点的LinkedList至少具有以下 invariants：\n列表默认存在前哨节点。 列表第一个元素（如果非空的话）总是在sentinel.next.item。 size变量始终是已添加的元素总数。 不变条件使得代码的推敲变得更加容易，同时给程序员提供了能够确保代码正常工作的具体目标。\n","permalink":"https://congchan.github.io/posts/java-08-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8D%95%E5%90%91%E9%93%BE%E8%A1%A8-singly-linked-list/","summary":"\u003ch2 id=\"链表\"\u003e链表\u003c/h2\u003e\n\u003cp\u003eLinked List\u003c/p\u003e\n\u003cp\u003e前面有介绍以array为基础搭建的列表，支持自动扩容, 各种插入，删除速度都很快. 这里再介绍另一种方案, 链表, 也可以实现列表自动扩容.\u003c/p\u003e\n\u003ch3 id=\"带链接的节点\"\u003e带链接的节点\u003c/h3\u003e\n\u003cp\u003e链表的核心组成是带链接的节点, 每个节点就像火车车厢, 有钩子连接下一节车厢.\u003cimg loading=\"lazy\" src=\"/images/408px-Singly-linked-list.png\"\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e以int节点为例:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eIntNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eitem\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eIntNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enext\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eIntNode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eIntNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eitem\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003enext\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003enext\u003c/code\u003e就是这个链接, 每一个节点就是其上一个节点的\u003ccode\u003enext\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"使用嵌套类\"\u003e使用嵌套类\u003c/h3\u003e\n\u003cp\u003e这个节点作为一个相对独立的数据结构, 我们更希望让他单独作为一个类来维护. 再另外创建一个名为\u003ccode\u003eLinkedList\u003c/code\u003e的class与用户进行交互. 这样还有另一个好处就是提供一个命名为\u003ccode\u003eLinkedList\u003c/code\u003e的类给用户交互，用户更直观地知道自己是在调用链表。如果直接与node类交互，用户可能会困扰. 但同时考虑到这个node类只有\u003ccode\u003eLinkedList\u003c/code\u003e会调用，所以我们可以把node类嵌套进\u003ccode\u003eLinkedList\u003c/code\u003e中。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eLinkedList\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eXXX\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eXXX\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eitem\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enext\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eXXX\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003eitem\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"n\"\u003enext\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003en\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003efirst\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eLinkedList\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eXXX\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003efirst\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eOneNode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003enull\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e//下面是各种方法...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e以上定义使用了\u003ca href=\"/java-05-variable-types#%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B\"\u003e泛型\u003c/a\u003e。声明\u003ccode\u003eOneNode\u003c/code\u003e实例\u003ccode\u003efirst\u003c/code\u003e为私有变量, 是为了防止用户错误地摆弄链接指向，\u003ccode\u003eprivate\u003c/code\u003e和\u003ccode\u003epublic\u003c/code\u003e的使用\u003ca href=\"/java-07-data-structures-array-based-list#%E5%85%AC%E5%85%B1%E4%B8%8E%E7%A7%81%E6%9C%89\"\u003e参考\u003c/a\u003e.\u003c/p\u003e","title":"Java 08 | 数据结构 - 单向链表 Singly Linked List"},{"content":"我学习CS历程，也包含基础知识的总结以及编程实现的整理。\n每一阶段里面, 都有很多个性化选项, 仅供参考\n入门\nCS入门 学习编写(至少)一种面向对象编程语言(C ++，Java®，Python®) 测试你的代码 夯实基础 你可以跳过这些而直接进入下面的进阶实践环节，并根据自己需要查漏补缺。 但是这些都是非常重要的基础，任何时候，只要有时间，就可以去学习了解。\n逻辑推理和离散数学 熟悉计算机操作系统 学习计算机网络 掌握数据库 了解编译器和优化 了解计算理论 学习其他编程语言 进阶与实践\n深入了解算法和数据结构 分布式，并行和大数据 安卓开发 iOS开发 网页开发 加密与区块链 参与项目 English template 英文模板\nCS入门 现在的入门课基本都是用Python语言。\n计算机科学导论，优达学城 CS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML CMU 15213: Introduction to Computer Systems (ICS) 面向对象编程语言 一般而言，建议先学Java 或 Python，再学C++。 这三种语言都基本掌握后，再根据自身的职业需求，选择其中一个语言（或者其他语言）进一步深入练习。\n面向初学者程序员的在线资源：\n编程方法学，斯坦福CS106A，Java 伯克利大学CS 61A计算机程序的结构与解读，Python Java编程简介，MIT Google Python Class 面向有经验的程序员的在线资源：\n数据结构，伯克利大学 CS 61B，Java 计算机程序设计，Udacity，Python 抽象编程，斯坦福 CS106B，C ++, 最新作业 http://web.stanford.edu/class/cs106b/ 《数据结构与算法分析:C++描述》, Mark A. Weiss 测试你的代码 了解如何捕获错误，创建测试和破解软件.\n软件测试，Udacity 软件调试，Udacity 操作系统 CMU 15213: Introduction to Computer Systems (ICS) CS162 UC Berkeley: Operating Systems and Systems Programming 逻辑推理和离散数学 数学计算机科学，麻省理工学院 数学思考导论，斯坦福大学，Coursera 概率图形模型，斯坦福大学，Coursera 博弈论，Coursera 计算理论 Introduction to the theory of computation, Michael Sipser 计算机网络 Computer Networking A Top-Down Approach, James F. Kurose, Keith W. Ross 数据库 SQL A First Course in Database Systems, Jeffrey D. Ullman, Jennifer Widom 编译器 算法和数据结构 了解基本数据类型(堆栈，队列和袋子)，排序算法(快速排序，合并，堆栈)，数据结构(二叉搜索树，红黑树，哈希表)和Big O.\n算法简介，麻省理工学院，2011秋季 算法，普林斯顿大学，Part1 算法，普林斯顿大学，Part2 算法：设计和分析，斯坦福大学 算法，第4版，by Robert Sedgewick and Kevin Wayne 分布式，并行和大数据 Hadoop 和 MapReduce 入门，优达学城 并行计算入门：MPI, openMP, and CUDA, 斯坦福 MapReduce 极限计算，爱丁堡大学 加密与区块链 Cryptography, Stanford, Coursera Applied Cryptography, Udacity 安卓开发 Google Developer Training for Android, on Udacity iOS开发 网页开发 参与项目 尝试课堂以外的项目：创建和维护网站，构建自己的服务器或构建机器人。 Capstone project: Analyzing (Social) Network Data - scroll down to bottom of page,UCSD, Coursera Capstone project: Java Programming: A DIY Version of Netflix and Amazon Recommendation Engines, Duke University, Coursera Project Directory, Apache Google Summer of Code Project Archive 参与大型系统（代码库）的一小部分，阅读并理解现有代码，跟踪文档和调试 Version control with Git GitHub®：关注github热门开源项目的issue 与其他程序员一起工作 参与开源项目： 尝试提出一个issue 针对一个issue，尝试给出解决方案，并提交 pull request 参与公司实习 其他编程语言 根据实际需要自行选择一种或多种学习, 一些在线资源：\nCS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML Codecademy JavaScript Bento JavaScript Learning Track(Bento) Egghead.io 学习如何编程：JavaScript - Epicodus Inc. 学习：查询 CSS ＆ HTML Bento CSS Learning Track(Bento) Bento HTML Learning Track(Bento) 用破折号建立个人网站 使用Webflow构建响应式网站 使用骨架构建SaaS着陆页 建立动态网站 在1小时内编写个人启动页面：实用HTML和CSS简介 学习如何编程：CSS - Epicodus Inc. 从头开始学习HTML5编程 Ruby 学习如何编程：Ruby - Epicodus Inc. RubyMonk - 交互式Ruby教程 Haskell C9：功能编程基础知识 - Erik Meijer CIS 194：Haskell简介 - Brent Yorgey CS240h：Haskell的功能系统 - Bryan O\u0026rsquo;Sullivan edX：功能编程简介 - Erik Meijer 亚琛大学：功能编程 - JürgenGiesl Lua Lua Interactive Crash Course Lua Tutorial PHP 学习如何编程：PHP - Epicodus Inc. GO Go Tutorial 参考资料 Guide to technical development from Google education OS Free Programming Books\n","permalink":"https://congchan.github.io/posts/computer-science-step-by-step/","summary":"\u003cp\u003e我学习CS历程，也包含基础知识的总结以及编程实现的整理。\u003c/p\u003e\n\u003cp\u003e每一阶段里面, 都有很多个性化选项, 仅供参考\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e入门\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#CS%e5%85%a5%e9%97%a8\"\u003eCS入门\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e9%9d%a2%e5%90%91%e5%af%b9%e8%b1%a1%e7%bc%96%e7%a8%8b%e8%af%ad%e8%a8%80\"\u003e学习编写(至少)一种面向对象编程语言(C ++，Java®，Python®)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e6%b5%8b%e8%af%95%e4%bd%a0%e7%9a%84%e4%bb%a3%e7%a0%81\"\u003e测试你的代码\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e夯实基础\u003c/strong\u003e\n你可以跳过这些而直接进入下面的进阶实践环节，并根据自己需要查漏补缺。\n但是这些都是非常重要的基础，任何时候，只要有时间，就可以去学习了解。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e9%80%bb%e8%be%91%e6%8e%a8%e7%90%86%e5%92%8c%e7%a6%bb%e6%95%a3%e6%95%b0%e5%ad%a6\"\u003e逻辑推理和离散数学\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e6%93%8d%e4%bd%9c%e7%b3%bb%e7%bb%9f\"\u003e熟悉计算机操作系统\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e8%ae%a1%e7%ae%97%e6%9c%ba%e7%bd%91%e7%bb%9c\"\u003e学习计算机网络\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e6%95%b0%e6%8d%ae%e5%ba%93\"\u003e掌握数据库\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e7%bc%96%e8%af%91%e5%99%a8\"\u003e了解编译器和优化\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e8%ae%a1%e7%ae%97%e7%90%86%e8%ae%ba\"\u003e了解计算理论\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e5%85%b6%e4%bb%96%e7%bc%96%e7%a8%8b%e8%af%ad%e8%a8%80\"\u003e学习其他编程语言\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e进阶与实践\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e7%ae%97%e6%b3%95%e5%92%8c%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84\"\u003e深入了解算法和数据结构\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e5%88%86%e5%b8%83%e5%bc%8f%ef%bc%8c%e5%b9%b6%e8%a1%8c%e5%92%8c%e5%a4%a7%e6%95%b0%e6%8d%ae\"\u003e分布式，并行和大数据\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e5%ae%89%e5%8d%93%e5%bc%80%e5%8f%91\"\u003e安卓开发\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#iOS%e5%bc%80%e5%8f%91\"\u003eiOS开发\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e7%bd%91%e9%a1%b5%e5%bc%80%e5%8f%91\"\u003e网页开发\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e5%8a%a0%e5%af%86%e4%b8%8e%e5%8c%ba%e5%9d%97%e9%93%be\"\u003e加密与区块链\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/computer-science-step-by-step/#%e5%8f%82%e4%b8%8e%e9%a1%b9%e7%9b%ae\"\u003e参与项目\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ShootingSpace/Guide-to-Computer-Science/blob/master/readme.md\"\u003eEnglish template 英文模板\u003c/a\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003chr\u003e\n\u003ch3 id=\"cs入门\"\u003eCS入门\u003c/h3\u003e\n\u003cp\u003e现在的入门课基本都是用Python语言。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.udacity.com/course/cs101\"\u003e计算机科学导论，优达学城\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://cs50.harvard.edu/\"\u003eCS50x 哈佛，语言包括C，Python，SQL和JavaScript加CSS和HTML\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.cs.cmu.edu/~213/\"\u003eCMU 15213: Introduction to Computer Systems (ICS)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"面向对象编程语言\"\u003e面向对象编程语言\u003c/h3\u003e\n\u003cp\u003e一般而言，建议先学Java 或 Python，再学C++。 这三种语言都基本掌握后，再根据自身的职业需求，选择其中一个语言（或者其他语言）进一步深入练习。\u003c/p\u003e\n\u003cp\u003e面向初学者程序员的在线资源：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://see.stanford.edu/Course/CS106A\"\u003e编程方法学，斯坦福CS106A，Java\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www-inst.eecs.berkeley.edu/~cs61a/sp14/\"\u003e伯克利大学CS 61A计算机程序的结构与解读，Python\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-092-introduction-to-programming-in-java-january-iap-2010/index.htm\"\u003eJava编程简介，MIT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://developers.google.com/edu/python/\"\u003eGoogle Python Class\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e面向有经验的程序员的在线资源：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://datastructur.es/sp16/\"\u003e数据结构，伯克利大学 CS 61B，Java\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.udacity.com/course/design-of-computer-programs--cs212\"\u003e计算机程序设计，Udacity，Python\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://see.stanford.edu/Course/CS106B\"\u003e抽象编程，斯坦福 CS106B，C ++\u003c/a\u003e, 最新作业 \u003ca href=\"http://web.stanford.edu/class/cs106b/\"\u003ehttp://web.stanford.edu/class/cs106b/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://book.douban.com/subject/1909336/\"\u003e《数据结构与算法分析:C++描述》, Mark A. Weiss\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"测试你的代码\"\u003e测试你的代码\u003c/h3\u003e\n\u003cp\u003e了解如何捕获错误，创建测试和破解软件.\u003c/p\u003e","title":"Computer Science Step by Step"},{"content":"记录学习AI的学习笔记，内容包含基础知识的总结以及编程实现的整理。\nEnglish template 英文模板\n机器学习 Coursera Machine Learning， 吴恩达的简化版机器学习 Machine Learning, 吴恩达的机器学习课程 这个比较深入 Deep Learning, 吴恩达的深度学习课程 Neural Networks for Machine Learning, Hinton的神经网络课程 深度学习 Deep learning, Coursera Machine Learning Practical: DNN, CNN, RNN 每个lab的答案在下一个lab 分支，即lab1的答案可以在lab2 branch里面看到。这个代码全部用Python class，比coursera的难度高点。 自然语言处理 自然语言处理, 斯坦福 加速自然语言处理, 爱丁堡大学 深度学习处理自然语言，斯坦福 计算机视觉 图像识别：卷积神经网络，李飞飞，斯坦福 机器人 机器人入门，斯坦福 ","permalink":"https://congchan.github.io/posts/learn-ai-step-by-step/","summary":"\u003cp\u003e记录学习AI的学习笔记，内容包含基础知识的总结以及编程实现的整理。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ShootingSpace/Guide-to-Computer-Science/blob/master/readme.md\"\u003eEnglish template 英文模板\u003c/a\u003e\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"机器学习\"\u003e机器学习\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/learn/machine-learning\"\u003eCoursera Machine Learning， 吴恩达的简化版机器学习\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://see.stanford.edu/Course/CS229\"\u003eMachine Learning, 吴恩达的机器学习课程\u003c/a\u003e 这个比较深入\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning, 吴恩达的深度学习课程\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/learn/neural-networks\"\u003eNeural Networks for Machine Learning, Hinton的神经网络课程\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"深度学习\"\u003e深度学习\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep learning, Coursera\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/CSTR-Edinburgh/mlpractical/tree/mlp2017-8/master\"\u003eMachine Learning Practical: DNN, CNN, RNN\u003c/a\u003e 每个lab的答案在下一个lab 分支，即lab1的答案可以在lab2 branch里面看到。这个代码全部用Python class，比coursera的难度高点。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"自然语言处理\"\u003e自然语言处理\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PL6397E4B26D00A269\"\u003e自然语言处理, 斯坦福\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ShootingSpace/Self-to-Computer-Science-and-Artificial-Intelligence/blob/master/Note%20-%20AI125%20Accelerated%20Natural%20Language%20Processing%20UoE.md\"\u003e加速自然语言处理, 爱丁堡大学\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://cs224d.stanford.edu/\"\u003e深度学习处理自然语言，斯坦福\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"计算机视觉\"\u003e计算机视觉\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=6niqTuYFZLQ\u0026amp;list=PLe7764SJVnV10-Nr7e0sBlC9J0LRf4sQo\"\u003e图像识别：卷积神经网络，李飞飞，斯坦福\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"机器人\"\u003e机器人\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://see.stanford.edu/Course/CS223A\"\u003e机器人入门，斯坦福\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Learn AI Step by Step"},{"content":"列表 List 前面说到Java的数组无法更改长度，那么也就无法实现插入或者删除数组成员。Java提供了功能更丰富的数据结构 - 列表（list）。所谓列表，即有序的集合（序列），用户可以精确地控制每个元素插入到列表中的哪个位置。用户可以通过整数索引（列表中的位置）来访问元素，并搜索列表中的元素（详细可进一步参考oracle官网）。\n这里我们尝试以java的array为基础实现一个列表，目标是实现自动扩容 (Java中的ArrayList不仅仅有自动扩容, 也继承了[List]的其他功能)。在探索的过程中, 可以顺带学习很多相关的内容. 使用自上而下的设计思想搭建一个框架: 先写出最基础的部分, 也就是一个构造器，前面学过了整数数组，我们直接拿来用\n/** Array based list. */ // index 0 1 2 3 4 5 6 7 // items: [6 9 -1 2 0 0 0 0 ...] // size: 5 public class AList { private int[] items; private int size; /** 构造一个初始容量100的数组，初始有效数据成员为0. */ public AList() { items = new int[100]; size = 0; } /** 下面添加其他方法 */ } 然后思考我们需要什么功能，把功能需求转化为实例方法instance method的形式，先把方法的外壳描绘出来，注释上该方法的功能（目的），输入值，返回值是什么之类的。具体的功能实现可以先空着，之后一步步丰富。\n公共与私有 Public vs. Private\n在上面的代码块中，可以看到 items 和 size 都被声明为 private 私有变量, 这样就只能被所在的java文件内调用.\n私有变量和方法的设计初衷是服务于程序的内部功能实现, 而不是用来和外部程序(用户)进行交互的. 设置成私有, 可以避免这些变量和方法被外部程序直接调用, 避免用户通过不恰当/容易出错的方式修改某些变量. 在程序说明文档中, 一般也会明确说明程序提供什么公共变量和方法给用户调用.\n因此我们这里也提供几个 public 方法让用户调用, 这样用户就能按照我们设计的方式来访问数据。分别是getLast() - 访问列表最后一个元素，get(int i)访问第i个元素, 和size()访问列表的大小.\n/** 程序内的方法可以访问 private 变量 */ /** 返回列表末尾的值. */ public int getLast() { return items[size - 1]; } /** 返回第 i 个值 (0 是第一个). */ public int get(int i) { return items[i]; } /** 返回列表元素长度. */ public int size() { return size; } 泛型数组 我们不仅希望我们的列表可以存整数，也可以存其他类型的数据，可以通过泛型解决，泛型的介绍参考这篇文章.\n泛型数组跟前面介绍的泛型示例有一个重要的语法差异：Java不允许我们创建一个通用对象的数组，原因这里不细展开。\n假如我们用Item来标识泛型, 那么在上面的列表类中构建泛型数组时, 我们不能用items = new Item[8];, 而要用items = (Item []) new Object[8];\npublic class AList\u0026lt;Item\u0026gt; { private Item[] items; private int size; /** 构造一个初始容量100的数组，初始有效数据成员为0. */ public AList() { items = (Item[]) new Object[100]; //会有编译警告, 暂时不管 size = 0; } } 即使这样也会产生一个编译警告，但不影响实际使用, 后面的Casting会更详细地讨论这个问题。\n% javac AList.java Note: AList.java uses unchecked or unsafe operations. Note: Recompile with -Xlint:unchecked for details. % javac -Xlint:unchecked AList.java AList.java:26: warning: [unchecked] unchecked cast found : java.lang.Object[] required: Item[] items = (Item[]) new Object[100]; ^ 1 warning 数组扩容 Resize\n一个列表应该支持基本的插入和删除数据的操作，但是因为数组本身无法更改长度，所以我们就需要一个方法，在给数组插入新数据时，先检查长度容量是否足够，如果不够，那么就要增加长度。我们考虑简单的情况, 即需要在数组末尾插入或者删除数据怎么办。\n插入元素：\n/** 把 X 插入到列表末尾. */ public void addLast(Item x) { /** 检查长度容量是否足够，如果不够，那么就要增加长度 */ if (size == items.length) { Item[] temp = (Item[]) new Object[size + 1]; System.arraycopy(items, 0, temp, 0, size); items = temp; } items[size] = x; size = size + 1; } 创建新array并把旧数据复制过去的过程通常称为“resizing”。其实用词不当，因为数组实际上并没有改变大小，只是把小数组上的数据复制到大数组上而已。\n为了让代码更易于维护，可以把上面的代码中负责大小调整的部分包装在一个独立的method中\n/** 改变列表容量, capacity为改变后的容量. */ private void resize(int capacity) { Item[] temp = (Item[]) new Object[capacity]; System.arraycopy(items, 0, temp, 0, size); items = temp; } /** 把 X 插入到列表末尾. */ public void addLast(Item x) { if (size == items.length) { resize(size + 1); } items[size] = x; size = size + 1; } 删除元素：\n/** 删去列表最后一个值，并返回该值 */ public int removeLast() { Item x = getLast(); items[size - 1] = null; // 曾经引用“删除”的元素的内存地址被清空 size = size - 1; return x; } 事实上即使没有items[size - 1] = null;,也可以达到删除元素的目的. 删除对存储对象的引用, 是为了避免“loitering”。所谓 loitering，可以理解为占着茅坑不拉屎的对象，它们已经没啥用了，却还是占用着内存。如果这个对象是些几十兆的高清图片，那么就会很消耗内存。这也是为什么安卓手机越用越慢的一个原因。\n当引用/内存地址丢失时，Java会销毁对象。如果我们不清空引用，那么Java将不会垃圾回收这些本来预计要删除的对象, 因为它们实际还被列表引用着。\n扩容效率分析 我们直觉也会感觉到，如果按照现在的设计，即每插入一个新元素，就重新复制一遍数组，这样随着数组越来越大，效率肯定会越来越差。事实上也是这样，如果数组目前长度是100个内存块，那么插入1000次，需要创建并填充大约50万个内存块（等差数列求和N(N+1)/2，101+102+\u0026hellip;+1000 ≈ 500000）。但假如我们第一次就扩容到1000，那么就省却了很多运算消耗。可惜我们不知道用户需要插入多少数据，所以要采取其他方法-几何调整。也就是与其按照size + FACTOR这样的速率增加容量, 不如按照size * RFACTOR成倍扩容, 前者的增加速率为1, 后者为 RFACTOR, 只要设置 RFACTOR 大于1, 就能减少扩容的次数.\n/** 把 X 插入到列表末尾. */ public void addLast(Item x) { if (size == items.length) { resize(size * RFACTOR); //用 RFACTOR 作为因子扩容数组, } items[size] = x; size = size + 1; } 目前我们解决了时间效率问题, 但代价是需要更大的内存空间, 也就是空间效率下降了. 假设我们插入了十亿个item，然后再删去九亿九千万个项目。在这种情况下，我们将只使用10,000,000个内存块，剩下99％完全没有使用到。\n为了解决这个问题，我们可以在数组容量利用率比较低时把容量降下来. 定义利用率 R 为列表的大小除以items数组的长度。一般当R下降到小于0.25时，我们将数组的大小减半。\n其他功能 比如排序等, 在后面介绍链表的文章中再讨论.\n","permalink":"https://congchan.github.io/posts/java-07-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%94%A8%E6%95%B0%E7%BB%84%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%88%97%E8%A1%A8-list/","summary":"\u003ch2 id=\"列表\"\u003e列表\u003c/h2\u003e\n\u003cp\u003eList\n前面说到Java的数组无法更改长度，那么也就无法实现插入或者删除数组成员。Java提供了功能更丰富的数据结构 - 列表（\u003ca href=\"https://docs.oracle.com/javase/8/docs/api/java/util/List.html\"\u003elist\u003c/a\u003e）。所谓列表，即有序的集合（序列），用户可以精确地控制每个元素插入到列表中的哪个位置。用户可以通过整数索引（列表中的位置）来访问元素，并搜索列表中的元素（详细可进一步参考\u003ca href=\"https://docs.oracle.com/javase/8/docs/api/java/util/List.html\"\u003eoracle官网\u003c/a\u003e）。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e这里我们尝试以java的array为基础实现一个列表，目标是实现自动扩容 (Java中的\u003ca href=\"https://docs.oracle.com/javase/8/docs/api/java/util/ArrayList.html\"\u003eArrayList\u003c/a\u003e不仅仅有自动扩容, 也继承了[List]的其他功能)。在探索的过程中, 可以顺带学习很多相关的内容.\n使用自上而下的设计思想搭建一个框架:\n先写出最基础的部分, 也就是一个构造器，前面学过了整数数组，我们直接拿来用\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e/** Array based list.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e// index   0 1  2 3 4 5 6 7\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e// items: [6 9 -1 2 0 0 0 0 ...]\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e// size: 5\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eAList\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eitems\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/** 构造一个初始容量100的数组，初始有效数据成员为0. */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eAList\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eitems\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e100\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"cm\"\u003e/** 下面添加其他方法\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e    */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e然后思考我们需要什么功能，把功能需求转化为实例方法instance method的形式，先把方法的外壳描绘出来，注释上该方法的功能（目的），输入值，返回值是什么之类的。具体的功能实现可以先空着，之后一步步丰富。\u003c/p\u003e","title":"Java 07 | 数据结构 - 用数组构建数据列表 list"},{"content":"数组（Array） 数组是一种特殊的对象，有一个固定的数组长度参数N，由一连串（N个）连续的带编号的内存块组成，每个都是相同的类型(不像Python可以包含不同类型)，索引从0到N-1编号。A[i]获得数组A的第i个元素。这与普通的类实例不同，类实例有具体变量名命名的内存块。\n数组实例化，包含对象的数组 Array Instantiation, Arrays of Objects\n要创建最简单的整数数组, 有三种方式:\nx = new int [3]; //创建一个指定长度的数组，并用默认值（0）填充每个内存块。 y = new int [] {1，2，3，4，5}; //创建一个合适大小的数组，以容纳指定的初始值 int [] z = {9，10，11，12，13}; //省略了new，只能结合变量声明使用。 创建一组实例化对象:\npublic class DogArrayDemo { public static void main(String[] args) { /* Create an array of two dogs. */ Dog[] dogs = new Dog[2]; dogs[0] = new Dog(8); dogs[1] = new Dog(20); /* Yipping will result, since dogs[0] has weight 8. */ dogs[0].makeNoise(); } } 注意到new有两种不同的使用方式：一种是创建一个可以容纳两个Dog对象的数组，另外两个创建各个实际的Dog实例。\n数组复制\nx = new int[]{-1, 2, 5, 4, 99}; int[] b = {9, 10, 11}; System.arraycopy(b, 0, x, 3, 2); //效果类似于Python的`x[3:5] = b[0:2]` System.arraycopy的五个参数分别代表：\n待复制的数组(源) 源数组复制起点 目标数组 目标数组粘贴起点 有多少项要复制 2D数组 Java的二维数组实质上是一数组的数组, 即每一个数组元素里面也是一个数组。\nint[][] matrix; //声明一个引用数组的数组 matrix = new int[4][]; //创建四个内存块, 用默认null值填充, 之后用于储存对整数数组的引用, 即地址, int[] rowZero = matrix[0]; /** 实例化整数数组, 把其地址/引用分别赋值给/储存到 matrix 的第N个内存块*/ matrix[0] = new int[]{1}; matrix[1] = new int[]{1, 1}; matrix[2] = new int[]{1, 2, 1}; matrix[3] = new int[]{1, 3, 3, 1}; int[] rowTwo = matrix[2]; rowTwo[1] = -5; /** 创建四个内存块, 其中每个被引用的整数数组长度为4,每个元素都是0.*/ matrix = new int[4][4]; int[][] matrixAgain = new int[][]{{1}, {1, 1},{1, 2, 1}, {1, 3, 3, 1}}; ","permalink":"https://congchan.github.io/posts/java-06-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-array-%E6%95%B0%E7%BB%84/","summary":"\u003ch2 id=\"数组array\"\u003e数组（Array）\u003c/h2\u003e\n\u003cp\u003e数组是一种特殊的对象，有一个固定的数组长度参数N，由一连串（N个）连续的带编号的内存块组成，每个都是相同的类型(不像Python可以包含不同类型)，索引从0到N-1编号。A[i]获得数组A的第i个元素。这与普通的类实例不同，类实例有具体变量名命名的内存块。\u003c/p\u003e\n\u003ch3 id=\"数组实例化包含对象的数组\"\u003e数组实例化，包含对象的数组\u003c/h3\u003e\n\u003cp\u003eArray Instantiation, Arrays of Objects\u003c/p\u003e\n\u003cp\u003e要创建最简单的整数数组, 有三种方式:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e3\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e//创建一个指定长度的数组，并用默认值（0）填充每个内存块。\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"err\"\u003e，\u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"err\"\u003e，\u003c/span\u003e\u003cspan class=\"n\"\u003e3\u003c/span\u003e\u003cspan class=\"err\"\u003e，\u003c/span\u003e\u003cspan class=\"n\"\u003e4\u003c/span\u003e\u003cspan class=\"err\"\u003e，\u003c/span\u003e\u003cspan class=\"n\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e};\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e//创建一个合适大小的数组，以容纳指定的初始值\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ez\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003e9\u003c/span\u003e\u003cspan class=\"err\"\u003e，\u003c/span\u003e\u003cspan class=\"n\"\u003e10\u003c/span\u003e\u003cspan class=\"err\"\u003e，\u003c/span\u003e\u003cspan class=\"n\"\u003e11\u003c/span\u003e\u003cspan class=\"err\"\u003e，\u003c/span\u003e\u003cspan class=\"n\"\u003e12\u003c/span\u003e\u003cspan class=\"err\"\u003e，\u003c/span\u003e\u003cspan class=\"n\"\u003e13\u003c/span\u003e\u003cspan class=\"p\"\u003e};\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e//省略了new，只能结合变量声明使用。\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c!-- more --\u003e\n\u003cp\u003e创建一组实例化对象:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDogArrayDemo\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"cm\"\u003e/* Create an array of two dogs. */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003edogs\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003edogs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003edogs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e20\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"cm\"\u003e/* Yipping will result, since dogs[0] has weight 8. */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003edogs\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e]\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003emakeNoise\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e注意到new有两种不同的使用方式：一种是创建一个可以容纳两个Dog对象的数组，另外两个创建各个实际的Dog实例。\u003c/p\u003e","title":"Java 06 | 数据结构 - array 数组"},{"content":"数据类型 数据类型是程序设计语言描述事物、对象的方法。Java数据类型分为基本类型（内置类型）和引用类型(扩展类型）两大类。基本类型就是Java语言本身提供的基本数据类型，比如，整型数，浮点数，字符，布尔值等等。而引用类型则是Java语言根据基本类型扩展出的其他类型，Java要求所有的引用扩展类型都必须包括在类定义里面，这就是Java为什么是面向对象编程语言的原因\u0026hellip;\n上面的定义有点抽象，要理解数据类型，需要先理解一个问题: 神秘的海象问题\n尝试预测下面的代码运行时会发生什么。b的变化是否会影响a？提示：类似Python。\nWalrus a = new Walrus(1000, 8.3); Walrus b; b = a; b.weight = 5; System.out.println(a); System.out.println(b); 同样尝试预测下面的代码运行时会发生什么。x的改变是否影响y？\nint x = 5; int y; y = x; x = 2; System.out.println(\u0026#34;x is: \u0026#34; + x); System.out.println(\u0026#34;y is: \u0026#34; + y); 答案是b的变化会影响a, 但x的改变不影响y，具体见可视化过程. 这里的差别虽然微妙, 但其背后的原理对于数据结构的效率来说是非常重要的，对这个问题的深入理解也将引导我们写出更安全，更可靠的代码。\n基本类型 Primative Types\n计算机中的所有信息都以一系列1和0的形式存储在内存中，这些二进制的0和1就是比特位（bits）。比如72和“H”在内存一般以01001000的形式存储，对他们的形式是一样的。一个引申问题就是：Java代码如何解释01001000，怎么知道应该解释为72还是“H”？ 通过类型types，预先定义好类型即可, 以下代码\nchar x = \u0026#39;H\u0026#39;; int y = x; System.out.println(x); System.out.println(y); 会分别得到“H”和72. 在这种情况下，x和y变量都包含几乎相同的bits，但是Java解释器在输出时对它们进行了不同的处理。\nJava有8种基本类型：byte，short，int，long，float，double，boolean和char。\n变量声明 Declaring Variables\n计算机的内存可以视为包含大量用于存储信息的内存比特位，每个位都有一个唯一的地址。现代计算机可以使用许多这样的位。 当你声明一个特定类型的变量时，Java会用一串连续的内存位存储它。例如，如果你声明一个int，你会得到一个长度32的内存list，里面有32bits。Java中的每个数据类型都有不同的比特数。\n除了留出内存空间外，Java解释器还会在一个内部表中创建一个条目，将每个变量名称映射到内存块中第一个位置（表头list head）。 例如，如果声明了int x和double y，那么Java可能会决定使用计算机内存的352到384位来存储x，而20800到20864位则用来存储y。然后解释器将记录int x从352开始，y从20800开始。\n在Java语言里无法知道变量的具体内存位置，例如你不能以某种方式发现x在位置352。不像C++这样的语言，可以获取一段数据的确切地址。Java的这个特性是一个折衷！隐藏内存位置自然意味着程序猿的控制权更少，就无法做某些类型的优化。但是，它也避免了一大类非常棘手的编程错误。在现在计算成本如此低廉的时代，不成熟的优化还不如少点bug。\n当声明一个变量时，Java不会在预留的内存位置中写入任何内容, 也即没有默认值。因此，如果没有赋值, Java编译器会阻止你使用变量。\n以上只是内存分配的简要说明, 堆和栈的介绍可以参考 CS106B 笔记。\n引用类型 Reference Types\n所有基本数据类型之外的类型都是引用类型。 引用类型顾名思义，就是对对象的引用。在java中内存位置是不开放给程序员的, 但我们可以通过引用类型访问内存中某处对象。所有引用类型都是 java.lang.Object 类型的子类。\n对象实例化 Object Instantiation\n对象实例化：当我们使用new（例 new Dog）实例化对象时，Java首先为类的每个实例变量分配一串长度合适的bits位，并用缺省值填充它们。然后，构造函数通常（但不总是）用其他值填充每个位置.\npublic static class Walrus { public int weight; public double tuskSize; public Walrus(int w, double ts) { weight = w; tuskSize = ts; } } 用new Walrus(1000, 8.3)创建一个Walrus实例后, 我们得到分别由一个32位(int weight = 1000)和一个64位(double tuskSize = 8.3)的内存块组成的实例： 通过程序可视化过程)来更好地理解. 当然在Java编程语言的实际实现中，实例化对象时都有一些额外的内存开销, 这里不展开.\n通过 new 实例化对象，new 会返回该对象的内存地址给我们，但假如我们没有用一个变量去接收这个地址，那么我们就无法访问这个对象。之后该对象会被作为垃圾回收。\n引用变量声明 Reference Variable Declaration\n前面有提到，我们需要声明变量来接受实例化的对象在内存中的地址。当声明任何引用类型的变量（比如array, 前面的Dog类等）时，Java都会分配一串64位的内存位置. 这个64位的内存块仅用于记录变量的内存地址, 所谓内存地址, 可以理解为内存(房子)的编号(地址), 一般是内存块的表头位置的64位表达式\nWalrus someWalrus; // 创建一个64位的内存位置 someWalrus = new Walrus(1000, 8.3); //创建一个新的实例 /** 内存地址由 new 返回, 并被复制/赋值给 someWalrus 对应的内存位置 */ 比如, 假设weight是从内存位5051956592385990207开始存储的，后面连续跟着其他实例变量，那么就可以把5051956592385990207存储在someWalrus变量中。5051956592385990207由64位的二进制0100011000011100001001111100000100011101110111000001111000111111表达，这样someWalrus的内存就可以抽象的理解为一个表 someWalrus: 0100011000011100001001111100000100011101110111000001111000111111 -\u0026gt; 具体存放实例的内存(Walrus: weight=1000, tuskSize=8.3) \u0026lsquo;-\u0026gt;\u0026lsquo;可以理解为指针.\n前面有提到，如果丢失了引用变量存储的内存地址，那么该地址对应的对象就找不回来了。例如，如果一个特定的 Walrus 地址的唯一副本存储在x中，那么x = null这行代码将删去地址，我们则丢失了这个 Walrus 对象。这也不一定是坏事，很多时候在完成了一个对象后就不在需要了，只需简单地丢弃这个参考地址就可以了。\n等值规则 Java Rule of Equals\n对于y = x，Java解释器会将x的位拷贝到y中,这个规则适用于java中任何使用=赋值的语法, 是理解开头的\u0026quot;神秘的海象\u0026quot;问题的关键.\n基本类型变量的位, 存储赋值的值（基本类型）在内存中值(具体位数取决于具体的类型) int x = 5; // 此时是把内存中的某一个地址 p 复制给 x int y; y = x; // y 也指向 p x = 2; // 把一个新的内存地址 new p 复制给x, 但y还是指向原来的p x的位存储的是基本类型`int 5`(32 bits), `x = 2`是把新的基本类型`int 2`复制给x, 但y还是指向原来的`int 5`， 所以y没变化。 引用类型 reference type 变量的位, 存储赋值的值（引用类型）在内存中的地址(固定的64 bits) Dog a = new Dog(5); // 创建一个64位的内存位, 并赋值一个新的实例 p Dog b; // 仅创建一个64位的内存位, 没有引用内存地址(null) b = a; // 把a的位（是实例 p 的内存地址）复制给b, 这样 b 也是指向实例 p b.weight = 21; // 此时修改b, 会改写b指向的内存实例 p a和b只存储地址, 而它们的地址都指向相同的实例； 如果对 b 的修改本质是对 p的修改, 那么输出`a.weight`的时候, 就会变成`21`. 参数传递 Parameter Passing\n给函数传递参数，本质上也是赋值操作，参考上面的等值规则，也即复制这些参数的bits给函数，也称之为pass by value。Java的参数传递都是pass by value。至于传递过去的参数会不会因为函数内部的操作而更改，其判断原理在上面的等值规则已经阐明。\n通用数据类型 Generic\n在定义类的时候，有时候我们可能希望这个类能够接受任何类型的数据，而不仅仅是限定了基本类型中的任何一种。比如我们想实现一个类似excel表格的类，自然需要这个表格类能够接收各种类型的字符，数字，并呈现出来。这个时候就需要使用泛型 Generic, 也即通用数据类型。\nGuiding principles. Welcome compile-time errors; avoid run-time errors.\n在2004年，Java的设计者在语言中加入了泛型，使​​我们能够创建包含任何引用类型的数据结构。方法就是在类声明的类名后面，使用一个任意的占位符，并用尖括号括住\u0026lt;随便什么字符\u0026gt;。然后，在任何你想使用泛型的地方，改用占位符。\npublic class table { public class table { public int item; ... } ... } 改为\npublic class table\u0026lt;xxx\u0026gt; { public class table { public xxx item; ... } ... } \u0026lt;xxx\u0026gt;里面的名称并不重要, 改成其他也行, 只是一个标识符, 用来接受参数, 当用户实例化这个类时, 必须使用特殊的语法table\u0026lt;String\u0026gt; d = new table\u0026lt;\u0026gt;(\u0026quot;hello\u0026quot;);\n由于泛型仅适用于引用类型，因此我们不能将基本类型int等放在尖括号内。相反，我们使用基本类型的引用版本，比如对于int, 用 Integer，table\u0026lt;Integer\u0026gt; d = new table\u0026lt;\u0026gt;(\u0026quot;10\u0026quot;);\n总结使用方法:\n在一个实现某数据结构的.java文件中，在类名后面, 只指定泛型类型一次。 在其他使用该数据结构的java文件中，声明实例变量时要指定所需的类型。 如果您需要在基本类型上实例化泛型，请使用Integer, Double, Character, Boolean, Long, Short, Byte, Float，而不是其基本类型。 ","permalink":"https://congchan.github.io/posts/java-05-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","summary":"\u003ch2 id=\"数据类型\"\u003e数据类型\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://zh.wikibooks.org/zh-hans/Java/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B\"\u003e数据类型\u003c/a\u003e是程序设计语言描述事物、对象的方法。Java数据类型分为基本类型（内置类型）和引用类型(扩展类型）两大类。基本类型就是Java语言本身提供的基本数据类型，比如，整型数，浮点数，字符，布尔值等等。而引用类型则是Java语言根据基本类型扩展出的其他类型，Java要求所有的引用扩展类型都必须包括在类定义里面，这就是Java为什么是面向对象编程语言的原因\u0026hellip;\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cp\u003e上面的定义有点抽象，要理解数据类型，需要先理解一个问题: 神秘的海象问题\u003c/p\u003e\n\u003cp\u003e尝试预测下面的代码运行时会发生什么。b的变化是否会影响a？提示：类似Python。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eWalrus\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eWalrus\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e1000\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eWalrus\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eweight\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e同样尝试预测下面的代码运行时会发生什么。x的改变是否影响y？\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;x is: \u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;y is: \u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e答案是b的变化会影响a, 但x的改变不影响y，具体见\u003ca href=\"http://cscircles.cemc.uwaterloo.ca/java_visualize/#code=public\u0026#43;class\u0026#43;PollQuestions\u0026#43;%7B%0A\u0026#43;\u0026#43;\u0026#43;public\u0026#43;static\u0026#43;void\u0026#43;main%28String%5B%5D\u0026#43;args%29\u0026#43;%7B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;Walrus\u0026#43;a\u0026#43;%3D\u0026#43;new\u0026#43;Walrus%281000,\u0026#43;8.3%29%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;Walrus\u0026#43;b%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;b\u0026#43;%3D\u0026#43;a%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;b.weight\u0026#43;%3D\u0026#43;5%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;System.out.println%28a%29%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;System.out.println%28b%29%3B\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;%0A%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;int\u0026#43;x\u0026#43;%3D\u0026#43;5%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;int\u0026#43;y%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;y\u0026#43;%3D\u0026#43;x%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;x\u0026#43;%3D\u0026#43;2%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;System.out.println%28%22x\u0026#43;is%3A\u0026#43;%22\u0026#43;%2B\u0026#43;x%29%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;System.out.println%28%22y\u0026#43;is%3A\u0026#43;%22\u0026#43;%2B\u0026#43;y%29%3B\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;%0A\u0026#43;\u0026#43;\u0026#43;%7D%0A\u0026#43;\u0026#43;\u0026#43;%0A\u0026#43;\u0026#43;\u0026#43;public\u0026#43;static\u0026#43;class\u0026#43;Walrus\u0026#43;%7B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;public\u0026#43;int\u0026#43;weight%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;public\u0026#43;double\u0026#43;tuskSize%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;public\u0026#43;Walrus%28int\u0026#43;w,\u0026#43;double\u0026#43;ts%29\u0026#43;%7B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;weight\u0026#43;%3D\u0026#43;w%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;tuskSize\u0026#43;%3D\u0026#43;ts%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;%7D%0A%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;public\u0026#43;String\u0026#43;toString%28%29\u0026#43;%7B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;return\u0026#43;String.format%28%22weight%3A\u0026#43;%25d,\u0026#43;tusk\u0026#43;size%3A\u0026#43;%25.2f%22,\u0026#43;weight,\u0026#43;tuskSize%29%3B%0A\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;\u0026#43;%7D%0A\u0026#43;\u0026#43;\u0026#43;%7D%0A%7D\u0026amp;mode=edit\"\u003e可视化过程\u003c/a\u003e.\n这里的差别虽然微妙, 但其背后的原理对于数据结构的效率来说是非常重要的，对这个问题的深入理解也将引导我们写出更安全，更可靠的代码。\u003c/p\u003e\n\u003ch3 id=\"基本类型\"\u003e基本类型\u003c/h3\u003e\n\u003cp\u003ePrimative Types\u003c/p\u003e\n\u003cp\u003e计算机中的所有信息都以一系列1和0的形式存储在内存中，这些二进制的0和1就是比特位（bits）。比如72和“H”在内存一般以01001000的形式存储，对他们的形式是一样的。一个引申问题就是：Java代码如何解释01001000，怎么知道应该解释为72还是“H”？ 通过类型types，预先定义好类型即可, 以下代码\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003echar\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"sc\"\u003e\u0026#39;H\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e会分别得到“H”和72. 在这种情况下，x和y变量都包含几乎相同的bits，但是Java解释器在输出时对它们进行了不同的处理。\u003c/p\u003e\n\u003cp\u003eJava有8种基本类型：byte，short，int，long，float，double，boolean和char。\u003c/p\u003e","title":"Java 05 | 数据类型"},{"content":"嵌套类 我们经常需要在某个类A中使用另一个类B，如果设计时我们知道类B只有在类A中有被使用的可能性, 那么我们可以把类B定义在类A中, 作为类A的嵌套类, 类A就称之为外部类. 这样做可以隐藏类名，减少全局的标识符，从而限制用户能否使用该类建立对象。这样可以提高类的抽象能力，并且强调了两个类(外围类和嵌套类)之间的主从关系。\nA nested class is any class whose declaration occurs within the body of another class or interface. A top level class is a class that is not a nested class.\n嵌套类分为两类：静态和非静态。声明为static的嵌套类简称为静态嵌套类。非静态嵌套类称为内部类(inner class)。\nclass OuterClass { ... static class StaticNestedClass { ... } class InnerClass { ... } } 作为OuterClass的成员，嵌套类可以声明为private，public，protected或package private。外部类只能声明为public或package private。更多详情参考官网.\n内部类 内部类可以直接访问外部类的方法和变量(即使是private的也可以)。如果从外部类程序代码中初始化内部类, 此内部对象会绑在该外部对象上. 一个内部类的实例作为成员存在于所属外部类的实例中。\npublic class Outer { private int outVar; class Inner { public int inVar; void go() { outVar += inVar; } } Inner myInner = new Inner(); public void do() { myInner.go(); } } public static void main(String[] args) { Outer O = new Outer(); Outer.Inner I = O.new Inner(); } 也可以从外部类之外的程序来初始化外部类的内部实例, 语法比较特殊\nclass Foo { public static void main (String[] args) { Outer A = new Outer(); Outer.Inner B = A.new Inner(); } } 因为内部类与一个实例相关联，所以它不能自己定义任何静态成员。\n内部类提供在一个类中实现同一个接口的多次机会. 使用了内部类, 就可以多次实现同一个方法.\n除此之外, 更重要的是, 如果外部类已经继承了一种父类, 但又需要实现其他类的行为. 如果只需要实现一次, 那么可以使用实现接口, 但如果要多次实现某种行为, 那么就需要一个内部类来创建多个内部实例, 已达成需要多次实现的行为. 而这个内部类是不太可能被其他非外部类的类使用的. 这种情况多见于GUI的使用中.\n静态嵌套类 如果嵌套类不需要使用外部类的任何实例方法或变量，那可以声明嵌套类为static。像静态类方法一样， 静态嵌套类不能直接引用其外部类中定义的实例变量或方法。外部类不能直接访问静态嵌套类的成员变量，要通过静态嵌套类来访问。\n","permalink":"https://congchan.github.io/posts/java-04-%E7%B1%BB-class-03-%E5%B5%8C%E5%A5%97%E7%B1%BB/","summary":"\u003ch2 id=\"嵌套类\"\u003e嵌套类\u003c/h2\u003e\n\u003cp\u003e我们经常需要在某个类A中使用另一个类B，如果设计时我们知道类B只有在类A中有被使用的可能性, 那么我们可以把类B定义在类A中, 作为类A的嵌套类, 类A就称之为外部类. 这样做可以隐藏类名，减少全局的标识符，从而限制用户能否使用该类建立对象。这样可以提高类的抽象能力，并且强调了两个类(外围类和嵌套类)之间的主从关系。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA nested class is any class whose declaration occurs within the body of another class or interface. A top level class is a class that is not a nested class.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e嵌套类分为两类：静态和非静态。声明为static的嵌套类简称为\u003cstrong\u003e静态嵌套类\u003c/strong\u003e。非静态嵌套类称为\u003cstrong\u003e内部类\u003c/strong\u003e(inner class)。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eOuterClass\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eStaticNestedClass\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eInnerClass\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e作为OuterClass的成员，嵌套类可以声明为private，public，protected或package private。外部类只能声明为public或package private。更多详情\u003ca href=\"https://docs.oracle.com/javase/tutorial/java/javaOO/nested.html\"\u003e参考官网\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"内部类\"\u003e内部类\u003c/h3\u003e\n\u003cp\u003e内部类可以直接访问外部类的方法和变量(即使是private的也可以)。如果从外部类程序代码中初始化内部类, 此内部对象会绑在该外部对象上. 一个内部类的实例作为成员存在于所属外部类的实例中。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eOuter\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eoutVar\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eInner\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003einVar\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003ego\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eoutVar\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e+=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003einVar\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eInner\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emyInner\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eInner\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003edo\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003emyInner\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003ego\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eOuter\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eO\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eOuter\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eOuter\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eInner\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eI\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eO\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003enew\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eInner\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e也可以从外部类之外的程序来初始化外部类的内部实例, 语法比较特殊\u003c/p\u003e","title":"Java 04 | 类 class - 03 嵌套类"},{"content":"Class 类的方法和变量细分为静态的和非静态的. 静态就是可以被类调用，所以静态方法/变量也称之为类方法/变量；非静态只能由实例调用，所以也称之为实例方法/变量。\n静态变量 类变量 Class Variables 有static声明(静态变量).\n静态变量一般是类本身固有的属性, 被该类所有实例共享。例如，我们可能需要用狗类的另一种生物学的统称“犬科”来作为类的说明， 这个时候可以用public static String binomen = \u0026quot;犬科\u0026quot;;，这个变量理论上是由类来访问的。静态方法也类似.\n以下代码定义了一个类来模拟狗，包含一个类变量作为这个类的说明，一个类方法用于发出叫声：\npublic class Dog { public static String instruction = \u0026#34;狗类实例\u0026#34;; //类变量, 说明 public static void makeNoise() { System.out.println(\u0026#34;汪!\u0026#34;); } } 这里没有定义main(), 在这种情况下如何直接运行这个类(java Dog), 程序是会报错的\n错误: 在类 Dog 中找不到 main 方法, 请将 main 方法定义为: public static void main(String[] args) 否则 JavaFX 应用程序类必须扩展javafx.application.Application. 你可以选择在里面添加一个main()方法. 但这次我们选择不定义具体的main(). 具体要如何运行, 我们可以另写一个类定义一个main()方法来调用这个类.\npublic class DogLauncher { public static void main(String[] args) { Dog.makeNoise(); } } 类变量和方法是有局限性的。现实世界中, 并不是所有的狗都是一样的特征，仅仅靠类这个概念是无法区分不同个体的狗, 除非你为不同的狗定义不同的类（以及里面的变量和方法）, 那么就会很繁琐痛苦.\n静态变量初始化 静态变量会在该类的任何对象创建之前就完成了初始化. 静态变量会在该类的任何静态方法执行之前就初始化.\nprimitive主数据类型整数默认值是0, 浮点数默认值是0.0, boolean默认值是false, 对象引用是null.\n静态的final变量是常数: 一个被标记final的变量代表一旦被初始化之后就不会改动. 常数变量的名称全部都要大写.\n静态final变量必须初始化后才能使用, 初始化有两种方法:\n声明时public class Foo { public static final int FOO_X = 25; } 使用静态初始化程序 public class Bar { public static final double BAR_SIGN; static { BAR_SIGN = (double) Math.random(); } } 实例 包括实例变量Instance Variables 和实例方法Instance method.\nJava的类定义就像定义一张蓝图, 我们可以在这个蓝图的基础上, 生成不同的实例instance. 实例是概念性的说法，本质上在Java里就是对象object。这样的特性提供了一个很自然而然地在java中模拟生成实体世界的方法：定义一个狗的类，在这个类的基础上，通过不同的特征参数实例化不同特征的狗（instances），并使类方法的输出取决于特定实例的狗的属性。\n/** 一只狗的类:*/ public class Dog { public int weight; public void makeNoise() { if (weight \u0026lt; 10) { System.out.println(\u0026#34;嘤嘤嘤!\u0026#34;); } else if (weight \u0026lt; 30) { System.out.println(\u0026#34;汪汪汪\u0026#34;); } else { System.out.println(\u0026#34;嗷呜!\u0026#34;); } } } 这里的方法和变量没有static, 所以是实例（非静态）方法和变量. 如果直接用 Dog 类来调用这些方法, 会报错:\npublic class DogLauncher { public static void main(String[] args) { Dog.weight = 21; Dog.makeNoise(); } } DogLauncher.java:3: 错误: 无法从静态上下文中引用非静态 变量 weight Dog.weight = 21; ^ DogLauncher.java:4: 错误: 无法从静态上下文中引用非静态 方法 makeNoise() Dog.makeNoise(); ^ 这个时候, 你需要实例化一只狗, 让这个实例来调用非静态变量和方法:\npublic class DogLauncher { public static void main(String[] args) { Dog bigDog = new Dog(); bigDog.weight = 5; bigDog.makeNoise(); } } 运行时，这个程序将会创建一个重量为5的狗，这个狗就会“嗷呜”叫。\n虽然Java在技术上允许使用实例变量来访问静态变量或静态方法，但合法的不一定是好的, 这样会产生容易误解的代码，所以还是少用为好。\n总的来说，之所以需要实例方法和变量，是因为我们需要模拟个体，一只具体的狗，并让它发出声音。这个weight和makeNoise()只能由具体的狗调用。狗类不能调用，也没有调用的意义, 毕竟每只狗的重量和声音都不同的. 在设计程序时, 如果其中一个方法我们只打算让特定的实例来调用它(而不让类去调用它), 那么这个方法应该设计成实例方法。\n静态方法与实例方法 类(静态)方法Class Methods由类调用Dog.makeNoise();.\n静态方法无法调用实例变量. 否则会报错non-static variable size cannot be referenced from a static context. 静态方法不能调用非静态的方法: 即使非静态方法中没有涉及实例变量, 也无法通过编译, 因为不能保证整个非静态方法在以后会不会改动涉及实例变量, 或者如果子类去覆盖此方法可能会用到实例变量. 实例方法Instance Methods只能由实例来调用bigDog.makeNoise();.\n实例方法访问本成员变量是不受限制的, 也就是它可以访问静态变量和静态方法. 可以看到实例方法更具体, 更贴近实体世界, 那我们仍需要静态方法, 因为:\n有些类不需要实例化, 毕竟我们也经常需要处理抽象的概念, 这些抽象概念在人类认知范畴内是统一的, 比如Java的Math包含很多数学运算的静态方法, 这是客观规律，比如x = Math.sqrt(100)不管创建多少个实例，数学运算的结果都是一样的, 所以没必要实例化来浪费空间. 有些类有静态方法, 是有实际作用的 - 每个实例都通用的方法。若想比较一个类里面的不同实例, 比如两只狗的重量。比较简单的方法就是使用一个比较狗的重量的类方法: public static Dog maxDog(Dog d1, Dog d2) { if (d1.weight \u0026gt; d2.weight) { return d1; } return d2; } Dog d = new Dog(15); Dog d2 = new Dog(100); Dog.maxDog(d, d2); * 这个时候, 若使用实例方法也可以, 但没那么直观： ```java /** 我们使用关键字this来引用当前对象d。*/ public Dog maxDog(Dog d2) { if (this.weight \u0026gt; d2.weight) { return this; } return d2; } Dog d = new Dog(15); Dog d2 = new Dog(100); d.maxDog(d, d2); ``` 如果一个类只有静态的方法, 可以将构造函数标记为private以避免被初始化, 就像Java的Math一样.\n","permalink":"https://congchan.github.io/posts/java-04-%E7%B1%BB-class-02-%E7%B1%BB%E4%B8%8E%E5%AE%9E%E4%BE%8B/","summary":"\u003ch2 id=\"class\"\u003eClass\u003c/h2\u003e\n\u003cp\u003e类的方法和变量细分为静态的和非静态的. 静态就是可以被类调用，所以静态方法/变量也称之为类方法/变量；非静态只能由实例调用，所以也称之为实例方法/变量。\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003ch3 id=\"静态变量\"\u003e静态变量\u003c/h3\u003e\n\u003cp\u003e类变量 Class Variables 有\u003ccode\u003estatic\u003c/code\u003e声明(静态变量).\u003c/p\u003e\n\u003cp\u003e静态变量一般是类本身固有的属性, 被该类所有实例共享。例如，我们可能需要用狗类的另一种生物学的统称“犬科”来作为类的说明， 这个时候可以用\u003ccode\u003epublic static String binomen = \u0026quot;犬科\u0026quot;;\u003c/code\u003e，这个变量理论上是由类来访问的。静态方法也类似.\u003c/p\u003e\n\u003cp\u003e以下代码定义了一个类来模拟狗，包含一个类变量作为这个类的说明，一个类方法用于发出叫声：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDog\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003einstruction\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;狗类实例\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e//类变量, 说明\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emakeNoise\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;汪!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这里没有定义\u003ccode\u003emain()\u003c/code\u003e, 在这种情况下如何直接运行这个类(\u003ccode\u003ejava Dog\u003c/code\u003e), 程序是会报错的\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nl\"\u003e错误\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e在类\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eDog\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e中找不到\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emain\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e方法\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e请将\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003emain\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e方法定义为\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003e否则\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eJavaFX\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e应用程序类必须扩展javafx\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eapplication\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eApplication\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e你可以选择在里面添加一个\u003ccode\u003emain()\u003c/code\u003e方法. 但这次我们选择不定义具体的\u003ccode\u003emain()\u003c/code\u003e. 具体要如何运行, 我们可以另写一个类定义一个main()方法来调用这个类.\u003c/p\u003e","title":"Java 04 | 类 class - 02 类与实例"},{"content":"Class Java的语法是为了更容易地模拟真实世界而设计的. 比如用程序实现一只狗, 可以用定义一个类class来描述它.\n类class里面包括变量Variable，方法method（可以理解为Python的函数function）。变量可以储存数据，方法可以处理数据。变量必须在类中声明(即不能离开类独立存在)，不像Python或Matlab这样的语言可以在运行时添加新的变量。\n构造对象的过程:\n声明(declaration)引用变量: Dog smalldog; 创建对象：实例化 new Dog(20), 如果没有把它作为值赋给一个类声明变量, 那么这个实例化的值会被垃圾回收. 连接对象和引用：赋值对象给引用Dog smalldog = new Dog(5) 创建对象这一步，调用了Dog(), 不是普通的方法, 而是类的构造函数 Constructors.\n构造函数 构造函数在初始化一个对象时执行, 构造函数与类名同名且没有返回类型, 而且可以带参数：\n/** 注意：构造函数与class类同名 但没有返回类型 */ public Dog(int w) { weight = w; } 然后在DogLauncher里实例化一只狗时, 直接Dog d = new Dog(20);即可.\n在以上代码的基础上, 后续当我们想使用new和参数创建一只狗时，可以随时调用public Dog(int w)构造函数。对于熟悉Python的人来说，你可以理解java的构造函数为Python的__init__。\nJava可以有与类同名的方法，只是要指明返回类型。 构造函数无法被继承 如果类有一个以上的构造函数，则参数一定要不一样，包括参数顺序和类型\n构造函数链 执行new指令会启动构造函数的连锁反应(Constructor Chaining), 首先会执行其父类的构造函数, 依此类推连锁反应到Object类为止. 就算是抽象类, 也会有构造函数, 虽然不能被直接实例化, 但也会被唤醒. 理论上，每个类的构造函数需要先调用其父类的构造函数super()，依次入栈\npublic class Duck extends Animal { int size; public Duck(int newSize) { super(); // 调用父类构造函数, 且必须是在函数中的第一行 size = newSize; } } 如果明确写了super();, 则必须位于构造函数第一行. 但很多构造函数没有写super();也可以编译通过, 甚至连自身的构造函数public Duck(int newSize)也不一定是必须的。\n如果没有super(), 编译器会帮我们加上. 如果连自身的构造函数都没有, 编译器会自动为没有构造函数的类提供一个无参数的默认构造函数。这个默认构造函数将调用其超类的（可调用的）无参构造函数。 此时, 如果其extends的父类没有无参数构造函数，编译会出错。 如果没有显式的超类，那么就调用隐式的超类Object的无参构造函数。 但如果在类中已经实现了带参数的构造函数，那么编译器不会再帮你构造无参数的构造函数，你需要自己编写。如果某个父类只有带参数的构造函数, 那么继承该父类的子类必须有构造函数, 且要有带参数的super(args).\n如果存在不同重载版本的构造函数, 其中有某个构造函数可以负责大部分构造工作, 我们这个时候肯定希望能够让同类的其他构造函数先调用该构造函数, 完成大部分构造工作, 已达到代码重用的目的, 这样可以更好地维护代码.\nthis()就是用来从某个构造函数中调用同一个类的另外一个构造函数.\nclass Mini extends Car { Color color; // 这里无参数的构造函数使用默认颜色调用阵阵的构造函数 public Mini() { this(Color.Red); } public Mini(Color c) { super(\u0026#34;Mini\u0026#34;); color = c; } } this()只能存在于构造函数中, 且必须要在第一行, 所以会跟super()冲突, 二者不能同时调用.\n","permalink":"https://congchan.github.io/posts/java-04-%E7%B1%BB-class-01-%E5%8F%98%E9%87%8F%E5%92%8C%E6%96%B9%E6%B3%95/","summary":"\u003ch2 id=\"class\"\u003eClass\u003c/h2\u003e\n\u003cp\u003eJava的语法是为了更容易地模拟真实世界而设计的. 比如用程序实现一只狗, 可以用定义一个类class来描述它.\u003c/p\u003e\n\u003cp\u003e类class里面包括变量Variable，方法method（可以理解为Python的函数function）。变量可以储存数据，方法可以处理数据。变量必须在类中声明(即不能离开类独立存在)，不像Python或Matlab这样的语言可以在运行时添加新的变量。\u003c/p\u003e\n\u003cp\u003e构造对象的过程:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e声明(declaration)引用变量: \u003ccode\u003eDog smalldog;\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e创建对象：实例化 \u003ccode\u003enew Dog(20)\u003c/code\u003e, 如果没有把它作为值赋给一个类声明变量, 那么这个实例化的值会被垃圾回收.\u003c/li\u003e\n\u003cli\u003e连接对象和引用：赋值对象给引用\u003ccode\u003eDog smalldog = new Dog(5)\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e创建对象这一步，调用了\u003ccode\u003eDog()\u003c/code\u003e, 不是普通的方法, 而是类的构造函数 Constructors.\u003c/p\u003e\n\u003ch3 id=\"构造函数\"\u003e构造函数\u003c/h3\u003e\n\u003cp\u003e构造函数在初始化一个对象时执行, 构造函数与类名\u003cstrong\u003e同名\u003c/strong\u003e且没有返回类型, 而且可以带参数：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e/** 注意：构造函数与class类同名\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e    但没有返回类型  */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eDog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ew\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"n\"\u003eweight\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ew\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e然后在\u003ccode\u003eDogLauncher\u003c/code\u003e里实例化一只狗时, 直接\u003ccode\u003eDog d = new Dog(20);\u003c/code\u003e即可.\u003c/p\u003e\n\u003cp\u003e在以上代码的基础上, 后续当我们想使用new和参数创建一只狗时，可以随时调用\u003ccode\u003epublic Dog(int w)\u003c/code\u003e构造函数。对于熟悉Python的人来说，你可以理解java的构造函数为Python的\u003ccode\u003e__init__\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJava可以有与类同名的方法，只是要指明返回类型。\u003c/strong\u003e\n\u003cstrong\u003e构造函数无法被继承\u003c/strong\u003e\n\u003cstrong\u003e如果类有一个以上的构造函数，则参数一定要不一样，包括参数顺序和类型\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"构造函数链\"\u003e构造函数链\u003c/h3\u003e\n\u003cp\u003e执行\u003ccode\u003enew\u003c/code\u003e指令会启动构造函数的连锁反应(Constructor Chaining), 首先会执行其父类的构造函数, 依此类推连锁反应到\u003ccode\u003eObject\u003c/code\u003e类为止. 就算是抽象类, 也会有构造函数, 虽然不能被直接实例化, 但也会被唤醒. 理论上，每个类的构造函数需要先调用其父类的构造函数\u003ccode\u003esuper()\u003c/code\u003e，依次入栈\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eDuck\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eextends\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eAnimal\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003eDuck\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enewSize\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"kd\"\u003esuper\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"c1\"\u003e// 调用父类构造函数, 且必须是在函数中的第一行\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enewSize\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e如果明确写了\u003ccode\u003esuper();\u003c/code\u003e, 则必须位于构造函数第一行. 但很多构造函数没有写\u003ccode\u003esuper();\u003c/code\u003e也可以编译通过, 甚至连自身的构造函数\u003ccode\u003epublic Duck(int newSize)\u003c/code\u003e也不一定是必须的。\u003c/p\u003e","title":"Java 04 | 类 class - 01 变量和方法"},{"content":"代码风格与注释 努力保持代码可读性。良好的编码风格的一些最重要的特点是：\n一致的风格（间距，变量命名，缩进风格等） 大小（线不太宽，源文件不要太长） 描述性命名（变量，函数，类），例如变量或函数名称为年份或getUserName而不是x或f。让代码本身提供可解读性。 避免重复的代码：若有两个重要的代码块及其相似，应该想办法合并。 适当的评论, 使其他读者也能轻松理解你的代码 行注释: //分隔符开头行被当做注释。 Block（又名多行注释）注释: /*, */ , 但我们更推荐javadoc形式的注释。 Javadoc Javadoc: / **，*/, 可以（但不总是）包含描述性标签。 借助javadoc工具可以生成HTML格式的API文档。 第一段是方法的描述。描述下面是不同的描述性标签, 比如参数 @param， 返回值 @return， 可能抛出的任何异常 @throws\n/** * @author 名字，邮箱\u0026lt;address @ example.com\u0026gt; * @version 1.6 版本 * @param * @return */ public class Test { // class body } ","permalink":"https://congchan.github.io/posts/java-03-%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC-%E6%B3%A8%E9%87%8A-javadoc/","summary":"\u003ch2 id=\"代码风格与注释\"\u003e代码风格与注释\u003c/h2\u003e\n\u003cp\u003e努力保持代码可读性。良好的编码风格的一些最重要的特点是：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e一致的风格（间距，变量命名，缩进风格等）\u003c/li\u003e\n\u003cli\u003e大小（线不太宽，源文件不要太长）\u003c/li\u003e\n\u003cli\u003e描述性命名（变量，函数，类），例如变量或函数名称为年份或getUserName而不是x或f。让代码本身提供可解读性。\u003c/li\u003e\n\u003cli\u003e避免重复的代码：若有两个重要的代码块及其相似，应该想办法合并。\u003c/li\u003e\n\u003cli\u003e适当的评论, 使其他读者也能轻松理解你的代码\n\u003cul\u003e\n\u003cli\u003e行注释: \u003ccode\u003e//\u003c/code\u003e分隔符开头行被当做注释。\u003c/li\u003e\n\u003cli\u003eBlock（又名多行注释）注释: \u003ccode\u003e/*\u003c/code\u003e,  \u003ccode\u003e*/ \u003c/code\u003e, 但我们更推荐javadoc形式的注释。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"javadoc\"\u003eJavadoc\u003c/h3\u003e\n\u003cp\u003eJavadoc: \u003ccode\u003e/ **\u003c/code\u003e，\u003ccode\u003e*/\u003c/code\u003e, 可以（但不总是）包含描述性标签。 借助\u003ca href=\"http://docs.oracle.com/javase/8/docs/technotes/tools/windows/javadoc.html\"\u003ejavadoc工具\u003c/a\u003e可以生成HTML格式的API文档。\n第一段是方法的描述。描述下面是不同的\u003ca href=\"https://en.wikipedia.org/wiki/Javadoc\"\u003e描述性标签\u003c/a\u003e, 比如参数 \u003ccode\u003e@param\u003c/code\u003e， 返回值 \u003ccode\u003e@return\u003c/code\u003e， 可能抛出的任何异常 \u003ccode\u003e@throws\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e/**\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * @author   名字，邮箱\u0026lt;address @ example.com\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * @version     1.6 版本\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * @param\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e * @return\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cm\"\u003e */\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTest\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c1\"\u003e// class body\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Java 03 | 代码风格 注释 Javadoc"},{"content":"Java基本语法 public class HelloWorld { public static void main(String[] args) { System.out.println(\u0026#34;Hello world!\u0026#34;); } } 上面的程序由一个类声明组成，该声明使用关键字public class声明。\nJava所有的代码都应该包含在class里面。 真正负责运行的代码，是一个名为main的method，它声明为public static void main(String[] args)。 public：公共的，大部分方法都是以这个关键字开始的，后面会进一步解释。 static：这是一个静态方法，不与任何特定的实例关联，后面会解释。 void：它没有返回类型。 main：这是方法的名称。 String [] args：这是传递给main方法的参数。 使用大括号{ }来表示一段代码的开始和结束。 声明必须以分号结尾 静态分类 Static Typing\n程序语言静态与动态的分类，可以参考oracle的说明文件，它解释了动态和静态类型之间的区别, 帮助你理解由程序的错误提示信息。 两个主要区别: 1. 动态类型语言在运行时执行类型检查，而静态类型语言在编译时执行类型检查。这意味如果以静态类型语言（如Java）编写的脚本包含错误，则在编译错误之前将无法编译. 而用动态类型语言编写的脚本可以编译，即使它们包含会阻止脚本正常运行（如果有的话）的错误。 2. 静态类型语言要求你在使用它们之前声明变量的数据类型，而动态类型语言则不需要。 考虑以下两个代码示例：\n// Java int num; num = 5; # Python num = 5 这两段代码都创建一个名为num的变量并赋值为5. 不同之处在于Java需要将num的数据类型明确定义为int。因为Java是静态类型的，因此它期望变量在被赋值之前被声明。\nPython是动态类型的，不需要定义类型, Python根据变量的值确定其数据类型。动态类型语言更加灵活，在编写脚本时可以节省时间和空间。但是，这可能会导致运行时出现问题。例如：\n# python number = 5 numbr = (number + 15) / 2 #注意错字 上面的代码本应创建一个值为5的可变数字，然后将其加上15并除以2以得到10. 但是，number 在第二行的开头拼写错误。由于Python不需要声明变量，因此会不由分说直接创建一个名为numbr的新变量，并把本应分配给number的值分配给它。这段代码会很顺利编译，但是如果程序试图用number来做某事，程序员假设它的值是10，那么后续就无法产生期望的结果,而且还很难注意到问题。\nJava的compiler其中一个关键作用是进行静态类型检查（static type check）。若前面定义了 int x = 0;, 那么后面若给x赋值其他的类型值x = 'horse';, compiler就会报错. 这样就保证了程序不会出现类型错误.\n除了错误检查外, static types 也可以让程序媛/猿知道自己处的是什么对象.\n总而言之，静态类型具有以下优点：\n编译器确保所有类型都是兼容的，这使得程序员更容易调试他们的代码。 由于代码保证没有类型错误，所以编译后程序的用户将永远不会遇到类型错误。例如，Android应用程序是用Java编写的，通常仅以.class文件的形式分发，即以编译的格式。因此，这样的应用程序不应该由于类型错误而崩溃。 每个变量，参数和函数都有一个声明的类型，使程序员更容易理解和推理代码。 ","permalink":"https://congchan.github.io/posts/java-02-%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80/","summary":"\u003ch2 id=\"java基本语法\"\u003eJava基本语法\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eHelloWorld\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;Hello world!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e上面的程序由一个类声明组成，该声明使用关键字\u003ccode\u003epublic class\u003c/code\u003e声明。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJava所有的代码都应该包含在class里面。\u003c/li\u003e\n\u003cli\u003e真正负责运行的代码，是一个名为main的method，它声明为\u003ccode\u003epublic static void main(String[] args)\u003c/code\u003e。\n\u003cul\u003e\n\u003cli\u003epublic：公共的，大部分方法都是以这个关键字开始的，后面会进一步解释。\u003c/li\u003e\n\u003cli\u003estatic：这是一个静态方法，不与任何特定的实例关联，后面会解释。\u003c/li\u003e\n\u003cli\u003evoid：它没有返回类型。\u003c/li\u003e\n\u003cli\u003emain：这是方法的名称。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eString [] args\u003c/code\u003e：这是传递给main方法的参数。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e使用大括号\u003ccode\u003e{ }\u003c/code\u003e来表示一段代码的开始和结束。\u003c/li\u003e\n\u003cli\u003e声明必须以分号结尾\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- more --\u003e\n\u003ch2 id=\"静态分类\"\u003e静态分类\u003c/h2\u003e\n\u003cp\u003eStatic Typing\u003c/p\u003e\n\u003cp\u003e程序语言静态与动态的分类，可以参考oracle的\u003ca href=\"https://docs.oracle.com/cd/E57471_01/bigData.100/extensions_bdd/src/cext_transform_typing.html\"\u003e说明文件\u003c/a\u003e，它解释了动态和静态类型之间的区别, 帮助你理解由程序的错误提示信息。\n两个主要区别:\n1. 动态类型语言在运行时执行类型检查，而静态类型语言在编译时执行类型检查。这意味如果以静态类型语言（如Java）编写的脚本包含错误，则在编译错误之前将无法编译. 而用动态类型语言编写的脚本可以编译，即使它们包含会阻止脚本正常运行（如果有的话）的错误。\n2. 静态类型语言要求你在使用它们之前声明变量的数据类型，而动态类型语言则不需要。\n考虑以下两个代码示例：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Java\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003enum\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003enum\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Python\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003enum\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e这两段代码都创建一个名为num的变量并赋值为5. 不同之处在于Java需要将num的数据类型明确定义为int。因为Java是静态类型的，因此它期望变量在被赋值之前被声明。\u003c/p\u003e\n\u003cp\u003ePython是动态类型的，不需要定义类型, Python根据变量的值确定其数据类型。动态类型语言更加灵活，在编写脚本时可以节省时间和空间。但是，这可能会导致运行时出现问题。例如：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# python\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003enumber\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003enumbr\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enumber\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e15\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e  \u003cspan class=\"c1\"\u003e#注意错字\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e上面的代码本应创建一个值为5的可变数字，然后将其加上15并除以2以得到10. 但是，\u003ccode\u003enumber \u003c/code\u003e在第二行的开头拼写错误。由于Python不需要声明变量，因此会不由分说直接创建一个名为\u003ccode\u003enumbr\u003c/code\u003e的新变量，并把本应分配给\u003ccode\u003enumber\u003c/code\u003e的值分配给它。这段代码会很顺利编译，但是如果程序试图用\u003ccode\u003enumber\u003c/code\u003e来做某事，程序员假设它的值是10，那么后续就无法产生期望的结果,而且还很难注意到问题。\u003c/p\u003e","title":"Java 02 | 语法基础"},{"content":"Hello World 参考了伯克利 Josh Hug 的 cs61b spring 2017 和 cs61b spring 2018. Lab, homework 和 project 代码实现参考 https://github.com/ShootingSpace/cs61b-data-structures.\nJava安装与配置 安装Java，前往Oracle下载java sdk，我用的是Java SE 8u151/ 8u152 版本。安装sdk时会同时安装sdr。\nWindows系统配置:\n推荐安装git bash, 一切按照默认安装就好. 更新系统环境变量: 直接在运行中搜索Environment Variables, 选择编辑系统环境变量, 在弹出的框中选择高级-\u0026gt;环境变量, 在弹出的框中系统变量里面 新建变量: 变量名 = JAVA_HOME, 变量值 = 你的jdk路径,如C:\\Program Files\\Java\\jdk1.8.0_151 编辑Path: 在前面加入%JAVA_HOME%\\bin;%PYTHON_HOME%;(请注意，不能有空格.) OS X系统配置:\n安装Homebrew，一个非常好用的包管理工具。要安装，请在terminal终端输入ruby -e \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026quot;(注意：在此过程中，可能会提示输入密码。当输入密码时，终端上不会显示任何内容，但计算机还是会记录你的密码的。这是一个安全措施, 让其他人在屏幕上看不到你的密码。只需输入您的密码，然后按回车。) 然后，通过输入以下命令来检查brew系统是否正常工作brew doctor. 如果遇到警告，要求下载命令行工具，则需要执行此操作。请参考这个StackOverflow。 安装git：输入brew install git 安装并配置好java后，测试是否成功: 随便在你喜欢的文件夹里新建一个java文件HelloWorld.java\npublic class HelloWorld { public static void main(String[] args) { System.out.println(\u0026#34;Hello world!\u0026#34;); } } 你可以选择用sublime来快速新建文件, 直接在你选择的文件里右键 git bash, 在git bash 里面键入subl HelloWorld.java, 还自动启动sublime并新建一个空白的HelloWorld.java文件, 把上面的代码复制进去并保存即可. (若出现类似提示: 找不到subl command, 解决办法请参考博文在Gitbash中直接启动sublime或atom等编辑器以打开或新建文件 ) 开始真正的测试。直接在之前打开的git bash中输入:\nls, 会看到HelloWorld.java这个文件, ls会列出这个目录中的文件/文件夹 javac HelloWorld.java, 理论上这一步不会有任何输出，有的话可能是设置有问题。现在，如果你继续ls，会看到多了一个HelloWorld.class文件， 这是javac创建的。 java HelloWorld (注意没有.java), 会看到输出Hello World, 表明你的Java设置没有问题 ","permalink":"https://congchan.github.io/posts/java-01-%E5%AE%89%E8%A3%85/","summary":"\u003ch2 id=\"hello-world\"\u003eHello World\u003c/h2\u003e\n\u003cp\u003e参考了伯克利 Josh Hug 的 \u003ca href=\"datastructur.es/sp17/\"\u003ecs61b spring 2017\u003c/a\u003e 和 \u003ca href=\"http://sp18.datastructur.es\"\u003ecs61b spring 2018\u003c/a\u003e. Lab, homework 和 project 代码实现参考 \u003ca href=\"https://github.com/ShootingSpace/cs61b-data-structures\"\u003ehttps://github.com/ShootingSpace/cs61b-data-structures\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"java安装与配置\"\u003eJava安装与配置\u003c/h2\u003e\n\u003cp\u003e安装Java，前往\u003ca href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html\"\u003eOracle\u003c/a\u003e下载java sdk，我用的是Java SE 8u151/ 8u152 版本。安装sdk时会同时安装sdr。\u003c/p\u003e\n\u003cp\u003eWindows系统配置:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e推荐安装\u003ca href=\"http://git-scm.com/download/\"\u003egit bash\u003c/a\u003e, 一切按照默认安装就好.\u003c/li\u003e\n\u003cli\u003e更新系统环境变量: 直接在\u003ccode\u003e运行\u003c/code\u003e中搜索\u003ccode\u003eEnvironment Variables\u003c/code\u003e, 选择\u003ccode\u003e编辑系统环境变量\u003c/code\u003e, 在弹出的框中选择\u003ccode\u003e高级-\u0026gt;环境变量\u003c/code\u003e, 在弹出的框中\u003ccode\u003e系统变量\u003c/code\u003e里面\n\u003cul\u003e\n\u003cli\u003e新建变量: 变量名 = \u003ccode\u003eJAVA_HOME\u003c/code\u003e, 变量值 = 你的jdk路径,如\u003ccode\u003eC:\\Program Files\\Java\\jdk1.8.0_151\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e编辑Path: 在前面加入\u003ccode\u003e%JAVA_HOME%\\bin;%PYTHON_HOME%;\u003c/code\u003e(请注意，不能有空格.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOS X系统配置:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e安装Homebrew，一个非常好用的包管理工具。要安装，请在terminal终端输入\u003ccode\u003eruby -e \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026quot;\u003c/code\u003e(注意：在此过程中，可能会提示输入密码。当输入密码时，终端上不会显示任何内容，但计算机还是会记录你的密码的。这是一个安全措施, 让其他人在屏幕上看不到你的密码。只需输入您的密码，然后按回车。)\u003c/li\u003e\n\u003cli\u003e然后，通过输入以下命令来检查brew系统是否正常工作\u003ccode\u003ebrew doctor\u003c/code\u003e. 如果遇到警告，要求下载命令行工具，则需要执行此操作。请参考这个\u003ca href=\"http://stackoverflow.com/questions/9329243/xcode-4-4-and-later-install-%20%20command-line-tools\"\u003eStackOverflow\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e安装git：输入\u003ccode\u003ebrew install git\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e安装并配置好java后，测试是否成功:\n随便在你喜欢的文件夹里新建一个java文件\u003ccode\u003eHelloWorld.java\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-java\" data-lang=\"java\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eHelloWorld\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"kd\"\u003epublic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kd\"\u003estatic\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003eargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"n\"\u003eSystem\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;Hello world!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e你可以选择用sublime来快速新建文件, 直接在你选择的文件里右键 git bash, 在git bash 里面键入\u003ccode\u003esubl HelloWorld.java\u003c/code\u003e, 还自动启动sublime并新建一个空白的\u003ccode\u003eHelloWorld.java\u003c/code\u003e文件, 把上面的代码复制进去并保存即可. (若出现类似提示: 找不到subl command, 解决办法请参考博文\u003ca href=\"/Launch-editor-in-Gitbash\"\u003e在Gitbash中直接启动sublime或atom等编辑器以打开或新建文件\u003c/a\u003e )\n开始真正的测试。直接在之前打开的git bash中输入:\u003c/p\u003e","title":"Java 01 | 安装"},{"content":"Cache for image\n","permalink":"https://congchan.github.io/images/readme/","summary":"\u003cp\u003eCache for image\u003c/p\u003e","title":""}]