<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Large Language Models on Cong&#39;s Log</title>
    <link>https://congchan.github.io/tags/large-language-models/</link>
    <description>Recent content in Large Language Models on Cong&#39;s Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Sat, 25 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://congchan.github.io/tags/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepSeek-R1</title>
      <link>https://congchan.github.io/posts/deepseek-r1/</link>
      <pubDate>Sat, 25 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/deepseek-r1/</guid>
      <description>&lt;p&gt;DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, &lt;a href=&#34;https://doi.org/10.48550/arXiv.2501.12948&#34;&gt;https://doi.org/10.48550/arXiv.2501.12948&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;incentivizing-reasoning-capability-in-llms-via-reinforcement-learning&#34;&gt;Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/h3&gt;
&lt;p&gt;Large language models (LLMs) have made remarkable strides in mimicking human-like cognition, but their ability to reason through complex problems—from math proofs to coding challenges—remains a frontier. In a recent breakthrough, DeepSeek-AI introduces &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, a family of reasoning-focused models that leverages reinforcement learning (RL) to unlock advanced reasoning capabilities, without relying on traditional supervised fine-tuning (SFT) as a crutch. The paper &lt;em&gt;&amp;ldquo;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&amp;rdquo;&lt;/em&gt; unveils a paradigm shift in how we train LLMs to think critically, with implications for both research and real-world applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CoT on BBH - Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them</title>
      <link>https://congchan.github.io/posts/cot-on-bbh-challenging-big-bench-tasks-and-whether-chain-of-thought-can-solve-them/</link>
      <pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/cot-on-bbh-challenging-big-bench-tasks-and-whether-chain-of-thought-can-solve-them/</guid>
      <description>&lt;p&gt;CoT on BBH：M. Suzgun et al., ‘Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them’. arXiv, Oct. 17, 2022. Available: &lt;a href=&#34;http://arxiv.org/abs/2210.09261&#34;&gt;http://arxiv.org/abs/2210.09261&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;p&gt;Applying chain-of-thought (CoT) prompting to BIG-Bench Hard tasks&lt;/p&gt;
&lt;p&gt;Evaluate few-shot performance via standard “answer-only” prompting and &lt;strong&gt;chain-of-thought prompting&lt;/strong&gt; on BIG-Bench Hard Benchmark&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://congchan.github.io/images/papers/paper17.png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;resultsanalysisfindings&#34;&gt;Results/Analysis/Findings&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Benchmark: &lt;strong&gt;BIG-Bench Hard (BBH)&lt;/strong&gt;. These are the task for which prior language model evaluations did not outperform the average human-rater. many tasks in BBH require multi-step reasoning&lt;/p&gt;</description>
    </item>
    <item>
      <title>Efficient Training of Language Models to Fill in the Middle</title>
      <link>https://congchan.github.io/posts/efficient-training-of-language-models-to-fill-in-the-middle/</link>
      <pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/efficient-training-of-language-models-to-fill-in-the-middle/</guid>
      <description>&lt;p&gt;Bavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, &lt;a href=&#34;http://arxiv.org/abs/2207.14255&#34;&gt;http://arxiv.org/abs/2207.14255&lt;/a&gt;.
data: &lt;a href=&#34;https://www.github.com/openai/human-eval-infilling&#34;&gt;https://www.github.com/openai/human-eval-infilling&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;tldr&#34;&gt;TL:DR&lt;/h1&gt;
&lt;p&gt;Autoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
