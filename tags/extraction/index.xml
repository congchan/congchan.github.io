<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Extraction on Cong&#39;s Log</title>
    <link>https://congchan.github.io/tags/extraction/</link>
    <description>Recent content in Extraction on Cong&#39;s Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Wed, 21 Apr 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://congchan.github.io/tags/extraction/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Matching the Blanks - Distributional Similarity for Relation Learning</title>
      <link>https://congchan.github.io/posts/matching-the-blanks-distributional-similarity-for-relation-learning/</link>
      <pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/matching-the-blanks-distributional-similarity-for-relation-learning/</guid>
      <description>&lt;p&gt;2019, ACL&lt;/p&gt;
&lt;p&gt;data: KBP37, SemEval 2010 Task 8, TACRED&lt;/p&gt;
&lt;p&gt;task: Entity and Relation Extraction&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;Build task agnostic relation representations solely from entity-linked text.&lt;/p&gt;
&lt;h1 id=&#34;缺陷&#34;&gt;缺陷&lt;/h1&gt;
&lt;p&gt;文章认为网页中, 相同的的实体对一般指代相同的实体关系, 把实体不同的构建为负样本. 这个在单份文件中可能大概率是对的.&lt;/p&gt;
&lt;p&gt;但是实体不完全一直不代表这个两对实体的关系不同. 所以这个作为负样本是本质上映射的是实体识别而不是关系.&lt;/p&gt;
&lt;p&gt;比较好的方式是把实体不同但是关系一样的也考虑进来.&lt;/p&gt;
&lt;h1 id=&#34;方法&#34;&gt;方法&lt;/h1&gt;
&lt;h2 id=&#34;define-relation-statement&#34;&gt;Define Relation Statement&lt;/h2&gt;
&lt;p&gt;We define a relation statement to be a block of text containing two marked entities. From this, we create training data that contains relation statements in which the entities have been replaced with a special [BLANK]&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Frustratingly Easy Approach for Joint Entity and Relation Extraction</title>
      <link>https://congchan.github.io/posts/a-frustratingly-easy-approach-for-joint-entity-and-relation-extraction/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/a-frustratingly-easy-approach-for-joint-entity-and-relation-extraction/</guid>
      <description>&lt;p&gt;2020, NAACL&lt;/p&gt;
&lt;p&gt;data: ACE 04, ACE 05, SciERC&lt;/p&gt;
&lt;p&gt;links: &lt;a href=&#34;https://github.com/princeton-nlp/PURE&#34;&gt;https://github.com/princeton-nlp/PURE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;task: Entity and Relation Extraction&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;提出了一种简单但是有效的pipeline方法:builds on two independent pre-trained encoders and merely uses the entity model to provide input features for the relation model.&lt;/p&gt;
&lt;p&gt;实验说明: validate the importance of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;learning distinct contextual representations for entities and relations&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;fusing entity information at the input layer of the relation model,&lt;/li&gt;
&lt;li&gt;and incorporating global context.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从效果上看, 似乎是因为cross sentence的context加成更大&lt;/p&gt;
&lt;h1 id=&#34;方法&#34;&gt;方法&lt;/h1&gt;
&lt;p&gt;Input: a sentence X consisting of n tokens &lt;code&gt;x1, . . . , xn&lt;/code&gt;. Let &lt;code&gt;S = {s1, . . . , sm}&lt;/code&gt; be all the possible spans in &lt;code&gt;X&lt;/code&gt; of up to length &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;START(i)&lt;/code&gt; and &lt;code&gt;END(i)&lt;/code&gt; denote start and end indices of &lt;code&gt;si&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Two are Better than One - Joint Entity and Relation Extraction with Table-Sequence Encoders</title>
      <link>https://congchan.github.io/posts/two-are-better-than-one-joint-entity-and-relation-extraction-with-table-sequence-encoders/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/two-are-better-than-one-joint-entity-and-relation-extraction-with-table-sequence-encoders/</guid>
      <description>&lt;p&gt;2020, EMNLP&lt;/p&gt;
&lt;p&gt;data: ACE 04, ACE 05, ADE, CoNLL04&lt;/p&gt;
&lt;p&gt;links: &lt;a href=&#34;https://github.com/LorrinWWW/two-are-better-than-one&#34;&gt;https://github.com/LorrinWWW/two-are-better-than-one&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;task: Entity and Relation Extraction&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;In this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process.&lt;/p&gt;
&lt;p&gt;这篇ACL 2020文章认为, 之前的Joint learning方法侧重于learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
