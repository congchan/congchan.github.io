<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DDDQN on Cong&#39;s Log</title>
    <link>https://congchan.github.io/tags/dddqn/</link>
    <description>Recent content in DDDQN on Cong&#39;s Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Tue, 09 Mar 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://congchan.github.io/tags/dddqn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DQN, Double DQN, Dueling DoubleQN, Rainbow DQN</title>
      <link>https://congchan.github.io/posts/dqn-double-dqn-dueling-doubleqn-rainbow-dqn/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/dqn-double-dqn-dueling-doubleqn-rainbow-dqn/</guid>
      <description>&lt;p&gt;深度强化学习DQN和Natural DQN, Double DQN, Dueling DoubleQN, Rainbow DQN 的演变和必看论文.&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h1 id=&#34;dqn的overestimate&#34;&gt;DQN的Overestimate&lt;/h1&gt;
&lt;p&gt;DQN 基于 Q-learning, Q-Learning 中有 Qmax, Qmax 会导致 Q现实 当中的过估计 (overestimate). 而 Double DQN 就是用来解决过估计的. 在实际问题中, 如果你输出你的 DQN 的 Q 值, 可能就会发现, Q 值都超级大. 这就是出现了 overestimate.&lt;/p&gt;
&lt;p&gt;DQN 的神经网络部分可以看成一个 最新的神经网络 + 老神经网络, 他们有相同的结构, 但内部的参数更新却有时差. Q现实 部分是这样的:&lt;/p&gt;
$$Y_t^\text{DQN} \equiv R_{t+1} + \gamma \max_a Q(S_{t+1}, a; \theta_t^-)$$&lt;p&gt;&lt;strong&gt;过估计&lt;/strong&gt; (overestimate) 是指对一系列数先求最大值再求平均，通常比先求平均再求最大值要大（或相等，数学表达为：&lt;/p&gt;
$$E(\max(X_1, X_2, ...)) \ge \max(E(X_1), E(X_2), ...)$$&lt;p&gt;一般来说Q-learning方法导致overestimation的原因归结于其更新过程，其表达为：&lt;/p&gt;
$$Q_{t+1} (s_t, a_t) = Q_t (s_t, a_t) + a_t(s_t, a_t)(r_t + \gamma \max a Q_t(s_{t+1}, a) - Q_t(s_t, a_t))$$&lt;p&gt;而更新最优化过程如下&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
