<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Survey on Cong&#39;s Log</title>
    <link>https://congchan.github.io/tags/survey/</link>
    <description>Recent content in Survey on Cong&#39;s Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Sat, 19 Jun 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://congchan.github.io/tags/survey/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Survey - Pre-Trained Models - Past, Present and Future</title>
      <link>https://congchan.github.io/posts/survey-pre-trained-models-past-present-and-future/</link>
      <pubDate>Sat, 19 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/survey-pre-trained-models-past-present-and-future/</guid>
      <description>&lt;p&gt;Links: &lt;a href=&#34;https://arxiv.org/abs/2106.07139&#34;&gt;https://arxiv.org/abs/2106.07139&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最新出炉的 Pre-Trained Models 综述速览。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;先确定综述中的一些名词的定义&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的&lt;/li&gt;
&lt;li&gt;Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。
&lt;ul&gt;
&lt;li&gt;一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题&lt;/li&gt;
&lt;li&gt;而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;2-background-脉络图谱&#34;&gt;2 Background 脉络图谱&lt;/h1&gt;
&lt;p&gt;Pre-training 可分为两大类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2.1 &lt;strong&gt;Transfer Learning&lt;/strong&gt; and &lt;strong&gt;Supervised&lt;/strong&gt; Pre-Training
&lt;ul&gt;
&lt;li&gt;此类可进一步细分为 feature transfer 和 parameter transfer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2.2 &lt;strong&gt;Self-Supervised Learning&lt;/strong&gt; and Self-Supervised Pre-Training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://congchan.github.io/images/papers/paper11.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Transfer learning 可细分为四个子类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007),&lt;/li&gt;
&lt;li&gt;transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006),&lt;/li&gt;
&lt;li&gt;self-taught learning (Raina et al., 2007; Dai et al., 2008)&lt;/li&gt;
&lt;li&gt;unsupervised transfer learning (Wang et al., 2008).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
