<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>2025 on Cong&#39;s Log</title>
    <link>https://congchan.github.io/tags/2025/</link>
    <description>Recent content in 2025 on Cong&#39;s Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Sun, 29 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://congchan.github.io/tags/2025/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Awesome Large Language Model (LLM) Post-training - [2025 Update]</title>
      <link>https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/</link>
      <pubDate>Fri, 30 May 2025 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/</guid>
      <description>&lt;p&gt;In the race to build truly helpful AI assistants, we&amp;rsquo;ve discovered a fundamental truth: raw intelligence isn&amp;rsquo;t enough. A model that masters calculus but can&amp;rsquo;t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.&lt;/p&gt;
&lt;p&gt;This is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, &lt;strong&gt;Reinforcement Learning from Human Feedback (RLHF)&lt;/strong&gt; reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF&amp;rsquo;s complexity was showing:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multi-token Prediction</title>
      <link>https://congchan.github.io/posts/multi-token-prediction/</link>
      <pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/multi-token-prediction/</guid>
      <description>&lt;h2 id=&#34;multi-token-prediction-vs-next-token-prediction&#34;&gt;Multi-token prediction vs Next-token prediction&lt;/h2&gt;
&lt;p&gt;Next-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \( x_{t+1} \) given the context \( x_{1:t} \) (all tokens up to position \( t \)).&lt;/p&gt;
&lt;p&gt;The cross-entropy loss for next-token prediction is defined as:&lt;br&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling</title>
      <link>https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/</link>
      <pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/</guid>
      <description>&lt;p&gt;Reward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Reading - Inference-Time Scaling for Generalist Reward Modeling</title>
      <link>https://congchan.github.io/posts/paper-reading-inference-time-scaling-for-generalist-reward-modeling/</link>
      <pubDate>Mon, 05 May 2025 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/paper-reading-inference-time-scaling-for-generalist-reward-modeling/</guid>
      <description>&lt;p&gt;Liu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, &lt;a href=&#34;https://doi.org/10.48550/arXiv.2504.02495&#34;&gt;https://doi.org/10.48550/arXiv.2504.02495&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h4&gt;
&lt;p&gt;Reinforcement Learning (RL) has become pivotal in post-training large language models (LLMs), but generating accurate reward signals for diverse domains remains challenging. Existing reward models (RMs) often rely on human-designed rules or verifiable tasks, struggling with generalizability and inference-time scalability. This paper addresses how to improve RM effectiveness through increased inference compute and adaptive learning methods for general queries.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DeepSeek-R1</title>
      <link>https://congchan.github.io/posts/deepseek-r1/</link>
      <pubDate>Sat, 25 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/deepseek-r1/</guid>
      <description>&lt;p&gt;DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, &lt;a href=&#34;https://doi.org/10.48550/arXiv.2501.12948&#34;&gt;https://doi.org/10.48550/arXiv.2501.12948&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;incentivizing-reasoning-capability-in-llms-via-reinforcement-learning&#34;&gt;Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/h3&gt;
&lt;p&gt;Large language models (LLMs) have made remarkable strides in mimicking human-like cognition, but their ability to reason through complex problems—from math proofs to coding challenges—remains a frontier. In a recent breakthrough, DeepSeek-AI introduces &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, a family of reasoning-focused models that leverages reinforcement learning (RL) to unlock advanced reasoning capabilities, without relying on traditional supervised fine-tuning (SFT) as a crutch. The paper &lt;em&gt;&amp;ldquo;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&amp;rdquo;&lt;/em&gt; unveils a paradigm shift in how we train LLMs to think critically, with implications for both research and real-world applications.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
