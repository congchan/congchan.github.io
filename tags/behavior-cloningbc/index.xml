<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Behavior Cloning(BC) on Cong&#39;s Log</title>
    <link>https://congchan.github.io/tags/behavior-cloningbc/</link>
    <description>Recent content in Behavior Cloning(BC) on Cong&#39;s Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Sun, 30 Apr 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://congchan.github.io/tags/behavior-cloningbc/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点</title>
      <link>https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/</link>
      <pubDate>Sun, 30 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://congchan.github.io/&#34;&gt;Cong Chen&lt;/a&gt;&lt;br&gt;
University of Edinburgh&lt;/p&gt;
&lt;p&gt;John Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，同时南大的俞扬教授也对BC和RL的对比进行了观点分享&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;归纳的核心观点有三个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Behavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决：
&lt;ul&gt;
&lt;li&gt;核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。&lt;/li&gt;
&lt;li&gt;除此之外，RL还允许负反馈，而 negative feedback is much more powerful&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易&lt;/li&gt;
&lt;li&gt;未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;详细分享---by-john-schulman&#34;&gt;详细分享 - by John Schulman&lt;/h2&gt;
&lt;h3 id=&#34;why-there-is-hallucination&#34;&gt;Why there is Hallucination&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;language-model-hallucination&#34; loading=&#34;lazy&#34; src=&#34;https://congchan.github.io/images/John-Schulman/John-Schulman-0-language-model-hallucination.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Hallucination-and-Behavior-Cloning&#34; loading=&#34;lazy&#34; src=&#34;https://congchan.github.io/images/John-Schulman/John-Schulman-1-Hallucination-and-Behavior-Cloning.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;is-if-a-model-know-something-a-meaningful-question&#34;&gt;Is “if a model know something” a meaningful question?&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;Does-Model-Know-About-Its-Uncertainty&#34; loading=&#34;lazy&#34; src=&#34;https://congchan.github.io/images/John-Schulman/John-Schulman-2-Does-Model-Know-About-Its-Uncertainty.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;rl-is-the-correct-ways&#34;&gt;RL is the correct ways&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;John-Schulman-3&#34; loading=&#34;lazy&#34; src=&#34;https://congchan.github.io/images/John-Schulman/John-Schulman-3.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;John-Schulman-4&#34; loading=&#34;lazy&#34; src=&#34;https://congchan.github.io/images/John-Schulman/John-Schulman-4.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Long form QA (LFQA)  is much difficult that short QA&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
