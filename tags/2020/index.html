<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2020 | Cong's Log</title><meta name=keywords content><meta name=description content="Hi, this is Cong. I’m documenting my learning notes in this blog."><meta name=author content="Cong"><link rel=canonical href=https://congchan.github.io/tags/2020/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://congchan.github.io/tags/2020/index.xml><link rel=alternate hreflang=en href=https://congchan.github.io/tags/2020/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/tags/2020/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="2020"><meta property="og:description" content="Hi, this is Cong. I’m documenting my learning notes in this blog."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="2020"><meta name=twitter:description content="Hi, this is Cong. I’m documenting my learning notes in this blog."></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/tags/>Tags</a></div><h1>2020
<a href=/tags/2020/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>The Curious Case of Neural Text Degeneration</h2></header><div class=entry-content><p>Holtzman, Ari, et al. The Curious Case of Neural Text Degeneration. arXiv:1904.09751, arXiv, 14 Feb. 2020. arXiv.org, http://arxiv.org/abs/1904.09751.
Introduction 从语言模型生成文本（例如生成故事）的最佳解码策略是什么仍然是一个悬而未决的问题。违反直觉的经验观察是，即使使用似然作为训练目标可以为广泛的语言理解任务生成高质量的模型，但基于maximization-based decoding的解码方法（例如beam search）会导致退化（degeneration）——输出文本平淡无奇，不连贯，或陷入重复循环。
文本生成中的decoding strategy主要可以分为两大类：
Argmax Decoding: 主要包括beam search, class-factored softmax等 Stochastic Decoding: 主要包括temperature sampling, top-k sampling等 为了解决这个问题，提出了 Nucleus Sampling（Top-p Sampling），这是一种简单但有效的方法，可以从神经语言模型中提取比以前的解码策略质量更高的文本。The key idea is to use the shape of the probability distribution to determine the set of tokens to be sampled from.
Method 通过截断概率分布的不可靠尾部分布、从包含绝大多数概率质量的标记的dynamic nucleus中采样来避免文本退化。
效果/Analysis/Findings 为了正确检查当前基于最大化和随机的解码方法，我们将这些方法中的每一种的生成与人类文本从几个方向（如可能性、多样性和重复）的分布进行了比较。
...</p></div><footer class=entry-footer><span title='2021-12-23 00:00:00 +0000 UTC'>2021-12-23</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to The Curious Case of Neural Text Degeneration" href=https://congchan.github.io/posts/the-curious-case-of-neural-text-degeneration/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Codex - Evaluating Large Language Models Trained on Code</h2></header><div class=entry-content><p>Codex：M. Chen et al., ‘Evaluating Large Language Models Trained on Code’. arXiv, Jul. 14, 2021. Available: http://arxiv.org/abs/2107.03374
Intro Codex, a GPT language model finetuned on publicly available code from GitHub
Task: docstring-conditional code generation
Method Codex: fine-tune GPT3 models containing up to 12B parameters on code to produce Codex.
Codex-S: fine-tune Codex on standalone, correctly implemented functions.
Inference: assemble each HumanEval problem into a prompt consisting of a header, a signature, and a docstring. We use nucleus sampling (Holtzman et al., 2020) with top p = 0.95 for all sampling evaluation in this work
...</p></div><footer class=entry-footer><span title='2021-12-20 00:00:00 +0000 UTC'>2021-12-20</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Codex - Evaluating Large Language Models Trained on Code" href=https://congchan.github.io/posts/codex-evaluating-large-language-models-trained-on-code/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Scaling Laws for Neural Language Models</h2></header><div class=entry-content><p>Kaplan, Jared, et al. ‘Scaling Laws for Neural Language Models’. arXiv:2001.08361 [Cs, Stat], Jan. 2020. arXiv.org, http://arxiv.org/abs/2001.08361.
TL:DR key findings for Transformer language models are are as follows:
Performance depends strongly on scale, weakly on model shape: Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embeddings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section 3) Smooth power laws: Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance must flatten out eventually before reaching zero loss. (Section 3) ...</p></div><footer class=entry-footer><span title='2021-12-19 00:00:00 +0000 UTC'>2021-12-19</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Scaling Laws for Neural Language Models" href=https://congchan.github.io/posts/scaling-laws-for-neural-language-models/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>CorefQA - Coreference resolution as query-based span prediction</h2></header><div class=entry-content><p>2020, ACL
data: CoNLL-2012, GAP
task: Coreference Resolution
通过QA方式处理coreference问题，A query is generated for each candidate mention using its surrounding con- text, and a span prediction module is em- ployed to extract the text spans of the corefer- ences within the document using the generated query.
近期的方法有consider all text spans in a document as potential mentions and learn to find an antecedent for each possible mention. There。这种仅依靠mention的做对比的方法的缺点：
At the task formalization level： 因为当前数据集有很多遗漏的mention， mentions left out at the mention proposal stage can never be recov- ered since the downstream module only operates on the proposed mentions. At the algorithm level：Semantic matching operations be- tween two mentions (and their contexts) are per- formed only at the output layer and are relatively superficial 方法 Speaker information： directly concatenates the speaker’s name with the corresponding utterance.
...</p></div><footer class=entry-footer><span title='2021-05-11 00:00:00 +0000 UTC'>2021-05-11</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to CorefQA - Coreference resolution as query-based span prediction" href=https://congchan.github.io/posts/corefqa-coreference-resolution-as-query-based-span-prediction/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>A Frustratingly Easy Approach for Joint Entity and Relation Extraction</h2></header><div class=entry-content><p>2020, NAACL
data: ACE 04, ACE 05, SciERC
links: https://github.com/princeton-nlp/PURE
task: Entity and Relation Extraction
提出了一种简单但是有效的pipeline方法:builds on two independent pre-trained encoders and merely uses the entity model to provide input features for the relation model.
实验说明: validate the importance of
learning distinct contextual representations for entities and relations, fusing entity information at the input layer of the relation model, and incorporating global context. 从效果上看, 似乎是因为cross sentence的context加成更大
方法 Input: a sentence X consisting of n tokens x1, . . . , xn. Let S = {s1, . . . , sm} be all the possible spans in X of up to length L and START(i) and END(i) denote start and end indices of si.
...</p></div><footer class=entry-footer><span title='2021-04-20 00:00:00 +0000 UTC'>2021-04-20</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to A Frustratingly Easy Approach for Joint Entity and Relation Extraction" href=https://congchan.github.io/posts/a-frustratingly-easy-approach-for-joint-entity-and-relation-extraction/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Two are Better than One - Joint Entity and Relation Extraction with Table-Sequence Encoders</h2></header><div class=entry-content><p>2020, EMNLP
data: ACE 04, ACE 05, ADE, CoNLL04
links: https://github.com/LorrinWWW/two-are-better-than-one.
task: Entity and Relation Extraction
In this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process.
这篇ACL 2020文章认为, 之前的Joint learning方法侧重于learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process.
...</p></div><footer class=entry-footer><span title='2021-03-27 00:00:00 +0000 UTC'>2021-03-27</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Two are Better than One - Joint Entity and Relation Extraction with Table-Sequence Encoders" href=https://congchan.github.io/posts/two-are-better-than-one-joint-entity-and-relation-extraction-with-table-sequence-encoders/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Improving Event Detection via Open-domain Trigger Knowledge</h2></header><div class=entry-content><p>2020, ACL
data: ACE 05
task: Event Detection
Propose a novel Enrichment Knowledge Distillation (EKD) model to efficiently distill external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations.
leverage the wealth of the open-domain trigger knowledge to improve ED propose a novel teacher-student model (EKD) that can learn from both labeled and unlabeled data 缺点 只能对付普遍情况, 即一般性的触发词; 但触发词不是在任何语境下都是触发词.
方法 empower the model with external knowledge called Open-Domain Trigger Knowledge, defined as a prior that specifies which words can trigger events without subject to pre-defined event types and the domain of texts.
...</p></div><footer class=entry-footer><span title='2021-03-25 00:00:00 +0000 UTC'>2021-03-25</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Improving Event Detection via Open-domain Trigger Knowledge" href=https://congchan.github.io/posts/improving-event-detection-via-open-domain-trigger-knowledge/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Cross-media Structured Common Space for Multimedia Event Extraction</h2></header><div class=entry-content><p>2020, ACL Task: MultiMedia Event Extraction
Introduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles
Propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities
...</p></div><footer class=entry-footer><span title='2021-03-24 00:00:00 +0000 UTC'>2021-03-24</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Cross-media Structured Common Space for Multimedia Event Extraction" href=https://congchan.github.io/posts/cross-media-structured-common-space-for-multimedia-event-extraction/></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>