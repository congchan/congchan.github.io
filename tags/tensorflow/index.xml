<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>TensorFlow on Cong&#39;s Log</title>
    <link>https://congchan.github.io/tags/tensorflow/</link>
    <description>Recent content in TensorFlow on Cong&#39;s Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Fri, 20 Jul 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://congchan.github.io/tags/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)</title>
      <link>https://congchan.github.io/posts/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-lstm%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E6%96%AF%E5%9D%A6%E7%A6%8F%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86-squad/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-lstm%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E6%96%AF%E5%9D%A6%E7%A6%8F%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86-squad/</guid>
      <description>&lt;p&gt;本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;注意力机制应用于阅读理解&#34;&gt;注意力机制应用于阅读理解&lt;/h2&gt;
&lt;p&gt;The Standford question and answer dataset &lt;a href=&#34;https://rajpurkar.github.io/SQuAD-explorer/&#34;&gt;(SQuAD)&lt;/a&gt; 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。&lt;/p&gt;
&lt;p&gt;由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。&lt;/p&gt;
&lt;p&gt;但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。&lt;/p&gt;
&lt;p&gt;虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用&lt;a href=&#34;%5Cattention#%E5%85%A8%E5%B1%80%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&#34;&gt;全局注意力机制&lt;/a&gt;中提到的
&lt;/p&gt;
$$
\begin{aligned}
    score_{general}(t&#39; t) &amp;= s^\top_{t&#39;} W_\alpha h_t, \\\
\end{aligned}
$$&lt;p&gt;
就是用一个交互矩阵$W_\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 &lt;strong&gt;Bilinear&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
