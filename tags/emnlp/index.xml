<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>EMNLP on Cong&#39;s Log</title>
    <link>https://congchan.github.io/tags/emnlp/</link>
    <description>Recent content in EMNLP on Cong&#39;s Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en</language>
    <lastBuildDate>Sat, 27 Mar 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://congchan.github.io/tags/emnlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Two are Better than One - Joint Entity and Relation Extraction with Table-Sequence Encoders</title>
      <link>https://congchan.github.io/posts/two-are-better-than-one-joint-entity-and-relation-extraction-with-table-sequence-encoders/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/two-are-better-than-one-joint-entity-and-relation-extraction-with-table-sequence-encoders/</guid>
      <description>&lt;p&gt;2020, EMNLP&lt;/p&gt;
&lt;p&gt;data: ACE 04, ACE 05, ADE, CoNLL04&lt;/p&gt;
&lt;p&gt;links: &lt;a href=&#34;https://github.com/LorrinWWW/two-are-better-than-one&#34;&gt;https://github.com/LorrinWWW/two-are-better-than-one&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;task: Entity and Relation Extraction&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;In this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process.&lt;/p&gt;
&lt;p&gt;这篇ACL 2020文章认为, 之前的Joint learning方法侧重于learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning</title>
      <link>https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/</guid>
      <description>&lt;p&gt;2017, EMNLP&lt;/p&gt;
&lt;p&gt;data: FB15K-237, FB15K&lt;/p&gt;
&lt;p&gt;task: Knowledge Graph Reasoning&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;Use a policy-based agent with continuous states based on knowledge graph embeddings, which &lt;strong&gt;reasons in a KG vector space&lt;/strong&gt; by sampling the most promising relation to extend its path.&lt;/p&gt;
&lt;h1 id=&#34;方法&#34;&gt;方法&lt;/h1&gt;
&lt;p&gt;RL 系统包含两部分，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。&lt;/li&gt;
&lt;li&gt;系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;/images/papers/paper7.png&#34; loading=&#34;lazy&#34; src=&#34;https://congchan.github.io/images/papers/paper7.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;关系推理的强化学习&#34;&gt;关系推理的强化学习&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;行为&lt;/strong&gt; 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;状态&lt;/strong&gt; 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
