<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="0p5a0VOKCc8etsdDimlaZoAC96x8VeV9Ab5HWs5NcVw" />








  <meta name="baidu-site-verification" content="EbMAKHjVzF" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Computer Science & AI" type="application/atom+xml" />






<meta name="description" content="Reference:http://www.inf.ed.ac.uk/teaching/courses/ppls/CMU 15213: Introduction to Computer Systems (ICS)Computer Systems: A Programmer’s PerspectiveA Comprehensive MPI Tutorial ResourceA chapter on M">
<meta property="og:type" content="article">
<meta property="og:title" content="Parallel Programming Language and Systems - Informatics - University of Edinburgh 爱丁堡">
<meta property="og:url" content="http://shukebeta.me/UoE-ppls/index.html">
<meta property="og:site_name" content="Computer Science &amp; AI">
<meta property="og:description" content="Reference:http://www.inf.ed.ac.uk/teaching/courses/ppls/CMU 15213: Introduction to Computer Systems (ICS)Computer Systems: A Programmer’s PerspectiveA Comprehensive MPI Tutorial ResourceA chapter on M">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://shukebeta.me/images/Shared_Memory_Architectures.png">
<meta property="og:image" content="http://shukebeta.me/images/Multicomputer_Architectures.png">
<meta property="og:image" content="http://shukebeta.me/images/symmetric_barrier.jpg">
<meta property="og:image" content="http://shukebeta.me/images/dissemination_barrier.png">
<meta property="og:image" content="http://shukebeta.me/images/Multiple_Producers_Consumers.png">
<meta property="og:image" content="http://shukebeta.me/images/signal_and_continue.png">
<meta property="og:image" content="https://www.tutorialspoint.com/java/images/Thread_Life_Cycle.jpg">
<meta property="og:image" content="http://shukebeta.me/images/SPMD_Compare_Exchange.png">
<meta property="og:image" content="http://shukebeta.me/images/Collective_Operations.jpg">
<meta property="og:image" content="http://shukebeta.me/images/Synchronization_in_MPI.png">
<meta property="og:image" content="http://shukebeta.me/images/Collective_Operations.jpg">
<meta property="og:image" content="http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_allreduce_1.png">
<meta property="og:image" content="http://shukebeta.me/images/1_d_jacobi_MPI.png">
<meta property="og:image" content="https://www.threadingbuildingblocks.org/docs/help/tbb_userguide/Images/image009.jpg">
<meta property="og:image" content="http://shukebeta.me/images/TBB_Scheduler.png">
<meta property="og:updated_time" content="2018-06-29T19:34:08.551Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Parallel Programming Language and Systems - Informatics - University of Edinburgh 爱丁堡">
<meta name="twitter:description" content="Reference:http://www.inf.ed.ac.uk/teaching/courses/ppls/CMU 15213: Introduction to Computer Systems (ICS)Computer Systems: A Programmer’s PerspectiveA Comprehensive MPI Tutorial ResourceA chapter on M">
<meta name="twitter:image" content="http://shukebeta.me/images/Shared_Memory_Architectures.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://shukebeta.me/UoE-ppls/"/>





  <title>Parallel Programming Language and Systems - Informatics - University of Edinburgh 爱丁堡 | Computer Science & AI</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Computer Science & AI</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://shukebeta.me/UoE-ppls/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Computer Science & AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Parallel Programming Language and Systems - Informatics - University of Edinburgh 爱丁堡</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-11T00:00:00+01:00">
                2018-05-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CS/" itemprop="url" rel="index">
                    <span itemprop="name">CS</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/UoE-ppls/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="UoE-ppls/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/UoE-ppls/" class="leancloud_visitors" data-flag-title="Parallel Programming Language and Systems - Informatics - University of Edinburgh 爱丁堡">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words&#58;</span>
                
                <span title="Words">
                  16,406
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Estimated &asymp;</span>
                
                <span title="Estimated">
                  96 min
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Reference:<br><a href="http://www.inf.ed.ac.uk/teaching/courses/ppls/" target="_blank" rel="noopener">http://www.inf.ed.ac.uk/teaching/courses/ppls/</a><br><a href="http://www.cs.cmu.edu/~213/" target="_blank" rel="noopener">CMU 15213: Introduction to Computer Systems (ICS)</a><br><a href="http://csapp.cs.cmu.edu/" target="_blank" rel="noopener">Computer Systems: A Programmer’s Perspective</a><br><a href="http://mpitutorial.com/" target="_blank" rel="noopener">A Comprehensive MPI Tutorial Resource</a><br><a href="http://www.mcs.anl.gov/~itf/dbpp/text/node94.html#SECTION03500000000000000000" target="_blank" rel="noopener">A chapter on MPI from Ian Foster’s online Book Designing and Building Parallel Programs</a><br><a id="more"></a></p>
<h2 id="Introduction-to-parallel-computer-architecture"><a href="#Introduction-to-parallel-computer-architecture" class="headerlink" title="Introduction to parallel computer architecture"></a>Introduction to parallel computer architecture</h2><p>Covering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model.</p>
<p>Bridging the gap between the parallel applications and algorithms which we can design and describe in abstract terms and the parallel computer architectures (and their lowest level programming interfaces) which it is practical to construct.</p>
<p>The ability to express parallelism (a.k.a concurrency) concisely, correctly and efficiently is important in several contexts:<br>• Performance Computing: parallelism is the means by which the execution time of computationally demanding applications can be reduced. In the era of static (or even falling) clock speeds and increasing core count, this class is entering the computing mainstream.<br>• Distributed Computing: when concurrency is inherent in the nature of the system and we have no choice but to express and control it.<br>• Systems Programming: when it is conceptually simpler to think of a system as being composed of concurrent components, even though these will actually be executed by time-sharing a single processor.</p>
<h2 id="Parallel-Architecture"><a href="#Parallel-Architecture" class="headerlink" title="Parallel Architecture"></a>Parallel Architecture</h2><p>Two types (mainstream):</p>
<ul>
<li>Shared Memory architectures: in which all processors can physically address the whole memory, usually with support for cache coherency (for example, a quad or oct core chip, or more expensive machines with tens or hundreds of cores)</li>
<li>Multicomputer architectures: in which processors can only physically address their “own” memory (for example, a networked cluster of PCs), which interact with messages across the network.</li>
</ul>
<p>Increasingly, systems will span both classes (e.g. cluster of manycore, or network-onchip manycores like the Intel SCC), and incorporate other specialized, constrained parallel hardware such as GPUs.</p>
<p>Real parallel machines are complex, with unforseen semantic and performance traps. We need to provide programming tools which simplify things, but without sacrificing too much performance.</p>
<h3 id="Shared-Memory-Architectures"><a href="#Shared-Memory-Architectures" class="headerlink" title="Shared Memory Architectures"></a>Shared Memory Architectures</h3><p>Uniform Memory Access (UMA) architectures have all memory “equidistant” from all CPUs.<br>For NUMA performance varies with data location. NUMA is also confusingly called Distributed Shared Memory as memory is physically distributed but logically shared.<br><img src="/images/Shared_Memory_Architectures.png" alt="" title="image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></p>
<p>Memory consistency challenge: when, and in what order should one processor public updates to the shared memory? Exactly what and when it is permissible for each processor to see is defined by the <strong>Consistency Model</strong>, which is effectively a contract between hardware and software, must be respected by application programmers (and compiler/library writers) to ensure program correctness.</p>
<p>Different consistency models trade off conceptual <strong>simplicity against cost</strong> (time/hardware complexity):</p>
<ul>
<li><strong>Sequential consistency</strong>: every processor “sees” the same sequential interleaving of the basic reads and writes. This is very intuitive, but expensive to implement.</li>
<li><strong>Release consistency</strong>: writes are only guaranteed to be visible after program specified synchronization points (triggered by special machine instructions). This is less intuitive, but allows faster implementations.</li>
</ul>
<p>Shared memory architectures also raise tricky performance issues: The unit of transfer between memory and cache is a cache-line or block, containing several words. <strong>False sharing</strong> occurs when two logically unconnected variables share the same cache-line. Updates to one cause remote copies of the line, including the other variable, to be invalidated.</p>
<h3 id="Multicomputer-architectures"><a href="#Multicomputer-architectures" class="headerlink" title="Multicomputer architectures"></a>Multicomputer architectures</h3><p>Lack of any hardware integration between cache/memory system and the interconnect. Each processor only accesses its <strong>own physical address space</strong>, so no consistency issues. Information is shared by explicit, co-operative message passing<br><img src="/images/Multicomputer_Architectures.png" alt="" title="image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"><br>Performance/correctness issues include the semantics of <strong>synchronization</strong> and constraints on <strong>message ordering</strong>.</p>
<h2 id="Parallel-Applications-and-Algorithms"><a href="#Parallel-Applications-and-Algorithms" class="headerlink" title="Parallel Applications and Algorithms"></a>Parallel Applications and Algorithms</h2><p>Three well-known parallel patterns: <strong>Bag of Tasks, Pipeline and Interacting Peers</strong>.</p>
<p>Here using the <code>co</code>, <code>&lt; &gt;</code>, <code>await</code> notation.</p>
<p>在<code>co oc</code>内的代码, 顺序是任意的.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 这里暂时用 // 表示并行的代码</span><br><span class="line">co</span><br><span class="line">    a=1; // a=2; // a=3; ## all happen at the same time, What is a in the end?</span><br><span class="line">oc</span><br></pre></td></tr></table></figure></p>
<p>To answer the above question, we need to define <strong>Atomic Actions</strong>: Reads and writes of single variables as being atomic. For more than one statements, if they appear to execute as a single indivisible step with no visible intermediate states, they are atomic, must be enclosed in <code>&lt; &gt;</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=0;</span><br><span class="line">co</span><br><span class="line">    a=1; // a=2; // b=a+a; ## what is b?</span><br><span class="line">oc</span><br></pre></td></tr></table></figure></p>
<p>The above code has no <code>&lt; &gt;</code>, each value accessed in an expression is a read. Each assignment is <code>a</code> write. Thus, <code>b</code> could be 0, 1, 2, 3, or 4.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=0;</span><br><span class="line">co</span><br><span class="line">    a=1; // a=2; // &lt;b=a+a;&gt;</span><br><span class="line">oc</span><br></pre></td></tr></table></figure></p>
<p>Now the only outcomes for b are 0, 2 or 4.</p>
<p><strong>Sequential memory consistency (SC)</strong><br>To make agreement on such inconsistency, we define the sequential memory consistency (SC), to be consistent with the following rules:</p>
<ol>
<li>ordering of atomic actions (particularly reads and writes to memory) from any one thread have to occur in normal program order</li>
<li>atomic actions from different threads are interleaved arbitrarily (ie in an unpredictable sequential order, subject only to rule 1)</li>
</ol>
<p><strong>It doesn’t mean that SC programs have to be executed sequentially</strong>!<br>It only means that the results we get must be the same as if the program had been executed in this way.</p>
<p><strong>Await</strong><br>The await notation <code>&lt; await (B) S &gt;</code> allows us to indicate that <code>S</code> must appear to be delayed until <code>B</code> is true, and must be executed within the same atomic action as a successful check of <code>B</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=0; flag=0;</span><br><span class="line">co</span><br><span class="line">&#123;a=25; flag=1;&#125;</span><br><span class="line">//</span><br><span class="line">&lt;await (flag==1) x=a;&gt; ## x = 25</span><br><span class="line">oc</span><br></pre></td></tr></table></figure></p>
<p>However, it is not guaranteed that, an await statement is executed right after its condition becomes true. If other atomic actions make the condition false again, before the await executes, it will have to wait for another chance.</p>
<h3 id="The-Bag-of-Tasks"><a href="#The-Bag-of-Tasks" class="headerlink" title="The Bag-of-Tasks"></a>The Bag-of-Tasks</h3><p>Example: Adaptive Quadrature, compute an approximation to the shaded integral by partitioning until the 梯形 trapezoidal approximation is “good enough”, compared with the sum of its two sub-divided trapezoidals’s area.<br><code>area = quad (a, b, f(a), f(b), (f(a)+f(b))*(b-a)/2);</code><br>The recursive calls to <code>quad</code> do not interfere with each other. So we can parallelize the program by changing the calls to<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 简单地并行</span><br><span class="line">co</span><br><span class="line">    larea = quad(left, mid, f(left), f(mid), larea); //</span><br><span class="line">    rarea = quad(mid, right, f(mid), f(right), rarea);</span><br><span class="line">oc</span><br></pre></td></tr></table></figure></p>
<p>In practice, there is very little work directly involved in each call to <code>quad</code>. The work involved in <strong>creating and scheduling a process or thread is substantial</strong> (much worse than a simple function call), program may be swamped by this overhead.</p>
<p>Using the Bag of Tasks pattern: a <strong>fixed number of worker processes/threads</strong> maintain and process a dynamic collection of homogeneous “tasks”. Execution of a particular task may lead to the <strong>creation</strong> of more task instances.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Bag of Tasks pattern</span><br><span class="line">co [w = 1 to P] &#123;</span><br><span class="line">    while (all tasks not done) &#123;</span><br><span class="line">        get a task;</span><br><span class="line">        execute the task;</span><br><span class="line">        possibly add new tasks to the bag;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>1, Shared bag: contains <code>task(a, b, f(a), f(b), area)</code><br>2, Get a task: remove a record from the bag, either:<br>    • adds its local area approximation to the total<br>    • or creates two more tasks for a better approximation (by adding them to the bag).</p>
<p>Advantage:<br>1, It constraints the number of processes/threads to avoid overhead.<br>2, Useful for independent tasks and to implement recursive parallelism<br>3, <strong>Naturally load-balanced</strong>: each worker will probably complete a different number of tasks, but will do roughly the same amount of work overall.</p>
<p>Bag of Tasks <strong>Implementation</strong>: The challenge is to make accessing the bag much cheaper than creating a new thread. With a shared address space, a simple implementation would make the bag an atomically accessed shared data structure.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">shared int size = 1, idle = 0;</span><br><span class="line">shared double total = 0.0;</span><br><span class="line">bag.insert (a, b, f(a), f(b), approxarea);</span><br><span class="line">co [w = 1 to P] &#123;</span><br><span class="line">    while (true) &#123;</span><br><span class="line">        &lt; idle++; &gt;</span><br><span class="line">        &lt; await ( size &gt; 0 || idle == P )             ## 检测 termination</span><br><span class="line">          if (size &gt; 0) &#123;                             ## get a task</span><br><span class="line">              bag.remove (left, right ...); size--; idle--;</span><br><span class="line">          &#125; else break; &gt;                             ## the work is done</span><br><span class="line">        mid = (left + right)/2; ..etc..               ## compute larea, etc</span><br><span class="line">        if (fabs(larea + rarea - lrarea) &gt; EPSILON) &#123; ## create new tasks</span><br><span class="line">            &lt; bag.insert (left, mid, fleft, fmid, larea);</span><br><span class="line">              bag.insert (mid, right, fmid, fright, rarea);</span><br><span class="line">              size = size + 2; &gt;</span><br><span class="line">        &#125; else &lt; total = total + larea + rarea; &gt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><a href="https://www2.cs.arizona.edu/~greg/mpdbook/lectures/lec09.html" target="_blank" rel="noopener">Detecting termination</a>:<br>不能仅仅因为 bag 空了就认为可以结束了, 因为还可能有还在工作的 workers 未来会产生新的任务. 所以需要让 workers 有能力把自己的工作完成状况告知 bag. When bag is empty AND all tasks are done; All tasks are done when all workers are waiting to get a new task.</p>
<p>If a bag of tasks algorithm has terminated, there are no tasks left. However, the inverse is not true. I.e. no tasks in a bag could mean that one of the workers is still processing a task which can lead to creation of multiple new tasks.<br>To solve this problem, workers could have the ability to notify the master/bag once they finish the current task. As a result, an implementation of bag of tasks can then contain a count of idle and active works to prevent early termination</p>
<p>A more sophisticated implementation (with less contention) might internally have a collection of bags, perhaps one per worker, with task-stealing to distribute the load as necessary.</p>
<p>With <strong>message passing</strong>, a simple scheme might allocate an explicit “farmer” node to maintain the bag. Again, a more sophisticated implementation could distribute the bag and the farmer, with task-stealing and termination checking via messages.</p>
<h3 id="Pipeline-Patterns"><a href="#Pipeline-Patterns" class="headerlink" title="Pipeline Patterns."></a>Pipeline Patterns.</h3><p>Example: <a href="https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes" target="_blank" rel="noopener">The Sieve of Eratosthenes</a> algorithms for finding all prime numbers.</p>
<p>To find all prime numbers in the range 2 to N. The algorithm write down all integers in the range, then repeatedly remove all multiples of the smallest remaining number. Before each removal phase, the new smallest remaining number is guaranteed to be prime.</p>
<p>Notice that, it is not necessarily to wait one Sieve completed then start another. As long as one Sieve stage finds out one candidate number could not be divided exactly by the sieve number, it could generate a new stage with this candidate number as Sieve. And different sieve just remove the multiples of its own Sieve number.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># a message-passing style pipeline pseudocode</span><br><span class="line">main() &#123;                                # the generator</span><br><span class="line">    spawn the first sieve process;</span><br><span class="line">    for (i=2; i&lt;=N; i++) &#123;</span><br><span class="line">        send i to first sieve;</span><br><span class="line">    &#125;</span><br><span class="line">    send -1 to first sieve;             # a &quot;stop&quot; signal</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sieve() &#123;</span><br><span class="line">    int myprime, candidate;</span><br><span class="line">    receive myprime from predecessor and record it;</span><br><span class="line">    do &#123;</span><br><span class="line">        receive candidate from predecessor;</span><br><span class="line">        if (candidate == -1) &#123;send -1 to successor if it exists&#125;</span><br><span class="line">        else if (myprime doesn&apos;t divide candidate exactly) &#123;</span><br><span class="line">            if (no successor yet) spawn successor sieve process;</span><br><span class="line">            send candidate to successor sieve process;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; while (candidate != -1)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>每一个数(2-N)都可能作为筛子, 筛掉能整除这个筛子的其他数，而筛子之间是互相独立的，所以可以以<a href="http://www.informit.com/articles/article.aspx?p=366887&amp;seqNum=8" target="_blank" rel="noopener">流水线模式 pipeline patterns</a>来并行操作，动态生成筛子。最开始最小的数字<code>2</code>会成为筛子。筛子可以理解为不同的工序，其余数字从小到大逐一通过这些工序加工（在 Sieve of Eratosthenes 问题中变为筛选排查），无法被筛子整除的数字会被传递到下个筛子（如果没有下一个筛子，则以这个数字创建新的筛子），这样保证生成的筛子就都是素数了。虽然工序是按顺序过的，但是所有工序可以同时对不同的产品（数字）开工，从而达到并行目的。</p>
<p>For pipeline patterns, the potential concurrency can be exploited by assigning each operation (stage of the pipeline) to a different worker and having them work simultaneously, with the data elements passing from one worker to the next as operations are completed. Despite the dependencies (order constraints) of the processing steps, the pipeline threads can work in parallel by applying their processing step to different data (products).</p>
<p>Think of pipeline patterns as the factory assembly line. We need to pick out prime number from a range of numbers N, each number is passed into a sequence of stages, each stages checks a pass in number based on the stages’s Sieve. The numbers that finally pass all stages without being removed is a prime number.</p>
<p>Pipelines are composed of a sequence of threads, in which each thread’s input is the previous thread’s output, (<strong>Producer-Consumer relationships</strong>).</p>
<p>The advantages of pipeline patterns is that construction of pipeline stages is dynamic and data-dependent.</p>
<p>To allow production and consumption to be loosely synchronized, we will need some buffering in the system.</p>
<p>The programming challenges are to ensure that no producer overwrites a buffer entry before a consumer has used it, and that no consumer tries to consume an entry which doesn’t really exist (or re-use an already consumed entry)</p>
<h3 id="Interacting-Peers-Pattern"><a href="#Interacting-Peers-Pattern" class="headerlink" title="Interacting Peers Pattern"></a>Interacting Peers Pattern</h3><p>Models of physical phenomena are often expressed as a system of partial differential equations. These can be approximately solved by “finite difference methods” which involve iteration on a matrix of points, in an interacting peers pattern. The “compute” step usually involves only a small number of neighbouring points. The termination test looks for convergence.</p>
<p>We could use a duplicate grid and <strong>barriers</strong> to enforce correct synchronization between iterations:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">shared real grid[n+2, n+2], newgrid[n+2, n+2];</span><br><span class="line">shared bool converged; local real diff;</span><br><span class="line">co [i = 1 to n, j = 1 to n] &#123;</span><br><span class="line">    initialise grid;</span><br><span class="line">    do &#123;</span><br><span class="line">        barrier();                                  ## before resetting test</span><br><span class="line">        converged = true;                           ## provisionally</span><br><span class="line">        newgrid[i,j] = (grid[i-1,j] + grid[i+1,j] +</span><br><span class="line">            grid[i,j-1] + grid[i,j+1])/4;           ## compute new value</span><br><span class="line">        diff = abs (newgrid[i,j] - grid[i,j]);      ## compute local change</span><br><span class="line">        barrier();                                  ## before converged update</span><br><span class="line">        if (diff &gt; EPSILON) converged = false;      ## any one will do</span><br><span class="line">        grid[i,j] = newgrid[i,j];                   ## copy back to real grid</span><br><span class="line">        barrier();                                  ## before global check</span><br><span class="line">    &#125; while (not converged);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>A <code>barrier()</code> in ppls makes any thread that arrive here has to wait all the other threads arriving here.</p>
<p>以方腔热对流的模拟计算模型为例，每个网格节点$(i,j)_{t+1}$ 的更新依赖于上一个迭代时间点的$(i,j)_t$以及其临近几个点的值，创建最多跟网格点数量一样的threads，然后并行地计算网格点的新值，更新的值用一个buffer层来缓存，用<code>barrier()</code>来保证所有网格点的更新值都计算完毕，再检查收敛情况，再用一个<code>barrier()</code>保证所有buffer层的值都更新到原网格上，再决定是否进行下一次计算。</p>
<p>Single Program Multiple Data (SPMD): A programming style, all processes execute more or less the same code, but on distinct partitions of the data.</p>
<h3 id="Other-Patterns"><a href="#Other-Patterns" class="headerlink" title="Other Patterns"></a>Other Patterns</h3><p>Other candidate patterns include MapReduce (championed by Google), Scan, Divide &amp; Conquer, Farm as well as application domain specific operations.</p>
<h2 id="Shared-Variable-Programming"><a href="#Shared-Variable-Programming" class="headerlink" title="Shared Variable Programming"></a>Shared Variable Programming</h2><p>In the <strong>shared-memory programming model</strong>, tasks share a common address space, which they read and write asynchronously. An advantage of this model from the programmer’s point is that the notion of data “ownership” is lacking, so there is no need to specify explicitly the communication of data between tasks. Program development can often be simplified.</p>
<p>There are two fundamentally different synchronization in shared variable programming. <strong>Mutual Exclusion</strong> and <strong>Condition Synchronization</strong>.</p>
<h3 id="Mutual-Exclusion"><a href="#Mutual-Exclusion" class="headerlink" title="Mutual Exclusion"></a>Mutual Exclusion</h3><p>Atomic actions, at most one thread is executing the critical section at a time. Prevent two or more threads from being active concurrently for some period, because their actions may interfere incorrectly. For example, we might require updates to a shared counter (e.g., count++) to execute with mutual exclusion.</p>
<h4 id="Critical-Sections-problem"><a href="#Critical-Sections-problem" class="headerlink" title="Critical Sections problem"></a>Critical Sections problem</h4><p>A simple pattern of mutual exclusion occurs in the <strong>critical section problem</strong> - when n threads execute code of the following form, in which it is essential that at most one thread is executing statements in the critical section at a time (because of potentially unsafe access to shared variables)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">co [i = 1 to n] &#123;</span><br><span class="line">    while (something) &#123;</span><br><span class="line">        lock(l);      #entry section</span><br><span class="line">        critical section;</span><br><span class="line">        unlock(l);    #exit section</span><br><span class="line">        non-critical section;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Design code to execute before (<strong>entry protocol</strong>) and after (<strong>exit protocol</strong>) the critical section to make the critical section <strong>atomic</strong>. If one thread lock the critical section, no one(thread) else could lock it or unlock it anymore, until the thread unlock it.</p>
<p>Important properties:</p>
<ol>
<li><strong>Mutual exclusion</strong>: When a thread is executing in its critical section, no other threads can be executing in their critical sections.</li>
<li><strong>Absence of Deadlock</strong> (or Livelock): If two or more threads are trying to enter the critical section, <strong>at least one succeeds</strong>.<blockquote>
<p>A deadlock is a state in which each member of a group is waiting for some other member to take action, such as sending a message or more commonly releasing a lock, so that neither of them take action.<br>类似两个人相遇互不相让, 没人肯挪动.<br>Livelock is a condition that takes place when two or more programs change their state continuously, with neither program making progress.<br>类似两个人相遇同时往相同方向避让.</p>
</blockquote>
</li>
<li><strong>Absence of Unnecessary Delay</strong>: If a thread is trying to enter its critical section and the other threads are executing their non-critical sections, or have terminated, the first thread is not prevented from entering its critical section.</li>
<li><strong>Eventual Entry</strong> (No Starvation): A thread that is attempting to enter its critical section will eventually succeed. May not matter in some “performance parallel” programs - as long as we are making progress elsewhere.</li>
</ol>
<p>Simple implementation of each lock with a <strong>shared boolean variable</strong>: if <code>false</code>, then one locking thread can set it to <code>true</code> and be allowed to proceed. Other attempted locks must be forced to wait.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># model assumes that the l = false;</span><br><span class="line"># write is already atomic</span><br><span class="line"># This might fail if the model is more relaxed than SC.</span><br><span class="line">lock_t l = false;</span><br><span class="line">co [i = 1 to n] &#123;</span><br><span class="line">    while (something) &#123;</span><br><span class="line">        &lt; await (!l) l = true; &gt; # guarantee the others waiting</span><br><span class="line">        critical section;</span><br><span class="line">        l = false; # unlock the lock, open the critical section</span><br><span class="line">        non-critical section;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>To implement the <code>&lt; await (!l) l = true; &gt;</code>, we rely on some <strong>simpler atomic primitive</strong>, implemented with hardware support. There are many possibilities, including “Fetch-and-Add”, “Test-and-Set” and the “Load-Linked, Store-Conditional” pairing.</p>
<h5 id="Test-and-Set-TS-instruction"><a href="#Test-and-Set-TS-instruction" class="headerlink" title="Test-and-Set (TS) instruction"></a>Test-and-Set (TS) instruction</h5><p>Behaving like a call-by-reference function, so that the variable passed in is read from and written to, but in reality it is a single machine instruction. The key feature is that this happens (or at least, appears to happen) atomically.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># A Test-and-Set (TS) instructionW</span><br><span class="line">bool TS (bool v) &#123;</span><br><span class="line">    &lt; bool initial = v;</span><br><span class="line">    v = true;</span><br><span class="line">    return initial; &gt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">lock_t l = false;</span><br><span class="line">co [i = 1 to n] &#123;</span><br><span class="line">    while (something) &#123;</span><br><span class="line">        while (TS(l)) ;  ## spin lock</span><br><span class="line">        critical section;</span><br><span class="line">        l = false;</span><br><span class="line">        non-critical section;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>This is called <strong>spin lock</strong>,</p>
<p>Simple spin locks don’t make good use of the cache (those spinning Test-And-Sets play havoc with contention and coherence performance). A pragmatically better spin locks is known as <strong>Test-and-Test-and-Set</strong> - mainly spinning on a read rather than a read-write function.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    <span class="keyword">while</span> (something) &#123;</span><br><span class="line">        <span class="keyword">while</span> (l || TS(l)); <span class="comment">/* only TS() if l was false*/</span></span><br><span class="line">        critical section;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>Simply read <code>l</code> until there is a chance that a Test-and-Set might succeed.</p>
<p><strong>Spin lock</strong> guarantees mutual exclusion, absence of deadlock and absence of delay, but does <strong>not guarantee eventual entry</strong>.</p>
<h5 id="Lamport’s-Bakery-Algorithm"><a href="#Lamport’s-Bakery-Algorithm" class="headerlink" title="Lamport’s Bakery Algorithm"></a><a href="https://en.wikipedia.org/wiki/Lamport%27s_bakery_algorithm" target="_blank" rel="noopener">Lamport’s Bakery Algorithm</a></h5><p>Implement critical sections using only simple atomic read and simple atomic write instructions (i.e. no need for atomic read-modify-write).</p>
<p>采用商店结账排队机制，顾客就是一个个threads，根据排队码，越小的优先级越高（0 除外，0 代表没有结账需求），最小的可以进入critical section。</p>
<p>The challenge is entry protocal, if a thread intends to access the critical section:</p>
<ol>
<li>排队取号：It sets its turn <code>turn[i] = max(turn[:])+1</code> (Threads not at or intend to access the critical section have a turn of 0)</li>
<li>等待叫号：This thread waits until its turn comes up (until it has the smallest turn).</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">int turn[n] = [0, 0, ... 0];</span><br><span class="line">co [i=1 to n] &#123;</span><br><span class="line">    while (true) &#123;</span><br><span class="line">        turn[i] = max (turn[1..n]) + 1;</span><br><span class="line">        for (j = 1 to n except i) &#123;</span><br><span class="line">            while ((turn[j]!=0 and (turn[i] &gt; (turn[j])) skip;</span><br><span class="line">        &#125;</span><br><span class="line">        critical section;</span><br><span class="line">        turn[i] = 0;</span><br><span class="line">        noncritical section;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为<code>max (turn[1..n]) + 1</code>不是atomic的, 所以会出现问题.</p>
<p>问题一: if turn setting is not atomic then two (or more) threads may claim the same turn.</p>
<blockquote>
<p>两个threads在取号阶段<code>turn[i] = max(turn[:])+1</code>出现并发，两个都先<code>max</code>, 之后再<code>+1</code>.</p>
</blockquote>
<p>问题二: there is possibility that a thread can claim a lower turn than another thread which enters the critical section before it!</p>
<blockquote>
<p>两个threads在取号阶段<code>turn[i] = max(turn[:])+1</code>出现并发, 并且在两个threads分别进行<code>max</code>的间隙, 刚好在CS中的thread完成并退出CS，导致两个thread看到的<code>max</code>值不一样了. 前者比后者看到的大, 但前者却因为更早进行<code>+1</code>操作而提前进入了CS.</p>
</blockquote>
<p>举例：假如同时有三个thread A B C, A 已经在CS中(turn(A)&gt;0)：</p>
<ol>
<li>B 先运行max比较(<code>max = turn(A)</code>),</li>
<li>C 在 A 退出后(<code>turn(A) = 0</code>)才进行比较(<code>max = 0</code>),</li>
<li>B 先进行<code>+1</code>操作(<code>turn(B) = turn(A)+1 &gt; 1</code>),</li>
<li>B 进行比较后允许进入CS (<strong>此时turn(C)还是0</strong>, 0是被忽略的);</li>
<li>之后C才 <code>+1</code>(<code>turn(C) = 0 + 1 = 1</code>);</li>
<li>这样导致B的值虽然比C大, 但B还是比C先进入CS; 之后因为 C 的 turn 比较小， 所以 C 也跟着进入 CS。</li>
</ol>
<p>问题一解决方案 - 使用线程ID（绝不相同）做二次区分, 在相同 turn 的情况下，具有较低ID的 thread 有限。</p>
<p>问题二解决方案 - 在<code>max (turn[1..n]) + 1</code>之前先<code>turn[i] = 1;</code>.<br>• 这样，任何 threads 想取号都要先标记为 1<br>• 标记后，才有资格跟其他 thread 比较<br>• 以<code>max+1</code>作为号码进入队列，这样任何的可能的 turn 值都必定大于 1<br>• B 无法提前进入CS (<strong>此时turn(C)不再是被忽略的0, 而是最小正整数1</strong>).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#  (x, a) &gt; (y,b) means (x&gt;y) || (x==y &amp;&amp; a&gt;b).</span><br><span class="line">while (true) &#123;</span><br><span class="line">    turn[i] = 1; turn[i] = max (turn[1..n]) + 1;</span><br><span class="line">    for (j = 1 to n except i) &#123;</span><br><span class="line">        while ((turn[j]!=0 and (turn[i], i) &gt; (turn[j], j)) skip;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Lamport’s algorithm has the strong property of guaranteeing eventual entry (unlike our spin lock versions). The algorithm is too inefficient to be practical if spin-locks are available.</p>
<h3 id="Condition-Synchronization"><a href="#Condition-Synchronization" class="headerlink" title="Condition Synchronization"></a>Condition Synchronization</h3><p>Delay an action until some condition (on the shared variables such as in producer-consumer, or with respect to the progress of other threads such as in a <strong>Barrier</strong>) becomes true.</p>
<h4 id="Barrier-synchronization"><a href="#Barrier-synchronization" class="headerlink" title="Barrier synchronization"></a>Barrier synchronization</h4><p><strong>Barrier synchronization</strong> is a particular pattern of condition synchronization, a kind of computation-wide waiting:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">co [i = 1 to n] &#123;</span><br><span class="line">    while (something) &#123;</span><br><span class="line">        do some work;</span><br><span class="line">        wait for all n workers to get here;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>A <strong>Counter Barriers</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;count = count + 1;&gt;</span><br><span class="line">&lt;await (count == n);&gt;</span><br></pre></td></tr></table></figure></p>
<p>is fine as a single-use barrier, but things get more complex if (as is more likely) we need the barrier to be <strong>reusable</strong>.</p>
<p>改良为<code>&lt;await (count == n); count = 0;&gt;</code>也不行: an inter-iteration race, 假如<code>count == n</code>, 那么n个threads都完成了前面的statements并准备执行<code>await</code>, 但其中任何一个 thread 先执行完整个代码都使<code>count = 0</code>,这样剩余的threads就无法通过await条件了.</p>
<h5 id="Sense-Reversing-Barrier"><a href="#Sense-Reversing-Barrier" class="headerlink" title="Sense Reversing Barrier"></a>Sense Reversing Barrier</h5><p>A shared variable <code>sense</code> is <strong>flipped after each use</strong> of the barrier to indicate that all threads may proceed. 关键每个 thread 都有自己的 private variable <code>mySense</code> 和 while spin lock。解决了前面的死锁问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">shared int count = 0; shared boolean sense = false;</span><br><span class="line">co [i = 1 to n] &#123;</span><br><span class="line">    private boolean mySense = !sense; ## one per thread</span><br><span class="line">    while (something) &#123;</span><br><span class="line">        do some work;</span><br><span class="line">        &lt; count = count + 1;</span><br><span class="line">          if (count == n) &#123; count = 0; sense = mySense; &#125;   ## flip sense</span><br><span class="line">        &gt;</span><br><span class="line">        while (sense != mySense);                           ## wait or pass</span><br><span class="line">        mySense = !mySense;                                 ## flip mySense</span><br><span class="line">        // 或者使用 &lt; await (sense==mySense) mySense = !sense;&gt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>所有thread的local variable <code>mySense</code>开始都被赋值为<code>!sense</code>(<code>true</code>)，前面n-1个thread都得在内循环<code>while</code>那里等着；直到最后一个thread完成工作后, <code>if</code>条件才满足, <code>count</code>重置为<code>0</code>, <strong>反转</strong><code>sense</code>(被赋值为<code>mySense</code>也即是<code>true</code>), 之后所有threads才能结束内部<code>while</code>循环，接着再次<strong>反转</strong><code>sense</code>(被赋值为<code>!mySense</code>也即是<code>false</code>), 然后进行下一轮大循环，借此达到重复利用barrier的目的. <code>sense</code>初始值是什么无所谓, 反转才是关键.</p>
<p>缺点：$O(n)$效率，count次数（同步次数）正比于thread数量。</p>
<h5 id="Symmetric-Barriers"><a href="#Symmetric-Barriers" class="headerlink" title="Symmetric Barriers"></a>Symmetric Barriers</h5><p>Symmetric barriers are designed to avoid the bottleneck at the counter.<br>通过 pair-threads barriers 多轮同步来构建一个完整的 n-threads barriers，让所有threads都知道大家已经完成任务。总共是$\log_2n$ 轮同步。每个thread在完成必要工作后, 开始进入下面的pairwise同步环节，自己(myid)的初始arrive状态为0:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># arrive[i] == 1 means arrive barrier</span><br><span class="line"># there will be log_2 #threads stages,</span><br><span class="line"># 每个stage代表一次pairwise同步</span><br><span class="line">for [s = 0 to stages-1] &#123;</span><br><span class="line">    &lt;await (arrive[myid] == 0);&gt;   # 1</span><br><span class="line">    arrive[myid] = 1;              # 2</span><br><span class="line">    work out who my friend is at stage s;</span><br><span class="line">    &lt;await (arrive[friend] == 1);&gt; # 3</span><br><span class="line">    arrive[friend] = 0;            # 4</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这样保证了，每个thread需要先把自己的arrive状态标记为1(#1，#2)，才可以去看同伴的状态（#3），假如同伴也是1，那么表明自己这一组是都到达了barrier状态（大家都是1），那么就会把对方的状态初始化为0 （#4），进入下一阶段，更换同伴，继续同步比较。<br><img src="/images/symmetric_barrier.jpg" alt="" title="Butterfly barrier for 8 processe. Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"><br>When used as <strong>a step within a multistage symmetric barrier</strong>, 会出现问题：假如有四个thread，那么就会有两个stages：第一次是1和2同步，3和4同步。2一直没到barrier，1一直卡在#3。而3和4 同步完后开始检查1的状况，发现<code>arrive[1] = 1</code>，就运行Lines (3) and (4), 结果1就被初始化了，而2还没是没到barrier。</p>
<p>解决办法是给每个stage分配新的arrive变量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for [s = 0 to stages-1] &#123;</span><br><span class="line">    &lt;await (arrive[myid][s] == 0);&gt;</span><br><span class="line">    arrive[myid][s] = 1;</span><br><span class="line">    work out who my friend is at this stage;</span><br><span class="line">    &lt;await (arrive[friend][s] == 1);&gt;</span><br><span class="line">    arrive[friend][s] = 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这样假如出现2一直没到barrier的情况, 那么1会卡在当前stage, 1的stage+1的arrive状态就无法更新为1.</p>
<h5 id="Dissemination-Barriers"><a href="#Dissemination-Barriers" class="headerlink" title="Dissemination Barriers"></a>Dissemination Barriers</h5><p>If n isn’t a power of 2, instead of pairwise synchs, we have two partners at each stage for each thread, one incoming and one outgoing.<br><img src="/images/dissemination_barrier.png" alt="" title=" Dissemination barrier for 6 processes. Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></p>
<h2 id="Structured-Primitives"><a href="#Structured-Primitives" class="headerlink" title="Structured Primitives"></a>Structured Primitives</h2><p>Instead of implementing directly in the user-address space, a number of more structured primitives have been devised for <strong>implementation with the assistance of the operating system</strong>, so that threads can be directly suspended and resumed by the OS’s scheduler.</p>
<blockquote>
<p>• Machine code, instructions and data directly understandable by a CPU;<br>• Language primitive, the simplest element provided by a programming language;<br>• Primitive data type, a datatype provided by a programming language.</p>
</blockquote>
<h3 id="Semaphores-信号灯"><a href="#Semaphores-信号灯" class="headerlink" title="Semaphores 信号灯"></a>Semaphores 信号灯</h3><p>A semaphore is a special shared variable, accessible only through two atomic operations, <strong>P(try to decrease)</strong> and <strong>V(increase)</strong>, defined by:<br>P(s): <code>&lt;await (s&gt;0) s=s-1;&gt;</code><br>V(s): <code>&lt;s=s+1;&gt;</code></p>
<p>Property: A thread executing <code>P()</code> on a 0 valued semaphore will be suspended on a queue until after some other thread has executed a <code>V()</code>.</p>
<p>Application: A semaphore appears to be a simple integer. A thread waits for permission to proceed a <strong>critical section</strong>, and then signals that it has proceeded by performing a <code>P()</code> operation on the semaphore.</p>
<p><strong>Binary semaphore</strong>: A semaphore whose usage is organised to only ever take the value (0, 1) as a mutex 互斥.<br><strong>Counting(split) semaphore</strong>: can take on arbitrary nonnegative values.</p>
<p>Semaphores still require careful programming: there is no explicit connection in the program source between “matching” semaphore operations. It is easy to get things wrong.</p>
<p>Similarly, there is no obvious indication of how semaphores are being used - some may be for mutual exclusion, others for condition synchronization. Again confusion is possible.</p>
<h4 id="Semaphores-for-Critical-Section-mutual-exclusion"><a href="#Semaphores-for-Critical-Section-mutual-exclusion" class="headerlink" title="Semaphores for Critical Section (mutual exclusion)"></a>Semaphores for Critical Section (mutual exclusion)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sem mutex = 1;</span><br><span class="line">co [i = 1 to n] &#123;</span><br><span class="line">    while (whatever) &#123;</span><br><span class="line">        P(mutex);</span><br><span class="line">        critical section;</span><br><span class="line">        V(mutex);</span><br><span class="line">        noncritical section;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Semaphores-for-Barrier-Synchronisation"><a href="#Semaphores-for-Barrier-Synchronisation" class="headerlink" title="Semaphores for Barrier Synchronisation"></a>Semaphores for Barrier Synchronisation</h4><p>实现 symmetric barrier: an array of arrive semaphores for each stage<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for [s = 1 to stages] &#123;</span><br><span class="line">    V(arrive[myid][s]);</span><br><span class="line">    work out who my friend is at stage s;</span><br><span class="line">    P(arrive[friend][s]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="Semaphores-for-Producer-Consumer-Buffering"><a href="#Semaphores-for-Producer-Consumer-Buffering" class="headerlink" title="Semaphores for Producer-Consumer Buffering"></a>Semaphores for Producer-Consumer Buffering</h4><p>针对单个producer和consumer，控制其接触单个容量的buffer权限：一个semaphores标识buffer已满<code>full</code>，一个标识空<code>empty</code>。这种情况下，只能有一个semaphore是<code>1</code>，故称之为<strong>split binary semaphore</strong>。 <code>P(full)</code> 执行 <code>wait full &gt; 0 : full -= 1</code>, <code>V(empty)</code>执行<code>empty += 1</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">T buf; sem empty = 1, full = 0;</span><br><span class="line">co</span><br><span class="line">  co [i = 1 to M] &#123;</span><br><span class="line">      while (whatever) &#123;</span><br><span class="line">          ...produce new data locally</span><br><span class="line">          P(empty);</span><br><span class="line">          buf = data;                # producer</span><br><span class="line">          V(full);</span><br><span class="line">  &#125;   &#125;</span><br><span class="line">//</span><br><span class="line">  co [j = 1 to N] &#123;</span><br><span class="line">      while (whatever) &#123;</span><br><span class="line">          P(full);</span><br><span class="line">          result = buf;              # consumer</span><br><span class="line">          V(empty);</span><br><span class="line">          ... handle result locally</span><br><span class="line">  &#125;   &#125;</span><br><span class="line">oc</span><br></pre></td></tr></table></figure></p>
<p><strong>Bounded Buffer</strong>: Control access to a multi-space buffer (the producer can run ahead of the consumer up to some limit)</p>
<ul>
<li>Implement the buffer itself with an array (circular),</li>
<li>and two integer indices, indicating the current front and rear of the buffer and use arithmetic modulo <code>n</code> (the buffer size), so that the buffer conceptually becomes circular</li>
<li>For a single producer and consumer, we protect the buffer with a split “counting” semaphore, initialised according to the buffer size.</li>
<li>Think of full as counting how many space in the buffer are full, and empty as counting how many are empty<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">T buf[n]; int front = 0, rear = 0;</span><br><span class="line">sem empty = n, full = 0;</span><br><span class="line">co ## Producer</span><br><span class="line">    while (whatever) &#123;</span><br><span class="line">        ...produce new data locally</span><br><span class="line">        P(empty);                 # empty&gt;0, 才能生产, empty-=1</span><br><span class="line">        buf[rear] = data; rear = (rear + 1) % n;</span><br><span class="line">        V(full);</span><br><span class="line">    &#125;</span><br><span class="line">// ## Consumer</span><br><span class="line">    while (whatever) &#123;</span><br><span class="line">        P(full);                  # full&gt;0, 才能消耗, full-=1</span><br><span class="line">        result = buf[front]; front = (front + 1) % n;</span><br><span class="line">        V(empty);</span><br><span class="line">        ... handle result locally</span><br><span class="line">    &#125;</span><br><span class="line">oc</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Multiple Producers/Consumers</strong>: Because each producer may access the same pointer to overide each other, so as consumer. Thus we need two levels of protection.</p>
<ul>
<li>Use a split counting semaphore to avoid buffer overflow (or underflow), as previously.</li>
<li>Use a mutual exclusion semaphores to prevent interference between producers (and another to prevent interference between consumers). This allows up to one consumer and one producer to be actively simultaneously within a non-empty, non-full buffer.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">T buf[n]; int front = 0, rear = 0; 86</span><br><span class="line">sem empty = n, full = 0, mutexP = 1, mutexC = 1;</span><br><span class="line">co</span><br><span class="line">  co [i = 1 to M] &#123;</span><br><span class="line">      while (whatever) &#123;</span><br><span class="line">          ...produce new data locally</span><br><span class="line">          P(empty);</span><br><span class="line">            P(mutexP); # stop the other producers from accessing the buffer</span><br><span class="line">              buf[rear] = data; rear = (rear + 1) % n;</span><br><span class="line">            V(mutexP);</span><br><span class="line">          V(full);</span><br><span class="line">&#125; &#125;</span><br><span class="line">//</span><br><span class="line">  co [j = 1 to N] &#123;</span><br><span class="line">      while (whatever) &#123;</span><br><span class="line">          P(full);</span><br><span class="line">            P(mutexC);</span><br><span class="line">              result = buf[front]; front = (front + 1) % n;</span><br><span class="line">            V(mutexC);</span><br><span class="line">          V(empty);</span><br><span class="line">          ... handle result locally</span><br><span class="line">    &#125; &#125;</span><br><span class="line">oc</span><br></pre></td></tr></table></figure>
<p><strong>Extending Multiple Producers/Consumers</strong>: If the buffered items are large and take a long time to read/write, we would like to relax this solution to allow several producers and/or consumers to be active within the buffer simultaneously.</p>
<ul>
<li>We need to ensure that these workers accesse distinct buffer locations, which require the index arithmetic to be kept atomic.</li>
<li>Make sure that the producer/consumers wait for that element to be empty/full before actually proceeding.<br><img src="/images/Multiple_Producers_Consumers.png" alt="" title="The producers are filling distinct slots, but not necessarily completing these fills in strict order - slot i+1 might finish filling before slot i. However, consumers only know that a slot has been filled and assume, possibly incorrectly, that it is the &quot;next&quot; one. Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></li>
</ul>
<p>The solution is to have <strong>extra semaphores pair for each buffer location</strong>.</p>
<h3 id="Monitors"><a href="#Monitors" class="headerlink" title="Monitors"></a>Monitors</h3><p>The monitor is a more structured mechanism which allows threads to have both <strong>mutual exclusion</strong> and the ability to <strong>wait</strong> (block) for a certain condition to become true. It has a mechanism for <strong>signaling</strong> other threads that their condition has been met. A monitor consists of a <strong>mutex (lock)</strong> object and <strong>condition variables</strong> (cv). A condition variable is basically a container of threads that are waiting for a certain condition.</p>
<p>For <strong>Mutual Exclusion</strong>: i.e. a mutex (lock) object, ensures that <strong>at most one thread is active within the monitor at each point in time</strong>. 不同线程的下一条即将执行的指令 (suspended) 可能是来自同一个 monitor (由os自行分配), 但同一时间内，至多只能有一个线程执行下一条指令，但可能不同线程各自收到了来自这个 monitor 代码的不同指令. It is as if the body of each monitor method is implicitly surrounded with <code>P()</code> and <code>V()</code> operations on a single hidden binary semaphore, shared by all methods.</p>
<p>For <strong>Condition Synchronization</strong>, using a cv with a monitor to control <strong>a queue of delayed threads</strong> by a kind of <strong>Signal and Continue (SC)</strong> scheme.<img src="/images/signal_and_continue.png" alt="" title="State transition diagram for &quot;signal-and-continue&quot; monitors. Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"><br>For a <code>condition_variables x;</code></p>
<ul>
<li><code>wait(x)</code>: Release lock; wait for the condition to become true; reacquire lock upon return (Java wait())</li>
<li><code>Signal(x)</code>: Wake up a waiter, if any (Java notify())</li>
<li><code>signal-all(x)</code>or<code>Broadcast(x)</code>: Wake up all the waiters (Java notifyAll())</li>
</ul>
<p>For the thread active inside a monitor method - <strong>executing in monitor state</strong></p>
<ul>
<li>If the thread could not proceed, it may call the <code>wait(cv)</code> operation to <strong>give up the (implicit) lock</strong> it holds on the monitor, and being <strong>suspended</strong> (push to the end of CV queue). Each CV has its unique block queue.</li>
<li>Or the thread could calls the operation <code>signal(cv)</code> to release the lock. This allow one previously blocked thread (normally chosen by a FIFO discipline) to <strong>become ready for scheduling</strong> again (only one will be allowed to enter the <strong>monitor entry queue</strong> at a time). The signalling thread <strong>continues</strong> uninterrupted.</li>
<li>Or <code>return()</code>.</li>
</ul>
<p>If no threads are waiting, then a <code>signal()</code> is <strong>“lost”</strong> or “forgotten”, whereas a <code>V()</code> in Semaphores allows a subsequent <code>P()</code> to proceed.</p>
<p>Monitor semantics mean that when a thread which was previously blocked on a condition is actually awakened again in the monitor.</p>
<p>The point to remember is that when the signal happened, the signalled thread was allowed to <strong>try to acquire the monitor lock</strong> again). It could be that some other thread acquires the lock first, and does something which negates the condition again (for example, it consumes the “new item” from a monitor protected buffer).</p>
<p>Thus it is often <strong>necessary</strong>, in all but the most tightly constrained situations, to wrap each conditional variable <code>wait()</code> call in a loop which rechecks the condition it was waiting for is still true.</p>
<p><strong>Single producer, single consumer bounder buffer</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">monitor Bounded_Buffer &#123;</span><br><span class="line">    typeT buf[n];                     # an array of some type T</span><br><span class="line">    int front = 0,                    # index of first full slot</span><br><span class="line">        rear = 0;                     # index of first empty slot</span><br><span class="line">        count = 0;                    # number of full slots</span><br><span class="line">    ## rear == (front + count) % n</span><br><span class="line">    condition_variables not_full,     # signaled when count &lt; n</span><br><span class="line">                        not_empty;    # signaled when count &gt; 0</span><br><span class="line">    procedure deposit(typeT data) &#123;   # 存</span><br><span class="line">        while (count == n) wait(not_full);</span><br><span class="line">        buf[rear] = data; rear = (rear+1) % n; count++;</span><br><span class="line">        signal(not_empty);</span><br><span class="line">    &#125;</span><br><span class="line">    procedure fetch(typeT &amp;result) &#123;  # 取</span><br><span class="line">        while (count == 0) wait(not_empty);</span><br><span class="line">        result = buf[front]; front = (front+1) % n; count--;</span><br><span class="line">        signal(not_full);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>Why the while loop is necessary</strong> as a safety check on the <code>wait</code> calls (why not use if)? - 因为<code>notify()</code>只会让正在 wait queue 的 thread 进入准备状态, 但不会直接控制其恢复工作（是否马上开始，谁先开始，都是由os内部控制的）, 所以导致不同 thread 进度不同; 而<code>while</code>可以保证当即使 thread 因为受到<code>notify()</code>而结束<code>wait()</code>开始进入准备状态(entry queue)后, 继续检查 buffer 状态, 这样假如发现自己是最优先安排的那个, 就可以跳出<code>while</code>循环进入工作状态; 假如发现自己优先度不是最高的(<code>while</code>循环条件继续满足), 则继续<code>wait()</code>.</p>
<p>The key difference to semaphores: <code>signal()</code> on a condition variable is not “remembered” in the way that <code>V()</code> on a semaphore is. If no threads are waiting, then a <code>signal()</code> is “lost” or “forgotten”, whereas a <code>V()</code> will allow a subsequent <code>P()</code> to proceed.</p>
<h2 id="Real-Shared-Variable-Programming-Systems"><a href="#Real-Shared-Variable-Programming-Systems" class="headerlink" title="Real Shared Variable Programming Systems"></a>Real Shared Variable Programming Systems</h2><p>Various concepts for shared variable programming have been embedded in real programming systems. In particular C’s <strong>Posix threads (Pthreads)</strong> library and <strong>Java’s threads and monitors</strong>.</p>
<h2 id="POSIX-Threads-Pthread"><a href="#POSIX-Threads-Pthread" class="headerlink" title="POSIX Threads (Pthread)"></a>POSIX Threads (Pthread)</h2><p>Create a new thread: Threads (type <code>pthread_t</code>) begin by executing a given function, and terminate when that function exits (or when killed off by another thread).<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pthread_create</span> <span class="params">(<span class="keyword">pthread_t</span> *thread, <span class="keyword">p_thread_attr_t</span> *attr,</span></span></span><br><span class="line">                    void *(*function) (void *), void *arguments);</span><br></pre></td></tr></table></figure></p>
<p>Wait for thread termination: <code>int pthread_join (pthread_t t, void ** result);</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//一个简单但是有错误的例子，</span></span><br><span class="line"><span class="keyword">int</span> target;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">adderthread</span> <span class="params">(<span class="keyword">void</span> *arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;N; i++) &#123;</span><br><span class="line">        target = target+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i; <span class="keyword">pthread_t</span> thread[P];</span><br><span class="line">    target = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;P; i++) &#123;</span><br><span class="line">        pthread_create(&amp;thread[i], <span class="literal">NULL</span>, adderthread, <span class="literal">NULL</span>);</span><br><span class="line">    &#125; .....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Variable <code>target</code> is accessible to all threads (shared memory). Its increment is <strong>not atomic</strong>, so we may get unpredictable results.</p>
<p>POSIX provides mechanisms to coordinate accesses including <strong>semaphores</strong> and building blocks for <strong>monitors</strong>.</p>
<h3 id="Pthreads-semaphores"><a href="#Pthreads-semaphores" class="headerlink" title="Pthreads semaphores"></a>Pthreads semaphores</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//用 pthread semaphores 改写前面的代码</span></span><br><span class="line"><span class="keyword">sem_t</span> lock;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">adderthread</span> <span class="params">(<span class="keyword">void</span> *arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;N; i++) &#123;</span><br><span class="line">        sem_wait(&amp;lock);</span><br><span class="line">        target = target+<span class="number">1</span>;</span><br><span class="line">        sem_post(&amp;lock);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    target = <span class="number">0</span>;</span><br><span class="line">    sem_init(&amp;lock, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    .....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><code>sem_init(&amp;sem, share, init)</code>, where init is the initial value and share is a “boolean” (in the C sense) indicating whether the semaphore will be shared between processes (true) or just threads within a process (false).</li>
<li><code>sem_wait(s)</code>, which is the Posix name for P(s)</li>
<li><code>sem_post(s)</code>, which is the Posix name for V(s)</li>
</ol>
<p>A Producers &amp; Consumers:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">sem_t</span> empty, full; <span class="comment">// the global semaphores</span></span><br><span class="line"><span class="keyword">int</span> data; <span class="comment">// shared buffer</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">pthread_t</span> pid, cid;</span><br><span class="line">    ....</span><br><span class="line">    sem_init(&amp;empty, <span class="number">0</span>, <span class="number">1</span>); <span class="comment">// sem empty = 1</span></span><br><span class="line">    sem_init(&amp;full, <span class="number">0</span>, <span class="number">0</span>); <span class="comment">// sem full = 0</span></span><br><span class="line">    pthread_create(&amp;pid, &amp;attr, Producer, <span class="literal">NULL</span>);</span><br><span class="line">    pthread_create(&amp;cid, &amp;attr, Consumer, <span class="literal">NULL</span>);</span><br><span class="line">    pthread_join(pid, <span class="literal">NULL</span>);</span><br><span class="line">    pthread_join(cid, <span class="literal">NULL</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">Producer</span> <span class="params">(<span class="keyword">void</span> *arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> produced;</span><br><span class="line">    <span class="keyword">for</span> (produced = <span class="number">0</span>; produced &lt; numIters; produced++) &#123;</span><br><span class="line">        sem_wait(&amp;empty);</span><br><span class="line">        data = produced;</span><br><span class="line">        sem_post(&amp;full);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">Consumer</span> <span class="params">(<span class="keyword">void</span> *arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> total = <span class="number">0</span>, consumed;</span><br><span class="line">    <span class="keyword">for</span> (consumed = <span class="number">0</span>; consumed &lt; numIters; consumed++) &#123;</span><br><span class="line">        sem_wait(&amp;full);</span><br><span class="line">        total = total+data;</span><br><span class="line">        sem_post(&amp;empty);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"after %d iterations, the total is %d (should be %d)\n"</span>, numIters, total, numIters*(numIters+<span class="number">1</span>)/<span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Pthreads-Monitors"><a href="#Pthreads-Monitors" class="headerlink" title="Pthreads Monitors"></a>Pthreads Monitors</h3><p>Pthreads provides <strong>locks</strong>, of type <code>pthread_mutex_t m;</code>. These can be</p>
<ul>
<li><strong>Initialized</strong> with <code>pthread_mutex_init(&amp;m, attr)</code>, where attr are attributes concerning scope (as with semaphore creation). If attr is <code>NULL</code>, the default mutex attributes (NONRECURSIVE) are used;</li>
<li><strong>Locked</strong> with <code>pthread_mutex_lock(&amp;m)</code>, which blocks the locking thread if <code>m</code> is already locked. There is also a non-blocking version <code>pthread_mutex_trylock(&amp;m)</code>.</li>
<li><strong>Unlocked</strong> with <code>pthread_mutex_unlock(&amp;m)</code>. Only a thread which holds a given lock, should unlock it!</li>
</ul>
<p>Pthreads provides <strong>condition variables</strong> <code>pthread_cond_t</code>. As well as the usual initialization, these can be:</p>
<ul>
<li><strong>Waited</strong> on with <code>pthread_cond_wait(&amp;cv, &amp;mut)</code> where <code>cv</code> is a condition variable, and <code>mut</code> must be a lock already held by this thread, and which is implictly released.</li>
<li><strong>Signalled</strong> with <code>pthread_cond_signal(&amp;cv)</code> by a thread which should (but doesn’t strictly have to) hold the associated mutex. The semantics are “Signal-and-Continue” as previously discussed.</li>
<li><strong>Signalled all</strong> with <code>pthread_cond_broadcast(&amp;cv)</code>. This is “signal-all”</li>
</ul>
<p>A simple Jacobi grid-iteration program with a re-usable Counter Barrier. To avoid copying between “new” and “old” grids, each iteration performs two Jacobi steps. Convergence testing could be added as before.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pthread_mutex_t</span> barrier; <span class="comment">// mutex semaphore for the barrier</span></span><br><span class="line"><span class="keyword">pthread_cond_t</span> go;       <span class="comment">// condition variable for leaving</span></span><br><span class="line"><span class="keyword">int</span> numArrived = <span class="number">0</span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Barrier</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    pthread_mutex_lock(&amp;barrier);</span><br><span class="line">    numArrived++;</span><br><span class="line">    <span class="keyword">if</span> (numArrived == numWorkers) &#123;</span><br><span class="line">        numArrived = <span class="number">0</span>;</span><br><span class="line">        pthread_cond_broadcast(&amp;go);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        pthread_cond_wait(&amp;go, &amp;barrier);</span><br><span class="line">    &#125;</span><br><span class="line">    pthread_mutex_unlock(&amp;barrier);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">pthread_t</span> workerid[MAXWORKERS];</span><br><span class="line">    pthread_mutex_init(&amp;barrier, <span class="literal">NULL</span>);</span><br><span class="line">    pthread_cond_init(&amp;go, <span class="literal">NULL</span>);</span><br><span class="line">    InitializeGrids();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; numWorkers; i++)</span><br><span class="line">        pthread_create(&amp;workerid[i], &amp;attr, Worker, (<span class="keyword">void</span> *) i);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; numWorkers; i++)</span><br><span class="line">        pthread_join(workerid[i], <span class="literal">NULL</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">Worker</span><span class="params">(<span class="keyword">void</span> *arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> myid = (<span class="keyword">int</span>) arg, rowA = myid*rowshare+<span class="number">1</span>, rowB = rowA+rowshare<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span> (iters = <span class="number">1</span>; iters &lt;= numIters; iters++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (i = rowA; i &lt;= rowB; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (j = <span class="number">1</span>; j &lt;= gridSize; j++) &#123;</span><br><span class="line">                grid2[i][j] = (grid1[i<span class="number">-1</span>][j] + grid1[i+<span class="number">1</span>][j] + grid1[i][j<span class="number">-1</span>] + grid1[i][j+<span class="number">1</span>]) * <span class="number">0.25</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        Barrier();</span><br><span class="line">        <span class="keyword">for</span> (i = rowA; i &lt;= rowB; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (j = <span class="number">1</span>; j &lt;= gridSize; j++) &#123;</span><br><span class="line">                grid1[i][j] = (grid2[i<span class="number">-1</span>][j] + grid2[i+<span class="number">1</span>][j] + grid2[i][j<span class="number">-1</span>] + grid2[i][j+<span class="number">1</span>]) * <span class="number">0.25</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        Barrier();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Memory-Consistency-in-Pthreads"><a href="#Memory-Consistency-in-Pthreads" class="headerlink" title="Memory Consistency in Pthreads"></a>Memory Consistency in Pthreads</h3><p>Weak consistency models can wreck naive DIY synchronization attempts!</p>
<p>To enable portability, Pthreads mutex, semaphore and condition variable operations <strong>implicitly act as memory fences</strong>, executing architecture specific instructions.</p>
<p>In effect, the C + Pthreads combination guarantees a <strong>weak consistency memory model</strong>, with the only certainties provided at uses of Pthreads primitives.</p>
<p>For example, all writes by a thread which has released some <strong>mutex</strong>, are guaranteed to be seen by any thread which then acquires it. Nothing can be assumed about the visibility of writes which cannot be seen to be ordered by their relationship to uses of Pthread primitives.</p>
<p>The programmer must also be careful to use only <strong>thread-safe</strong> code, which works irrespective of how many threads are active.</p>
<blockquote>
<p>Thread-safe code only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specification without unintended interaction. Implementation is guaranteed to be free of race conditions when accessed by multiple threads simultaneously.</p>
</blockquote>
<p>Typical problems involve the use of non-local data. For example, imagine a non-thread safe <code>malloc</code>. Unluckily interleaved calls might break the underlying free space data structure. Some libraries will provide thread-safe versions (but of course, which pay an unnecessary performance penalty when used in a single threaded program).</p>
<h2 id="Java-Concurrency"><a href="#Java-Concurrency" class="headerlink" title="Java Concurrency"></a>Java Concurrency</h2><p>Java是一种多线程 multi-threaded 编程语言，其同步模型是基于 monitor 概念，可用于开发多线程程序。多任务 multtasking 就是多个进程共享公共处理资源（如CPU）的时候。多线程将多任务的思想扩展到可以将单个应用程序中的特定操作细分为单独线程的应用程序。每个线程都可以并行运行。操作系统不仅在不同的应用程序之间分配处理时间，而且在应用程序内的每个线程之间分配处理时间。</p>
<h3 id="Java-Threads"><a href="#Java-Threads" class="headerlink" title="Java Threads"></a>Java Threads</h3><p><img src="https://www.tutorialspoint.com/java/images/Thread_Life_Cycle.jpg" alt="" title="Life Cycle of a Thread. image from: http://www.tutorialspoint.com/java/java_multithreading.htm"><br>Threads can be created from classes which extend <code>java.lang.Thread</code><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Simple</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"this is a thread"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">new</span> Simple().start(); <span class="comment">// implicitly calls the run() method</span></span><br></pre></td></tr></table></figure></p>
<p>Or implement <code>java.lang.Runnable</code> (so we can extend some other class too).<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bigger</span> <span class="keyword">extends</span> <span class="title">Whatever</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123; .... &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">new</span> Thread( <span class="keyword">new</span> Bigger (...) ).start();</span><br></pre></td></tr></table></figure></p>
<p>Wait to join with another thread<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Friend</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> me;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Friend</span> <span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; me = i; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Hello from thread "</span> + me);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span> <span class="title">throws</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">InterruptedException</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> n = <span class="number">5</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i; Friend t[] = <span class="keyword">new</span> Friend[n];</span><br><span class="line">        System.out.println (<span class="string">"Hello from the main thread"</span>);</span><br><span class="line">        <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;n; i++) &#123;</span><br><span class="line">            t[i] = <span class="keyword">new</span> Friend(i);</span><br><span class="line">            t[i].start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;n; i++) &#123;</span><br><span class="line">            t[i].join(); <span class="comment">// might throw java.lang.InterruptedException</span></span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println (<span class="string">"Goodbye from the main thread"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Java-“Monitors”"><a href="#Java-“Monitors”" class="headerlink" title="Java “Monitors”"></a>Java “Monitors”</h3><p>Java provides an implementation of the <strong>monitor</strong> concept (but doesn’t actually have monitor as a keyword).</p>
<p>Any object in a Java program can, in effect, become a monitor, simply by declaring one or more of its methods to be <strong>synchronized</strong>, or by including a synchronized block of code.</p>
<p>Each such object is associated with one, <strong>implicit lock</strong>. A thread executing any <code>synchronized</code> code must first acquire this lock. This happens implicitly (i.e. there is no source syntax). Similarly, upon leaving the synchronized block the lock is <strong>implicitly released</strong>.</p>
<p>Java’s <strong>condition variable</strong> mechanism uses <strong>Signal-and-Continue</strong> semantics (The signalling thread continues uninterrupted). Each synchronizable object is associated with a <strong>single implicit condition variable</strong>. Manipulated with methods <code>wait()</code>, <code>notify()</code> and <code>notifyAll()</code>. We can only have <strong>one conditional variable queue per monitor</strong> (hence the absence of any explicit syntax for the condition variable itself).</p>
<p><code>wait()</code>: has three variance, one which waits indefinitely for any other thread to call notify or notifyAll method on the object to wake up the current thread. Other two variances puts the current thread in wait for specific amount of time before they wake up.</p>
<p><code>notify()</code>: <strong>wakes up only one thread</strong> waiting on the object and that thread starts execution.</p>
<p><code>notifyAll()</code>: <strong>wakes up all the threads</strong> waiting on the object, although which one will process first depends on the OS implementation.</p>
<p>These methods can be used to implement producer consumer problem where consumer threads are waiting for the objects in Queue and producer threads put object in queue and notify the waiting threads.</p>
<p><strong>Readers &amp; Writers problem</strong> requires control access to some shared resource, such that there may be many concurrent readers, but only one writer (with exclusive access) at a time.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 2 readers and 2 writers making 5 accesses each</span></span><br><span class="line"><span class="comment">with concurrent read or exclusive write.  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReadWrite</span> </span>&#123; <span class="comment">// driver program -- two readers and two writers</span></span><br><span class="line">    <span class="keyword">static</span> Database RW = <span class="keyword">new</span> Database(); <span class="comment">// the monitor</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] arg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> rounds = Integer.parseInt(arg[<span class="number">0</span>],<span class="number">10</span>);</span><br><span class="line">        <span class="keyword">new</span> Reader(rounds, RW).start();</span><br><span class="line">        <span class="keyword">new</span> Reader(rounds, RW).start();</span><br><span class="line">        <span class="keyword">new</span> Writer(rounds, RW).start();</span><br><span class="line">        <span class="keyword">new</span> Writer(rounds, RW).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reader</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> rounds; Database RW;</span><br><span class="line">    <span class="keyword">private</span> Random generator = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Reader</span><span class="params">(<span class="keyword">int</span> rounds, Database RW)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.rounds = rounds;</span><br><span class="line">        <span class="keyword">this</span>.RW = RW;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;rounds; i++) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(generator.nextInt(<span class="number">500</span>));</span><br><span class="line">            &#125; <span class="keyword">catch</span> (java.lang.InterruptedException e) &#123;&#125;</span><br><span class="line">            System.out.println(<span class="string">"read: "</span> + RW.read());</span><br><span class="line">        &#125;    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Writer</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> rounds; Database RW;</span><br><span class="line">    <span class="keyword">private</span> Random generator = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Writer</span><span class="params">(<span class="keyword">int</span> rounds, Database RW)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.rounds = rounds;</span><br><span class="line">        <span class="keyword">this</span>.RW = RW;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;rounds; i++) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(generator.nextInt(<span class="number">500</span>));</span><br><span class="line">            &#125; <span class="keyword">catch</span> (java.lang.InterruptedException e) &#123;&#125;</span><br><span class="line">            RW.write();</span><br><span class="line">        &#125;    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Implement the “database”. Allowing several readers to be actively concurrently. The last reader to leave will signal a waiting writer.</p>
<p>Thus we need to count readers, which implies <strong>atomic update</strong> of the count. A reader needs <strong>two protected sections</strong> to achieve this.</p>
<p>Notice that while readers are actually reading the data they do not hold the lock.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Database</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> data = <span class="number">0</span>; <span class="comment">// the data</span></span><br><span class="line">    <span class="keyword">int</span> nr = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// synchronized means no more than one thread could do that</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">startRead</span><span class="params">()</span> </span>&#123; nr++; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">endRead</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        nr--;</span><br><span class="line">        <span class="keyword">if</span> (nr==<span class="number">0</span>) notify(); &#125;<span class="comment">// awaken a waiting writer</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">read</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> snapshot;</span><br><span class="line">        startRead();</span><br><span class="line">        snapshot = data;      <span class="comment">// read data</span></span><br><span class="line">        endRead();</span><br><span class="line">        <span class="keyword">return</span> snapshot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> temp;</span><br><span class="line">        <span class="keyword">while</span> (nr&gt;<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">try</span> &#123; wait(); &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;<span class="keyword">return</span>;&#125;</span><br><span class="line"></span><br><span class="line">        temp = data; <span class="comment">// next six lines are the ‘‘database’’ update!</span></span><br><span class="line">        data = <span class="number">99999</span>; <span class="comment">// to simulate an inconsistent temporary state</span></span><br><span class="line">        <span class="keyword">try</span> &#123; Thread.sleep(generator.nextInt(<span class="number">500</span>)); <span class="comment">// wait a bit, for demo purposes only</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (java.lang.InterruptedException e) &#123;&#125;</span><br><span class="line">        data = temp+<span class="number">1</span>; <span class="comment">// back to a safe state</span></span><br><span class="line">        System.out.println(<span class="string">"wrote: "</span> + data);</span><br><span class="line">        notify(); <span class="comment">// awaken another waiting writer</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>We could express the same effect with synchronized blocks<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">read</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> snapshot;</span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="keyword">this</span>) &#123; nr++; &#125; <span class="comment">// this - the database object</span></span><br><span class="line">    snapshot = data;</span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="keyword">this</span>) &#123;</span><br><span class="line">        nr--;</span><br><span class="line">        <span class="keyword">if</span> (nr==<span class="number">0</span>) notify(); <span class="comment">// awaken a waiting writer</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> snapshot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Would it be OK to use <code>notifyAll()</code> in <code>read()</code>? - <strong>Yes</strong>, but with extra transmission cost.</p>
<p><strong>Buffer for One Producer - One Consumer</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** (borrowed from Skansholm, Java from the Beginning) */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Buffer</span> <span class="keyword">extends</span> <span class="title">Vector</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">putLast</span> <span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">        addElement(obj); <span class="comment">// Vectors grow implicitly</span></span><br><span class="line">        notify();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> Object <span class="title">getFirst</span> <span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (isEmpty())</span><br><span class="line">            <span class="keyword">try</span> &#123;wait();&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;<span class="keyword">return</span> <span class="keyword">null</span>;&#125;</span><br><span class="line">        Object obj = elementAt(<span class="number">0</span>);</span><br><span class="line">        removeElementAt(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> obj;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="The-java-util-concurrent-package"><a href="#The-java-util-concurrent-package" class="headerlink" title="The java.util.concurrent package"></a>The <code>java.util.concurrent</code> package</h3><p>Including a re-usable <strong>barrier</strong> and <strong>semaphores</strong> (with P() and V() called <code>acquire()</code> and <code>release()</code>). It also has some thread-safe concurrent data structures (queues, hash tables).</p>
<p>The <code>java.util.concurrent.atomic</code> package provides implementations of <strong>atomically accessible</strong> integers, booleans and so on, with atomic operations like <code>addAndGet</code>, <code>compareAndSet</code>.</p>
<p>The <code>java.util.concurrent.locks</code> package provides implementations of <strong>locks and condition variables</strong>, to allow a finer grained, more explicit control than that provided by the built-in synchronized monitors.</p>
<h2 id="Message-Passing-Programming"><a href="#Message-Passing-Programming" class="headerlink" title="Message Passing Programming"></a>Message Passing Programming</h2><p>When the underyling archictecture doesn’t support physically shared memory (for example, by distributing the OS and virtual memory system, i.e. <strong>Multicomputer architectures</strong>), we can make the disjoint nature of the address spaces apparent to the programmer, who must make decisions about data distribution and invoke explicit operations to allow interaction across these.</p>
<p><strong>Message passing</strong>, which is a approache to abstract and implement such a model, dominates the performance-oriented parallel computing world.</p>
<p>Message passing is characterized as requiring the <strong>explicit participation</strong> of both interacting processes, since each address space can only be directly manipulated by its owner. The basic requirement is thus for <strong>send</strong> and <strong>receive</strong> primitives for transferring data out of and into local address spaces.</p>
<p>The resulting programs can seem quite fragmented: we express algorithms as a collection of local perspectives. These are often captured in a single program source using <strong>Single Program Multiple Data (SPMD)</strong> style, with different processes following different paths through the same code, branching with respect to local data values and/or to some process identifier.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// SPMD Compare-Exchange</span><br><span class="line">co [me = 0 to P-1] &#123; // assumes P is even</span><br><span class="line">  int a, temp;       // these are private to each process now</span><br><span class="line">  ......</span><br><span class="line"></span><br><span class="line">  // typical one step within a parallel sorting algorithm</span><br><span class="line">  if (me%2 == 0) &#123;</span><br><span class="line">      send (me+1, a);    // send from a to process me+1</span><br><span class="line">      recv (me+1, temp); // receive into temp from process me+1</span><br><span class="line">      a = (a&lt;=temp) ? a : temp; // 取较小值</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">      send (me-1, a);</span><br><span class="line">      recv (me-1, temp);</span><br><span class="line">      a = (a&gt;temp) ? a : temp; // 取较大值</span><br><span class="line">  &#125; ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/SPMD_Compare_Exchange.png" alt="" title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"><br>1, <strong>Synchronization</strong>: Must a sending process pause until a matching receive has been executed (<strong>synchronous</strong>), or not (<strong>asynchronous</strong>)? Asynchronous semantics require the implementation to buffer messages which haven’t yet been, and indeed may never be, received. If we use synchronous semantics, the compare-exchange code above will deadlock. Can you fix it?</p>
<blockquote>
<p>One way s to make the send be a  non-blocking one (<code>MPI_Isend</code>)<br>Another way is to reverse the order of one of the send/receive pairs:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#125; else &#123;</span><br><span class="line">    recv (me-1, temp);</span><br><span class="line">    send (me-1, a);</span><br><span class="line">    a = (a&gt;temp) ? a : temp; // 取较大值</span><br><span class="line">&#125; ......</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>2, <strong>Addressing</strong>: When we invoke a send (or receive) do we have to specify a unique destination (or source) process or can we use <strong>wild-cards</strong>? Do we require program-wide process naming, or can we create process groups and aliases?<br>3, <strong>Collective Operations</strong>: Do we restrict the programmer to single-source, single-destination, point-to-point messages, or do we provide abstractions of more complex data exchanges involving several partners?<img src="/images/Collective_Operations.jpg" alt="" title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf">• Broadcast: Everyone gets a copy of the same value.<br>• Scatter: Data is partitioned and spread across the group.<br>• Gather: Data is gathered from across the group.<br>• Reduction: Combine the gathered values with an associative operation.<br>• Scan (Prefix): Reduce and also compute all the ordered partial reductions.</p>
<h2 id="Message-Passing-Interface-MPI"><a href="#Message-Passing-Interface-MPI" class="headerlink" title="Message Passing Interface (MPI)"></a>Message Passing Interface (MPI)</h2><blockquote>
<p>Message Passing Interface (MPI) is a standardized and portable message-passing standard. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran.</p>
</blockquote>
<p>Processes can be created <strong>statically</strong> when the program is invoked (e.g. using the mpirun command) or spawned <strong>dynamically</strong>.</p>
<p>All communications take place within the context of “communication spaces” called <strong>communicators</strong>, which denote sets of processes, allows the MPI programmer to define <strong>modules</strong> that encapsulate internal communication structures. A process can belong to many communicators simultaneously. New communicators can be defined dynamically.</p>
<p>Simple send/receives operate with respect to other processes in a communicator. <strong>Send must specify a target</strong> but receive can wild card on matching sender.</p>
<p>Messages can be tagged with an extra value to aid disambiguation.</p>
<p>Message-passing programming models are by default <strong>nondeterministic</strong>: the arrival order of messages sent from two processes A and B, to a third process C, is not defined. (However, MPI does guarantee that two messages sent from one process A, to another process B, will arrive in the order sent.)</p>
<p>There are many <strong>synchronization modes</strong> and a range of <strong>collective operations</strong>.</p>
<h3 id="MPI-Primitives-6-basics-functions"><a href="#MPI-Primitives-6-basics-functions" class="headerlink" title="MPI Primitives (6 basics functions)"></a>MPI Primitives (6 basics functions)</h3><p>1, <code>int MPI_Init(int *argc, char ***argv)</code>: Initiate an MPI computation.<br>2, <code>int MPI_Finalize()</code>: Terminate a computation.<br>These must be called <strong>once</strong> by every participating process, before/after any other MPI calls. They return <strong>MPI_SUCCESS</strong> if successful, or an error code.</p>
<p>Each process has a <strong>unique identifier</strong> in each communicator of which it is a member (range 0…members-1). <code>MPI_COMM_WORLD</code> is the built-in <strong>global communicator</strong>, to which all processes belong by default.</p>
<p>A process can find the size of a communicator, and its own rank within it:<br>3, <code>int MPI_Comm_Size (MPI_Comm comm, int *np)</code>: Determine number of processes (comm - communicator). The processes in a process group are identified with unique, contiguous integers numbered from 0 to <code>np-1</code>.<br>4, <code>int MPI_Comm_rank (MPI_Comm comm, int *me)</code>: Determine my process identifier.</p>
<p>5, <code>MPI_SEND</code>: Send a message.<br>6, <code>MPI_RECV</code>: Receive a message.</p>
<h3 id="MPI-Task-Farm"><a href="#MPI-Task-Farm" class="headerlink" title="MPI Task Farm"></a>MPI Task Farm</h3><p>A task farm is bag-of-tasks in which <strong>all the tasks are known</strong> from the beginning. The challenge is to assign them <strong>dynamically</strong> to worker processes, to allow for the possibility that some tasks may take much longer to compute than others.</p>
<p>To simplify the code, we assume that there are <strong>at least as many tasks as processors</strong> and that tasks and results are just integers. In a real application these would be more complex data structures.</p>
<p>Notice the handling of the characteristic <strong>non-determinism</strong> in the order of task completion, with tags used to identify tasks and results. We also use a special tag to indicate an “end of tasks” message.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** SPMD style</span></span><br><span class="line"><span class="comment">农场主分配任务给工人 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_TASKS 100</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> NO_MORE_TASKS MAX_TASKS+1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FARMER 0  <span class="comment">// 第一个 process 是farmer,其余是worker</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> np, rank;</span><br><span class="line">    MPI_Init(&amp;argc, &amp;argv);</span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);</span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;np);</span><br><span class="line">    <span class="keyword">if</span> (rank == FARMER) &#123;</span><br><span class="line">        farmer(np<span class="number">-1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        worker();</span><br><span class="line">    &#125;</span><br><span class="line">    MPI_Finalize();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">farmer</span> <span class="params">(<span class="keyword">int</span> workers)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, task[MAX_TASKS], result[MAX_TASKS], temp, tag, who; MPI_Status status;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1, 给每个人发送任务</span></span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;workers; i++) &#123;</span><br><span class="line">        MPI_Send(&amp;task[i], <span class="number">1</span>, MPI_INT, i+<span class="number">1</span>, i, MPI_COMM_WORLD);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2, 收取任务结果, 继续发放剩余任务</span></span><br><span class="line">    <span class="keyword">while</span> (i&lt;MAX_TASKS) &#123;</span><br><span class="line">        MPI_Recv(&amp;temp, <span class="number">1</span>, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);</span><br><span class="line">        who = status.MPI_SOURCE; tag = status.MPI_TAG;</span><br><span class="line">        result[tag] = temp;</span><br><span class="line">        MPI_Send(&amp;task[i], <span class="number">1</span>, MPI_INT, who, i, MPI_COMM_WORLD);</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3, 所有任务已经完成, 收集最后一个任务结果, 并且发出结束任务信号</span></span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;workers; i++) &#123;</span><br><span class="line">        MPI_Recv(&amp;temp, <span class="number">1</span>, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);</span><br><span class="line">        who = status.MPI_SOURCE; tag = status.MPI_TAG;</span><br><span class="line">        result[tag] = temp;</span><br><span class="line">        MPI_Send(&amp;task[i], <span class="number">1</span>, MPI_INT, who, NO_MORE_TASKS, MPI_COMM_WORLD);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Notice that the final loop, which gathers the last computed tasks, has a predetermined bound. We know that this loop begins after dispatch of the last uncomputed task, so there must be exactly as many results left to gather as there are workers.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">worker</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> task, result, tag;</span><br><span class="line">    MPI_Status status;</span><br><span class="line">    MPI_Recv(&amp;task, <span class="number">1</span>, MPI_INT, FARMER, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);</span><br><span class="line">    tag = status.MPI_TAG;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (tag != NO_MORE_TASKS) &#123;</span><br><span class="line">        result = somefunction(task);</span><br><span class="line">        MPI_Send(&amp;result, <span class="number">1</span>, MPI_INT, FARMER, tag, MPI_COMM_WORLD);</span><br><span class="line">        MPI_Recv(&amp;task, <span class="number">1</span>, MPI_INT, FARMER, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);</span><br><span class="line">        tag = status.MPI_TAG;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>A worker is only concerned with its interaction with the farmer. 这样速度较快的worker可以自动接更多的任务，最终整体上达成 load balance。</p>
<h3 id="Send-in-standard-mode"><a href="#Send-in-standard-mode" class="headerlink" title="Send in standard mode"></a>Send in standard mode</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Send</span><span class="params">(<span class="keyword">void</span> *buf, <span class="keyword">int</span> count, MPI_Datatype datatype, <span class="keyword">int</span> dest,</span></span></span><br><span class="line"><span class="function"><span class="params">             <span class="keyword">int</span> tag, MPI_Comm comm)</span></span></span><br></pre></td></tr></table></figure>
<p>Send <code>count</code> items of given type starting in position <code>buf</code> to process <code>dest</code> in communicator <code>comm</code>, tagging the message with <code>tag</code> (which must be non-negative).</p>
<p>There are corresponding datatypes for each basic C type, <code>MPI_INT</code>, <code>MPI_FLOAT</code> etc, and also facilities for constructing <strong>derived types</strong> which group these together.</p>
<p>Are <code>MPI_Send</code> and <code>MPI_Recv</code> synchronous or asynchronous?</p>
<h3 id="Receive-in-standard-mode"><a href="#Receive-in-standard-mode" class="headerlink" title="Receive in standard mode"></a>Receive in standard mode</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Recv</span><span class="params">(<span class="keyword">void</span> *buf, <span class="keyword">int</span> count, MPI_Datatype datatype, <span class="keyword">int</span> source,</span></span></span><br><span class="line"><span class="function"><span class="params">             <span class="keyword">int</span> tag, MPI_Comm comm, MPI_Status *status)</span></span></span><br></pre></td></tr></table></figure>
<p>Receive <code>count</code> items of given type starting in position <code>buf</code>, from process <code>source</code> in communicator <code>comm</code>, tagged by <code>tag</code>. It attempts to receive a message that has an envelope corresponding to the specified <code>tag</code>, <code>source</code>, and <code>comm</code>, <strong>blocking</strong> until such a message is available. When the message arrives, elements of the specified datatype are placed into the buffer at address <code>buf</code>. This buffer is guaranteed to be large enough to contain at least <code>count</code> elements.</p>
<p><strong>Non-determinism</strong> (within a communicator) is achieved with “wild cards”, by naming <code>MPI_ANY_SOURCE</code> and/or <code>MPI_ANY_TAG</code> as the source or tag respectively.</p>
<p>A receive can match any available message sent to the receiver which has the specified communicator, tag and source, subject to the constraint that messages sent <strong>between any particular pair of processes</strong> are guaranteed to appear to be <strong>non-overtaking</strong>. In other words, a receive cannot match message B in preference to message A if A was sent before B by the same process, the receive will receive the first one which was sent, not the first one to arrive.</p>
<p>The <code>status</code> variable can be used subsequently to inquire about the <code>size</code>, <code>tag</code>, and <code>source</code> of the received message. <strong>Status information</strong> is returned in a structure with <code>status.MPI_SOURCE</code> and <code>status.MPI_TAG</code> fields. This is useful in conjunction with <strong>wild card</strong> receives, allowing the receiver to determine the actual source and tag associated with the received message.</p>
<h3 id="Prime-Sieve-Generator"><a href="#Prime-Sieve-Generator" class="headerlink" title="Prime Sieve Generator"></a>Prime Sieve Generator</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    MPI_Comm nextComm; <span class="keyword">int</span> candidate = <span class="number">2</span>, N = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">    MPI_Init(&amp;argc, &amp;argv);</span><br><span class="line">    MPI_Comm_spawn(<span class="string">"sieve"</span>, argv, <span class="number">1</span>, MPI_INFO_NULL, <span class="number">0</span>, MPI_COMM_WORLD, &amp;nextComm, MPI_ERRCODES_IGNORE);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (candidate&lt;N) &#123;</span><br><span class="line">        MPI_Send(&amp;candidate, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, nextComm);</span><br><span class="line">        candidate++;</span><br><span class="line">    &#125;</span><br><span class="line">    candidate = <span class="number">-1</span>;</span><br><span class="line">    MPI_Send(&amp;candidate, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, nextComm);</span><br><span class="line">    MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>We use <code>MPI_Comm_spawn</code> to <strong>dynamically create</strong> new sieve processes as we need them, and <code>MPI_Comm_get_parent</code> to find an inter-communicator to the process group which created us.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    MPI_Comm predComm, succComm; MPI_Status status;</span><br><span class="line">    <span class="keyword">int</span> myprime, candidate;</span><br><span class="line">    <span class="keyword">int</span> firstoutput = <span class="number">1</span>;            <span class="comment">// a C style boolean</span></span><br><span class="line">    MPI_Init (&amp;argc, &amp;argv);</span><br><span class="line">    MPI_Comm_get_parent (&amp;predComm);</span><br><span class="line">    MPI_Recv(&amp;myprime, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, predComm, &amp;status);</span><br><span class="line">    <span class="built_in">printf</span> (<span class="string">"%d is a prime\n"</span>, myprime);</span><br><span class="line">    MPI_Recv(&amp;candidate, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, predComm, &amp;status);</span><br><span class="line">    <span class="keyword">while</span> (candidate!=<span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (candidate%myprime != <span class="number">0</span>) &#123;    <span class="comment">// not sieved out</span></span><br><span class="line">            <span class="keyword">if</span> (firstoutput) &#123;      <span class="comment">// create my successor if necessary</span></span><br><span class="line">                MPI_Comm_spawn(<span class="string">"sieve"</span>, argv, <span class="number">1</span>, MPI_INFO_NULL, <span class="number">0</span>,  MPI_COMM_WORLD, &amp;succComm, MPI_ERRCODES_IGNORE);</span><br><span class="line">                firstoutput = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            MPI_Send(&amp;candidate, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, succComm) <span class="comment">// pass on the candidate</span></span><br><span class="line">        &#125;</span><br><span class="line">        MPI_Recv(&amp;candidate, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, predComm, &amp;status); <span class="comment">// next candidate</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!firstoutput) MPI_Send(&amp;candidate, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, succComm); <span class="comment">// candidate=-1, shut down</span></span><br><span class="line">    MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>The message flow is insured by the method in which new processes are spawned/created. Every time a new “sieve” process is spawned, MPI creates it in a new group/communicator. succComm is a handle to this new group which always contains only one process. Therefore, when a candidate is sent to the process, there is only one process in the succComm group and it has id 0.</p>
<p>The Recv function works in the same way predComm is a handle of the parent group (i.e. group of the process that created this sieve). And because the parent was the only process in this group/communicator, it can be identified by id 0.</p>
<p>In conclusion, a process creates at most one successor. This successor is the only process in its group/communicator. The succCom and predComm are handles to the children and parent groups respectively, both of which contain a single process with id 0 which is unique in its own group/communicator.</p>
<p>Spawning New MPI Processes<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Comm_spawn</span> <span class="params">(<span class="keyword">char</span> *command, <span class="keyword">char</span> *argv[], <span class="keyword">int</span> p, MPI_Info info,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">int</span> root, MPI_Comm comm, MPI_Comm *intercomm, <span class="keyword">int</span> errcodes[])</span></span></span><br></pre></td></tr></table></figure></p>
<p>This <strong>spawns</strong> p new processes, each executing a copy of program <code>command</code>, in a new communicator returned as <code>intercomm</code>. To the new processes, <code>intercomm</code> appears as <code>MPI_COMM_WORLD</code>. It must be<br>called by <strong>all processes</strong> in <code>comm</code> (it is “collective”), with process root computing the parameters. <code>info</code> and <code>errcodes</code> are used in system dependent ways to control/monitor process placement, errors etc.</p>
<p><code>MPI_Comm_get_parent</code> gives the new processes a reference to the communicator which created them.</p>
<h3 id="Synchronization-in-MPI"><a href="#Synchronization-in-MPI" class="headerlink" title="Synchronization in MPI"></a>Synchronization in MPI</h3><p>MPI uses the term <strong>blocking</strong> in a slightly unconventional way, to refer to the relationship between the <strong>caller</strong> of a communication operation and the <strong>implementation</strong> of that operation (i.e. nothing to do with any matching operation).<img src="/images/Synchronization_in_MPI.png" alt="" title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"><br>Thus, a <strong>blocking send</strong> complete only when it is safe to reuse the specified output buffer (because the data has been copied somewhere safe by the system). 注意这里跟前面提到的<strong>synchronous</strong>概念不一样，synchronous 强调<strong>接收成功</strong>才是判断发送成功与否的标识，而 blocking 只需要保证缓存可以被安全改写即可。</p>
<p>In contrast, a process calling a <strong>non-blocking send</strong> continues immediately with unpredictable effects on the value actually sent. Similarly, there is a <strong>non-blocking receive</strong> operation which allows the calling process to continue immediately, with similar issues concerning the values which appear in the buffer. 意义在于，当需要发送的信息字节非常巨大时，发送和接收耗时都非常久，这时候如果可以不需要等待这些巨量信息的传输而直接继续下一个任务，则能提高效率。</p>
<p>To manage these effects, there are MPI operations for <strong>monitoring</strong> the progress of non-blocking communications (effectively, to ask, “is it OK to use this variable now?”). - The idea is that with <strong>careful use</strong> these can allow the process to get on with other useful work even before the user-space buffer has been safely stored.</p>
<h3 id="Blocking-Communication-Semantics-in-MPI"><a href="#Blocking-Communication-Semantics-in-MPI" class="headerlink" title="Blocking Communication Semantics in MPI"></a>Blocking Communication Semantics in MPI</h3><p>MPI provides different blocking send operations, vary <strong>in the level of synchronization</strong> they provide. Each makes different demands on the underlying communication protocol (i.e. the implementation).</p>
<p>1, <strong>Synchronous mode</strong> send (<code>MPI_Ssend</code>) is blocking and synchronous, only complete when a matching receive has been found.</p>
<p>2, <strong>Standard mode</strong> send (<code>MPI_Send</code>) is blocking. Its synchronicity depends upon the state of the implementation buffers, in that it will be <strong>asynchronous</strong> unless the relevant buffers are full, in which case it will wait for buffer space (and so may appear to behave in a “semi” synchronous fashion).</p>
<p>3, <strong>Buffered mode</strong> send (<code>MPI_Bsend</code>) is blocking and asynchronous, but the programmer must previously have made enough buffer space available (otherwise an error is reported). There are associated operations for <strong>allocating</strong> the buffer space.</p>
<p><strong>Receiving</strong> with <code>MPI_Recv</code> blocks until a matching message has been completely received into the buffer (so it is blocking and <strong>synchronous</strong>).</p>
<p>MPI also provides <strong>non-blocking</strong> sends and receives which return <strong>immediately</strong> (i.e. possibly before it is safe to use/reuse the buffer). There are immediate versions of all the blocking operations (with an extra “I” in the name). For example, <code>MPI_Isend</code> is the <strong>standard mode immediate send</strong>, and <code>MPI_Irecv</code> is the immediate receive.</p>
<p>Non-blocking operations have an extra parameter, called a ‘request’ which is a <strong>handle on the communication</strong>, used with <code>MPI_Wait</code> and <code>MPI_Test</code> to <strong>wait</strong> or <strong>check</strong> for <strong>completion</strong> of the communication (in the sense of the corresponding blocking version of the operation).</p>
<h3 id="Probing-for-Messages"><a href="#Probing-for-Messages" class="headerlink" title="Probing for Messages"></a>Probing for Messages</h3><p>A receiving process may want to <strong>check</strong> for a <strong>potential receive</strong> without actually receiving it. For example, we may not know the incoming message size, and want to create a suitable receiving buffer.</p>
<p><code>int MPI_Probe(int src, int tag, MPI_Comm comm, MPI_Status *status)</code> behaves like <code>MPI_Recv</code> , filling in <code>*status</code>, without actually receiving the message.</p>
<p>There is also a version which tests whether a message is available immediately <code>int MPI_Iprobe(int src, int tag, MPI_Comm comm, int *flag, MPI_Status *status)</code> leaving a (C-style) boolean result in <code>*flag</code> (i.e. message/no message).</p>
<p>We can then determine the <strong>size</strong> of the incoming message by inspecting its status information. <code>int MPI_Get_count(MPI_Status *status, MPI_Datatype t, int *count)</code> sets <code>*count</code> to the <strong>number of items</strong> of type <code>t</code> in message with status <code>*status</code>.</p>
<p>We could use these functions to receive (for example) a message containing an <strong>unknown number</strong> of integers from an <strong>unknown source</strong>, but with <strong>tag</strong> <code>75</code>, in a given communicator comm.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MPI_Probe(MPI_ANY_SOURCE, <span class="number">75</span>, comm, &amp;status);</span><br><span class="line">MPI_Get_count(&amp;status, MPI_INT, &amp;count);</span><br><span class="line">buf = (<span class="keyword">int</span> *) <span class="built_in">malloc</span>(count*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">source = status.MPI_SOURCE;</span><br><span class="line">MPI_Recv(buf, count, MPI_INT, source, <span class="number">75</span>, comm, &amp;status);</span><br></pre></td></tr></table></figure></p>
<h3 id="Collective-Operations"><a href="#Collective-Operations" class="headerlink" title="Collective Operations"></a>Collective Operations</h3><p>MPI offers a range of more complex operations which would otherwise require <strong>complex sequences</strong> of sends, receives and computations.</p>
<p>These are called <strong>collective</strong> operations, because they must be called by <strong>all</strong> processes in a communicator.<img src="/images/Collective_Operations.jpg" alt="" title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></p>
<p>1, <code>MPI_Bcast</code> <strong>broadcasts</strong> <code>count</code> items of type <code>t</code> from <code>buf</code> in <code>root</code> to <code>buf</code> in all other processes in <code>comm</code>:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Bcast</span> <span class="params">(<span class="keyword">void</span> *buf, <span class="keyword">int</span> count, MPI_Datatype t, <span class="keyword">int</span> root,</span></span></span><br><span class="line"><span class="function"><span class="params">               MPI_Comm comm)</span></span></span><br></pre></td></tr></table></figure></p>
<p>2, <code>MPI_Scatter</code> is used to <strong>divide the contents of a buffer</strong> across all processes.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Scatter</span> <span class="params">(<span class="keyword">void</span> *sendbuf, <span class="keyword">int</span> sendcount, MPI_Datatype sendt,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *recvbuf, <span class="keyword">int</span> recvcount, MPI_Datatype recvt, <span class="keyword">int</span> root, MPI_Comm comm)</span></span></span><br></pre></td></tr></table></figure></p>
<p>$i^{th}$ chunk (size <code>sendcount</code>) of <code>root</code>‘s <code>sendbuf</code> is sent to <code>recvbuf</code> on process $i$ (including the root process itself). The first three parameters are only significant at the root. Counts, types, root and communicator parameters must match between root and all receivers.</p>
<p>3, <code>MPI_Gather</code> is the inverse of <code>MPI_Scatter</code>. Instead of spreading elements from one process to many processes, <code>MPI_Gather</code> takes elements from many processes and gathers them to one single process.</p>
<blockquote>
<p><code>MPI_Gather</code> takes elements from each process and gathers them to the root process. The elements are ordered by the rank of the process from which they were received. Only the <code>root</code> process needs to have a valid receive buffer. The <code>recv_count</code> parameter is <strong>the count of elements received per process</strong>, not the total summation of counts from all processes.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MPI_Gather( <span class="keyword">void</span>* send_data, <span class="keyword">int</span> send_count, MPI_Datatype send_datatype,</span><br><span class="line">    <span class="keyword">void</span>* recv_data, <span class="keyword">int</span> recv_count, MPI_Datatype recv_datatype,</span><br><span class="line">    <span class="keyword">int</span> root, MPI_Comm communicator)</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>4, <code>MPI_Allreduce</code> computes a <strong>reduction</strong>, such as adding a collection<br>of values together. No root, all Processes receive the reduced result.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Allreduce</span> <span class="params">(<span class="keyword">void</span> *sendbuf, <span class="keyword">void</span> *recvbuf, <span class="keyword">int</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">                   MPI_Datatype sendt, MPI_Op op, MPI_Comm comm)</span></span></span><br></pre></td></tr></table></figure></p>
<p><strong>Reduces</strong> elements from all send buffers, point-wise, to count single values, using <code>op</code>, storing result(s) in <strong>all</strong> receive buffers. The <code>op</code> is chosen from a <strong>predefined set</strong> (<code>MPI_SUM</code>, <code>MPI_MAX</code> etc) or <strong>constructed</strong> with user code and <code>MPI_Op_create</code>. <code>MPI_Allreduce</code> is the equivalent of doing <code>MPI_Reduce</code> followed by an <code>MPI_Bcast</code>.<img src="http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_allreduce_1.png" alt=""></p>
<p>Jacobi (1-dimensional wrapped), each neighour is owned by distinct process, thus could not read each other’s data - introduce a layer of message passing, introduce halo as buffer.<img src="/images/1_d_jacobi_MPI.png" alt="" title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// here for convenience MPI_Sendrecv combines a send and a receive.</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;p);</span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);</span><br><span class="line">    <span class="keyword">if</span> (rank == <span class="number">0</span>) read_problem(&amp;n, work); <span class="comment">// 数据存在 root - 0号进程</span></span><br><span class="line"></span><br><span class="line">    MPI_Bcast(&amp;n, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, MPI_COMM_WORLD); <span class="comment">// 广播数据</span></span><br><span class="line">    mysize = n/p;             <span class="comment">// assume p divides n, for simplicity</span></span><br><span class="line">    local = (<span class="keyword">float</span> *) <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * (mysize+<span class="number">2</span>)); <span class="comment">//include fringe/halo</span></span><br><span class="line">    MPI_Scatter(work, mysize, MPI_FLOAT, &amp;local[<span class="number">1</span>], mysize,</span><br><span class="line">                MPI_FLOAT, <span class="number">0</span>, MPI_COMM_WORLD); <span class="comment">// scatter 分发数据到各process主位置</span></span><br><span class="line">    left = (rank+p<span class="number">-1</span>)%p;      <span class="comment">// who is my left neighour?</span></span><br><span class="line">    right = (rank+<span class="number">1</span>)%p;       <span class="comment">// who is my right neighour?</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">do</span> &#123; <span class="comment">//[0]和[mysize+1]halo</span></span><br><span class="line">        MPI_Sendrecv(&amp;local[<span class="number">1</span>], <span class="number">1</span>, MPI_FLOAT, left, <span class="number">0</span>,        <span class="comment">// send this</span></span><br><span class="line">                     &amp;local[mysize+<span class="number">1</span>], <span class="number">1</span>, MPI_FLOAT, right, <span class="number">0</span>, <span class="comment">// receive this</span></span><br><span class="line">                     MPI_COMM_WORLD, &amp;status);               <span class="comment">// anti-clockwise</span></span><br><span class="line">        MPI_Sendrecv(&amp;local[mysize], <span class="number">1</span>, MPI_FLOAT, right, <span class="number">0</span>,</span><br><span class="line">                     &amp;local[<span class="number">0</span>], <span class="number">1</span>, MPI_FLOAT, left, <span class="number">0</span>,</span><br><span class="line">                     MPI_COMM_WORLD, &amp;status);               <span class="comment">// clockwise</span></span><br><span class="line">        do_one_step(local, &amp;local_error);</span><br><span class="line">        MPI_Allreduce(&amp;local_error, &amp;global_error, <span class="number">1</span>,</span><br><span class="line">                      MPI_FLOAT, MPI_MAX, MPI_COMM_WORLD);</span><br><span class="line">    &#125; <span class="keyword">while</span> (global_error &gt; acceptable_error);</span><br><span class="line"></span><br><span class="line">    MPI_Gather (&amp;local[<span class="number">1</span>], mysize, MPI_FLOAT,</span><br><span class="line">                work, mysize, MPI_FLOAT, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    <span class="keyword">if</span> (rank == <span class="number">0</span>) print_results(n, work);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Sendrecv</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span> *sendbuf, <span class="keyword">int</span> sendcount, MPI_Datatype sendtype,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> dest, <span class="keyword">int</span> sendtag,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">void</span> *recvbuf, <span class="keyword">int</span> recvcount, MPI_Datatype recvtype,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> source, <span class="keyword">int</span> recvtag,</span></span></span><br><span class="line"><span class="function"><span class="params">                MPI_Comm comm, MPI_Status *status)</span></span></span><br></pre></td></tr></table></figure>
<h3 id="Communicators"><a href="#Communicators" class="headerlink" title="Communicators"></a>Communicators</h3><p>Communicators define contexts within which groups of processes interact. All processes belong to <code>MPI_COMM_WORLD</code> from the MPI initialisation call onwards.</p>
<p>Create new communicators from old ones by collectively calling<br><code>MPI_Comm_split(MPI_Comm old, int colour, int key, MPI_Comm *newcomm)</code> to create new communicators based on <strong><code>colors</code></strong> and <strong><code>keys</code></strong>:<br><strong><code>color</code></strong> - control of subset assignment (nonnegative integer). Processes with the same color are in the same new communicator.<br><strong><code>key</code></strong> - control of rank assignment (integer).</p>
<p>Within each new communicator, processes are assigned a new rank in the range $0…p^{\prime} − 1$, where $p^{\prime}$ is the size of the new communicator. Ranks are ordered by (but not necessarily equal to) the value passed in as the <code>key</code> parameter, with ties broken by considering process rank in the parent communicator.</p>
<p>This can be helpful in expressing algorithms which contain nested structure. For example, many <strong>divide-and-conquer</strong> algorithms split the data and machine in half, process recursively within the halves, then unwind to process the recursive results back at the upper level.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Divide &amp; Conquer Communicators</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">some_DC_algorithm</span> <span class="params">( ..., MPI_Comm comm)</span> </span>&#123;</span><br><span class="line">    MPI_Comm_size(comm, &amp;p); MPI_Comm_rank(comm, &amp;myrank);</span><br><span class="line">    ... pre-recursion work ...</span><br><span class="line">    <span class="keyword">if</span> (p&gt;<span class="number">1</span>) &#123;</span><br><span class="line">        MPI_Comm_split (comm, myrank&lt;(p/<span class="number">2</span>), <span class="number">0</span>, &amp;subcomm); <span class="comment">// two sub-machines</span></span><br><span class="line">        some_DC_algorithm (..., subcomm); <span class="comment">// recursive step</span></span><br><span class="line">        <span class="comment">// in both sub-machines</span></span><br><span class="line">    &#125; <span class="keyword">else</span> do_base_case_solution_locally();</span><br><span class="line">    ... post-recursion work ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="Task-and-Pattern-Based-Models"><a href="#Task-and-Pattern-Based-Models" class="headerlink" title="Task and Pattern Based Models"></a>Task and Pattern Based Models</h2><p>Programming explicitly with threads (or processes) has some drawbacks:<br>• Natural expression of many highly parallel algorithms involves creation of far more threads than there are cores. Thread creation and scheduling have higher overheads than simpler activities like function calls (by a factor of 50-100).<br>• The OS has control over the scheduling of threads to processor cores, but it does not have the application specific knowledge required to make intelligent assignments (for example to optimize cache re-use). Traditional OS concerns for fairness may be irrelevant or even counter-productive.</p>
<p>To avoid this, programmers resort to complex scheduling and synchronization of a smaller number of coarser grained threads. How to avoid this?</p>
<p>A number of languages and libraries have emerged which<br>• separate the responsibility for identifying potential parallelism, which remains the application programmer’s job, from detailed scheduling of this work to threads and cores, which becomes the language/library run-time’s job.<br>• provide abstractions of common patterns of parallelism, which can be specialized with application specific operations, leaving implementation of the pattern and its inherent synchronization to the system.</p>
<p>These are sometimes called <strong>task based</strong> approaches, in contrast to traditional threaded models. Examples include <strong>OpenMP</strong>, which is a compiler/language based model, and Intel’s <strong>Threading Building Blocks</strong> library.</p>
<h2 id="Threading-Building-Blocks"><a href="#Threading-Building-Blocks" class="headerlink" title="Threading Building Blocks"></a>Threading Building Blocks</h2><p>Threading Building Blocks (TBB) is a shared variable model, C++ <strong>template-based</strong> library. It uses <strong>generic programming</strong> techniques to provide a collection of <strong>parallel algorithms</strong>, each of which is an <strong>abstraction of a parallel pattern</strong>. It also provides a direct mechanism for specifying task graphs and a collection of concurrent data structures and synchronization primitives.</p>
<blockquote>
<p>泛型程序设计（generic programming）是程序设计语言的一种风格或范式，允许程序员在强类型程序设计语言中编写代码时使用一些以后才指定的类型，在实例化时作为参数指明这些类型。</p>
</blockquote>
<p>It handles <strong>scheduling</strong> of tasks, whether explicit programmed or inferred from pattern calls, to a fixed number of threads internally. In effect, this is a hidden Bag-of-Tasks, leaving the OS with almost nothing to do.</p>
<p>Game of Life (<a href="http://web.stanford.edu/class/archive/cs/cs106b/cs106b.1186//assn/life.html" target="_blank" rel="noopener">cs106b 作业1</a>) Original Code for a Step<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> State &#123;DEAD,ALIVE&#125; ; <span class="comment">// cell status</span></span><br><span class="line"><span class="keyword">typedef</span> State **Grid;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">NextGen</span><span class="params">(Grid oldMap, Grid newMap)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> row, col, ncount;</span><br><span class="line">    State current;</span><br><span class="line">    <span class="keyword">for</span> (row = <span class="number">1</span>; row &lt;= MAXROW; row++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (col = <span class="number">1</span>; col &lt;= MAXCOL; col++) &#123;</span><br><span class="line">            current = oldMap[row][col];</span><br><span class="line">            ncount = NeighborCount(oldMap, row, col);</span><br><span class="line">            newMap[row][col] = CellStatus(current, ncount);</span><br><span class="line">&#125;   &#125;   &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="TBB-parallel-for"><a href="#TBB-parallel-for" class="headerlink" title="TBB parallel_for"></a>TBB <code>parallel_for</code></h3><p>假设我们想将上面的函数<code>NextGen</code>应用到数组(网格)的每个元素，这个例子是可以放心使用并行处理模式的。函数模板<code>tbb::parallel_for</code> 将此迭代空间(<code>Range</code>)分解为一个个块，并把每个块运行在不同的线程上。要并行化这个循环，第一步是将循环体转换为可以在一个块上运行的形式 - 一个STL风格的函数对象，称为<code>body</code>对象，其中由<code>operator()</code>中处理。<br>Game of Life Step Using <code>parallel_for</code><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">NextGen</span><span class="params">(Grid oldMap, Grid newMap)</span> </span>&#123;</span><br><span class="line">    parallel_for (blocked_range&lt;<span class="keyword">int</span>&gt;(<span class="number">1</span>, maxrow+<span class="number">1</span>), <span class="comment">// Range</span></span><br><span class="line">                  CompNextGen(oldMap, newMap),     <span class="comment">// Body</span></span><br><span class="line">                  affinity_partitioner());         <span class="comment">// Partitioner</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>Range</strong> defines a task(iteration) space, and its sub-division (partition) technique;<br><strong>Body</strong> defines the code which processes a range;<br><strong>Partitioner</strong> (optional parameter) influencing partitioning and scheduling strategy.</p>
<p>The <code>parallel_for</code> Template:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Range, <span class="keyword">typename</span> Body&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">parallel_for</span><span class="params">(<span class="keyword">const</span> Range&amp; range, <span class="keyword">const</span> Body &amp;body)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>Requires definition of:</p>
<ul>
<li>A <code>range</code> space to iterate over<ul>
<li>Must define a copy constructor and a destructor</li>
<li>a <strong>destructor</strong> to destroy these copies</li>
<li>Defines <code>is_empty()</code></li>
<li>Defines i<code>s_divisible()</code></li>
<li>Defines a <strong>splitting constructor</strong>, <code>R(R &amp;r, split)</code></li>
</ul>
</li>
<li>A <code>body</code> type that operates on the range (or a subrange)<ul>
<li>Must define a <strong>copy constructor</strong>, which is invoked to create a separate copy (or copies) for each worker thread.</li>
<li>Defines <code>operator()</code></li>
</ul>
</li>
</ul>
<blockquote>
<p>In the C++ programming language, a <strong>copy constructor</strong> is a special constructor for creating a new object as a copy of an existing object.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//通用形式</span></span><br><span class="line">classname (<span class="keyword">const</span> classname &amp;obj) &#123;</span><br><span class="line">   <span class="comment">// body of constructor</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//实例</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Line</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">      <span class="function"><span class="keyword">int</span> <span class="title">getLength</span><span class="params">( <span class="keyword">void</span> )</span></span>;</span><br><span class="line">      Line( <span class="keyword">int</span> len );             <span class="comment">// simple constructor</span></span><br><span class="line">      Line( <span class="keyword">const</span> Line &amp;obj);      <span class="comment">// copy constructor</span></span><br><span class="line">      ~Line();                     <span class="comment">// destructor</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">int</span> *ptr;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h4 id="Range-Class"><a href="#Range-Class" class="headerlink" title="Range Class"></a>Range Class</h4><p>A <code>blocked_range&lt;T&gt;</code> is a template class provided by the library. It describes a one-dimensional iteration space over type <code>T</code>. and be queried for the beginning (<code>r.begin()</code>) and end (<code>r.end()</code>) of the range.</p>
<p>The TBB runtime can break a <code>blocked_range</code> into two smaller ranges, each (roughly) half the size.</p>
<p>Note that a <code>blocked_range</code> carries no problem data. The values in the range can be used as we choose, for example to index into arrays.<br><strong>Range is Generic</strong>:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">R::R (<span class="keyword">const</span> R&amp;) <span class="comment">// Copy constructor</span></span><br><span class="line">R::~R()         <span class="comment">// Destructor</span></span><br><span class="line"><span class="keyword">bool</span> R::is_empty() <span class="keyword">const</span> <span class="comment">// True if range is empty</span></span><br><span class="line"><span class="keyword">bool</span> R::is_divisible() <span class="keyword">const</span> <span class="comment">// True if range can be partitioned</span></span><br><span class="line">R::R (R&amp; r, split) <span class="comment">// Splitting constructor; splits r into two subranges</span></span><br></pre></td></tr></table></figure></p>
<p>Besides the provided <code>blocked_range</code> and <code>blocked_range2d</code>, users can define their own ranges. TBB DIY Range Example: Compute Fibonacci numbers.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FibRange</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="keyword">int</span> n_ ; <span class="comment">// represents the range corresponding to fib(n)</span></span><br><span class="line">        FibRange(<span class="keyword">int</span> n) : n_(n) &#123; &#125;</span><br><span class="line">        FibRange(FibRange&amp; other, split) <span class="comment">// split constructor</span></span><br><span class="line">        : n_(other.n_ - <span class="number">2</span>) <span class="comment">// initialize the new object</span></span><br><span class="line">        &#123; other.n_ = other.n_ - <span class="number">1</span>;&#125; <span class="comment">// reuse the other range object</span></span><br><span class="line">        <span class="function"><span class="keyword">bool</span> <span class="title">is_divisible</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> (n_ &gt; <span class="number">10</span>); &#125; <span class="comment">// sequential threshold</span></span><br><span class="line">        <span class="function"><span class="keyword">bool</span> <span class="title">is_empty</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> n_ &lt; <span class="number">0</span>; &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h4 id="Body-Class"><a href="#Body-Class" class="headerlink" title="Body Class"></a>Body Class</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CompNextGen</span> &#123;</span></span><br><span class="line">    Grid oldMap, newMap;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">    CompNextGen (Grid omap, Grid nmap) : oldMap(omap), newMap(nmap) &#123;&#125;</span><br><span class="line">    <span class="comment">// 分割迭代空间的方式多种多样</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">( <span class="keyword">const</span> blocked_range&lt;<span class="keyword">int</span>&gt;&amp; r )</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> row = r.begin(); row &lt; r.end(); row++)&#123; <span class="comment">// 这里按行分割</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> col = <span class="number">1</span>; col &lt;= maxcol; col++) &#123;</span><br><span class="line">                nState current = oldMap[row][col];</span><br><span class="line">                <span class="keyword">int</span> ncount = NeighborCount(oldMap, row, col);</span><br><span class="line">                newMap[row][col] = CellStatus(current, ncount);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Body is Generic</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Body::Body(<span class="keyword">const</span> Body&amp;) \\ Copy constructor</span><br><span class="line">Body::~Body()           \\ Destructor</span><br><span class="line"><span class="keyword">void</span> Body::<span class="keyword">operator</span>() (Range&amp; subrange) <span class="keyword">const</span>  \\ Apply the body to subrange.</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>Because the body object might be copied, its <code>operator()</code> should not modify the body hence should be declared <code>const</code>. Otherwise the modification might or might not become visible to the thread that invoked parallel_for, depending upon whether <code>operator()</code> is acting on the original or a copy.<br>Credit from <a href="https://www.threadingbuildingblocks.org/docs/help/tbb_userguide/parallel_for.html" target="_blank" rel="noopener">www.threadingbuildingblocks.org</a></p>
</blockquote>
<p><code>parallel_for</code> partitions original range into subranges, and deals out subranges to worker threads in a way that: Balances load, Uses cache efficiently, and Scales.</p>
<p>Game of Life 1D with C++11 Lambda Function, an alternative interface to <code>parallel_for</code> allows us to use a C++ lambda expression to avoid writing a body class.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">NextGen</span><span class="params">(Grid oldMap, Grid newMap)</span> </span>&#123;</span><br><span class="line">    parallel_for (blocked_range&lt;<span class="keyword">int</span>&gt;(<span class="number">1</span>, maxrow+<span class="number">1</span>),</span><br><span class="line">                [&amp;](<span class="keyword">const</span> blocked_range&lt;<span class="keyword">int</span>&gt;&amp; r)&#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> row = r.begin(); row &lt; r.end(); row++)&#123;</span><br><span class="line">                        <span class="keyword">for</span> (<span class="keyword">int</span> col = <span class="number">1</span>; col &lt;= MAXCOL; col++) &#123;</span><br><span class="line">                            State current = oldMap[row][col];</span><br><span class="line">                            <span class="keyword">int</span> ncount = NeighborCount(oldMap, row, col);</span><br><span class="line">                            newMap[row][col] = CellStatus(current, ncount);</span><br><span class="line">                        &#125;                    &#125;                &#125;    );&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>[&amp;]</code>引入 lambda 表达式. 该表达式创建一个类似于<code>CompNextGen</code>的函数对象. 当局部变量在 lambda expression 之外声明，但又在lambda表达式内使用时, 它们被”捕获”为函数对象内的字段. <code>[&amp;]</code>指定引用，<code>[=]</code>指定按值捕获.</p>
<h3 id="TBB-Partitioners"><a href="#TBB-Partitioners" class="headerlink" title="TBB Partitioners"></a>TBB Partitioners</h3><p>TBB supports different partitioning strategy:<br>1, <code>tbb::parallel_for( range, body, tbb::simple_partitioner() );</code> <strong>forces</strong> all ranges to be <strong>fully partitioned</strong> (i.e. until <code>is_divisible()</code> fails).<br>2, <code>tbb::parallel_for( range, body, tbb::auto_partitioner() );</code> allows the TBB runtime to <strong>decide</strong> whether to partition the range (to improve <strong>granularity</strong>).<br>3, <code>tbb::parallel_for( range, body, tbb::affinity_partitioner );</code> is like <code>auto_partitioner()</code> but also, when the <code>parallel_for</code> is inside a loop, tries to allocate the same range to the same processor core across iterations to <strong>improve cache behaviour</strong>.</p>
<p>Game of Life Using a 2D decomposition<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">NextGen</span><span class="params">(Grid oldMap, Grid newMap)</span> </span>&#123;</span><br><span class="line">    parallel_for (blocked_range2d&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; (<span class="number">1</span>, maxrow+<span class="number">1</span>, <span class="number">1</span>, maxcol+<span class="number">1</span>), <span class="comment">// Range</span></span><br><span class="line">                  CompNextGen(oldMap, newMap));                  <span class="comment">// Body</span></span><br><span class="line">                  auto_partitioner());                           <span class="comment">// Partitioner</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CompNextGen</span> &#123;</span></span><br><span class="line">    Grid oldMap, Grid newMap;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        CompNextGen (Grid omap, Grid nmap) : oldMap(omap), newMap(nmap) &#123;&#125;</span><br><span class="line">    <span class="comment">// 二维分割</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">( <span class="keyword">const</span> blocked_range2d&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&amp; r )</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> row = r.rows().begin(); row &lt; r.rows().end(); row++)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> col = r.cols().begin(); col &lt; r.cols().end(); col++) &#123;</span><br><span class="line">                State current = oldMap[row][col];</span><br><span class="line">                <span class="keyword">int</span> ncount = NeighborCount(oldMap, row, col);</span><br><span class="line">                newMap[row][col] = CellStatus(current, ncount);</span><br><span class="line">            &#125;        &#125;    &#125;;&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>blocked_range2d</code> is partitioned in alternating dimensions, level by level.</p>
<h3 id="TBB-parallel-reduce-Template"><a href="#TBB-parallel-reduce-Template" class="headerlink" title="TBB parallel_reduce Template"></a>TBB <code>parallel_reduce</code> Template</h3><p>TBB <code>parallel_reduce</code> has similar structure to <code>parallel_for</code> but additionally allows bodies to <code>gather results</code> internally as they go along.</p>
<p>We could parallelize a loop reduction (iterations are independent), as in a Numerical Integration example, with a <code>parallel_for</code>, but we would need a <strong>critical section</strong> of some kind to accumulate the partial results. <strong><code>parallel_reduce</code></strong> structures and hides this, with one further generic operation, called <code>join</code>.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Range, <span class="keyword">typename</span> Body&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">parallel_reduce</span> <span class="params">(<span class="keyword">const</span> Range&amp; range, Body &amp;body)</span></span>;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Body::Body( <span class="keyword">const</span> Body&amp;, split ) <span class="comment">//Splitting constructor</span></span><br><span class="line">Body::~Body()                    <span class="comment">// Destructor</span></span><br><span class="line"><span class="keyword">void</span> Body::<span class="keyword">operator</span>() (Range&amp; subrange) <span class="keyword">const</span> <span class="comment">// Accumulate results from subrange</span></span><br><span class="line"><span class="keyword">void</span> Body::join( Body&amp; rhs ); <span class="comment">// Merge result of rhs into the result of this.</span></span><br></pre></td></tr></table></figure>
<p>When a worker thread is <strong>available</strong>, as decided by the task scheduler, <code>parallel_reduce</code> invokes the <code>splitting constructor</code> to create a subtask for the worker. When the subtask <strong>completes</strong>, <code>parallel_reduce</code> uses method <code>join</code> to accumulate the result of the subtask. It reuses <strong>Range</strong> concept from <code>parallel_for</code>.<br><img src="https://www.threadingbuildingblocks.org/docs/help/tbb_userguide/Images/image009.jpg" alt="" title="Graph of the Split-join Sequence. An arc indicates order in time. image from https://www.threadingbuildingblocks.org/docs/help/tbb_userguide/Images/image009.jpg"><br>The Fib Body Class (with <code>operator()</code>), using the DIY range example - <code>FibRange</code> from above<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fib</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="keyword">int</span> fsum_ ;</span><br><span class="line">        Fib() : fsum_(<span class="number">0</span>) &#123; &#125;</span><br><span class="line">        Fib(Fib&amp; other, split) : fsum_(<span class="number">0</span>) &#123; &#125;</span><br><span class="line">        <span class="comment">// use += since each body may accumulate more than one range</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span> <span class="params">(FibRange&amp; range)</span> </span>&#123; fsum_ += fib(range.n_ ); &#125;</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;<span class="keyword">if</span> (n &lt; <span class="number">2</span>) <span class="keyword">return</span> <span class="number">1</span>; <span class="keyword">else</span> <span class="keyword">return</span> fib(n<span class="number">-1</span>)+fib(n<span class="number">-2</span>);&#125;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">join</span><span class="params">(Fib&amp; rhs)</span> </span>&#123; fsum_ += rhs.fsum_; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">( <span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[] )</span> </span>&#123;</span><br><span class="line">    <span class="function">task_scheduler_init <span class="title">init</span><span class="params">(<span class="number">2</span>)</span></span>;</span><br><span class="line">    Fib f;</span><br><span class="line">    parallel_reduce(FibRange(FIBSEED), f, simple_partitioner());</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Fib "</span> &lt;&lt; FIBSEED &lt;&lt; <span class="string">" is "</span> &lt;&lt; f.fsum_ &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Using a <code>simple_partitioner</code> forces full splitting of the ranges. We could use <code>auto_partitioner</code> to let the TBB run-time system control this.</p>
<h3 id="The-Task-Scheduler"><a href="#The-Task-Scheduler" class="headerlink" title="The Task Scheduler"></a>The Task Scheduler</h3><p>如果一个算法不能自然地映射到前面提到的任何其中一种 high-level loop templates，可以使用 task scheduler 直接操作任务, 可以构建新的高级模板。</p>
<p>All of TBB’s parallel pattern constructs are implemented via the same underlying <strong>task scheduler</strong>, which executes a task graph representing the pattern.</p>
<p>TBB also allows the programmer to (carefully!) <strong>create task graphs directly</strong>. This allows expression of unstructured task graphs, or the implementation and abstraction of further patterns.</p>
<p>There are functions to create new tasks as children of existing tasks and to specify the control dependencies between them.</p>
<p>How to code Fibonacci using tasks directly? The key method is <code>task::execute</code>, which we override with our application specific behaviour.</p>
<p>Recursion is typically used to calculate Fibonacci number but leads to unbalanced task graph.</p>
<p>Fibonacci - Task Spawning Solution - Use TBB tasks to thread creation and execution of task graph:</p>
<ol>
<li>Allocate space for the task by a special “overloaded new” and method <code>task::allocate_root</code> - Create root of a task tree. Tasks must be allocated by special methods so that the space can be efficiently recycled when the task completes.</li>
<li>Construct task with the constructor <code>FibTask(n, &amp;sum)</code> invoked by <code>new</code>. When the task is run in step 3, it computes the nth Fibonacci number and stores it into <code>*sum</code>.</li>
<li>Run the task and wait for completion with <code>task::spawn_root_and_wait</code>.</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tbb/task.h"</span></span></span><br><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">ParallelFib</span><span class="params">( <span class="keyword">long</span> n )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> sum;</span><br><span class="line">    FibTask&amp; a = *<span class="keyword">new</span>(task::allocate_root()) FibTask(n, &amp;sum);</span><br><span class="line">    task::spawn_root_and_wait(a);</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FibTask</span>:</span> <span class="keyword">public</span> task &#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">long</span> n;</span><br><span class="line">        <span class="keyword">long</span>* <span class="keyword">const</span> sum;</span><br><span class="line">        FibTask( <span class="keyword">long</span> n_, <span class="keyword">long</span>* sum_ ) : n(n_), sum(sum_) &#123;&#125;</span><br><span class="line">        <span class="function">task* <span class="title">execute</span><span class="params">()</span> </span>&#123; <span class="comment">// Overrides virtual function task::execute</span></span><br><span class="line">            <span class="keyword">if</span>( n &lt; CutOff ) &#123;</span><br><span class="line">                *sum = SerialFib(n);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">long</span> x, y;</span><br><span class="line">                FibTask&amp; a = *<span class="keyword">new</span>( allocate_child() ) FibTask(n<span class="number">-1</span>,&amp;x);</span><br><span class="line">                FibTask&amp; b = *<span class="keyword">new</span>( allocate_child() ) FibTask(n<span class="number">-2</span>,&amp;y);</span><br><span class="line">                set_ref_count(<span class="number">3</span>); <span class="comment">// Set to 3  = 2 children + 1 for wait</span></span><br><span class="line">                spawn( b ); <span class="comment">// Start b running.</span></span><br><span class="line">                <span class="comment">// Start a running and wait for all children (a and b).</span></span><br><span class="line">                spawn_and_wait_for_all(a);</span><br><span class="line">                *sum = x+y; <span class="comment">// Do the sum</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>The TBB scheduler runs tasks in a way that tends to minimize both memory demands and cross-thread communication. The intuition is that a balance must be reached between depth-first and breadth-first execution.</p>
<p>At any point in execution, the collection of known tasks is maintained as a <strong>shared</strong> graph. Each thread maintains its own <strong>double-ended queue</strong> of tasks (roughly, as pointers into the shared graph).</p>
<p>Newly <strong>spawned</strong> tasks are added to the front of the local queue.</p>
<p>当一条线程参与 task graph 时，它会不断按照优先原则执行下面的规则来获取任务:</p>
<ol>
<li>looks at the <strong>front of its local queue</strong>, which encourages locality within one thread’s work; 如果 deque 为空，则此规则不适用；</li>
<li>假如失败, steal a task from the <strong>back of one other (randomly chosen) thread’s queue</strong>, which encourages stealing of big tasks, and discourages locality across threads.<br><img src="/images/TBB_Scheduler.png" alt="" title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></li>
</ol>
<h2 id="Linda"><a href="#Linda" class="headerlink" title="Linda"></a>Linda</h2><p>Linda presents an alternative conceptual model for parallelism, based around a small library of operations. The Linda model saw something of a revival in distributed java systems programming, under the name <strong>JavaSpaces</strong>.</p>
<p>The key concept is that processes interact through <strong>tuple space</strong>, a global, <strong>content addressable</strong> memory, which is thread safe, with no race conditions, therefore does not require explicit <strong>locks</strong>. Each tuple is an <strong>ordered</strong> collection of typed data fields. Duplicate tuples are allowed.</p>
<p>The tuple space itself acts like a <strong>monitor</strong>. If a process tries to access a tuplen, it is blocked until a matching tuple becomes available.</p>
<p><strong>Semaphore</strong> - Linda have tuple (or a set of tuples for a counting semaphore) that represent the locks. If someone needs to enter the lock, it waits until a tuple is available in the bag, pull it out of the bag and inserts it back into the tuple space.</p>
<p>Processes run <strong>asynchronously</strong> and can operate on tuple space with six operations.</p>
<p>1, Add new tuple to tuple space: <code>out(exp1, exp2, ...., expN);</code> - evaluates the expressions in the parameter list before <strong>atomically</strong> placing a copy of the results as a new tuple in tuple space. It could be considered as an <strong>asynchronous send with a wild-card destination</strong> in message-passing. <code>out(&quot;sum&quot;, 2, 3)</code>, <code>out(&quot;Green&quot;, x*y, square(2));</code></p>
<p>2, To take a tuple from tuple space: <code>in(tuple-template);</code> - <strong>atomically removes</strong> from tuple space a tuple which <strong>matches the template</strong>. <code>template</code> contains actual values and formal parameters (indicated by <code>?</code>) to be assigned during the match. 匹配包含与实际值的匹配，以及与形式参数类型 types 相匹配. <code>in</code> is <strong>blocking</strong>, in the sense that the caller is <strong>suspended</strong> until a matching tuple becomes available. For example: <code>in(&quot;sum&quot;,?i,?j)</code> matches <code>&quot;sum&quot;</code>, assigns <code>2</code> to <code>i</code> and <code>3</code> to <code>j</code> and the tuple is removed from the tuple space. <code>in(&quot;Green&quot;, ?y, ?r, FALSE);</code>. We could think of <code>in</code> as a <strong>blocking, asynchronous receive, with wild-card source</strong>, but with additional constraints implied by the pattern matching.</p>
<p>3, <strong>Atomically</strong> read a tuple from tuple space with <code>rd(tuple-template);</code></p>
<p>4, Tuples may also be created with <code>eval(expr, expr, ...)</code> which is like <code>out</code>, but <strong>dynamically creates new processes</strong> to evaluate each field of the tuple which has been expressed as a function call. The calling process continues immediately, and the resulting tuple enters tuple space atomically when all the newly sparked processes have terminated</p>
<p>5, Finally, there are <strong>non-blocking</strong> forms <code>inp</code>, <code>rdp</code> (<code>p</code> for “predicate”) which complete “immediately”, returning a boolean indicating whether or not a match occurred. This allow a process to carry on with a different task and then try again later.</p>
<p><strong>Bag of Tasks</strong> Implementation:<br>同样以前面的 Adaptive Quadrature 为例. Use a <code>(&quot;counts&quot;, x, y)</code> tuple, in effect as a shared variable, to count the number of tasks and number of idle workers. The final field in a task tuple indicates whether this is a real task or a “no more tasks” signal.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span> </span>&#123;</span><br><span class="line">    out(<span class="string">"total"</span>, <span class="number">0.0</span>); out(<span class="string">"counts"</span>, <span class="number">1</span>, P); <span class="comment">// set initial #task and #idle</span></span><br><span class="line">    out(<span class="string">"task"</span>, a, b, f(a), f(b), approxarea, FALSE); <span class="comment">// make initial task</span></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i&lt;P; i++) eval(worker());             <span class="comment">// create P workers</span></span><br><span class="line">    in (<span class="string">"counts"</span>, <span class="number">0</span>, P);              <span class="comment">// no tasks left, and P workers idle</span></span><br><span class="line">    in (<span class="string">"total"</span>, ?total);                             <span class="comment">// get the result</span></span><br><span class="line">    out (<span class="string">"task"</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, TRUE);      <span class="comment">// indicate no more tasks</span></span><br><span class="line">    ...                                               <span class="comment">//use total</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">worker</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        in(<span class="string">"task"</span>, ?left, ?right, ?fleft, ?fright, ?lrarea, ?gameOver);</span><br><span class="line">        <span class="keyword">if</span> (gameOver) &#123;  <span class="comment">// if gameOver == TRUE</span></span><br><span class="line">            out (<span class="string">"task"</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, TRUE); <span class="comment">// for others to see</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        in(<span class="string">"counts"</span>, ?size, ?idle); out(<span class="string">"counts"</span>, size<span class="number">-1</span>, idle<span class="number">-1</span>);</span><br><span class="line">        ... usual task calculations ...</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">abs</span> (larea + rarea - lrarea) &gt; EPSILON) &#123; <span class="comment">// create new tasks</span></span><br><span class="line">            out(<span class="string">"task"</span>, left, mid, fleft, fmid, larea, FALSE);</span><br><span class="line">            out(<span class="string">"task"</span>, mid, right, fmid, fright, rarea, FALSE);</span><br><span class="line">            in(<span class="string">"counts"</span>, ?size, ?idle); out(<span class="string">"counts"</span>, size+<span class="number">2</span>, idle+<span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            in (<span class="string">"total"</span>, ?total); out (<span class="string">"total"</span>, total+larea+rarea);</span><br><span class="line">            in(<span class="string">"counts"</span>, ?size, ?idle); out(<span class="string">"counts"</span>, size, idle+<span class="number">1</span>);</span><br><span class="line">        &#125;  &#125;    &#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>Pipeline</strong> Implementation:<br>Use <code>eval()</code> to create the sieve processes <strong>dynamically</strong> as we need them. The sieve processes eventually <strong>turn into</strong> part of an “array” of primes in tuple space. We ensure <strong>pipelined message flow by tagging</strong> tuples with their destination and position in the sequence.<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span> <span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    eval(<span class="string">"prime"</span>, <span class="number">1</span>, sieve(<span class="number">1</span>)); <span class="comment">// the 1st prime number, the 1st worker</span></span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">2</span>; i&lt;LIMIT; i++) &#123;</span><br><span class="line">        out(<span class="string">"number"</span>, <span class="number">1</span>, i<span class="number">-1</span>, i); <span class="comment">// send number to sieve</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sieve</span> <span class="params">(<span class="keyword">int</span> me)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n, p, in_seq=<span class="number">1</span>, out_seq=<span class="number">1</span>, stop=FALSE;</span><br><span class="line">    in(<span class="string">"number"</span>, me, in_seq, ?p);   <span class="comment">// in_seq = 1, first arrival is prime</span></span><br><span class="line">    <span class="keyword">while</span> (!stop) &#123;</span><br><span class="line">        in_seq++;</span><br><span class="line">        in(<span class="string">"number"</span>, me, in_seq, ?n);      <span class="comment">// get the next candidate</span></span><br><span class="line">        <span class="keyword">if</span> (n==LIMIT) &#123;</span><br><span class="line">            stop = TRUE; out(<span class="string">"number"</span>, me+<span class="number">1</span>, out_seq, n); <span class="comment">// pass on the signal</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (n%p !=<span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// if never created a successor before</span></span><br><span class="line">            <span class="keyword">if</span> (out_seq == <span class="number">1</span>) eval(<span class="string">"prime"</span>, me+<span class="number">1</span>, sieve(me+<span class="number">1</span>)); <span class="comment">// new sieve</span></span><br><span class="line">            out(<span class="string">"number"</span>, me+<span class="number">1</span>, out_seq, n);       <span class="comment">// and its first input</span></span><br><span class="line">            out_seq++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Tuple-Space"><a href="#Tuple-Space" class="headerlink" title="Tuple Space"></a>Tuple Space</h3><p>Linda’s powerful matching model sets a demanding implementation challenge, way beyond the associative memory hardware used in on-chip caches.</p>
<p><strong>Indexing</strong> and <strong>hashing</strong> techniques adapted from relational database technology can help (e.g. Linda <code>rd</code> and SQL <code>select</code>).</p>
<p>Advanced Linda implementations perform considerable compile-time analysis of program specific tuple usage. For example, possible tuples (in a given program) can be categorised into a set of classes by <strong>type signature</strong>, and stored separately.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/python-digest/" rel="next" title="Python Digest 之奇技淫巧">
                <i class="fa fa-chevron-left"></i> Python Digest 之奇技淫巧
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/UoE-sapm/" rel="prev" title="Software Architecture, Process, and Management - Informatics - University of Edinburgh 爱丁堡大学">
                Software Architecture, Process, and Management - Informatics - University of Edinburgh 爱丁堡大学 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- Go to www.addthis.com/dashboard to customize your tools -->
<div class="addthis_inline_share_toolbox">
  <script type = "text/javascript" src = "//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b35f789bd238372" async = "async" ></script>
</div>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="Cong" />
            
              <p class="site-author-name" itemprop="name">Cong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/congchan/" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:shooterbeta@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-to-parallel-computer-architecture"><span class="nav-number">1.</span> <span class="nav-text">Introduction to parallel computer architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parallel-Architecture"><span class="nav-number">2.</span> <span class="nav-text">Parallel Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Shared-Memory-Architectures"><span class="nav-number">2.1.</span> <span class="nav-text">Shared Memory Architectures</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multicomputer-architectures"><span class="nav-number">2.2.</span> <span class="nav-text">Multicomputer architectures</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parallel-Applications-and-Algorithms"><span class="nav-number">3.</span> <span class="nav-text">Parallel Applications and Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Bag-of-Tasks"><span class="nav-number">3.1.</span> <span class="nav-text">The Bag-of-Tasks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pipeline-Patterns"><span class="nav-number">3.2.</span> <span class="nav-text">Pipeline Patterns.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Interacting-Peers-Pattern"><span class="nav-number">3.3.</span> <span class="nav-text">Interacting Peers Pattern</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-Patterns"><span class="nav-number">3.4.</span> <span class="nav-text">Other Patterns</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Shared-Variable-Programming"><span class="nav-number">4.</span> <span class="nav-text">Shared Variable Programming</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mutual-Exclusion"><span class="nav-number">4.1.</span> <span class="nav-text">Mutual Exclusion</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Critical-Sections-problem"><span class="nav-number">4.1.1.</span> <span class="nav-text">Critical Sections problem</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Test-and-Set-TS-instruction"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">Test-and-Set (TS) instruction</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Lamport’s-Bakery-Algorithm"><span class="nav-number">4.1.1.2.</span> <span class="nav-text">Lamport’s Bakery Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Condition-Synchronization"><span class="nav-number">4.2.</span> <span class="nav-text">Condition Synchronization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Barrier-synchronization"><span class="nav-number">4.2.1.</span> <span class="nav-text">Barrier synchronization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Sense-Reversing-Barrier"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">Sense Reversing Barrier</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Symmetric-Barriers"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">Symmetric Barriers</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dissemination-Barriers"><span class="nav-number">4.2.1.3.</span> <span class="nav-text">Dissemination Barriers</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Structured-Primitives"><span class="nav-number">5.</span> <span class="nav-text">Structured Primitives</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Semaphores-信号灯"><span class="nav-number">5.1.</span> <span class="nav-text">Semaphores 信号灯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Semaphores-for-Critical-Section-mutual-exclusion"><span class="nav-number">5.1.1.</span> <span class="nav-text">Semaphores for Critical Section (mutual exclusion)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Semaphores-for-Barrier-Synchronisation"><span class="nav-number">5.1.2.</span> <span class="nav-text">Semaphores for Barrier Synchronisation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Semaphores-for-Producer-Consumer-Buffering"><span class="nav-number">5.1.3.</span> <span class="nav-text">Semaphores for Producer-Consumer Buffering</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Monitors"><span class="nav-number">5.2.</span> <span class="nav-text">Monitors</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Real-Shared-Variable-Programming-Systems"><span class="nav-number">6.</span> <span class="nav-text">Real Shared Variable Programming Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#POSIX-Threads-Pthread"><span class="nav-number">7.</span> <span class="nav-text">POSIX Threads (Pthread)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pthreads-semaphores"><span class="nav-number">7.1.</span> <span class="nav-text">Pthreads semaphores</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pthreads-Monitors"><span class="nav-number">7.2.</span> <span class="nav-text">Pthreads Monitors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Memory-Consistency-in-Pthreads"><span class="nav-number">7.3.</span> <span class="nav-text">Memory Consistency in Pthreads</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Java-Concurrency"><span class="nav-number">8.</span> <span class="nav-text">Java Concurrency</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Java-Threads"><span class="nav-number">8.1.</span> <span class="nav-text">Java Threads</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Java-“Monitors”"><span class="nav-number">8.2.</span> <span class="nav-text">Java “Monitors”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-java-util-concurrent-package"><span class="nav-number">8.3.</span> <span class="nav-text">The java.util.concurrent package</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Message-Passing-Programming"><span class="nav-number">9.</span> <span class="nav-text">Message Passing Programming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Message-Passing-Interface-MPI"><span class="nav-number">10.</span> <span class="nav-text">Message Passing Interface (MPI)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MPI-Primitives-6-basics-functions"><span class="nav-number">10.1.</span> <span class="nav-text">MPI Primitives (6 basics functions)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MPI-Task-Farm"><span class="nav-number">10.2.</span> <span class="nav-text">MPI Task Farm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Send-in-standard-mode"><span class="nav-number">10.3.</span> <span class="nav-text">Send in standard mode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Receive-in-standard-mode"><span class="nav-number">10.4.</span> <span class="nav-text">Receive in standard mode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prime-Sieve-Generator"><span class="nav-number">10.5.</span> <span class="nav-text">Prime Sieve Generator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Synchronization-in-MPI"><span class="nav-number">10.6.</span> <span class="nav-text">Synchronization in MPI</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Blocking-Communication-Semantics-in-MPI"><span class="nav-number">10.7.</span> <span class="nav-text">Blocking Communication Semantics in MPI</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Probing-for-Messages"><span class="nav-number">10.8.</span> <span class="nav-text">Probing for Messages</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Collective-Operations"><span class="nav-number">10.9.</span> <span class="nav-text">Collective Operations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Communicators"><span class="nav-number">10.10.</span> <span class="nav-text">Communicators</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Task-and-Pattern-Based-Models"><span class="nav-number">11.</span> <span class="nav-text">Task and Pattern Based Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Threading-Building-Blocks"><span class="nav-number">12.</span> <span class="nav-text">Threading Building Blocks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TBB-parallel-for"><span class="nav-number">12.1.</span> <span class="nav-text">TBB parallel_for</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Range-Class"><span class="nav-number">12.1.1.</span> <span class="nav-text">Range Class</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Body-Class"><span class="nav-number">12.1.2.</span> <span class="nav-text">Body Class</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TBB-Partitioners"><span class="nav-number">12.2.</span> <span class="nav-text">TBB Partitioners</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TBB-parallel-reduce-Template"><span class="nav-number">12.3.</span> <span class="nav-text">TBB parallel_reduce Template</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Task-Scheduler"><span class="nav-number">12.4.</span> <span class="nav-text">The Task Scheduler</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linda"><span class="nav-number">13.</span> <span class="nav-text">Linda</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tuple-Space"><span class="nav-number">13.1.</span> <span class="nav-text">Tuple Space</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://shootingspace.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://shukebeta.me/UoE-ppls/';
          this.page.identifier = 'UoE-ppls/';
          this.page.title = 'Parallel Programming Language and Systems - Informatics - University of Edinburgh 爱丁堡';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://shootingspace.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("KJ3aRNAv0BvPIe1SoKj9frht-gzGzoHsz", "gm1RJIiLJ5g6f6lmDxkpWzVG");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
