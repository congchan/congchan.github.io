<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="yandex-verification" content="0da69d506cf33dfe">
  <meta name="baidu-site-verification" content="Elnplp8Jq5">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"congchan.github.io","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="Word2vec Mikolov et al.">
<meta property="og:type" content="article">
<meta property="og:title" content="深入理解word2vec细节以及TensorFlow实现">
<meta property="og:url" content="https://congchan.github.io/word2vec/index.html">
<meta property="og:site_name" content="Computer Science &amp; AI">
<meta property="og:description" content="Word2vec Mikolov et al.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tensorflow.google.cn/images/softmax-nplm.png">
<meta property="og:image" content="https://tensorflow.google.cn/images/nce-nplm.png">
<meta property="article:published_time" content="2019-07-21T16:00:00.000Z">
<meta property="article:modified_time" content="2019-07-21T16:00:00.000Z">
<meta property="article:author" content="Cong">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Programming Language">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tensorflow.google.cn/images/softmax-nplm.png">


<link rel="canonical" href="https://congchan.github.io/word2vec/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>深入理解word2vec细节以及TensorFlow实现 | Computer Science & AI</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Computer Science & AI</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-represent-meanings"><span class="nav-number">1.</span> <span class="nav-text">How to represent meanings?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.</span> <span class="nav-text">优化目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Negative-sampling"><span class="nav-number">1.2.</span> <span class="nav-text">Negative sampling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8TensorFlow%E5%AE%9E%E7%8E%B0word2vec"><span class="nav-number">2.</span> <span class="nav-text">用TensorFlow实现word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-the-graph"><span class="nav-number">2.1.</span> <span class="nav-text">Building the graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-the-model"><span class="nav-number">2.2.</span> <span class="nav-text">Training the model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating-the-model"><span class="nav-number">2.3.</span> <span class="nav-text">Evaluating the model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">3.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">93</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/congchan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;congchan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/Ginntonic" title="你乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;Ginntonic" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>你乎</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/congchan" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://congchan.github.io/word2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Computer Science & AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深入理解word2vec细节以及TensorFlow实现
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-07-22 00:00:00" itemprop="dateCreated datePublished" datetime="2019-07-22T00:00:00+08:00">2019-07-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
        </span>
    </span>

  
    <span id="/word2vec/" class="post-meta-item leancloud_visitors" data-flag-title="深入理解word2vec细节以及TensorFlow实现" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/word2vec/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="word2vec/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Word2vec <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al.</a></p>
<a id="more"></a>
<h2 id="How-to-represent-meanings"><a href="#How-to-represent-meanings" class="headerlink" title="How to represent meanings?"></a>How to represent meanings?</h2><p>如何在数学上表达词义？</p>
<p>Vector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上<strong>语义相似</strong>的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis">Distributional Hypothesis</a>, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent Semantic Analysis</a>))和Predictive models(例如 <a target="_blank" rel="noopener" href="http://www.scholarpedia.org/article/Neural_net_language_models">neural net language models (NNLM)</a>)。</p>
<p>具体的区别详见<a target="_blank" rel="noopener" href="http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf">Baroni et al.</a>. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。</p>
<p>Word2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1301.3781.pdf">Mikolov et al.</a>).</p>
<p>从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words (<code>&#39;the cat sits on the&#39;</code>)预测目标单词(例如<code>&quot;mat&quot;</code>); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。</p>
<h3 id="优化目标函数"><a href="#优化目标函数" class="headerlink" title="优化目标函数"></a>优化目标函数</h3><p>NNLM 的训练是利用 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Maximum_likelihood">最大似然 maximum likelihood</a> (ML) 原则来最大化给定上文单词\(h\) (for “history”) 预测下一个词的概率 \(w_t\) (for “target”)。</p>
<p>$$<br>\begin{align}<br>P(w_t | h) &amp;= \text{softmax}(\text{score}(w_t, h)) \<br>           &amp;= \frac{\exp { \text{score}(w_t, h) } }<br>             {\sum_\text{Word w’ in Vocab} \exp { \text{score}(w’, h) } }<br>\end{align}<br>$$</p>
<p>其中 \(\text{score}(w_t, h)\) 计算 word \(w_t\) 和 context \(h\) 的相关性 (一般用点乘). 训练时，最大化</p>
<p>$$<br>\begin{align}<br> J_\text{ML} &amp;= \log P(w_t | h) \<br>  &amp;= \text{score}(w_t, h) -<br>     \log \left( \sum_\text{Word w’ in Vocab} \exp { \text{score}(w’, h) } \right).<br>\end{align}<br>$$</p>
<p>这么计算成本很高， 因为在每一训练步，需要为词汇表 \(V\) 中的每一个词汇 \(w’\) 计算在当前上下文 \(h\) 的分数概率。</p>
<p><img src="https://tensorflow.google.cn/images/softmax-nplm.png"></p>
<h3 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h3><p>但是，word2vec的目的是特征学习，而不是学习完整的概率语言模型。所以word2vec（CBOW和Skip gram一样）的训练目标函数其实是一个二分类模型(<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>)，给定一个上下文，在 \(k\) 个噪声词（根据算法选出）和一个真正的目标词汇\(w_t\)中识别出目标词\(w_t\)。如下图(以CBOW为例, Skip gram方向反过来)</p>
<p><img src="https://tensorflow.google.cn/images/nce-nplm.png"></p>
<p>目标函数变为最大化:<br>$$<br>J_\text{NEG} = \log Q_\theta(D=1 |w_t, h) + k \mathop{\mathbb{E}}_{\tilde w \sim P_n}<br>\left[ \log Q_\theta(D = 0 |\tilde w, h) \right]<br>$$</p>
<p>where \(Q_\theta(D=1 | w, h)\) is the binary logistic regression probability<br>under the model of seeing the word \(w\) in the context \(h\) in the dataset<br>\(D\), calculated in terms of the learned embedding vectors \(\theta\). In<br>practice we approximate the expectation by drawing \(k\) contrastive words<br>from the noise distribution (i.e. we compute a<br><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo average</a>).</p>
<p>负采样是指每个训练样本仅更新模型权重的一小部分：</p>
<p>负采样的选择是基于 unigram 分布 $f(w_i)$: 一个词作为负面样本被选择的概率与其出现的频率有关，更频繁的词更可能被选作负面样本。<br>$$<br>P(w_i) = \frac{  {f(w_i)}^{3/4}  }{\sum_{j=0}^{n}\left(  {f(w_j)}^{3/4} \right) }<br>$$<br>负采样带来的好处是</p>
<ol>
<li>训练速度不再受限于 vocabulary size</li>
<li>能够并行实现</li>
<li>模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。<br>模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。</li>
</ol>
<p>每个词由两个向量表示：</p>
<ol>
<li>$v_w$, 表示这个词作为中心词 (Focus Word) 时的样子。</li>
<li>$u_w$, 表示它作为另一个中心词的上下文 (Context Word) 时的样子。</li>
</ol>
<p>这样, 对于一个中心词 $c$ 和外围词$o$:<br>$$<br>P(o|c) = \frac{exp(u^T_o v_c)}{\sum_{w \in V} \left( {exp(u^T_w v_c)} \right)}<br>$$</p>
<p>在C语言的源代码里，这些向量由两个数组 (Array) 分别负责：<br><code>syn0</code>数组，负责某个词作为中心词时的向量。是<strong>随机初始化</strong>的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369</span></span><br><span class="line"><span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++) &#123;</span><br><span class="line">     next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>)<span class="number">25214903917</span> + <span class="number">11</span>;</span><br><span class="line">     syn0[a * layer1_size + b] =</span><br><span class="line">        (((next_random &amp; <span class="number">0xFFFF</span>) / (real)<span class="number">65536</span>) - <span class="number">0.5</span>) / layer1_size;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p><code>syn1neg</code>数组，负责这个词作为上下文时的向量。是<strong>零初始化</strong>的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L365</span></span><br><span class="line"><span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++)</span><br><span class="line">   syn1neg[a * layer1_size + b] = <span class="number">0</span>;</span><br></pre></td></tr></table></figure>
<p>训练时，先选出一个中心词。在正、负样本训练的时候，这个中心词就保持不变 (Constant) 了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L226</span></span><br><span class="line"><span class="comment"># Negative sampling.</span></span><br><span class="line">sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(</span><br><span class="line">    true_classes=labels_matrix,</span><br><span class="line">    num_true=<span class="number">1</span>,</span><br><span class="line">    num_sampled=opts.num_samples,</span><br><span class="line">    unique=<span class="literal">True</span>,</span><br><span class="line">    range_max=opts.vocab_size,</span><br><span class="line">    distortion=<span class="number">0.75</span>,</span><br><span class="line">    unigrams=opts.vocab_counts.tolist()))</span><br></pre></td></tr></table></figure>


<h2 id="用TensorFlow实现word2vec"><a href="#用TensorFlow实现word2vec" class="headerlink" title="用TensorFlow实现word2vec"></a>用TensorFlow实现word2vec</h2><p>Negative Sampling 可以利用原理近似的 <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf">noise-contrastive estimation (NCE) loss</a>, 已经在TF的<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/nn/nce_loss">tf.nn.nce_loss()</a>实现了.</p>
<h3 id="Building-the-graph"><a href="#Building-the-graph" class="headerlink" title="Building the graph"></a>Building the graph</h3><p>初始化一个在<code>-1: 1</code>之间随机均匀分布的矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">embeddings = tf.Variable(</span><br><span class="line">    tf.random_uniform([vocabulary_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br></pre></td></tr></table></figure>
<p>NCE loss 依附于 logistic regression 模型。为此, 我们需要定义词汇中每个单词的weights和bias。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nce_weights = tf.Variable(</span><br><span class="line">  tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                      stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">nce_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br></pre></td></tr></table></figure>
<p>参数已经就位, 接下来定义模型图.</p>
<p>The skip-gram model 有两个输入. 一个是以word indice表达的一个batch的context words, 另一个是目标单词。为这些输入创建placeholder节点, 以便后续馈送数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Placeholders for inputs</span></span><br><span class="line">train_inputs = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>利用<code>embedding_lookup</code>来高效查找word indice对应的vector.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br></pre></td></tr></table></figure>
<p>使用NCE作为训练目标函数来预测target word:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute the NCE loss, using a sample of the negative labels each time.</span></span><br><span class="line">loss = tf.reduce_mean(</span><br><span class="line">  tf.nn.nce_loss(weights=nce_weights,</span><br><span class="line">                 biases=nce_biases,</span><br><span class="line">                 labels=train_labels,</span><br><span class="line">                 inputs=embed,</span><br><span class="line">                 num_sampled=num_sampled,</span><br><span class="line">                 num_classes=vocabulary_size))</span><br></pre></td></tr></table></figure>
<p>然后添加计算梯度和更新参数等所需的节点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We use the SGD optimizer.</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">1.0</span>).minimize(loss)</span><br></pre></td></tr></table></figure>

<h3 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h3><p>使用<code>feed_dict</code>推送数据到<code>placeholders</code>, 调用<code>tf.Session.run</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> generate_batch(...):</span><br><span class="line">  feed_dict = &#123;train_inputs: inputs, train_labels: labels&#125;</span><br><span class="line">  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)</span><br></pre></td></tr></table></figure>

<h3 id="Evaluating-the-model"><a href="#Evaluating-the-model" class="headerlink" title="Evaluating the model"></a>Evaluating the model</h3><p>Embeddings 对于 NLP 中的各种下游预测任务非常有用。可以利用analogical reasoning, 也就是预测句法和语义关系来简单而直观地评估embeddings, 如<code>king is to queen as father is to ?</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L292</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_eval_graph</span>(<span class="params">self</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Build the eval graph.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># Eval graph</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Each analogy task is to predict the 4th word (d) given three</span></span><br><span class="line">  <span class="comment"># words: a, b, c.  E.g., a=italy, b=rome, c=france, we should</span></span><br><span class="line">  <span class="comment"># predict d=paris.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># The eval feeds three vectors of word ids for a, b, c, each of</span></span><br><span class="line">  <span class="comment"># which is of size N, where N is the number of analogies we want to</span></span><br><span class="line">  <span class="comment"># evaluate in one batch.</span></span><br><span class="line">  analogy_a = tf.placeholder(dtype=tf.int32)  <span class="comment"># [N]</span></span><br><span class="line">  analogy_b = tf.placeholder(dtype=tf.int32)  <span class="comment"># [N]</span></span><br><span class="line">  analogy_c = tf.placeholder(dtype=tf.int32)  <span class="comment"># [N]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalized word embeddings of shape [vocab_size, emb_dim].</span></span><br><span class="line">  nemb = tf.nn.l2_normalize(self._emb, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Each row of a_emb, b_emb, c_emb is a word&#x27;s embedding vector.</span></span><br><span class="line">  <span class="comment"># They all have the shape [N, emb_dim]</span></span><br><span class="line">  a_emb = tf.gather(nemb, analogy_a)  <span class="comment"># a&#x27;s embs</span></span><br><span class="line">  b_emb = tf.gather(nemb, analogy_b)  <span class="comment"># b&#x27;s embs</span></span><br><span class="line">  c_emb = tf.gather(nemb, analogy_c)  <span class="comment"># c&#x27;s embs</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># We expect that d&#x27;s embedding vectors on the unit hyper-sphere is</span></span><br><span class="line">  <span class="comment"># near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].</span></span><br><span class="line">  target = c_emb + (b_emb - a_emb)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Compute cosine distance between each pair of target and vocab.</span></span><br><span class="line">  <span class="comment"># dist has shape [N, vocab_size].</span></span><br><span class="line">  dist = tf.matmul(target, nemb, transpose_b=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># For each question (row in dist), find the top 4 words.</span></span><br><span class="line">  _, pred_idx = tf.nn.top_k(dist, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Nodes for computing neighbors for a given word according to</span></span><br><span class="line">  <span class="comment"># their cosine distance.</span></span><br><span class="line">  nearby_word = tf.placeholder(dtype=tf.int32)  <span class="comment"># word id</span></span><br><span class="line">  nearby_emb = tf.gather(nemb, nearby_word)</span><br><span class="line">  nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=<span class="literal">True</span>)</span><br><span class="line">  nearby_val, nearby_idx = tf.nn.top_k(nearby_dist,</span><br><span class="line">                                       min(<span class="number">1000</span>, self._options.vocab_size))</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/tutorials/representation/word2vec">https://tensorflow.google.cn/tutorials/representation/word2vec</a></li>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf">Learning word embeddings efficiently with noise-contrastive estimation</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Programming-Language/" rel="tag"># Programming Language</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/NLP-word-lattice/" rel="prev" title="Word Lattice">
                  <i class="fa fa-chevron-left"></i> Word Lattice
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/NLP-attention-01/" rel="next" title="从头理解注意力机制">
                  从头理解注意力机制 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cong Chan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b35f789bd238372" async="async"></script>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="/js/local-search.js"></script>






  


<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      const visitors = document.querySelector('.leancloud_visitors');
      const url = decodeURI(visitors.id);
      const title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            const counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      const visitors = document.querySelectorAll('.leancloud_visitors');
      const entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            const target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    const { app_id, app_key, server_url } = {"enable":true,"app_id":"KJ3aRNAv0BvPIe1SoKj9frht-gzGzoHsz","app_key":"gm1RJIiLJ5g6f6lmDxkpWzVG","server_url":null,"security":true};
    function fetchData(api_server) {
      const Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    const api_server = app_id.slice(-9) === '-MdYXbMMI' ? `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com` : server_url;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


  


<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://shootingspace.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://congchan.github.io/word2vec/";
    this.page.identifier = "word2vec/";
    this.page.title = "深入理解word2vec细节以及TensorFlow实现";
    };
  NexT.utils.loadComments('#disqus_thread', () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://shootingspace.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
