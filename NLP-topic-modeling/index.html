<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="yandex-verification" content="0da69d506cf33dfe">
  <meta name="baidu-site-verification" content="Elnplp8Jq5">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"congchan.github.io","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="本篇介绍 topic modeling, 以及一个经典的算法Latent Dirichlet allocation, 文本挖掘与语义理解的集大成者(至少在深度学习统治之前). 当然LDA不仅仅局限于文本, 还可应用于涉及大量数据集的各种问题，包括协同过滤，基于内容的图像检索和生物信息学等领域的数据。">
<meta property="og:type" content="article">
<meta property="og:title" content="Topic Modelling - 主题建模以及隐变量模型">
<meta property="og:url" content="https://congchan.github.io/NLP-topic-modeling/index.html">
<meta property="og:site_name" content="Computer Science &amp; AI">
<meta property="og:description" content="本篇介绍 topic modeling, 以及一个经典的算法Latent Dirichlet allocation, 文本挖掘与语义理解的集大成者(至少在深度学习统治之前). 当然LDA不仅仅局限于文本, 还可应用于涉及大量数据集的各种问题，包括协同过滤，基于内容的图像检索和生物信息学等领域的数据。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://congchan.github.io/images/document_term.png">
<meta property="og:image" content="https://congchan.github.io/images/ducument_topic_term.png">
<meta property="og:image" content="https://congchan.github.io/images/lsa_process.png">
<meta property="og:image" content="https://congchan.github.io/images/plsa_illustrations.png">
<meta property="og:image" content="https://congchan.github.io/images/topic_models_geometric_interpretation.png">
<meta property="article:published_time" content="2018-12-22T16:00:00.000Z">
<meta property="article:modified_time" content="2018-12-22T16:00:00.000Z">
<meta property="article:author" content="Cong">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://congchan.github.io/images/document_term.png">


<link rel="canonical" href="https://congchan.github.io/NLP-topic-modeling/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>Topic Modelling - 主题建模以及隐变量模型 | Computer Science & AI</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Computer Science & AI</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Topic-Modelling"><span class="nav-number">1.</span> <span class="nav-text">Topic Modelling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Latent-Semantic-Analysis-LSA"><span class="nav-number">1.1.</span> <span class="nav-text">Latent Semantic Analysis (LSA)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Probabilistic-Latent-Semantic-Analysis-PLSA"><span class="nav-number">1.2.</span> <span class="nav-text">Probabilistic Latent Semantic Analysis (PLSA)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Latent-Dirichlet-allocation-LDA"><span class="nav-number">1.3.</span> <span class="nav-text">Latent Dirichlet allocation(LDA)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LDA%E6%8E%A8%E7%90%86%E5%92%8C%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="nav-number">1.4.</span> <span class="nav-text">LDA推理和参数估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LDA%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8"><span class="nav-number">1.5.</span> <span class="nav-text">LDA的实际应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">2.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">93</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/congchan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;congchan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/Ginntonic" title="你乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;Ginntonic" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>你乎</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/congchan" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://congchan.github.io/NLP-topic-modeling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Computer Science & AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Topic Modelling - 主题建模以及隐变量模型
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2018-12-23 00:00:00" itemprop="dateCreated datePublished" datetime="2018-12-23T00:00:00+08:00">2018-12-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span id="/NLP-topic-modeling/" class="post-meta-item leancloud_visitors" data-flag-title="Topic Modelling - 主题建模以及隐变量模型" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/NLP-topic-modeling/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="NLP-topic-modeling/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本篇介绍 topic modeling, 以及一个经典的算法Latent Dirichlet allocation, 文本挖掘与语义理解的集大成者(至少在深度学习统治之前). 当然LDA不仅仅局限于文本, 还可应用于涉及大量数据集的各种问题，包括协同过滤，基于内容的图像检索和生物信息学等领域的数据。</p>
<a id="more"></a>

<h2 id="Topic-Modelling"><a href="#Topic-Modelling" class="headerlink" title="Topic Modelling"></a>Topic Modelling</h2><p>大规模文本挖掘的核心问题, 就是用数学模型代替人力来理解文本语义，目标是找到对集合成员（如一堆文本）的数学/统计描述，以便能够对这些大型集合进行高效处理，同时保留对基本任务（如分类，检测，摘要以及相似性和相关性判断）有用的基本统计关系。</p>
<p>在这方面的研究方法很多，特别是信息检索(IR)领域. 一个基本方法是将语料库中的每个文档向量化，向量中的每个实数代表计数率。比如经典的tf-idf方法，用<strong>Document-Term Matrix</strong>来表达不同词在不同文档出现的情况差异, 一般term就是word作为features, 所以在这里我们表示document-word matrix(DWM), 就是<code>DWM[i][j] = The number of occurrences of word_j in document_i</code>.<br>Doc 1: I have a fluffy cat.<br>Doc 2: I see a fluffy dog. </p>
<table>
<thead>
<tr>
<th>DWM</th>
<th>I</th>
<th>have</th>
<th>a</th>
<th>fluffy</th>
<th>cat</th>
<th>see</th>
<th>dog</th>
</tr>
</thead>
<tbody><tr>
<td>doc1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>doc2</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<p>然后进行normalization, 去和 inverse document frequency count(IDF)进行比较. IDF统计每个词在整个文档集合中出现的总次数, 通常转化为log scale, 并进行适当的normalization. </p>
<p><img src="/images/document_term.png" title="image from https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"></p>
<p>这个矩阵把文档表示为向量，使得不同文档之间可以从几何上衡量相似性，根据相似性<strong>聚类文本</strong>.</p>
<p>虽然tf-idf有很多很好的特性, 但是它的降维程度非常有限, 而且无法揭示文档间或文档内的统计结构.</p>
<p>比如我们需要知道文本包含什么<strong>信息</strong>, 却又不清楚什么信息是重要的, 所以我们希望能把信息也归纳成几类。我们称这种信息为<strong>主题</strong>, 一种粗粒度的信息. 那么就有了一个很重要的任务, 就是挖掘出这一堆文本包含的主题都有哪几大类. 每个文本都可能包含多种不同主题, 而且包含的侧重也不一样, 所以进一步的, 我们希望能够挖掘出每个文本的主题分布, 也就是主题类别在各个文本中的权重. 这种对文本信息的挖掘和理解方法, 称之为<strong>主题建模(Topic Modelling)</strong>.</p>
<p>此时主题数量就是一个超参数, 通过主题建模，构建了单词的clusters而不是文本的clusters。因此，文本被表达为多个主题的混合，每个主题都有一定的权重。</p>
<p>因为主题建模不再是用词频来表达, 而是用主题权重<code>&#123;Topic_i: weight(Topic_i, T) for Topic_i in Topics&#125;</code>, 所以主题建模也是一种 Dimensionality Reduction. </p>
<p>主题建模也可以理解为文本主题的tagging任务, 只是无监督罢了.</p>
<h3 id="Latent-Semantic-Analysis-LSA"><a href="#Latent-Semantic-Analysis-LSA" class="headerlink" title="Latent Semantic Analysis (LSA)"></a>Latent Semantic Analysis (LSA)</h3><p>通过引入Latent的概念，把主题表达为隐藏的信息, 也就是假设主题已经存在, 只是我们看不到. LSA使用DWM矩阵的SVD奇异值分解来确定tf-idf特征空间中的线性子空间，该子空间捕获了集合中的大部分variance。</p>
<ol>
<li>假设单词使用中存在一些latent的结构, 由于单词选择的多样性而被掩盖了.</li>
<li>与其将文本表示为单词的t维空间中的向量，不如将相似词有效地组合在一起的”概念”(topic), 作为维度, 将文本（以及词本身）表示为低维空间中的向量. 这些低维的轴就是通过PCA得出的Principal Components</li>
<li>然后就可以在 latent semantic 空间中为下游任务服务, 如计算文本相似度(通过内积等cosine相似度计算).</li>
</ol>
<p><img src="/images/ducument_topic_term.png" title="image from https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"></p>
<p>把DWM矩阵表达为 DTM(Document Topic Matrix) 和 TWM(Topic Word Matrix) 两个矩阵, 它们维度更小，相乘的结果应该尽可能近似原始的DWM矩阵。</p>
<p>假设词汇$V$有$1024$个, 文档$W$有$64$篇, 用 $DWM = W \times V$来表达Document-Word Matrix, 需要$64 \times 1024 = 65,536$的参数量. 假如我们设定topic参数为$8$, 那么就可以用$DWM = DTM \times TWM $来近似表达Document-Term Matrix, 参数量减少为$64 \times 8 + 8 \times 1024 = 8,704$, 缩减了将近$90%$</p>
<p>所以LSA核心思想是构造 Document-Term Matrix 的低阶近似.</p>
<ol>
<li>用 tf-idf 计算加权DWM. tf-idf(term frequency–inverse document frequency)是DWM矩阵的一种经典加权表达。</li>
<li>然后对DWM矩阵进行 Singular Value Decomposition (SVD).</li>
</ol>
<p><img src="/images/lsa_process.png" title="https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/plsa.pdf"></p>
<p>降维的意义不仅仅是减少下游任务的计算负担：</p>
<ol>
<li>tf-idf向量一般很长很庞大。因此降维操作对于聚类或分类等进一步计算是能节省很多资源。</li>
<li>原始DTM矩阵被认为是有噪声的：近似矩阵被解释为去噪矩阵（比原始矩阵更好的矩阵）。</li>
<li>假定原始的DTM矩阵相对于“真实的”DTM矩阵过于稀疏，降维也可以看作一种泛化。也就是说，原始矩阵仅列出每个文档中实际的单词，而我们可能会对与每个文档相关的所有单词感兴趣-如同义词等。</li>
</ol>
<p><code>(car), (truck), (flower)&#125; --&gt; &#123;(1.3452 * car + 0.2828 * truck), (flower)&#125;</code></p>
<h3 id="Probabilistic-Latent-Semantic-Analysis-PLSA"><a href="#Probabilistic-Latent-Semantic-Analysis-PLSA" class="headerlink" title="Probabilistic Latent Semantic Analysis (PLSA)"></a>Probabilistic Latent Semantic Analysis (PLSA)</h3><p>也称之为aspect model, 尝试从统计学的角度改进LSA. 将文档中的每个单词建模为混合模型中的样本. 其中这个混合模型混合的成分是multinomial随机变量，可以视为“主题”的表示形式。因此，每个单词都是由单个主题生成的，每一个文档中的不同单词可能从不同的主题生成。每个文档都表示为这些混合成分根据不同比例混合的列表，从而简化为固定主题集的概率分布.</p>
<p>把潜在的topics视作 Latent variable 隐变量z, 而文本Documents和词汇Words就是观察变量 observed variables. 共现(co‐occurrence)的数据都关联有隐含的话题类别, 做出条件独立假设, D和W是基于隐变量z的条件独立变量,<br>$$ P(w|d) = \sum_{z\in Z} P(w|z)P(z|d)$$</p>
<p>$$ P(d, w) = P(d)P(w|d) = P(d) \sum_{z\in Z} P(w|z)P(z|d) \<br>           = \sum_{z\in Z} P(d) P(w|z)P(z|d) \<br>           = \sum_{z\in Z} P(z) P(w|z)P(d|z)<br>$$</p>
<p><img src="/images/plsa_illustrations.png" title="https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/plsa.pdf"></p>
<p>利用隐变量可以解决稀疏性问题，也就是避免文档中未出现的word的概率为零, 可以理解为一种平滑.</p>
<p>使用EM算法:<br>E-Step: 计算隐变量的posterior probabilities, </p>
<ul>
<li>$P(z|d, w) = \frac{ P(z) P(w|z)P(d|z) }{ \sum_{z’\in Z} P(z’) P(w|z’)P(d|z’) }$</li>
</ul>
<p>M-Step: 更新参数</p>
<ul>
<li>$P(w|z) \propto \sum_{d \in D} n(d, w) P(z|d, w) $ </li>
<li>$P(d|z) \propto \sum_{w \in W} n(d, w) P(z|d, w) $ </li>
<li>$P(z) \propto \sum_{d \in D}\sum_{w \in W} n(d, w) P(z|d, w) $ </li>
</ul>
<p>PLSA是一种生成式的概率模型. PLSA的$P(w, d)$可以解释为LSA中的$P = U Σ V^T$, 其中$U$包含$P(d|z)$, $Σ$作为$P(z)$的对角矩阵, $V$包含$P(w|z)$ </p>
<p>PLSA有助于处理多义词(Polysemous words), 通过$P(w|z)$排序比较, 比如<code>SEGMENT</code>在topic1中更靠近<code>image</code>, 意味着<code>Image region</code>; 在topic2中更靠近<code>sound</code>, 意味着<code>Phonetic segment</code>.</p>
<p>虽然PPCA也是概率模型, 但是PPCA假设了正态分布(normal distribution), 局限性很大. PLSA将每个共现的概率建模为条件独立的多项式分布(multinomial distributions)的混合. 多项式分布在此领域是更好的选择。</p>
<p>因为有了$p(z|d)$充当特定文档的主题混合权重, pLSA可以捕获文档包含多个主题的可能性. 但是, $d$是训练集文档列表的虚拟索引, 因此$d$是一个多项式随机变量，其值可能与训练文档一样多, 这样pLSA仅针对训练集的文档学习主题混合$p(z|d)$, 对于训练集之外的document而言, 不知道如何分配概率. 等于说pLSA并不是一个定义明确的文档级别的概率生成模型。</p>
<p>除此之外, 因为使用训练集文档索引的分布，另一个困难就是需要估计的参数数量随着训练集数量增加而线性增加. 具体地说, 一个k-topic pLSA的参数是$k$个latent topic上大小为$V$的多项式分布, 以及$M$个mixtures, 参数量是$kV + kM$，随着$M$线性增长。所以pLSA容易过拟合(虽然可以用Tempered EM算法来稍微缓解). </p>
<h3 id="Latent-Dirichlet-allocation-LDA"><a href="#Latent-Dirichlet-allocation-LDA" class="headerlink" title="Latent Dirichlet allocation(LDA)"></a>Latent Dirichlet allocation(LDA)</h3><p>回顾LSA和pLSA, 都基于“词袋”的假设。从概率论的角度来说，是对文档中单词有exchangeability的假设（Aldous，1985）。此外，尽管很少正式地陈述这些方法，但这些方法还假定文档是可交换的。语料库中文档的特定顺序也可以忽略。Finetti（1990）提出的经典表示定理认为任何可交换随机变量的集合都具有表示为混合分布的形式 - 通常是无限混合。因此，如果我们希望考虑文档和单词的可交换表示形式，则需要考虑能够同时捕获单词和文档的可交换性的混合模型。这就是<a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Latent Dirichlet allocation. David Blei, Andrew Ng, and Michael Jordan. 2003.</a>这篇文章的动机.</p>
<p>LDA对主题分布的基本设定是, 每个文档被表达为latent variables(topics)的随机混合, 其中各个topics可以由单词的概率分布来描述.</p>
<p>对于语料库$D$中的每个文档$\boldsymbol{w}$, LDA假设如下的生成过程:</p>
<ol>
<li>选择参数$N ∼ Poisson(ξ)$,</li>
<li>用Dirichlet分布$Dir(\alpha)$生成一个多项式分布参数$θ$, 即$p(θ|\alpha)$</li>
<li>对于文档中的每一个词$w_n$:<ol>
<li>基于多项式概率分布$Multinomial(θ)$选择一个topic$z_n$, 即$p(z_n |θ)$</li>
<li>基于$p(w_n | z_n, β)$, 即以topic $z_n$为条件的multinomial概率, 选择一个词$w_n$</li>
</ol>
</li>
</ol>
<p>这个过程做了几个假设. 一个是, $\beta$作为单词概率的参数, 是一个$k \times V$的矩阵, $\beta_{ij} = p(w^j = 1 | z^i = 1)$, 是需要估计的固定变量. 这里要注意，$N$是独立于所有其他数据生成变量（$θ$和$z$）, 因此是一个辅助变量，通常会忽略它的随机性。其余的假设有兴趣可以去读论文.</p>
<p>给定了参数$\alpha$和$\beta$, 可以估计topic mixture θ，一组$N$个主题$\boldsymbol{z}$和一组$N$个单词$\boldsymbol{w}$的联合分布: $$ p(θ,\boldsymbol{z}, \boldsymbol{w}|α,β) = p(θ|α) \prod^N_{n=1} p(z_n | \theta) p(w_n|z_n, \beta) $$</p>
<p>$p(z_n |θ)$在这里就是第$i$个$\theta_i$, 这个独特的$i$使得$z^i_n = 1$. 沿着$θ$求积分并在$z$上求和，得到文档的 marginal distribution $$p(\boldsymbol{w}|α,β) = \int p(θ|α) \Bigg( \prod\limits^N_{n=1} \sum\limits_{z_n} p(z_n | \theta) p(w_n|z_n, \beta)  \Bigg) d\theta$$</p>
<p>最后，取各个文档的marginal distribution的乘积，得到整个语料库(corpus)的概率 $$ p(D|α,β)  = \prod\limits^M_{d=1} p(\boldsymbol{w}|α,β) $$</p>
<p>参数$\alpha$和$\beta$是语料库级别的参数，假定在生成语料库的过程中只采样一次。$\theta_d$是文档级别的变量, 每个文档采样一次. $z_{dn}$和$w_{dn}$是词级别的变量, 每个文档的每个词采样一次.</p>
<p>LDA通过将主题混合权重视为k-parameter隐随机变量，而不是与训练集显式关联的一大套参数, 解决pLSA的缺陷。而且k-topic的LDA模型参数量是$k + kV$, 不会随着训练语料库的增加而增长.</p>
<p>如果在几何上比较和理解pLSA和LDA, 模型都可以视为在words的分布空间上操作, 每个这样的分布都可以看作<code>(V-1)-simplex</code>(称之为word simplex)上的一个点. 如图, 假设有<code>3</code>个单词, 假设选择<code>k=3</code>的topic simplex包含在三个单词的word simplex中。word simplex的三个角对应于三个特殊的分布(<code>[1, 0, 0], [0, 1, 0], [0, 0, 1]</code>)，即其中各有一个单词的概率为<code>1</code>。topic simplex的三个点对应于三个不同的单词分布(比如类似<code>[0.7, 0.2, 0.1], [0.05, 0.9, 0.05], [0.3, 0.05, 0.65]</code>)。<img src="/images/topic_models_geometric_interpretation.png" title="image from Blei 2003"></p>
<p>最简单的unigram模型在word simplex上找到一个点，并假设语料库中的所有单词都来自相应的分布。而隐变量模型考虑(选择)word simplex上的<code>k</code>个点(在图中是<code>k=3</code>个)，并基于这些点形成sub-simplex，即topic simplex。</p>
<ul>
<li>pLSA模型假定训练集文档的每个单词各来自一个随机选择的主题。主题本身来自document-specific的主题分布，即topic simplex上的一个个点<code>x</code>。每个文档都有一个这样的分布；因此，文档训练集定义了关于topic simplex的经验分布。</li>
<li>LDA假定，不管是训练集还是测试集的文档, 每个单词都是由随机选择的主题生成的，该主题是从一个以随机选择的$θ_d$为参数的分布中得出的。参数$θ_d$的采样方法是每个文档采样一个topic simplex的平滑分布, 就是图中的等高线。</li>
</ul>
<h3 id="LDA推理和参数估计"><a href="#LDA推理和参数估计" class="headerlink" title="LDA推理和参数估计"></a>LDA推理和参数估计</h3><p>LDA推理关键的一步是计算给定的一个文档的隐变量的后验分布(posterior distribution): $$<br>p(θ, \boldsymbol{z} | \boldsymbol{w}, α, β) = \frac{p(θ, \boldsymbol{z}, \boldsymbol{w} | α, β)}{p( \boldsymbol{w} | α, β)} $$</p>
<p>其中的$p(\boldsymbol{w}|α,β)$由于latent topics的求和中$θ$和$β$之间的耦合而变得很难求解(Dickey, 1983). 尽管因为后验分布导致精确的推理是很难，但对于LDA，可以考虑使用各种近似算法，包括Laplace逼近，变分(variational)逼近和Markov chain Monte Carlo(Jordan, 1999)。</p>
<p>论文中介绍了一种convexity-based variational inference方法, 基本思想是利用Jensen’s inequality获得log likelihood的可调下限(Jordan, et al., 1999)</p>
<p>使用迭代逼近来计算DLA模型：</p>
<ol>
<li>初始化：每个单词随机分配给一个主题。</li>
<li>循环遍历每个单词，基于以下信息将单词重新分配给一个主题：</li>
</ol>
<ul>
<li>training: repeat until converge<ol>
<li>assign each word in each document to one of T topics.</li>
<li>For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t)</li>
<li>Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d)</li>
</ol>
</li>
</ul>
<h3 id="LDA的实际应用"><a href="#LDA的实际应用" class="headerlink" title="LDA的实际应用"></a>LDA的实际应用</h3><p><a target="_blank" rel="noopener" href="https://github.com/congchan/Chinese-nlp/blob/master/latent-dirichlet-allocation-topic-model.ipynb">LDA模型实战案例</a></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>主题建模的算法:</p>
<ol>
<li>(p)LSA: (Probabilistic) Latent Semantic Analysis – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra. SVD假设了Gaussian distributed. </li>
<li>LDA: latent Dirichlet allocation, 假设了multinomial distribution。<blockquote>
<p>LDA是pLSA的generalization, LDA的hyperparameter设为特定值的时候，就specialize成 pLSA 了。从工程应用价值的角度看，这个数学方法的generalization，允许我们用一个训练好的模型解释任何一段文本中的语义。而pLSA只能理解训练文本中的语义。（虽然也有ad hoc的方法让pLSA理解新文本的语义，但是大都效率低，并且并不符合pLSA的数学定义。）这就让继续研究pLSA价值不明显了。</p>
</blockquote>
</li>
<li>NMF – Non-Negative Matrix Factorization</li>
<li>Generalized matrix decomposition 实际上是 collaborative filtering 的 generalization，是用户行为分析和文本语义理解的共同基础.</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://nlpforhackers.io/topic-Modelling/">https://nlpforhackers.io/topic-Modelling/</a></li>
<li><a target="_blank" rel="noopener" href="http://cocosci.berkeley.edu/tom/papers/SteyversGriffiths.pdf">Steyvers and Griffiths (2007)</a>. Probabilistic topic models. Distributional semantic models and topic models have been extensively investigated not just in NLP, but also as models of human cognition. This paper provides a brief introduction to topic models as cognitive models. A much more thorough investigation can be found in Griffiths, Steyvers, and Tenenbaum (2007).</li>
<li><a target="_blank" rel="noopener" href="http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/">Latent Semantic Analysis (LSA) for Text Classification Tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158">Intuitive Guide to Latent Dirichlet Allocation</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/NLP-HMM-CRF/" rel="prev" title="概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归">
                  <i class="fa fa-chevron-left"></i> 概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/NLP-word-lattice/" rel="next" title="Word Lattice">
                  Word Lattice <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cong Chan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b35f789bd238372" async="async"></script>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="/js/local-search.js"></script>






  


<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      const visitors = document.querySelector('.leancloud_visitors');
      const url = decodeURI(visitors.id);
      const title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            const counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      const visitors = document.querySelectorAll('.leancloud_visitors');
      const entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            const target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    const { app_id, app_key, server_url } = {"enable":true,"app_id":"KJ3aRNAv0BvPIe1SoKj9frht-gzGzoHsz","app_key":"gm1RJIiLJ5g6f6lmDxkpWzVG","server_url":null,"security":true};
    function fetchData(api_server) {
      const Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    const api_server = app_id.slice(-9) === '-MdYXbMMI' ? `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com` : server_url;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://shootingspace.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://congchan.github.io/NLP-topic-modeling/";
    this.page.identifier = "NLP-topic-modeling/";
    this.page.title = "Topic Modelling - 主题建模以及隐变量模型";
    };
  NexT.utils.loadComments('#disqus_thread', () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://shootingspace.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
