<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.147.9"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Cong's Log</title><meta name=description content="Hi, this is Cong. I’m documenting my learning notes in this blog."><meta name=author content="Cong"><link rel=canonical href=https://congchan.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://congchan.github.io/index.xml><link rel=alternate type=application/json href=https://congchan.github.io/index.json><link rel=alternate hreflang=en href=https://congchan.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Cong's Log"><meta property="og:description" content="Hi, this is Cong. I’m documenting my learning notes in this blog."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Cong's Log"><meta name=twitter:description content="Hi, this is Cong. I’m documenting my learning notes in this blog."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Cong's Log","url":"https://congchan.github.io/","description":"Hi, this is Cong. I’m documenting my learning notes in this blog.","logo":"https://congchan.github.io/favicons/favicon.ico","sameAs":["https://github.com/congchan"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning</h2></header><div class=entry-content><p>2017, EMNLP
data: FB15K-237, FB15K
task: Knowledge Graph Reasoning
Use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.
方法 RL 系统包含两部分，
第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。 系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。 关系推理的强化学习 行为 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。
状态 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量：
...</p></div><footer class=entry-footer><span title='2020-03-11 00:00:00 +0000 UTC'>2020-03-11</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning" href=https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Knowledge-Graph-Embedding的Translate族（TransE，TransH，TransR，TransD）</h2></header><div class=entry-content><p>data: WN18, WN11, FB15K, FB13, FB40K
task: Knowledge Graph Embedding
TransE Translating Embeddings for Modeling Multi-relational Data（2013）
https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf
这是转换模型系列的第一部作品。该模型的基本思想是使head向量和relation向量的和尽可能靠近tail向量。这里我们用L1或L2范数来衡量它们的靠近程度。
损失函数 $\mathrm{L}(h, r, t)=\max \left(0, d_{\text {pos }}-d_{\text {neg }}+\text { margin }\right)$使损失函数值最小化，当这两个分数之间的差距大于margin的时候就可以了(我们会设置这个值，通常是1)
但是这个模型只能处理一对一的关系，不适合一对多/多对一关系，例如，有两个知识，(skytree, location, tokyo)和(gundam, location, tokyo)。经过训练，“sky tree”实体向量将非常接近“gundam”实体向量。但实际上它们没有这样的相似性。
with tf.name_scope("embedding"): self.ent_embeddings = tf.get_variable(name = "ent_embedding", shape = [entity_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) self.rel_embeddings = tf.get_variable(name = "rel_embedding", shape = [relation_total, size], initializer = tf.contrib.layers.xavier_initializer(uniform = False)) pos_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_h) pos_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.pos_t) pos_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.pos_r) neg_h_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_h) neg_t_e = tf.nn.embedding_lookup(self.ent_embeddings, self.neg_t) neg_r_e = tf.nn.embedding_lookup(self.rel_embeddings, self.neg_r) if config.L1_flag: pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - pos_t_e), 1, keep_dims = True) neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - neg_t_e), 1, keep_dims = True) self.predict = pos else: pos = tf.reduce_sum((pos_h_e + pos_r_e - pos_t_e) ** 2, 1, keep_dims = True) neg = tf.reduce_sum((neg_h_e + neg_r_e - neg_t_e) ** 2, 1, keep_dims = True) self.predict = pos with tf.name_scope("output"): self.loss = tf.reduce_sum(tf.maximum(pos - neg + margin, 0)) TransH Knowledge Graph Embedding by Translating on Hyperplanes（2014）
...</p></div><footer class=entry-footer><span title='2020-03-05 00:00:00 +0000 UTC'>2020-03-05</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Knowledge-Graph-Embedding的Translate族（TransE，TransH，TransR，TransD）" href=https://congchan.github.io/posts/knowledge-graph-embedding%E7%9A%84translate%E6%97%8Ftransetranshtransrtransd/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications</h2></header><div class=entry-content><p>Survey: https://arxiv.org/abs/2002.00388v4
A knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.
Entities can be real-world objects and abstract concepts, Relationships represent the relation between entities, Semantic descriptions of entities and their relationships contain types and properties with a well-defined meaning G: A knowledge graph F: A set of facts (h, r, t): A triple of head, relation and tail $(\mathbf{h}, \mathbf{r}, \mathbf{t})$: Embedding of head, relation and tail
...</p></div><footer class=entry-footer><span title='2020-02-01 00:00:00 +0000 UTC'>2020-02-01</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to 综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications" href=https://congchan.github.io/posts/%E7%BB%BC%E8%BF%B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification</h2></header><div class=entry-content><p>2019, ACL
data: SemEval 2014, SemEval 2014 ABSA, SemEval 2015, SemEval 2016
task: ABSA
propose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations.
优点：
用指针网络选取target，避免了序列标注的搜索空间过大问题 用span边界+极性的标注方式，解决多极性的target问题 方法 Input:
sentence x =(x1,..., xn) with length n,
Target list T = {t1,..., tm}： each target ti is annotated with its start, end position, and its sentiment polarity
...</p></div><footer class=entry-footer><span title='2020-01-24 00:00:00 +0000 UTC'>2020-01-24</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification" href=https://congchan.github.io/posts/open-domain-targeted-sentiment-analysis-via-span-based-extraction-and-classification/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>A Lite BERT(AlBERT) 原理和源码解析</h2></header><div class=entry-content><p>A Lite BERT BERT(Devlin et al., 2019)的参数很多, 模型很大, 内存消耗很大, 在分布式计算中的通信开销很大.
但是BERT的高内存消耗边际收益并不高, 如果继续增大BERT-large这种大模型的隐含层大小, 模型效果不升反降.
针对这些问题, 启发于mobilenet, Alert使用了两种减少参数的方法来降低模型大小和提高训练速度, 分别是Factorized embedding parameterization和Cross-layer parameter sharing. 这些设计让ALBERT增加参数大小的边界收益远远大于BERT.
除此之外, 在句子关系任务上抛弃了bert的nsp任务, 改为sop任务.
整体而言, ALBERT是当前众多BERT系列模型的集大成者, 其思路值得学习, 代码也写得很清楚. 下面仔细过一遍.
Factorized embedding parameterization BERT以及后续的XLNet(Yang et al., 2019), RoBERTa(Liu et al., 2019)等, WordPiece embedding的维度E是和隐层维度H绑定的. WordPiece embedding本意是学习context-independent的表达，而hidden-layer旨在学习context-dependent的表达。将WordPiece embedding大小E与隐层大小H解绑，可以更有效地利用建模所需的总模型参数.
从实用性的角度看, 这样可以减少词汇量对模型大小的影响. 在NLP中词汇量一般都很大, 所以这个解绑收益是很明显的.
具体的做法就是对embedding进行因式分解, 把非常大的单词embedding分解成两个小的矩阵, O(V × H)变成O(V × E + E × H), 可以显著减少单词映射embedding的参数量. 这个在topic models一文中的隐变量模型中类似的思路体现.
Cross-layer parameter sharing 各个 transformer blocks 所有参数共享, 这样参数不再随着模型层数加深而增大.
...</p></div><footer class=entry-footer><span title='2020-01-11 00:00:00 +0000 UTC'>2020-01-11</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to A Lite BERT(AlBERT) 原理和源码解析" href=https://congchan.github.io/posts/a-lite-bertalbert-%E5%8E%9F%E7%90%86%E5%92%8C%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Entity Linking</h2></header><div class=entry-content><p>Entity Linking
Knowledge Graph (知识图谱)：一种语义网络，旨在描述客观世界的概念实体及其之间的关系，有时也称为Knowledge Base (知识库)。 图谱由三元组构成：&lt;实体1，关系，实体2> 或者 &lt;实体，属性，属性值>； 例如：&lt;姚明，plays-in，NBA>、&lt;姚明，身高，2.29m>； 常见的KB有：Wikidata、DBpedia、YAGO。 Entity 实体：实体是知识图谱的基本单元，也是文本中承载信息的重要语言单位。 Mention 提及：自然文本中表达实体的语言片段。 应用方向
Question Answering：EL是KBQA的刚需，linking到实体之后才能查询图数据库； Content Analysis：舆情分析、内容推荐、阅读增强； Information Retrieval：基于语义实体的搜索引擎，google搜索一些实体，右侧会出现wikipedia页面； Knowledge Base population：扩充知识库，更新实体和关系。 候选实体和消歧
Entity linking system consists of two components:
candidate entity generation：从mention出发，找到KB中所有可能的实体，组成候选实体集 (candidate entities)； Entity Disambiguation：从candidate entities中，选择最可能的实体作为预测实体。 Entity Disambiguation (ED) 是最重要的部分
Features Context-Independent Features： LinkCount：#(m->e)，知识库中某个提及m指向实体e的次数； Entity Attributes：Popularity、Type； Context-Dependent Features： Textual Context：BOW, Concept Vector Coherence Between Entities：WLM、PMI、Jaccard Distance Context-Independent Features mention到实体的LinkCount、实体自身的一些属性（比如热度、类型等等）
LinkCount作为一个先验知识，在消歧时，往往很有用 Context-Dependent Features 全局地进行entities的消歧实际上是一个NP-hard的问题，因此核心问题是如何更加快速有效地利用一致性特征
...</p></div><footer class=entry-footer><span title='2020-01-02 00:00:00 +0000 UTC'>2020-01-02</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Entity Linking" href=https://congchan.github.io/posts/entity-linking/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>知识图谱补全</h2></header><div class=entry-content><p>知识图谱补全
基于知识表示的方法 知识表示学习：对知识图谱中的实体和关系学习其低维度的嵌入式表示。
常见的知识表示学习方法：主要是以 TransE 法及其变种为核心，针对空间映射等场景做的改进
基于实体和关系的表示对缺失三元组进行预测；
利用实体描述信息，可以解决开放域实体补全的问题；
基于路径查找的方法 可使用基于路径查找的方法来处理这类多步推理问题。
传统的路径查找方法主要是 PRA 方法（Path Ranking Algorithm）；但是这种方法对于包含较大规模的知识图谱来说，会由于路径数量爆炸式增长，导致特征空间急剧膨胀
可以尝试用 embedding 的方式表示关系，对关系进行泛化，并基于此对知识的补全进行建模，以缓解路径数量过多导致的特征空间膨胀问题。
给定实体对集合，利用 PRA 查找一定数量的路径； 路径计算过程中加入实体类型信息（减少长尾实体影响）； 使用 RNN 沿着路径进行向量化建模；RNN 模型参数在不同关系之间共享； 通过比较路径向量与待预测关系向量间的关联度来进行关系补全。 基于强化学习的方法 前面提到的两种方法，仍然存在若干的问题：
需要基于 random walk 来查找路径； 而 random walk 算法在离散空间中运行，难以评价知识图谱中相似的实体和关系； 超级结点可能影响 random walk 算法运行速度。 强化学习方法：
在连续空间中进行路径搜索； 通过引入多种奖励函数，使得路径查找更加灵活、可控。 DeepPath DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning
xwhan/DeepPath
任务：查找 Band of Brothers 和 English 之间的关系。 路径起点：Band of Brothers 状态：实体中的 embedding 动作：图谱中的关系； 奖励 Binary，是否到达终点 路径长度 路径多样性 策略网络：使用全连接网络。 DeepPath 方法仍然存在一些缺陷：知识图谱本身的不完善很可能对路径查找造成影响。
...</p></div><footer class=entry-footer><span title='2020-01-01 00:00:00 +0000 UTC'>2020-01-01</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to 知识图谱补全" href=https://congchan.github.io/posts/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Deep Q Networks</h2></header><div class=entry-content><p>Combining reinforcement learning and deep neural networks at scale. The algorithm was developed by enhancing a classic RL algorithm called Q-Learning with deep neural networks and a technique called experience replay.
Q-Learning Q-Learning is based on the notion of a Q-function. The Q-function (a.k.a the state-action value function) of a policy $\pi$，$Q^{\pi}(s, a)$ ，measures the expected return or discounted sum of rewards obtained from state $s$ by taking action $a$ first and following policy $\pi$ thereafter.
...</p></div><footer class=entry-footer><span title='2019-03-10 00:00:00 +0000 UTC'>2019-03-10</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Deep Q Networks" href=https://congchan.github.io/posts/deep-q-networks/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>BERT的Adam Weight Decay</h2></header><div class=entry-content><p>Adam Weight Decay in BERT 在看BERT(Devlin et al., 2019)的源码中优化器部分的实现时，发现有这么一段话
# Just adding the square of the weights to the loss function is *not* # the correct way of using L2 regularization/weight decay with Adam, # since that will interact with the m and v parameters in strange ways. # # Instead we want ot decay the weights in a manner that doesn't interact # with the m/v parameters. This is equivalent to adding the square # of the weights to the loss with plain (non-momentum) SGD. 其针对性地指出一些传统的Adam weight decay实现是错误的.
...</p></div><footer class=entry-footer><span title='2019-03-03 00:00:00 +0000 UTC'>2019-03-03</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to BERT的Adam Weight Decay" href=https://congchan.github.io/posts/bert%E7%9A%84adam-weight-decay/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Word Lattice</h2></header><div class=entry-content><p>What is Word Lattices?
A word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)
语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的k-best条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。
在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。
所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题：
...</p></div><footer class=entry-footer><span title='2019-01-24 00:00:00 +0000 UTC'>2019-01-24</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan</footer><a class=entry-link aria-label="post link to Word Lattice" href=https://congchan.github.io/posts/word-lattice/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://congchan.github.io/page/3/>«&nbsp;Prev&nbsp;3/14
</a><a class=next href=https://congchan.github.io/page/5/>Next&nbsp;5/14&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>