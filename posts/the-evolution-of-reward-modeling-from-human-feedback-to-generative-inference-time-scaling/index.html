<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling | Cong's Log</title><meta name=keywords content="2025,Large Language Model,Reward Modeling"><meta name=description content="Reward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM."><meta name=author content="Cong"><link rel=canonical href=https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling"><meta property="og:description" content="Reward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-25T00:00:00+00:00"><meta property="article:tag" content="2025"><meta property="article:tag" content="Large Language Model"><meta property="article:tag" content="Reward Modeling"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling"><meta name=twitter:description content="Reward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling","item":"https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling","name":"The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling","description":"Reward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM.\n","keywords":["2025","Large Language Model","Reward Modeling"],"articleBody":"Reward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM.\n1. Scoreing preference by parameters Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155.\nThe paper presents a reward modeling (RM) approach as a core component of reinforcement learning from human feedback (RLHF) to align language models with human intent. Below is a detailed breakdown of the paper’s views and methods on reward modeling:\n1.1 Core Objectives of Reward Modeling The reward model aims to:\nQuantify Human Preferences: Convert subjective human judgments about model outputs into a scalar reward signal, enabling models to learn what constitutes “desirable behavior.” Guide Model Alignment: Direct language models to follow instructions, prioritize truthfulness, and avoid harmful outputs by optimizing against human-derived rewards. 1.2. Data Collection for Reward Modeling Input Source: Prompts from the OpenAI API (filtered to remove PII) and labeler-written prompts, covering tasks like generation, QA, and summarization . Labeling Process: Labelers rank 4–9 model outputs per prompt from best to worst, generating pairwise comparisons (e.g., “Output A is preferred over Output B”) . To avoid bias, labelers undergo a screening test to assess sensitivity to sensitive content and alignment with research criteria . 1.3. Reward Model Architecture and Training Model Structure:\nBased on the GPT-3 architecture, initialized from a supervised fine-tuned (SFT) model with the final unembedding layer replaced by a projection layer to output a scalar reward . Uses a 6B parameter model for computational efficiency, as 175B models showed training instability . Training Methodology:\nLoss Function: Cross-entropy loss to predict human-preferred outputs, formulated as: $$ \\text{loss}(\\theta) = -\\frac{1}{\\binom{K}{2}} \\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim D} \\left[ \\log \\left( \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right) \\right] $$ where $y_w$ and $y_l$ are the preferred and less preferred outputs, respectively, and $K$ is the number of outputs per prompt . Batch Processing: Treats all $\\binom{K}{2}$ comparisons from a prompt as a single batch element to prevent overfitting and improve computational efficiency . Normalization: Adjusts rewards so that labeler demonstrations have a mean score of 0 before RL training . 1.4. Key Innovations and Insights Generalization to Held-Out Labelers: Reward models trained on one group of labelers generalize to new labelers, with cross-validation showing 69.6% accuracy in predicting preferences of unseen labelers . Trade-off with Public NLP Datasets: RM-based RLHF may cause performance regressions on standard NLP tasks (e.g., SQuAD, DROP), but mixing pretraining gradients (PPO-ptx) mitigates this while preserving human preference . Role in InstructGPT: The RM is crucial for improving model behavior: InstructGPT (PPO-ptx) outperforms GPT-3 despite having 100x fewer parameters, with 1.3B InstructGPT preferred over 175B GPT-3 in 85% of cases . 1.5. Limitations and Future Directions Alignment Scope: The RM aligns models to specific labelers and researchers, not broader human values, raising questions about fairness and representativeness . Toxicity and Bias: While InstructGPT reduces toxicity, it shows minimal improvement on bias metrics (e.g., Winogender, CrowS-Pairs), indicating RM needs better signals for these dimensions . Scalability: Future work may explore combining RM with adversarial data collection or constraint optimization to address harmful outputs and improve generalization . In summary, the paper demonstrates that reward modeling via RLHF is a powerful tool for aligning language models, but ongoing research is needed to address its limitations and expand its applicability to diverse human values.\n2. Constitutional AI: Bootstrapping Harmlessness with AI Feedback Bai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2212.08073.\nIn “Constitutional AI: Harmlessness from AI Feedback” (Bai et al., 2022), researchers introduced a paradigm that replaces human labels for harmfulness with AI-generated feedback. The approach uses a “constitution” of principles to guide self-critique and revision, enabling models to learn harmless behavior without direct human supervision.\nKey Innovation: The framework combines supervised learning (critique → revision cycles) and RL from AI Feedback (RLAIF), where a preference model (PM) is trained on AI-generated comparisons. For example, models generate pairs of responses and evaluate which aligns better with constitutional principles (e.g., “avoid harmful advice”). Impact: As shown in Figure 2 of the paper, Constitutional AI achieves a Pareto improvement in harmlessness and helpfulness, outperforming RLHF models that trade off these traits. The approach reduces reliance on human labeling, a critical step toward scalable supervision. This work laid the groundwork for self-supervised RM, demonstrating that models can learn to evaluate their own behavior using explicit principles.\n3. DeepSeek-R1: Incentivizing Reasoning via Reinforcement Learning DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948.\nThe paper employs reward modeling strategically to enhance reasoning capabilities in LLMs through reinforcement learning (RL). Here’s a detailed breakdown of how reward modeling is utilized:\n3.1. Rule-Based Rewards for DeepSeek-R1-Zero DeepSeek-R1-Zero relies on a rule-based reward system to avoid the complexity and potential pitfalls of neural reward models. This system consists of two main components:\nAccuracy Rewards: Evaluate the correctness of responses. For example: In math problems, the model must provide answers in a specified format (e.g., within a box) for rule-based verification. In coding tasks (e.g., LeetCode), a compiler checks solutions against predefined test cases. Format Rewards: Enforce structural consistency by requiring the model to place reasoning processes between specific tags (e.g., , \u003c\\think\u003e, \u003c｜tool▁calls▁begin｜\u003e\u003c｜tool▁call▁begin｜\u003e and \u003c｜tool▁call▁end｜\u003e\u003c｜tool▁calls▁end｜\u003e\u003c｜end▁of▁sentence｜\u003e). The paper explicitly avoids neural reward models (both outcome and process-based) for DeepSeek-R1-Zero, citing risks of reward hacking and the additional computational overhead of retraining reward models.\n3.2. Enhanced Reward Systems for DeepSeek-R1 DeepSeek-R1 incorporates additional reward mechanisms to address readability and generalizability:\nLanguage Consistency Reward: Introduced to mitigate language mixing in Chain-of-Thought (CoT) reasoning. This reward measures the proportion of target language words in the CoT and is summed with accuracy rewards. While this slightly reduces reasoning performance, it improves human readability. Multi-Stage RL with Diverse Rewards: Reasoning-Oriented RL: Uses rule-based rewards (accuracy + format) for tasks like math and coding. General Scenario RL: Employs neural reward models to align with human preferences for helpfulness and harmlessness. For example: Helpfulness: Focuses on the utility of the final summary. Harmlessness: Evaluates the entire response (reasoning + summary) to prevent biased or harmful content. 3.3. Reward Design for Cold Start and Distillation Cold Start Data: Thousands of CoT examples are curated to fine-tune the model before RL. These examples include human-readable formats (e.g., summaries) and serve as a foundation for reward-aligned behavior. Distillation: The reasoning patterns of DeepSeek-R1 are distilled into smaller models using 800K training samples. While distillation itself does not use RL rewards, the teacher model (DeepSeek-R1) is trained with the aforementioned reward systems, ensuring smaller models inherit optimized reasoning behaviors. 3.4. Key Trade-offs and Design Choices Rule-Based vs. Neural Rewards: Rule-based rewards are prioritized for simplicity and to avoid reward hacking in large-scale RL. Neural rewards are introduced only when necessary (e.g., for general task alignment in DeepSeek-R1). Balancing Performance and Readability: The language consistency reward in DeepSeek-R1 trades off slight performance degradation for improved human interpretability, highlighting the importance of practical usability. 3.5. Experimental Validation of Reward Models DeepSeek-R1-Zero: Achieves significant reasoning gains (e.g., AIME 2024 Pass@1 from 15.6% to 71.0%) using purely rule-based rewards, demonstrating that complex reasoning can emerge without neural reward models. DeepSeek-R1: Outperforms DeepSeek-R1-Zero on readability and matches OpenAI-o1-1217 on reasoning tasks by combining rule-based and language consistency rewards. Distilled Models: Smaller models (e.g., 14B, 32B) trained on DeepSeek-R1’s rewarded outputs outperform state-of-the-art open-source models, validating the transferability of reward-aligned reasoning patterns. 3.6 Conclusion The paper demonstrates that reward modeling in RL can be tailored to balance reasoning performance, readability, and human alignment. Rule-based rewards enable pure RL-driven reasoning emergence in DeepSeek-R1-Zero, while DeepSeek-R1 enhances this with language consistency and general preference rewards. This approach highlights the flexibility of reward systems in shaping LLM behavior without heavy reliance on neural reward models, paving the way for efficient and interpretable reasoning enhancements.\n4. Inference-Time Scaling for Generalist Reward Modeling Liu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2504.02495.\nLiu et al. (2025) address a critical gap in RM: generalizability across domains and efficient resource use. Their approach, Self-Principled Critique Tuning (SPCT), combines generative reward modeling (GRM) with inference-time scaling to improve RM performance without increasing training compute.\nCore Contributions: SPCT: A two-phase RL method where models learn to generate adaptive principles and critiques, enhancing reward quality. For example, DeepSeek-GRM-27B with SPCT outperforms scalar RMs on benchmarks like Reward Bench and PPE (Table 2). Inference-Time Scaling: Parallel sampling and a meta RM guide voting on multiple reward samples, expanding the reward space and improving granularity. As shown in Figure 1, DeepSeek-GRM-27B with meta RM achieves 72.8% overall performance, surpassing models like Nemotron-4-340B-Reward. Connection to Prior Work: SPCT builds on Constitutional AI’s self-critique and DeepSeek-R1’s RL efficiency, but shifts focus to generalist domains. The meta RM integrates self-evaluation insights (from Kadavath et al.) to filter low-quality rewards, aligning with P(IK)-like confidence metrics. The Developmental Arc: From Specialization to Generalization The evolution of RM reflects a shift from human-dependent, task-specific approaches to self-supervised, generalizable frameworks:\nEarly RLHF (2020s): Relied on massive human labeling, limited to specific domains. Constitutional AI (2022): Introduced AI-generated feedback and principles, reducing human overhead. Self-Evaluation (2022): Uncovered models’ ability to assess their own knowledge, enabling confidence-aware RM. Task-Oriented RL (2025, DeepSeek-R1): Optimized reasoning via RL, demonstrating task-specific RM scaling. Generalist Inference-Time Scaling (2025, Liu et al.): Extended RM to diverse domains using generative models and inference-time compute, balancing efficiency and performance. Challenges and Future Directions Bias and Calibration: While SPCT reduces domain bias, models like DeepSeek-GRM still struggle with specific tasks (e.g., verifiable problems, Appendix B). Computational Overhead: Inference-time scaling requires more compute per query, necessitating efficiency improvements. Cross-Domain Generalization: Combining task-specific RM (DeepSeek-R1) with generalist GRM (Liu et al.) remains an open challenge. Future work may integrate self-supervised RM with external tools (e.g., code execution for verification) and explore hybrid frameworks that balance training and inference-time scaling.\nConclusion The landscape of reward modeling is evolving rapidly, driven by innovations that prioritize scalability, self-supervision, and generalizability. From Constitutional AI’s principles to SPCT’s inference-time scaling, these methods collectively push LLMs toward more aligned, transparent, and efficient behavior. As shown in the comparative results across papers, the field is moving toward a future where RM serves as a versatile interface for LLM alignment, enabling models to reason, evaluate, and adapt to diverse human needs.\nKey Citations Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155. Bai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2212.08073. DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948. Liu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2504.02495. Kadavath, Saurav, et al. Language Models (Mostly) Know What They Know. arXiv:2207.05221, arXiv, 21 Nov. 2022. arXiv.org, http://arxiv.org/abs/2207.05221. ","wordCount":"1882","inLanguage":"en","datePublished":"2025-05-25T00:00:00Z","dateModified":"2025-05-25T00:00:00Z","author":{"@type":"Person","name":"Cong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling</h1><div class=post-meta><span title='2025-05-25 00:00:00 +0000 UTC'>2025-05-25</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Cong&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/llm-Evolution-of-Reward-Modeling.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-scoreing-preference-by-parameters aria-label="1. Scoreing preference by parameters">1. Scoreing preference by parameters</a><ul><li><a href=#11-core-objectives-of-reward-modeling aria-label="1.1 Core Objectives of Reward Modeling">1.1 Core Objectives of Reward Modeling</a></li><li><a href=#12-data-collection-for-reward-modeling aria-label="1.2. Data Collection for Reward Modeling">1.2. Data Collection for Reward Modeling</a></li><li><a href=#13-reward-model-architecture-and-training aria-label="1.3. Reward Model Architecture and Training">1.3. Reward Model Architecture and Training</a></li><li><a href=#14-key-innovations-and-insights aria-label="1.4. Key Innovations and Insights">1.4. Key Innovations and Insights</a></li><li><a href=#15-limitations-and-future-directions aria-label="1.5. Limitations and Future Directions">1.5. Limitations and Future Directions</a></li></ul></li><li><a href=#2-constitutional-ai-bootstrapping-harmlessness-with-ai-feedback aria-label="2. Constitutional AI: Bootstrapping Harmlessness with AI Feedback">2. Constitutional AI: Bootstrapping Harmlessness with AI Feedback</a></li><li><a href=#3-deepseek-r1-incentivizing-reasoning-via-reinforcement-learning aria-label="3. DeepSeek-R1: Incentivizing Reasoning via Reinforcement Learning">3. DeepSeek-R1: Incentivizing Reasoning via Reinforcement Learning</a><ul><li><a href=#31-rule-based-rewards-for-deepseek-r1-zero aria-label="3.1. Rule-Based Rewards for DeepSeek-R1-Zero">3.1. Rule-Based Rewards for DeepSeek-R1-Zero</a></li><li><a href=#32-enhanced-reward-systems-for-deepseek-r1 aria-label="3.2. Enhanced Reward Systems for DeepSeek-R1">3.2. Enhanced Reward Systems for DeepSeek-R1</a></li><li><a href=#33-reward-design-for-cold-start-and-distillation aria-label="3.3. Reward Design for Cold Start and Distillation">3.3. Reward Design for Cold Start and Distillation</a></li><li><a href=#34-key-trade-offs-and-design-choices aria-label="3.4. Key Trade-offs and Design Choices">3.4. Key Trade-offs and Design Choices</a></li><li><a href=#35-experimental-validation-of-reward-models aria-label="3.5. Experimental Validation of Reward Models">3.5. Experimental Validation of Reward Models</a></li><li><a href=#36-conclusion aria-label="3.6 Conclusion">3.6 Conclusion</a></li></ul></li><li><a href=#4-inference-time-scaling-for-generalist-reward-modeling aria-label="4. Inference-Time Scaling for Generalist Reward Modeling">4. Inference-Time Scaling for Generalist Reward Modeling</a></li><li><a href=#the-developmental-arc-from-specialization-to-generalization aria-label="The Developmental Arc: From Specialization to Generalization">The Developmental Arc: From Specialization to Generalization</a></li><li><a href=#challenges-and-future-directions aria-label="Challenges and Future Directions">Challenges and Future Directions</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#key-citations aria-label="Key Citations">Key Citations</a></li></ul></div></details></div><div class=post-content><p>Reward modeling (RM) has emerged as a cornerstone of large language model (LLM) alignment, guiding models to align with human values and perform complex tasks. Early approaches relied heavily on Reinforcement Learning from Human Feedback (RLHF), but recent research has shifted toward more scalable, efficient, and generalizable RM frameworks. This blog explores the developmental arc of RM, connecting four seminal papers that have shaped the field: from Constitutional AI and self-evaluation mechanisms to inference-time scaling for generalist RM.</p><h3 id=1-scoreing-preference-by-parameters>1. Scoreing preference by parameters<a hidden class=anchor aria-hidden=true href=#1-scoreing-preference-by-parameters>#</a></h3><p>Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, <a href=http://arxiv.org/abs/2203.02155>http://arxiv.org/abs/2203.02155</a>.</p><p>The paper presents a reward modeling (RM) approach as a core component of reinforcement learning from human feedback (RLHF) to align language models with human intent. Below is a detailed breakdown of the paper&rsquo;s views and methods on reward modeling:</p><h4 id=11-core-objectives-of-reward-modeling><strong>1.1 Core Objectives of Reward Modeling</strong><a hidden class=anchor aria-hidden=true href=#11-core-objectives-of-reward-modeling>#</a></h4><p>The reward model aims to:</p><ul><li><strong>Quantify Human Preferences</strong>: Convert subjective human judgments about model outputs into a scalar reward signal, enabling models to learn what constitutes &ldquo;desirable behavior.&rdquo;</li><li><strong>Guide Model Alignment</strong>: Direct language models to follow instructions, prioritize truthfulness, and avoid harmful outputs by optimizing against human-derived rewards.</li></ul><h4 id=12-data-collection-for-reward-modeling><strong>1.2. Data Collection for Reward Modeling</strong><a hidden class=anchor aria-hidden=true href=#12-data-collection-for-reward-modeling>#</a></h4><ul><li><strong>Input Source</strong>: Prompts from the OpenAI API (filtered to remove PII) and labeler-written prompts, covering tasks like generation, QA, and summarization .</li><li><strong>Labeling Process</strong>:<ol><li>Labelers rank 4–9 model outputs per prompt from best to worst, generating pairwise comparisons (e.g., &ldquo;Output A is preferred over Output B&rdquo;) .</li><li>To avoid bias, labelers undergo a screening test to assess sensitivity to sensitive content and alignment with research criteria .</li></ol></li></ul><h4 id=13-reward-model-architecture-and-training><strong>1.3. Reward Model Architecture and Training</strong><a hidden class=anchor aria-hidden=true href=#13-reward-model-architecture-and-training>#</a></h4><ul><li><p><strong>Model Structure</strong>:</p><ul><li>Based on the GPT-3 architecture, initialized from a supervised fine-tuned (SFT) model with the final unembedding layer replaced by a projection layer to output a scalar reward .</li><li>Uses a 6B parameter model for computational efficiency, as 175B models showed training instability .</li></ul></li><li><p><strong>Training Methodology</strong>:</p><ul><li><strong>Loss Function</strong>: Cross-entropy loss to predict human-preferred outputs, formulated as:
$$
\text{loss}(\theta) = -\frac{1}{\binom{K}{2}} \mathbb{E}_{\left(x, y_w, y_l\right) \sim D} \left[ \log \left( \sigma(r_\theta(x, y_w) - r_\theta(x, y_l)) \right) \right]
$$
where $y_w$ and $y_l$ are the preferred and less preferred outputs, respectively, and $K$ is the number of outputs per prompt .</li><li><strong>Batch Processing</strong>: Treats all $\binom{K}{2}$ comparisons from a prompt as a single batch element to prevent overfitting and improve computational efficiency .</li><li><strong>Normalization</strong>: Adjusts rewards so that labeler demonstrations have a mean score of 0 before RL training .</li></ul></li></ul><h4 id=14-key-innovations-and-insights><strong>1.4. Key Innovations and Insights</strong><a hidden class=anchor aria-hidden=true href=#14-key-innovations-and-insights>#</a></h4><ul><li><strong>Generalization to Held-Out Labelers</strong>: Reward models trained on one group of labelers generalize to new labelers, with cross-validation showing 69.6% accuracy in predicting preferences of unseen labelers .</li><li><strong>Trade-off with Public NLP Datasets</strong>: RM-based RLHF may cause performance regressions on standard NLP tasks (e.g., SQuAD, DROP), but mixing pretraining gradients (PPO-ptx) mitigates this while preserving human preference .</li><li><strong>Role in InstructGPT</strong>: The RM is crucial for improving model behavior: InstructGPT (PPO-ptx) outperforms GPT-3 despite having 100x fewer parameters, with 1.3B InstructGPT preferred over 175B GPT-3 in 85% of cases .</li></ul><h4 id=15-limitations-and-future-directions><strong>1.5. Limitations and Future Directions</strong><a hidden class=anchor aria-hidden=true href=#15-limitations-and-future-directions>#</a></h4><ul><li><strong>Alignment Scope</strong>: The RM aligns models to specific labelers and researchers, not broader human values, raising questions about fairness and representativeness .</li><li><strong>Toxicity and Bias</strong>: While InstructGPT reduces toxicity, it shows minimal improvement on bias metrics (e.g., Winogender, CrowS-Pairs), indicating RM needs better signals for these dimensions .</li><li><strong>Scalability</strong>: Future work may explore combining RM with adversarial data collection or constraint optimization to address harmful outputs and improve generalization .</li></ul><p>In summary, the paper demonstrates that reward modeling via RLHF is a powerful tool for aligning language models, but ongoing research is needed to address its limitations and expand its applicability to diverse human values.</p><h3 id=2-constitutional-ai-bootstrapping-harmlessness-with-ai-feedback><strong>2. Constitutional AI: Bootstrapping Harmlessness with AI Feedback</strong><a hidden class=anchor aria-hidden=true href=#2-constitutional-ai-bootstrapping-harmlessness-with-ai-feedback>#</a></h3><p>Bai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2212.08073>https://doi.org/10.48550/arXiv.2212.08073</a>.</p><p>In &ldquo;Constitutional AI: Harmlessness from AI Feedback&rdquo; (Bai et al., 2022), researchers introduced a paradigm that replaces human labels for harmfulness with AI-generated feedback. The approach uses a &ldquo;constitution&rdquo; of principles to guide self-critique and revision, enabling models to learn harmless behavior without direct human supervision.</p><ul><li><strong>Key Innovation</strong>: The framework combines supervised learning (critique → revision cycles) and RL from AI Feedback (RLAIF), where a preference model (PM) is trained on AI-generated comparisons. For example, models generate pairs of responses and evaluate which aligns better with constitutional principles (e.g., &ldquo;avoid harmful advice&rdquo;).</li><li><strong>Impact</strong>: As shown in Figure 2 of the paper, Constitutional AI achieves a Pareto improvement in harmlessness and helpfulness, outperforming RLHF models that trade off these traits. The approach reduces reliance on human labeling, a critical step toward scalable supervision.</li></ul><p>This work laid the groundwork for self-supervised RM, demonstrating that models can learn to evaluate their own behavior using explicit principles.</p><h3 id=3-deepseek-r1-incentivizing-reasoning-via-reinforcement-learning><strong>3. DeepSeek-R1: Incentivizing Reasoning via Reinforcement Learning</strong><a hidden class=anchor aria-hidden=true href=#3-deepseek-r1-incentivizing-reasoning-via-reinforcement-learning>#</a></h3><p>DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2501.12948>https://doi.org/10.48550/arXiv.2501.12948</a>.</p><p>The paper employs reward modeling strategically to enhance reasoning capabilities in LLMs through reinforcement learning (RL). Here’s a detailed breakdown of how reward modeling is utilized:</p><h4 id=31-rule-based-rewards-for-deepseek-r1-zero><strong>3.1. Rule-Based Rewards for DeepSeek-R1-Zero</strong><a hidden class=anchor aria-hidden=true href=#31-rule-based-rewards-for-deepseek-r1-zero>#</a></h4><p>DeepSeek-R1-Zero relies on a <strong>rule-based reward system</strong> to avoid the complexity and potential pitfalls of neural reward models. This system consists of two main components:</p><ul><li><strong>Accuracy Rewards</strong>: Evaluate the correctness of responses. For example:<ul><li>In math problems, the model must provide answers in a specified format (e.g., within a box) for rule-based verification.</li><li>In coding tasks (e.g., LeetCode), a compiler checks solutions against predefined test cases.</li></ul></li><li><strong>Format Rewards</strong>: Enforce structural consistency by requiring the model to place reasoning processes between specific tags (e.g., <code>&lt;think></code>, <code>&lt;\think></code>, <code>&lt;｜tool▁calls▁begin｜>&lt;｜tool▁call▁begin｜></code> and <code>&lt;｜tool▁call▁end｜>&lt;｜tool▁calls▁end｜>&lt;｜end▁of▁sentence｜></code>).</li></ul><p>The paper explicitly avoids neural reward models (both outcome and process-based) for DeepSeek-R1-Zero, citing risks of <strong>reward hacking</strong> and the additional computational overhead of retraining reward models.</p><h4 id=32-enhanced-reward-systems-for-deepseek-r1><strong>3.2. Enhanced Reward Systems for DeepSeek-R1</strong><a hidden class=anchor aria-hidden=true href=#32-enhanced-reward-systems-for-deepseek-r1>#</a></h4><p>DeepSeek-R1 incorporates additional reward mechanisms to address readability and generalizability:</p><ul><li><strong>Language Consistency Reward</strong>: Introduced to mitigate language mixing in Chain-of-Thought (CoT) reasoning. This reward measures the proportion of target language words in the CoT and is summed with accuracy rewards. While this slightly reduces reasoning performance, it improves human readability.</li><li><strong>Multi-Stage RL with Diverse Rewards</strong>:<ul><li><strong>Reasoning-Oriented RL</strong>: Uses rule-based rewards (accuracy + format) for tasks like math and coding.</li><li><strong>General Scenario RL</strong>: Employs neural reward models to align with human preferences for helpfulness and harmlessness. For example:<ul><li><strong>Helpfulness</strong>: Focuses on the utility of the final summary.</li><li><strong>Harmlessness</strong>: Evaluates the entire response (reasoning + summary) to prevent biased or harmful content.</li></ul></li></ul></li></ul><h4 id=33-reward-design-for-cold-start-and-distillation><strong>3.3. Reward Design for Cold Start and Distillation</strong><a hidden class=anchor aria-hidden=true href=#33-reward-design-for-cold-start-and-distillation>#</a></h4><ul><li><strong>Cold Start Data</strong>: Thousands of CoT examples are curated to fine-tune the model before RL. These examples include human-readable formats (e.g., summaries) and serve as a foundation for reward-aligned behavior.</li><li><strong>Distillation</strong>: The reasoning patterns of DeepSeek-R1 are distilled into smaller models using 800K training samples. While distillation itself does not use RL rewards, the teacher model (DeepSeek-R1) is trained with the aforementioned reward systems, ensuring smaller models inherit optimized reasoning behaviors.</li></ul><h4 id=34-key-trade-offs-and-design-choices><strong>3.4. Key Trade-offs and Design Choices</strong><a hidden class=anchor aria-hidden=true href=#34-key-trade-offs-and-design-choices>#</a></h4><ul><li><strong>Rule-Based vs. Neural Rewards</strong>: Rule-based rewards are prioritized for simplicity and to avoid reward hacking in large-scale RL. Neural rewards are introduced only when necessary (e.g., for general task alignment in DeepSeek-R1).</li><li><strong>Balancing Performance and Readability</strong>: The language consistency reward in DeepSeek-R1 trades off slight performance degradation for improved human interpretability, highlighting the importance of practical usability.</li></ul><h4 id=35-experimental-validation-of-reward-models><strong>3.5. Experimental Validation of Reward Models</strong><a hidden class=anchor aria-hidden=true href=#35-experimental-validation-of-reward-models>#</a></h4><ul><li><strong>DeepSeek-R1-Zero</strong>: Achieves significant reasoning gains (e.g., AIME 2024 Pass@1 from 15.6% to 71.0%) using purely rule-based rewards, demonstrating that complex reasoning can emerge without neural reward models.</li><li><strong>DeepSeek-R1</strong>: Outperforms DeepSeek-R1-Zero on readability and matches OpenAI-o1-1217 on reasoning tasks by combining rule-based and language consistency rewards.</li><li><strong>Distilled Models</strong>: Smaller models (e.g., 14B, 32B) trained on DeepSeek-R1’s rewarded outputs outperform state-of-the-art open-source models, validating the transferability of reward-aligned reasoning patterns.</li></ul><h4 id=36-conclusion><strong>3.6 Conclusion</strong><a hidden class=anchor aria-hidden=true href=#36-conclusion>#</a></h4><p>The paper demonstrates that reward modeling in RL can be tailored to balance reasoning performance, readability, and human alignment. Rule-based rewards enable pure RL-driven reasoning emergence in DeepSeek-R1-Zero, while DeepSeek-R1 enhances this with language consistency and general preference rewards. This approach highlights the flexibility of reward systems in shaping LLM behavior without heavy reliance on neural reward models, paving the way for efficient and interpretable reasoning enhancements.</p><h3 id=4-inference-time-scaling-for-generalist-reward-modeling><strong>4. Inference-Time Scaling for Generalist Reward Modeling</strong><a hidden class=anchor aria-hidden=true href=#4-inference-time-scaling-for-generalist-reward-modeling>#</a></h3><p>Liu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2504.02495>https://doi.org/10.48550/arXiv.2504.02495</a>.</p><p>Liu et al. (2025) address a critical gap in RM: generalizability across domains and efficient resource use. Their approach, Self-Principled Critique Tuning (SPCT), combines generative reward modeling (GRM) with inference-time scaling to improve RM performance without increasing training compute.</p><ul><li><strong>Core Contributions</strong>:<ul><li><strong>SPCT</strong>: A two-phase RL method where models learn to generate adaptive principles and critiques, enhancing reward quality. For example, DeepSeek-GRM-27B with SPCT outperforms scalar RMs on benchmarks like Reward Bench and PPE (Table 2).</li><li><strong>Inference-Time Scaling</strong>: Parallel sampling and a meta RM guide voting on multiple reward samples, expanding the reward space and improving granularity. As shown in Figure 1, DeepSeek-GRM-27B with meta RM achieves 72.8% overall performance, surpassing models like Nemotron-4-340B-Reward.</li></ul></li><li><strong>Connection to Prior Work</strong>: SPCT builds on Constitutional AI’s self-critique and DeepSeek-R1’s RL efficiency, but shifts focus to generalist domains. The meta RM integrates self-evaluation insights (from Kadavath et al.) to filter low-quality rewards, aligning with P(IK)-like confidence metrics.</li></ul><h3 id=the-developmental-arc-from-specialization-to-generalization><strong>The Developmental Arc: From Specialization to Generalization</strong><a hidden class=anchor aria-hidden=true href=#the-developmental-arc-from-specialization-to-generalization>#</a></h3><p>The evolution of RM reflects a shift from human-dependent, task-specific approaches to self-supervised, generalizable frameworks:</p><ol><li><strong>Early RLHF (2020s)</strong>: Relied on massive human labeling, limited to specific domains.</li><li><strong>Constitutional AI (2022)</strong>: Introduced AI-generated feedback and principles, reducing human overhead.</li><li><strong>Self-Evaluation (2022)</strong>: Uncovered models’ ability to assess their own knowledge, enabling confidence-aware RM.</li><li><strong>Task-Oriented RL (2025, DeepSeek-R1)</strong>: Optimized reasoning via RL, demonstrating task-specific RM scaling.</li><li><strong>Generalist Inference-Time Scaling (2025, Liu et al.)</strong>: Extended RM to diverse domains using generative models and inference-time compute, balancing efficiency and performance.</li></ol><h3 id=challenges-and-future-directions><strong>Challenges and Future Directions</strong><a hidden class=anchor aria-hidden=true href=#challenges-and-future-directions>#</a></h3><ul><li><strong>Bias and Calibration</strong>: While SPCT reduces domain bias, models like DeepSeek-GRM still struggle with specific tasks (e.g., verifiable problems, Appendix B).</li><li><strong>Computational Overhead</strong>: Inference-time scaling requires more compute per query, necessitating efficiency improvements.</li><li><strong>Cross-Domain Generalization</strong>: Combining task-specific RM (DeepSeek-R1) with generalist GRM (Liu et al.) remains an open challenge.</li></ul><p>Future work may integrate self-supervised RM with external tools (e.g., code execution for verification) and explore hybrid frameworks that balance training and inference-time scaling.</p><h3 id=conclusion><strong>Conclusion</strong><a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>The landscape of reward modeling is evolving rapidly, driven by innovations that prioritize scalability, self-supervision, and generalizability. From Constitutional AI’s principles to SPCT’s inference-time scaling, these methods collectively push LLMs toward more aligned, transparent, and efficient behavior. As shown in the comparative results across papers, the field is moving toward a future where RM serves as a versatile interface for LLM alignment, enabling models to reason, evaluate, and adapt to diverse human needs.</p><h3 id=key-citations><strong>Key Citations</strong><a hidden class=anchor aria-hidden=true href=#key-citations>#</a></h3><ul><li>Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, <a href=http://arxiv.org/abs/2203.02155>http://arxiv.org/abs/2203.02155</a>.</li><li>Bai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2212.08073>https://doi.org/10.48550/arXiv.2212.08073</a>.</li><li>DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2501.12948>https://doi.org/10.48550/arXiv.2501.12948</a>.</li><li>Liu, Zijun, et al. Inference-Time Scaling for Generalist Reward Modeling. arXiv:2504.02495, arXiv, 5 Apr. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2504.02495>https://doi.org/10.48550/arXiv.2504.02495</a>.</li><li>Kadavath, Saurav, et al. Language Models (Mostly) Know What They Know. arXiv:2207.05221, arXiv, 21 Nov. 2022. arXiv.org, <a href=http://arxiv.org/abs/2207.05221>http://arxiv.org/abs/2207.05221</a>.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/2025/>2025</a></li><li><a href=https://congchan.github.io/tags/large-language-model/>Large Language Model</a></li><li><a href=https://congchan.github.io/tags/reward-modeling/>Reward Modeling</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/multi-token-prediction/><span class=title>« Prev</span><br><span>Multi-token Prediction</span>
</a><a class=next href=https://congchan.github.io/posts/paper-reading-inference-time-scaling-for-generalist-reward-modeling/><span class=title>Next »</span><br><span>Paper Reading - Inference-Time Scaling for Generalist Reward Modeling</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling on x" href="https://x.com/intent/tweet/?text=The%20Evolution%20of%20Reward%20Modeling%20-%20From%20Human%20Feedback%20to%20Generative%20Inference-Time%20Scaling&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fthe-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling%2f&amp;hashtags=2025%2cLargeLanguageModel%2cRewardModeling"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fthe-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling%2f&amp;title=The%20Evolution%20of%20Reward%20Modeling%20-%20From%20Human%20Feedback%20to%20Generative%20Inference-Time%20Scaling&amp;summary=The%20Evolution%20of%20Reward%20Modeling%20-%20From%20Human%20Feedback%20to%20Generative%20Inference-Time%20Scaling&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fthe-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fthe-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling%2f&title=The%20Evolution%20of%20Reward%20Modeling%20-%20From%20Human%20Feedback%20to%20Generative%20Inference-Time%20Scaling"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fthe-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling on whatsapp" href="https://api.whatsapp.com/send?text=The%20Evolution%20of%20Reward%20Modeling%20-%20From%20Human%20Feedback%20to%20Generative%20Inference-Time%20Scaling%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fthe-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling on telegram" href="https://telegram.me/share/url?text=The%20Evolution%20of%20Reward%20Modeling%20-%20From%20Human%20Feedback%20to%20Generative%20Inference-Time%20Scaling&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fthe-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling on ycombinator" href="https://news.ycombinator.com/submitlink?t=The%20Evolution%20of%20Reward%20Modeling%20-%20From%20Human%20Feedback%20to%20Generative%20Inference-Time%20Scaling&u=https%3a%2f%2fcongchan.github.io%2fposts%2fthe-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>