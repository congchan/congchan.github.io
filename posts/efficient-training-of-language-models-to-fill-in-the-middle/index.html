<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Efficient Training of Language Models to Fill in the Middle | Cong's Log</title><meta name=keywords content="2022,Large Language Models"><meta name=description content="Bavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, http://arxiv.org/abs/2207.14255.
data: https://www.github.com/openai/human-eval-infilling
TL:DR
Autoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research."><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/efficient-training-of-language-models-to-fill-in-the-middle/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/efficient-training-of-language-models-to-fill-in-the-middle/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/efficient-training-of-language-models-to-fill-in-the-middle/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Efficient Training of Language Models to Fill in the Middle"><meta property="og:description" content="Bavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, http://arxiv.org/abs/2207.14255. data: https://www.github.com/openai/human-eval-infilling
TL:DR Autoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-11T00:00:00+00:00"><meta property="article:modified_time" content="2022-11-11T00:00:00+00:00"><meta property="article:tag" content="2022"><meta property="article:tag" content="Large Language Models"><meta name=twitter:card content="summary"><meta name=twitter:title content="Efficient Training of Language Models to Fill in the Middle"><meta name=twitter:description content="Bavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, http://arxiv.org/abs/2207.14255.
data: https://www.github.com/openai/human-eval-infilling
TL:DR
Autoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Efficient Training of Language Models to Fill in the Middle","item":"https://congchan.github.io/posts/efficient-training-of-language-models-to-fill-in-the-middle/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Efficient Training of Language Models to Fill in the Middle","name":"Efficient Training of Language Models to Fill in the Middle","description":"Bavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, http://arxiv.org/abs/2207.14255. data: https://www.github.com/openai/human-eval-infilling\nTL:DR Autoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research.\n","keywords":["2022","Large Language Models"],"articleBody":"Bavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, http://arxiv.org/abs/2207.14255. data: https://www.github.com/openai/human-eval-infilling\nTL:DR Autoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research.\nFIM-for-free property, just transform a portion of the training dataset by randomly splitting documents into three parts and moving the middle section to the end, document → (prefix, middle, suffix) → (prefix, suffix, middle), which can be concatenated by sentinel tokens. Best practices for FIM in pretraining: They conducted comprehensive ablations to clarify the effects of various hyperparameters related to training FIM models. Specifically, the FIM rate (probability of applying FIM transformation), different FIM transformation variants, and middle span selection. Finetuning inefficiency: they demonstrate that finetuning with FIM is computationally inefficient. Learning FIM during finetuning requires a significant amount of additional compute to achieve comparable performance levels. New infilling benchmarks: random span infilling and random span infilling light. Need for sampling evaluations: modifying different hyperparameters during FIM training can result in minimal changes in FIM test losses, but significant differences in sampling-based benchmarks. These benchmarks are not only more representative of real-world use cases, but they also reveal improvements that may be overlooked by relying solely on test losses. This finding is crucial, as scaling laws analysis often relies solely on test losses, which can be misleading without additional evaluations. Why Infilling is Important? Iinfilling is important for applications that require context before and after the point of generation, such as coding assistants, docstring generation, import statement generation, or completing a partially written function.\nWhy FIM Transformer based language models can be divided into three broad classes: encoder-only models like BERT, encoder-decoder models like T5, and causal decoder-based models like GPT.\nEncoder-only models are trained with a masked language modeling objective, Encoder-decoder models are trained with a span prediction objective. Causal decoder-based models are trained using the left-to-right next token prediction objective. All model classes are limited in infilling, which involves generating text within a prompt while conditioning on both a prefix and a suffix.\nLeft-to-right models can only condition on the prefix. Encoder-only and encoder-decoder models can condition on suffixes, but the lengths of infill regions seen during training are typically too short for practical use. Method FIM Apply a random transformation to the dataset. They explore two distinct implementations: document-level and context-level. The key difference between the two lies in the stage of the data loading pipeline where the FIM transformation takes place. This decision is driven by the fact that a lengthy document can be divided into multiple contexts, while a context can encompass multiple documents if the documents are relatively short.\nDocument-level FIM: With a probability parameter p (referred to as the FIM rate, p = 0.5 is used for the primary set of models), each document is divided into a prefix, a middle, and a suffix. the term FIM model is used to refer to any model that is trained on a mixture of FIM transformed and normal left-to-right data. Models that are trained without any FIM data, i.e., with a 0% FIM rate, are referred to as AR models. This split occurs before tokenization, when the document is still represented as a character sequence. The document is randomly split uniformly, such that the expected length of each part (prefix, middle, and suffix) is 1/3 of the full document length. Each of the three sections is encoded separately, and sentinel tokens (, , and ) are prepended to the beginning of each section. The three sections are concatenated in the order prefix, suffix, and middle, along with their sentinel tokens, to form the tokenized version of the FIM document: ◦ Enc(prefix) ◦ ◦ Enc(suffix) ◦ ◦ Enc(middle) \\tag{(PSM)} , where ◦ denotes concatenation. Different documents, whether FIM or AR, are concatenated with and given to the model during training. The loss is kept on all three sections (prefix, middle, and suffix), so FIM training does not cause a decrease in the autoregressive learning signal. Preliminary experiments suggest that keeping the loss on all three sections is crucial for the FIM-for-free property to hold. It is important to always train on the tokens as it signals a successful join to the suffix. During inference, the prefix and suffix are encoded and used to prompt the model with ◦ Enc(prefix) ◦ ◦ Enc(suffix) ◦ . \\tag{(PSM inference)}. The model generates samples until it produces the token, indicating successful connection of the prefix and suffix. If the token is not generated within a reasonable inference token budget, it suggests difficulty in connecting the prefix and suffix, and EOT aware best-of-n sampling is used to improve sample quality. SPM mode To improve key-value caching during inference, SPM mode is introduced, where the suffix, prefix, and middle order is swapped, use the ordering [suffix, prefix, middle]. This is because SPM avoids invalidation of keys and values computed in the suffix section when tokens are appended to the prefix. Note that superiority of SPM caching is not universal and may depend on the applications. Two variants of SPM encoding are presented. SPM variant 1 uses ◦ Enc(suffix) ◦ ◦ Enc(prefix) ◦ ◦ Enc(middle) ◦ . (SPM variant 1), while SPM variant 2 uses ◦ ◦ Enc(suffix) ◦ ◦ Enc(prefix) ◦ Enc(middle) ◦ . (SPM variant 2). The reason for using SPM variant 2 is to avoid creating a separation between PSM and SPM, which may result in less transfer between them. My understanding is that this is compatible with empty prefix in PSM mode. Since SPM is already a swap mode, it does not have to strictly follow the sentinel A ◦ Enc(A) format. However, SPM variant 1 has its own advantages, such as being stronger in handling subtokens at the end of the prefix. The choice of which variant to use may depend on the application. In this work, SPM variant 2 is used to emphasize joint training of PSM and SPM and to maximize transfer between them. However, minor changes to the suffix may invalidate the cache for the prefix in SPM mode. SPM also has a slight edge over PSM in infilling benchmarks. The FIM transformation is applied with 50% probability in both PSM and SPM modes to handle both types of formatting in inference. The placement of sentinel tokens in SPM is important when training jointly on SPM and PSM. Context-level FIM In language model training, documents are often joined with a boundary token, referred to as , and are then chunked to the model context length.\nTraining data contains lots of documents. When applying FIM to long documents, a joined by then chunked operation can result in fragmented FIM data where the entire prefix or suffix could get cut out of the context during chunking. To address this issue, FIM can be applied after the chunking step. A context slice may have multiple documents in them joined with the boundary token. The context slice is split based on . At this point, these documents are already tokenized, so applying FIM at the token level is straightforward. Some of the resulting documents are randomly selected to be turned into FIM examples based on a given FIM rate. The examples are then joined again with , and the resulting slice is trimmed to the model’s context length. This technique can boost performance relative to document-level FIM, and adopt context-level FIM in all the main FIM runs in this work. Evaluation The left-to-right test loss is unaffected even though FIM models see the data in its original form half the time, and are simultaneously learning a new skill. They trained a series of models with varying numbers of parameters, ranging from 50M to 6.9B, from scratch with and without 50% FIM augmentation on both natural language and code domains. They found that the left-to-right test loss was not affected by the FIM augmentation, even though the FIM models saw the data in its original form only half the time and were simultaneously learning a new skill.\nTest los\nJoint FIM pretraining does not result in any degradation in standard AR benchmarks as the performance matches within error for both natural language and code.\nHowever, the authors noted that test loss alone is not always sufficient to evaluate model performance. To strengthen their results, they evaluated their models on a suite of standard downstream benchmarks. The performance of the FIM models matched that of the non-FIM models within the margin of error for both natural language and code. The results are presented in Figure 3.\nNLP benchmarks\nThe left plot in Figure 4 provides evidence that a FIM rate even up to 90% does not cause any degradation in left-to-right capabilities.\nHowever, there is a clear sign of degradation in ordinary AR test loss with 100% FIM rate. This suggests that evaluating the FIM capabilities of the models cannot be done solely by considering language modeling perplexity measures such as test loss, but non-loss based evaluations should also be taken into account. SPM is slightly stronger than PSM in the benchmarks in general as evidenced by Figure 6\n","wordCount":"1589","inLanguage":"en","datePublished":"2022-11-11T00:00:00Z","dateModified":"2022-11-11T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/efficient-training-of-language-models-to-fill-in-the-middle/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Efficient Training of Language Models to Fill in the Middle</h1><div class=post-meta><span title='2022-11-11 00:00:00 +0000 UTC'>2022-11-11</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/paper-Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20middle.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tldr aria-label=TL:DR>TL:DR</a></li><li><a href=#why-infilling-is-important aria-label="Why Infilling is Important?">Why Infilling is Important?</a></li><li><a href=#why-fim aria-label="Why FIM">Why FIM</a></li><li><a href=#method aria-label=Method>Method</a><ul><li><a href=#fim aria-label=FIM>FIM</a></li><li><a href=#document-level-fim aria-label="Document-level FIM:">Document-level FIM:</a></li><li><a href=#spm-mode aria-label="SPM mode">SPM mode</a></li><li><a href=#context-level-fim aria-label="Context-level FIM">Context-level FIM</a></li></ul></li><li><a href=#evaluation aria-label=Evaluation>Evaluation</a></li></ul></div></details></div><div class=post-content><p>Bavarian, Mohammad, et al. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255, arXiv, 28 July 2022. arXiv.org, <a href=http://arxiv.org/abs/2207.14255>http://arxiv.org/abs/2207.14255</a>.
data: <a href=https://www.github.com/openai/human-eval-infilling>https://www.github.com/openai/human-eval-infilling</a></p><h1 id=tldr>TL:DR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h1><p>Autoregressive language models can effectively learn to infill text by moving a span of text from the middle of a document to its end, without harming the original generative capability. The training models with this technique, called fill-in-the-middle (FIM), is useful, simple, and efficient, and should be used by default in future autoregressive language models. The study provides best practices and strong default settings for training FIM models and releases infilling benchmarks to aid future research.</p><ul><li>FIM-for-free property, just transform a portion of the training dataset by randomly splitting documents into three parts and moving the middle section to the end, <code>document → (prefix, middle, suffix) → (prefix, suffix, middle)</code>, which can be concatenated by sentinel tokens.</li><li>Best practices for FIM in pretraining: They conducted comprehensive ablations to clarify the effects of various hyperparameters related to training FIM models. Specifically, the FIM rate (probability of applying FIM transformation), different FIM transformation variants, and middle span selection.</li><li>Finetuning inefficiency: they demonstrate that finetuning with FIM is computationally inefficient. Learning FIM during finetuning requires a significant amount of additional compute to achieve comparable performance levels.</li><li>New infilling benchmarks: random span infilling and random span infilling light.</li><li>Need for sampling evaluations: modifying different hyperparameters during FIM training can result in minimal changes in FIM test losses, but significant differences in sampling-based benchmarks. These benchmarks are not only more representative of real-world use cases, but they also reveal improvements that may be overlooked by relying solely on test losses. This finding is crucial, as scaling laws analysis often relies solely on test losses, which can be misleading without additional evaluations.</li></ul><h1 id=why-infilling-is-important>Why Infilling is Important?<a hidden class=anchor aria-hidden=true href=#why-infilling-is-important>#</a></h1><p>Iinfilling is important for applications that require context before and after the point of generation, such as coding assistants, docstring generation, import statement generation, or completing a partially written function.</p><h1 id=why-fim>Why FIM<a hidden class=anchor aria-hidden=true href=#why-fim>#</a></h1><p>Transformer based language models can be divided into three broad classes: encoder-only models like BERT, encoder-decoder models like T5, and causal decoder-based models like GPT.</p><ul><li>Encoder-only models are trained with a masked language modeling objective,</li><li>Encoder-decoder models are trained with a span prediction objective.</li><li>Causal decoder-based models are trained using the left-to-right next token prediction objective.</li></ul><p>All model classes are limited in infilling, which involves generating text within a prompt while conditioning on both a prefix and a suffix.</p><ul><li>Left-to-right models can only condition on the prefix.</li><li>Encoder-only and encoder-decoder models can condition on suffixes, but the lengths of infill regions seen during training are typically too short for practical use.</li></ul><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><h2 id=fim>FIM<a hidden class=anchor aria-hidden=true href=#fim>#</a></h2><p>Apply a random transformation to the dataset. They explore two distinct implementations: document-level and context-level. The key difference between the two lies in the stage of the data loading pipeline where the FIM transformation takes place. This decision is driven by the fact that a lengthy document can be divided into multiple contexts, while a context can encompass multiple documents if the documents are relatively short.</p><h2 id=document-level-fim>Document-level FIM:<a hidden class=anchor aria-hidden=true href=#document-level-fim>#</a></h2><ol><li>With a probability parameter p (referred to as the FIM rate, p = 0.5 is used for the primary set of models), each document is divided into a prefix, a middle, and a suffix. the term FIM model is used to refer to any model that is trained on a mixture of FIM transformed and normal left-to-right data. Models that are trained without any FIM data, i.e., with a 0% FIM rate, are referred to as AR models.</li><li>This split occurs before tokenization, when the document is still represented as a character sequence.</li><li>The document is randomly split uniformly, such that the expected length of each part (prefix, middle, and suffix) is 1/3 of the full document length.</li><li>Each of the three sections is encoded separately, and sentinel tokens (<pre>, <mid>, and <suf>) are prepended to the beginning of each section.</li>
<li>The three sections are concatenated in the order prefix, suffix, and middle, along with their sentinel tokens, to form the tokenized version of the FIM document:  <code>&lt;PRE&gt; ◦ Enc(prefix) ◦ &lt;SUF&gt; ◦ Enc(suffix) ◦ &lt;MID&gt; ◦ Enc(middle) \tag{(PSM)}</code> , where ◦ denotes concatenation.</li>
<li>Different documents, whether FIM or AR, are concatenated with <eot> and given to the model during training.</li>
<li>The loss is kept on all three sections (prefix, middle, and suffix), so FIM training does not cause a decrease in the autoregressive learning signal. Preliminary experiments suggest that keeping the loss on all three sections is crucial for the FIM-for-free property to hold.</li>
<li>It is important to always train on the <eot> tokens as it signals a successful join to the suffix.</li>
<li>During inference, the prefix and suffix are encoded and used to prompt the model with <code>&lt;PRE&gt; ◦ Enc(prefix) ◦ &lt;SUF&gt; ◦ Enc(suffix) ◦ &lt;MID&gt;. \tag{(PSM inference)}</code>. The model generates samples until it produces the <eot> token, indicating successful connection of the prefix and suffix. If the <eot> token is not generated within a reasonable inference token budget, it suggests difficulty in connecting the prefix and suffix, and EOT aware best-of-n sampling is used to improve sample quality.</li>
</ol>
<h2 id=spm-mode>SPM mode<a hidden class=anchor aria-hidden=true href=#spm-mode>#</a></h2>
<ul>
<li>To improve key-value caching during inference, SPM mode is introduced, where the suffix, prefix, and middle order is swapped, use the ordering <code>[suffix, prefix, middle]</code>. This is because SPM avoids invalidation of keys and values computed in the suffix section when tokens are appended to the prefix. Note that superiority of SPM caching is not universal and may depend on the applications.</li>
<li>Two variants of SPM encoding are presented. SPM variant 1 uses <code>&lt;SUF&gt; ◦ Enc(suffix) ◦ &lt;PRE&gt; ◦ Enc(prefix) ◦ &lt;MID&gt; ◦ Enc(middle) ◦ &lt;EOT&gt;. (SPM variant 1)</code>, while SPM variant 2 uses <code>&lt;PRE&gt; ◦ &lt;SUF&gt; ◦ Enc(suffix) ◦ &lt;MID&gt; ◦ Enc(prefix) ◦ Enc(middle) ◦ &lt;EOT&gt;. (SPM variant 2)</code>. The reason for using SPM variant 2 is to avoid creating a separation between PSM and SPM, which may result in less transfer between them. My understanding is that this is compatible with empty prefix in PSM mode. Since SPM is already a swap mode, it does not have to strictly follow the <code>sentinel A ◦ Enc(A)</code> format.</li>
<li>However, SPM variant 1 has its own advantages, such as being stronger in handling subtokens at the end of the prefix. The choice of which variant to use may depend on the application. In this work, SPM variant 2 is used to emphasize joint training of PSM and SPM and to maximize transfer between them.</li>
<li>However, minor changes to the suffix may invalidate the cache for the prefix in SPM mode.</li>
<li>SPM also has a slight edge over PSM in infilling benchmarks.</li>
<li>The FIM transformation is applied with 50% probability in both PSM and SPM modes to handle both types of formatting in inference. The placement of sentinel tokens in SPM is important when training jointly on SPM and PSM.</li>
</ul>
<h2 id=context-level-fim>Context-level FIM<a hidden class=anchor aria-hidden=true href=#context-level-fim>#</a></h2>
<p>In language model training, documents are often joined with a boundary token, referred to as <eot>, and are then chunked to the model context length.</p>
<ul>
<li>Training data contains lots of documents. When applying FIM to long documents, a joined by <eot> then chunked operation can result in fragmented FIM data where the entire prefix or suffix could get cut out of the context during chunking.</li>
<li>To address this issue, FIM can be applied after the chunking step. A context slice may have multiple documents in them joined with the <eot> boundary token. The context slice  is split based on <eot>. At this point, these documents are already tokenized, so applying FIM at the token level is straightforward. Some of the resulting documents are randomly selected to be turned into FIM examples based on a given FIM rate. The examples are then joined again with <eot>, and the resulting slice is trimmed to the model&rsquo;s context length.</li>
<li>This technique can boost performance relative to document-level FIM, and adopt context-level FIM in all the main FIM runs in this work.</li>
<li></li>
</ul>
<h1 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h1>
<ul>
<li>The left-to-right test loss is unaffected even though FIM models see the data in its original form half the time, and are simultaneously learning a new skill.
<ul>
<li>
<p>They trained a series of models with varying numbers of parameters, ranging from 50M to 6.9B, from scratch with and without 50% FIM augmentation on both natural language and code domains. They found that the left-to-right test loss was not affected by the FIM augmentation, even though the FIM models saw the data in its original form only half the time and were simultaneously learning a new skill.</p>
</li>
<li>
<p>Test los</p>
<p><img loading=lazy src=/images/papers/paper15.png></p>
</li>
</ul>
</li>
</ul>
<p><img loading=lazy src=/images/papers/paper15-1.png></p>
<ul>
<li>
<p>Joint FIM pretraining does not result in any degradation in standard AR benchmarks as the performance matches within error for both natural language and code.</p>
<ul>
<li>
<p>However, the authors noted that test loss alone is not always sufficient to evaluate model performance. To strengthen their results, they evaluated their models on a suite of standard downstream benchmarks. The performance of the FIM models matched that of the non-FIM models within the margin of error for both natural language and code. The results are presented in Figure 3.</p>
</li>
<li>
<p>NLP benchmarks</p>
<p><img loading=lazy src=/images/papers/paper15-2.png></p>
</li>
</ul>
</li>
<li>
<p>The left plot in Figure 4 provides evidence that a FIM rate even up to 90% does not cause any degradation in left-to-right capabilities.</p>
<ul>
<li>However, there is a clear sign of degradation in ordinary AR test loss with 100% FIM rate.</li>
<li>This suggests that evaluating the FIM capabilities of the models cannot be done solely by considering language modeling perplexity measures such as test loss, but non-loss based evaluations should also be taken into account.</li>
</ul>
<p><img loading=lazy src=/images/papers/paper15-3.png></p>
</li>
<li>
<p>SPM is slightly stronger than PSM in the benchmarks in general as evidenced by Figure 6</p>
<p><img loading=lazy src=/images/papers/paper15-4.png></p>
</li>
</ul>


  </div>

  <footer class=post-footer>
    <ul class=post-tags>
      <li><a href=https://congchan.github.io/tags/2022/>2022</a></li>
      <li><a href=https://congchan.github.io/tags/large-language-models/>Large Language Models</a></li>
    </ul>
<nav class=paginav>
  <a class=prev href=https://congchan.github.io/posts/cot-on-bbh-challenging-big-bench-tasks-and-whether-chain-of-thought-can-solve-them/>
    <span class=title>« Prev</span>
    <br>
    <span>CoT on BBH - Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them</span>
  </a>
  <a class=next href=https://congchan.github.io/posts/the-curious-case-of-neural-text-degeneration/>
    <span class=title>Next »</span>
    <br>
    <span>The Curious Case of Neural Text Degeneration</span>
  </a>
</nav>


<ul class=share-buttons>
    <li>
        <a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training of Language Models to Fill in the Middle on x" href="https://x.com/intent/tweet/?text=Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fefficient-training-of-language-models-to-fill-in-the-middle%2f&amp;hashtags=2022%2cLargeLanguageModels">
            <svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg>
        </a>
    </li>
    <li>
        <a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training of Language Models to Fill in the Middle on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fefficient-training-of-language-models-to-fill-in-the-middle%2f&amp;title=Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle&amp;summary=Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fefficient-training-of-language-models-to-fill-in-the-middle%2f">
            <svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
        </a>
    </li>
    <li>
        <a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training of Language Models to Fill in the Middle on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fefficient-training-of-language-models-to-fill-in-the-middle%2f&title=Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle">
            <svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
        </a>
    </li>
    <li>
        <a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training of Language Models to Fill in the Middle on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fefficient-training-of-language-models-to-fill-in-the-middle%2f">
            <svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
        </a>
    </li>
    <li>
        <a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training of Language Models to Fill in the Middle on whatsapp" href="https://api.whatsapp.com/send?text=Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fefficient-training-of-language-models-to-fill-in-the-middle%2f">
            <svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
        </a>
    </li>
    <li>
        <a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training of Language Models to Fill in the Middle on telegram" href="https://telegram.me/share/url?text=Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fefficient-training-of-language-models-to-fill-in-the-middle%2f">
            <svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
        </a>
    </li>
    <li>
        <a target=_blank rel="noopener noreferrer" aria-label="share Efficient Training of Language Models to Fill in the Middle on ycombinator" href="https://news.ycombinator.com/submitlink?t=Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle&u=https%3a%2f%2fcongchan.github.io%2fposts%2fefficient-training-of-language-models-to-fill-in-the-middle%2f">
            <svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class=footer>
        <span>&copy; 2025 <a href=https://congchan.github.io/>Cong&#39;s Log</a></span> · 

    <span>
        Powered by
        <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
    </span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g>
    <svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a>

<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script>
<script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script>
</body>

</html>
