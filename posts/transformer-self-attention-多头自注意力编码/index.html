<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformer & Self-Attention (多头)自注意力编码 | Cong's Log</title><meta name=keywords content="Attention,NLP"><meta name=description content="注意力机制的原理是计算query和每个key之间的相关性$\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。

注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章Attention is all you need提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。
近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。
因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。
所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用self-attention 自注意力机制(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时Query, Key和Value都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。
Transformer总体架构
与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:

Transformer使用注意力来编码(不需要LSTM/CNN之类的)。
引入自注意力机制
Multi-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。

Transformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。

Transformer的编码阶段概括起来就是：

首先为每个单词生成初始表达或embeddings。这些由空心圆表示。
然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。

Encoder的self-attention中, 所有Key, Value和Query都来自同一位置, 即上一层encoder的输出。
解码器类似，所有Key, Value和Query都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前query位置之前的部分。生成Query时, 不仅关注前一步的输出，还参考编码器的最后一层输出。

N = 6, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。"><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/transformer-self-attention-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BC%96%E7%A0%81/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/transformer-self-attention-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BC%96%E7%A0%81/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/transformer-self-attention-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BC%96%E7%A0%81/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Transformer & Self-Attention (多头)自注意力编码"><meta property="og:description" content="注意力机制的原理是计算query和每个key之间的相关性$\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。
注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章Attention is all you need提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。
近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。
因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。
所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用self-attention 自注意力机制(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时Query, Key和Value都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。
Transformer总体架构 与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:
Transformer使用注意力来编码(不需要LSTM/CNN之类的)。 引入自注意力机制 Multi-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。 Transformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。
Transformer的编码阶段概括起来就是：
首先为每个单词生成初始表达或embeddings。这些由空心圆表示。 然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。 Encoder的self-attention中, 所有Key, Value和Query都来自同一位置, 即上一层encoder的输出。
解码器类似，所有Key, Value和Query都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前query位置之前的部分。生成Query时, 不仅关注前一步的输出，还参考编码器的最后一层输出。
N = 6, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-11-30T00:00:00+00:00"><meta property="article:modified_time" content="2018-11-30T00:00:00+00:00"><meta property="article:tag" content="Attention"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformer & Self-Attention (多头)自注意力编码"><meta name=twitter:description content="注意力机制的原理是计算query和每个key之间的相关性$\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。

注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章Attention is all you need提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。
近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。
因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。
所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用self-attention 自注意力机制(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时Query, Key和Value都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。
Transformer总体架构
与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:

Transformer使用注意力来编码(不需要LSTM/CNN之类的)。
引入自注意力机制
Multi-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。

Transformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。

Transformer的编码阶段概括起来就是：

首先为每个单词生成初始表达或embeddings。这些由空心圆表示。
然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。

Encoder的self-attention中, 所有Key, Value和Query都来自同一位置, 即上一层encoder的输出。
解码器类似，所有Key, Value和Query都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前query位置之前的部分。生成Query时, 不仅关注前一步的输出，还参考编码器的最后一层输出。

N = 6, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Transformer \u0026 Self-Attention (多头)自注意力编码","item":"https://congchan.github.io/posts/transformer-self-attention-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BC%96%E7%A0%81/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer \u0026 Self-Attention (多头)自注意力编码","name":"Transformer \u0026 Self-Attention (多头)自注意力编码","description":"注意力机制的原理是计算query和每个key之间的相关性$\\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。\n注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章Attention is all you need提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。\n近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。\n因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。\n所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用self-attention 自注意力机制(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时Query, Key和Value都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。\nTransformer总体架构 与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:\nTransformer使用注意力来编码(不需要LSTM/CNN之类的)。 引入自注意力机制 Multi-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。 Transformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。\nTransformer的编码阶段概括起来就是：\n首先为每个单词生成初始表达或embeddings。这些由空心圆表示。 然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。 Encoder的self-attention中, 所有Key, Value和Query都来自同一位置, 即上一层encoder的输出。\n解码器类似，所有Key, Value和Query都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前query位置之前的部分。生成Query时, 不仅关注前一步的输出，还参考编码器的最后一层输出。\nN = 6, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。\n","keywords":["Attention","NLP"],"articleBody":"注意力机制的原理是计算query和每个key之间的相关性$\\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。\n注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章Attention is all you need提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。\n近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。\n因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。\n所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用self-attention 自注意力机制(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时Query, Key和Value都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。\nTransformer总体架构 与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:\nTransformer使用注意力来编码(不需要LSTM/CNN之类的)。 引入自注意力机制 Multi-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。 Transformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。\nTransformer的编码阶段概括起来就是：\n首先为每个单词生成初始表达或embeddings。这些由空心圆表示。 然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。 Encoder的self-attention中, 所有Key, Value和Query都来自同一位置, 即上一层encoder的输出。\n解码器类似，所有Key, Value和Query都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前query位置之前的部分。生成Query时, 不仅关注前一步的输出，还参考编码器的最后一层输出。\nN = 6, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。\n编码器:\nStage 1 - 输入编码: 序列的顺序信息是非常重要的。由于没有循环，也没有卷积，因此使用“位置编码”表示序列中每个标记的绝对（或相对）位置的信息。 positional encodings $\\oplus$ embedded input Stage 2 – Multi-head self-attention 和 Stage 3 – position-wise FFN. 两个阶段都是用来残差连接, 接着正则化输出层 Stage1_out = Embedding512 + TokenPositionEncoding512 Stage2_out = layer_normalization(multihead_attention(Stage1_out) + Stage1_out) Stage3_out = layer_normalization(FFN(Stage2_out) + Stage2_out) out_enc = Stage3_out 解码器的架构类似，但它在第3阶段采用了附加层, 在输出层上的 mask multi-head attention:\nStage 1 – 输入解码: 输入 output embedding，偏移一个位置以确保对位置i的预测仅取决于\u003c i的位置。 def shift_right_3d(x, pad_value=None): \"\"\"Shift the second dimension of x right by one.\"\"\" if pad_value is None: shifted_targets = tf.pad(x, [[0, 0], [1, 0], [0, 0]])[:, :-1, :] else: shifted_targets = tf.concat([pad_value, x], axis=1)[:, :-1, :] return shifted_targets Stage 2 - Masked Multi-head self-attention: 需要有一个mask来防止当前位置i的生成任务看到后续\u003e i位置的信息。 # 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html def subsequent_mask(size): \"Mask out subsequent positions.\" attn_shape = (1, size, size) subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') return torch.from_numpy(subsequent_mask) == 0 # # The attention mask shows the position each tgt word (row) is allowed to look at (column). # Words are blocked for attending to future words during training. plt.figure(figsize=(5,5)) plt.imshow(subsequent_mask(20)[0]) 阶段2,3和4同样使用了残差连接，然后在输出使用归一化层。\nStage1_out = OutputEmbedding512 + TokenPositionEncoding512 Stage2_Mask = masked_multihead_attention(Stage1_out) Stage2_Norm1 = layer_normalization(Stage2_Mask) + Stage1_out Stage2_Multi = multihead_attention(Stage2_Norm1 + out_enc) + Stage2_Norm1 Stage2_Norm2 = layer_normalization(Stage2_Multi) + Stage2_Multi Stage3_FNN = FNN(Stage2_Norm2) Stage3_Norm = layer_normalization(Stage3_FNN) + Stage2_Norm2 out_dec = Stage3_Norm 可以利用开源的Tensor2Tensor，通过调用几个命令来训练Transformer网络进行翻译和解析。\n通过Self Attention对比Attention有什么增益呢？可以看到，自注意力算法可以捕获同一个句子中单词之间的语义特征, 比如共指消解（coreference resolution），例如句子中的单词“it”可以根据上下文引用句子的不同名词。除此之外, 理论上也可以捕捉一些语法特征. ![](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/CoreferenceResolution.png “Co-reference resolution. 两边的\"it\"指向不同的词. Adopted from Google Blog.”)\n其实在LSTM_encoder-LSTM_decoder架构上的Attention也可以做到相同的操作, 但效果却不太好. 问题可能在于此时的Attention处理的不是纯粹的一个个序列编码, 而是经过LSTM(复杂的门控记忆与遗忘)编码后的包含前面时间步输入信息的一个个序列编码, 这个导致Attention的软寻址难度增大. 而现在是2019年, 几乎主流的文本编码方案都转投Transformer了, 可见单纯利用self-attention编码其实效率更高.\nAttention Vaswani, 2017明确定义了使用的注意力算法\n$$\\begin{eqnarray} Attention (Q,K,V) = softmax \\Big( \\frac{QK^T}{\\sqrt{d_k}} \\Big) V \\end{eqnarray},$$其中$\\mathbb{Q}\\in\\mathbb{R}^{n\\times d_k}, \\mathbb{K}\\in\\mathbb{R}^{m\\times d_k}, \\mathbb{V}\\in\\mathbb{R}^{m\\times d_v}$. 这就是传统的Scaled Dot-Product Attention, 把这个Attention理解为一个神经网络层，将$n\\times d_k$的序列$Q$编码成了一个新的$n\\times d_v$的序列。因为对于较大的$d_k$，内积会数量级地放大, 太大的话softmax可能会被推到梯度消失区域, softmax后就非0即1(那就是hardmax), 所以$q \\cdot k = \\sum_{i=1}^{d_k}q_i k_i$按照比例因子$\\sqrt{d_k}$缩放.\nBERT/ALBERT中的点积attention实现:\n# dot_product_attention from bert implementation def dot_product_attention(q, k, v, bias, dropout_rate=0.0): \"\"\"Dot-product attention. Args: q: Tensor with shape [..., length_q, depth_k]. k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must match with q. v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must match with q. bias: bias Tensor (see attention_bias()) dropout_rate: a float. Returns: Tensor with shape [..., length_q, depth_v]. \"\"\" logits = tf.matmul(q, k, transpose_b=True) # [..., length_q, length_kv] logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1]))) if bias is not None: # `attention_mask` = [B, T] from_shape = get_shape_list(q) if len(from_shape) == 4: broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32) elif len(from_shape) == 5: # from_shape = [B, N, Block_num, block_size, depth]# broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], from_shape[3], 1], tf.float32) bias = tf.matmul(broadcast_ones, tf.cast(bias, tf.float32), transpose_b=True) # Since attention_mask is 1.0 for positions we want to attend and 0.0 for # masked positions, this operation will create a tensor which is 0.0 for # positions we want to attend and -10000.0 for masked positions. adder = (1.0 - bias) * -10000.0 # Since we are adding it to the raw scores before the softmax, this is # effectively the same as removing these entirely. logits += adder else: adder = 0.0 attention_probs = tf.nn.softmax(logits, name=\"attention_probs\") attention_probs = dropout(attention_probs, dropout_rate) return tf.matmul(attention_probs, v) # 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html def attention(query, key, value, mask=None, dropout=0.0): \"Compute 'Scaled Dot Product Attention'\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) \\ / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = F.softmax(scores, dim = -1) # (Dropout described below) p_attn = F.dropout(p_attn, p=dropout) return torch.matmul(p_attn, value), p_attn 这只是注意力的一种形式，还有其他比如query跟key的运算方式是拼接后再内积一个参数向量，权重也不一定要归一化，等等。\nSelf-Attention (SA) 在实际的应用中, 不同的场景的$Q,K,V$是不一样的, 如果是SQuAD的话，$Q$是文章的向量序列，$K=V$为问题的向量序列，输出就是Aligned Question Embedding。\nGoogle所说的自注意力(SA), 就是$Attention(\\mathbb{X},\\mathbb{X},\\mathbb{X})$, 通过在序列自身做Attention，寻找序列自身内部的联系。Google论文的主要贡献之一是它表明了SA在序列编码部分是相当重要的，甚至可以替代传统的RNN(LSTM), CNN, 而之前关于Seq2Seq的研究基本都是关注如何把注意力机制用在解码部分。\n编码时，自注意力层处理来自相同位置的输入$queries, keys, value$，即编码器前一层的输出。编码器中的每个位置都可以关注前一层的所有位置.\n在解码器中，SA层使每个位置能够关注解码器中当前及之前的所有位置。为了保持 auto-regressive 属性，需要阻止解码器中的向左信息流, 所以要在scaled dot-product attention层中屏蔽（设置为-∞）softmax输入中与非法连接相对应的所有值.\n作者使用SA层而不是CNN或RNN层的动机是:\n最小化每层的总计算复杂度: SA层通过$O(1)$数量的序列操作连接所有位置. ($O(n)$ in RNN) 最大化可并行化计算：对于序列长度$n$ \u003c representation dimensionality $d$（对于SOTA序列表达模型，如word-piece, byte-pair）。对于非常长的序列$n \u003e d$, SA可以仅考虑以相应输出位置为中心的输入序列中的某个大小$r$的邻域，从而将最大路径长度增加到$O(n/r)$ 最小化由不同类型层组成的网络中任意两个输入和输出位置之间的最大路径长度。任何输入和输出序列中的位置组合之间的路径越短，越容易学习长距离依赖。 Multi-head Attention Transformer的SA将关联输入和输出序列中的（特别是远程）位置的计算量减少到$O(1)$。然而，这是以降低有效分辨率为代价的，因为注意力加权位置被平均了。为了弥补这种损失, 文章提出了 Multi-head Attention:\n$h=8$ attention layers (“heads”): 将key $K$ 和 query $Q$ 线性投影到 $d_k$ 维度, 将value $V$ 投影到$d_v$维度, (线性投影的目的是减少维度) $$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\\dots,h$$ 投影是参数矩阵$W^Q_i, W^K_i\\in\\mathbb{R}^{d_{model}\\times d_k}, W^V_i\\in\\mathbb{R}^{d_{model}\\times d_v}$ $d_k=d_v=d_{model}/h = 64$ 每层并行地应用 scaled-dot attention(用不同的线性变换), 得到$d_v$维度的输出 把每一层的输出拼接在一起 $Concat(head_1,\\dots,head_h)$ 再线性变换上一步的拼接向量$MultiHeadAttention(Q,K,V) = Concat(head_1,\\dots,head_h) W^O$, where $W^0\\in\\mathbb{R}^{d_{hd_v}\\times d_{model}}$ 因为Transformer只是把原来$d_{model}$维度的注意力函数计算并行分割为$h$个独立的$d_{model}/h$维度的head, 所以计算量相差不大.\nBERT/ALBERT中的multi-head attention层实现:\ndef attention_layer(from_tensor, to_tensor, attention_mask=None, num_attention_heads=1, query_act=None, key_act=None, value_act=None, attention_probs_dropout_prob=0.0, initializer_range=0.02, batch_size=None, from_seq_length=None, to_seq_length=None): \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`. Args: from_tensor: float Tensor of shape [batch_size, from_seq_length, from_width]. to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width]. attention_mask: (optional) int32 Tensor of shape [batch_size, from_seq_length, to_seq_length]. The values should be 1 or 0. The attention scores will effectively be set to -infinity for any positions in the mask that are 0, and will be unchanged for positions that are 1. num_attention_heads: int. Number of attention heads. query_act: (optional) Activation function for the query transform. key_act: (optional) Activation function for the key transform. value_act: (optional) Activation function for the value transform. attention_probs_dropout_prob: (optional) float. Dropout probability of the attention probabilities. initializer_range: float. Range of the weight initializer. batch_size: (Optional) int. If the input is 2D, this might be the batch size of the 3D version of the `from_tensor` and `to_tensor`. from_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `from_tensor`. to_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `to_tensor`. Returns: float Tensor of shape [batch_size, from_seq_length, num_attention_heads, size_per_head]. Raises: ValueError: Any of the arguments or tensor shapes are invalid. \"\"\" from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) to_shape = get_shape_list(to_tensor, expected_rank=[2, 3]) size_per_head = int(from_shape[2]/num_attention_heads) if len(from_shape) != len(to_shape): raise ValueError( \"The rank of `from_tensor` must match the rank of `to_tensor`.\") if len(from_shape) == 3: batch_size = from_shape[0] from_seq_length = from_shape[1] to_seq_length = to_shape[1] elif len(from_shape) == 2: if (batch_size is None or from_seq_length is None or to_seq_length is None): raise ValueError( \"When passing in rank 2 tensors to attention_layer, the values \" \"for `batch_size`, `from_seq_length`, and `to_seq_length` \" \"must all be specified.\") # Scalar dimensions referenced here: # B = batch size (number of sequences) # F = `from_tensor` sequence length # T = `to_tensor` sequence length # N = `num_attention_heads` # H = `size_per_head` # `query_layer` = [B, F, N, H] q = dense_layer_3d(from_tensor, num_attention_heads, size_per_head, create_initializer(initializer_range), query_act, \"query\") # `key_layer` = [B, T, N, H] k = dense_layer_3d(to_tensor, num_attention_heads, size_per_head, create_initializer(initializer_range), key_act, \"key\") # `value_layer` = [B, T, N, H] v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head, create_initializer(initializer_range), value_act, \"value\") q = tf.transpose(q, [0, 2, 1, 3]) k = tf.transpose(k, [0, 2, 1, 3]) v = tf.transpose(v, [0, 2, 1, 3]) if attention_mask is not None: attention_mask = tf.reshape( attention_mask, [batch_size, 1, to_seq_length, 1]) # 'new_embeddings = [B, N, F, H]' new_embeddings = dot_product_attention(q, k, v, attention_mask, attention_probs_dropout_prob) return tf.transpose(new_embeddings, [0, 2, 1, 3]) 可以看到k和v都是to_tensor.\n# 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \"Take in model size and number of heads.\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.p = dropout self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None def forward(self, query, key, value, mask=None): if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model =\u003e h x d_k query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention(query, key, value, mask=mask, dropout=self.p) # 3) \"Concat\" using a view and apply a final linear. x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) return self.linears[-1](x) NMT中Transformer以三种不同的方式使用Multi-head Attention：\n在encoder-decoder attention层中，queries来自前一层decoder层，并且 memory keys and values 来自encoder的输出。这让decoder的每个位置都可以注意到输入序列的所有位置。这其实还原了典型的seq2seq模型里常用的编码器 - 解码器注意力机制（例如Bahdanau et al., 2014或Conv2S2）。 编码器本身也包含了self-attention layers。在self-attention layers中，所有 keys, values and queries 来自相同的位置，在这里是编码器中前一层的输出。这样，编码器的每个位置都可以注意到前一层的所有位置。 with tf.variable_scope(\"attention_1\"): with tf.variable_scope(\"self\"): attention_output = attention_layer( from_tensor=layer_input, to_tensor=layer_input, attention_mask=attention_mask, num_attention_heads=num_attention_heads, attention_probs_dropout_prob=attention_probs_dropout_prob, initializer_range=initializer_range) 类似地，解码器中的 self-attention layers 允许解码器的每个位置注意到解码器中包括该位置在内的所有前面的位置（有mask屏蔽了后面的位置）。需要阻止解码器中的向左信息流以保持自回归属性(auto-regressive 可以简单理解为时序序列的特性, 只能从左到右, 从过去到未来)。我们通过在scaled dot-product attention层中屏蔽（设置为-∞）softmax输入中与非法连接相对应的所有值来维持该特性。 Position-wise Feed-Forward Networks 在编码器和解码器中，每个层都包含一个全连接的前馈网络(FFN)，FFN 分别应用于每个位置，使用相同的两个线性变换和一个ReLU $$FFN(x) = max(0, xW_1+b_1) W_2 + b_2$$ 虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。它的工作方式类似于两个内核大小为1的卷积层. 输入/输出维度是$d_{model}=512$, 内层的维度$d_{ff} = 2048$.\n# 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html class PositionwiseFeedForward(nn.Module): \"Implements FFN equation.\" def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() # Torch linears have a `b` by default. self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) Positional Encoding 在解码时序信息时，LSTM模型通过时间步的概念以输入/输出流一次一个的形式编码的. 而Transformer选择把时序编码为正弦波。这些信号作为额外的信息加入到输入和输出中以表达时序信息.\n这种编码使模型能够感知到当前正在处理的是输入（或输出）序列的哪个部分。位置编码可以学习或者使用固定参数。作者进行了测试（PPL，BLEU），显示两种方式表现相似。文中作者选择使用固定的位置编码参数:\n$$ \\begin{eqnarray} PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \\end{eqnarray} $$ $$ \\begin{eqnarray} PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\\end{eqnarray} $$ 其中$pos$是位置，$i$是维度。\n也就是说，位置编码的每个维度对应于正弦余弦曲线的拼接。波长形成从2π到10000⋅2π的几何级数。选择这个函数，是因为假设它能让模型容易地学习相对位置，因为对于任意固定偏移$k$，$PE_{pos + k}$可以表示为$PE_{pos}$的线性函数。\n# 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html class PositionalEncoding(nn.Module): \"Implement the PE function.\" def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) return self.dropout(x) 位置编码将根据位置添加正弦余弦波。每个维度的波的频率和偏移是不同的。\nplt.figure(figsize=(15, 5)) pe = PositionalEncoding(20, 0) y = pe.forward(Variable(torch.zeros(1, 100, 20))) plt.plot(np.arange(100), y[0, :, 4:8].data.numpy()) plt.legend([\"dim %d\"%p for p in [4,5,6,7]]) 直观的理解是，将这些值添加到embedding中，一旦它们被投影到$Q / K / V$向量和dot product attention中，就给embedding向量之间提供了有意义的相对距离。\nShared-Weight Embeddings and Softmax 与其他序列转导模型类似，使用可学习的Embeddings将 input tokens and output tokens 转换为维度$d_{model}$的向量。通过线性变换和softmax函数将解码器的输出向量转换为预测的token概率。在Transformer模型中，两个嵌入层和pre-softmax线性变换之间共享相同的权重矩阵，在Embeddings层中，将权重乘以$\\sqrt{d_{\\text{model}}}$. 这些都是当前主流的操作。\n# 使用pytorch版本的教程中提供的范例 # http://nlp.seas.harvard.edu/2018/04/03/attention.html class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) 启发 作者已经进行了一系列测试（论文表3），其中他们讨论N = 6层的建议，模型大小为512，基于h = 8个heads，键值维度为64，使用100K步。\n还指出，由于模型质量随着$d_k$（行B）的减小而降低，因此可以进一步优化点积兼容性功能。\n其声称提出的固定正弦位置编码，与学习到的位置编码相比，产生几乎相等的分数。\n算法适合哪些类型的问题？ 序列转导（语言翻译） 语法选区解析的经典语言分析任务 syntactic constituency parsing 共指消解 coreference resolution 参考资料 https://research.googleblog.com/2017/08/transformer-novel-neural-network.html https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/ http://nlp.seas.harvard.edu/2018/04/03/attention.html\n","wordCount":"1539","inLanguage":"en","datePublished":"2018-11-30T00:00:00Z","dateModified":"2018-11-30T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/transformer-self-attention-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BC%96%E7%A0%81/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Transformer & Self-Attention (多头)自注意力编码</h1><div class=post-meta><span title='2018-11-30 00:00:00 +0000 UTC'>2018-11-30</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/NLP-attention-03-self-attention.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#transformer%e6%80%bb%e4%bd%93%e6%9e%b6%e6%9e%84 aria-label=Transformer总体架构>Transformer总体架构</a></li><li><a href=#attention aria-label=Attention>Attention</a></li><li><a href=#self-attention-sa aria-label="Self-Attention (SA)">Self-Attention (SA)</a></li><li><a href=#multi-head-attention aria-label="Multi-head Attention">Multi-head Attention</a></li><li><a href=#position-wise-feed-forward-networks aria-label="Position-wise Feed-Forward Networks">Position-wise Feed-Forward Networks</a></li><li><a href=#positional-encoding aria-label="Positional Encoding">Positional Encoding</a></li><li><a href=#shared-weight-embeddings-and-softmax aria-label="Shared-Weight Embeddings and Softmax">Shared-Weight Embeddings and Softmax</a></li><li><a href=#%e5%90%af%e5%8f%91 aria-label=启发>启发</a></li><li><a href=#%e7%ae%97%e6%b3%95%e9%80%82%e5%90%88%e5%93%aa%e4%ba%9b%e7%b1%bb%e5%9e%8b%e7%9a%84%e9%97%ae%e9%a2%98 aria-label=算法适合哪些类型的问题？>算法适合哪些类型的问题？</a></li><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label=参考资料>参考资料</a></li></ul></div></details></div><div class=post-content><p>注意力机制的原理是计算query和每个key之间的相关性$\alpha_c(q,k_i)$以获得注意力分配权重。在大部分NLP任务中，key和value都是输入序列的编码。</p><p>注意力机制一般是用于提升seq2seq或者encoder-decoder架构的表现。但这篇2017 NIPS的文章<a href=https://arxiv.org/abs/1706.03762>Attention is all you need</a>提出我们可以仅依赖注意力机制就可以完成很多任务. 文章的动机是LSTM这种时序模型速度实在是太慢了。</p><p>近些年来，RNN（及其变种 LSTM, GRU）已成为很多nlp任务如机器翻译的经典网络结构。RNN从左到右或从右到左的方式顺序处理语言。RNN的按顺序处理的性质也使得其更难以充分利用现代快速计算设备，例如GPU等优于并行而非顺序处理的计算单元。虽然卷积神经网络（CNN）的时序性远小于RNN，但CNN体系结构如ByteNet或ConvS2S中，糅合远距离部分的信息所需的步骤数仍随着距离的增加而增长。</p><p>因为一次处理一个单词，RNN需要处理多个时序的单词来做出依赖于长远离单词的决定。但各种研究和实验逐渐表明，决策需要的步骤越多，循环网络就越难以学习如何做出这些决定。而本身LSTM就是为了解决long term dependency问题，但是解决得并不好。很多时候还需要额外加一层注意力层来处理long term dependency。</p><p>所以这次他们直接在编码器和解码器之间直接用attention，这样句子单词的依赖长度最多只有1，减少了信息传输路径。他们称之为Transformer。Transformer只执行一小段constant的步骤（根据经验选择）。在encoder和decoder中，分别应用<strong>self-attention 自注意力机制</strong>(也称为intra Attention), 顾名思义，指的不是传统的seq2seq架构中target和source之间的Attention机制，而是source或者target自身元素之间的Attention机制。也就是说此时<code>Query</code>, <code>Key</code>和<code>Value</code>都一样, 都是输入或者输出的序列编码. 具体计算过程和其他attention一样的，只是计算对象发生了变化. Self-attention 直接模拟句子中所有单词之间的关系，不管它们之间的位置如何。比如子“I arrived at the bank after crossing the river”，要确定“bank”一词是指河岸而不是金融机构，Transformer可以学会立即关注“river”这个词并在一步之内做出这个决定。</p><h3 id=transformer总体架构>Transformer总体架构<a hidden class=anchor aria-hidden=true href=#transformer总体架构>#</a></h3><p>与过去流行的使用基于自回归网络的Seq2Seq模型框架不同:</p><ol><li>Transformer使用注意力来编码(不需要LSTM/CNN之类的)。</li><li>引入自注意力机制</li><li>Multi-Headed Attention Mechanism: 在编码器和解码器中使用 Multi-Headed self-attention。</li></ol><p>Transformer也是基于encoder-decoder的架构。具体地说，为了计算给定单词的下一个表示 - 例如“bank” - Transformer将其与句子中的所有其他单词进行比较。这些比较的结果就是其他单词的注意力权重。这些注意力权重决定了其他单词应该为“bank”的下一个表达做出多少贡献。在计算“bank”的新表示时，能够消除歧义的“river”可以获得更高的关注。将注意力权重用来加权平均所有单词的表达，然后将加权平均的表达喂给一个全连接网络以生成“bank”的新表达，以反映出该句子正在谈论的是“河岸”。</p><p><img loading=lazy src=/images/transform20fps.gif title="image from: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"></p><p>Transformer的编码阶段概括起来就是：</p><ol><li>首先为每个单词生成初始表达或embeddings。这些由空心圆表示。</li><li>然后，对于每一个词, 使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达，由实心球表示。并基于前面生成的表达, 连续地构建新的表达（下一层的实心圆）对每个单词并行地重复多次这种处理。</li></ol><p>Encoder的self-attention中, 所有<code>Key</code>, <code>Value</code>和<code>Query</code>都来自同一位置, 即上一层encoder的输出。</p><p>解码器类似，所有<code>Key</code>, <code>Value</code>和<code>Query</code>都来自同一位置, 即上一层decoder的输出, 不过只能看到上一层对应当前<code>query</code>位置之前的部分。生成<code>Query</code>时, 不仅关注前一步的输出，还参考编码器的最后一层输出。</p><p><img loading=lazy src=/images/transformer.png title="单层编码器（左）和解码器（右），由 N = 6 个相同的层构建。">
<code>N = 6</code>, 这些“层”中的每一个由两个子层组成：position-wise FNN 和一个（编码器），或两个（解码器），基于注意力的子层。其中每个还包含4个线性投影和注意逻辑。</p><p>编码器:</p><ol><li>Stage 1 - 输入编码: 序列的顺序信息是非常重要的。由于没有循环，也没有卷积，因此使用“位置编码”表示序列中每个标记的绝对（或相对）位置的信息。<ul><li>positional encodings $\oplus$ embedded input</li></ul></li><li>Stage 2 – Multi-head self-attention 和 Stage 3 – position-wise FFN. 两个阶段都是用来残差连接, 接着正则化输出层</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Stage1_out</span> <span class=o>=</span> <span class=n>Embedding512</span> <span class=o>+</span> <span class=n>TokenPositionEncoding512</span>
</span></span><span class=line><span class=cl><span class=n>Stage2_out</span> <span class=o>=</span> <span class=n>layer_normalization</span><span class=p>(</span><span class=n>multihead_attention</span><span class=p>(</span><span class=n>Stage1_out</span><span class=p>)</span> <span class=o>+</span> <span class=n>Stage1_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Stage3_out</span> <span class=o>=</span> <span class=n>layer_normalization</span><span class=p>(</span><span class=n>FFN</span><span class=p>(</span><span class=n>Stage2_out</span><span class=p>)</span> <span class=o>+</span> <span class=n>Stage2_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>out_enc</span> <span class=o>=</span> <span class=n>Stage3_out</span>
</span></span></code></pre></div><p>解码器的架构类似，但它在第3阶段采用了附加层, 在输出层上的 mask multi-head attention:</p><ol><li>Stage 1 – 输入解码: 输入 output embedding，偏移一个位置以确保对位置<code>i</code>的预测仅取决于<code>&lt; i</code>的位置。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>shift_right_3d</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>pad_value</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Shift the second dimension of x right by one.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>pad_value</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>shifted_targets</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>pad</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]])[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>  <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>shifted_targets</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>([</span><span class=n>pad_value</span><span class=p>,</span> <span class=n>x</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>shifted_targets</span>
</span></span></code></pre></div><ol start=2><li>Stage 2 - Masked Multi-head self-attention: 需要有一个mask来防止当前位置<code>i</code>的生成任务看到后续<code>> i</code>位置的信息。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用pytorch版本的教程中提供的范例</span>
</span></span><span class=line><span class=cl><span class=c1># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>subsequent_mask</span><span class=p>(</span><span class=n>size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Mask out subsequent positions.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>attn_shape</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>subsequent_mask</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>attn_shape</span><span class=p>),</span> <span class=n>k</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;uint8&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>subsequent_mask</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># # The attention mask shows the position each tgt word (row) is allowed to look at (column).</span>
</span></span><span class=line><span class=cl><span class=c1># Words are blocked for attending to future words during training.</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span><span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>subsequent_mask</span><span class=p>(</span><span class=mi>20</span><span class=p>)[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></div><p><img loading=lazy src=http://nlp.seas.harvard.edu/images/the-annotated-transformer_31_0.png></p><p>阶段2,3和4同样使用了残差连接，然后在输出使用归一化层。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Stage1_out</span> <span class=o>=</span> <span class=n>OutputEmbedding512</span> <span class=o>+</span> <span class=n>TokenPositionEncoding512</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Stage2_Mask</span> <span class=o>=</span> <span class=n>masked_multihead_attention</span><span class=p>(</span><span class=n>Stage1_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Stage2_Norm1</span> <span class=o>=</span> <span class=n>layer_normalization</span><span class=p>(</span><span class=n>Stage2_Mask</span><span class=p>)</span> <span class=o>+</span> <span class=n>Stage1_out</span>
</span></span><span class=line><span class=cl><span class=n>Stage2_Multi</span> <span class=o>=</span> <span class=n>multihead_attention</span><span class=p>(</span><span class=n>Stage2_Norm1</span> <span class=o>+</span> <span class=n>out_enc</span><span class=p>)</span> <span class=o>+</span>  <span class=n>Stage2_Norm1</span>
</span></span><span class=line><span class=cl><span class=n>Stage2_Norm2</span> <span class=o>=</span> <span class=n>layer_normalization</span><span class=p>(</span><span class=n>Stage2_Multi</span><span class=p>)</span> <span class=o>+</span> <span class=n>Stage2_Multi</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Stage3_FNN</span> <span class=o>=</span> <span class=n>FNN</span><span class=p>(</span><span class=n>Stage2_Norm2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Stage3_Norm</span> <span class=o>=</span> <span class=n>layer_normalization</span><span class=p>(</span><span class=n>Stage3_FNN</span><span class=p>)</span> <span class=o>+</span> <span class=n>Stage2_Norm2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>out_dec</span> <span class=o>=</span> <span class=n>Stage3_Norm</span>
</span></span></code></pre></div><p>可以利用开源的<a href=https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py>Tensor2Tensor</a>，通过调用几个命令来训练Transformer网络进行翻译和解析。</p><p>通过Self Attention对比Attention有什么增益呢？可以看到，自注意力算法可以捕获同一个句子中单词之间的语义特征, 比如共指消解（coreference resolution），例如句子中的单词“it”可以根据上下文引用句子的不同名词。除此之外, 理论上也可以捕捉一些语法特征. ![](<a href=https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/CoreferenceResolution.png>https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/CoreferenceResolution.png</a> &ldquo;Co-reference resolution. 两边的"it"指向不同的词. Adopted from Google Blog.&rdquo;)</p><p>其实在LSTM_encoder-LSTM_decoder架构上的Attention也可以做到相同的操作, 但效果却不太好. 问题可能在于此时的Attention处理的不是纯粹的一个个序列编码, 而是经过LSTM(复杂的门控记忆与遗忘)编码后的包含前面时间步输入信息的一个个序列编码, 这个导致Attention的软寻址难度增大. 而现在是2019年, 几乎主流的文本编码方案都转投Transformer了, 可见单纯利用self-attention编码其实效率更高.</p><h3 id=attention>Attention<a hidden class=anchor aria-hidden=true href=#attention>#</a></h3><p><a href=https://arxiv.org/pdf/1706.03762.pdf>Vaswani, 2017</a>明确定义了使用的注意力算法</p>$$\begin{eqnarray} Attention (Q,K,V) = softmax \Big( \frac{QK^T}{\sqrt{d_k}} \Big) V \end{eqnarray},$$<p>其中$\mathbb{Q}\in\mathbb{R}^{n\times d_k}, \mathbb{K}\in\mathbb{R}^{m\times d_k}, \mathbb{V}\in\mathbb{R}^{m\times d_v}$. 这就是传统的Scaled Dot-Product Attention, 把这个Attention理解为一个神经网络层，将$n\times d_k$的序列$Q$编码成了一个新的$n\times d_v$的序列。因为对于较大的$d_k$，内积会数量级地放大, 太大的话softmax可能会被推到梯度消失区域, softmax后就非0即1(那就是hardmax), 所以$q \cdot k = \sum_{i=1}^{d_k}q_i k_i$按照比例因子$\sqrt{d_k}$缩放.</p><p>BERT/ALBERT中的点积attention实现:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># dot_product_attention from bert implementation</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>dot_product_attention</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span> <span class=n>bias</span><span class=p>,</span> <span class=n>dropout_rate</span><span class=o>=</span><span class=mf>0.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Dot-product attention.
</span></span></span><span class=line><span class=cl><span class=s2>  Args:
</span></span></span><span class=line><span class=cl><span class=s2>    q: Tensor with shape [..., length_q, depth_k].
</span></span></span><span class=line><span class=cl><span class=s2>    k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must
</span></span></span><span class=line><span class=cl><span class=s2>      match with q.
</span></span></span><span class=line><span class=cl><span class=s2>    v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must
</span></span></span><span class=line><span class=cl><span class=s2>      match with q.
</span></span></span><span class=line><span class=cl><span class=s2>    bias: bias Tensor (see attention_bias())
</span></span></span><span class=line><span class=cl><span class=s2>    dropout_rate: a float.
</span></span></span><span class=line><span class=cl><span class=s2>  Returns:
</span></span></span><span class=line><span class=cl><span class=s2>    Tensor with shape [..., length_q, depth_v].
</span></span></span><span class=line><span class=cl><span class=s2>  &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>logits</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>transpose_b</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>  <span class=c1># [..., length_q, length_kv]</span>
</span></span><span class=line><span class=cl>  <span class=n>logits</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=nb>float</span><span class=p>(</span><span class=n>get_shape_list</span><span class=p>(</span><span class=n>q</span><span class=p>)[</span><span class=o>-</span><span class=mi>1</span><span class=p>])))</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># `attention_mask` = [B, T]</span>
</span></span><span class=line><span class=cl>    <span class=n>from_shape</span> <span class=o>=</span> <span class=n>get_shape_list</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>from_shape</span><span class=p>)</span> <span class=o>==</span> <span class=mi>4</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>broadcast_ones</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>([</span><span class=n>from_shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>from_shape</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=mi>1</span><span class=p>],</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=nb>len</span><span class=p>(</span><span class=n>from_shape</span><span class=p>)</span> <span class=o>==</span> <span class=mi>5</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=c1># from_shape = [B, N, Block_num, block_size, depth]#</span>
</span></span><span class=line><span class=cl>      <span class=n>broadcast_ones</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>ones</span><span class=p>([</span><span class=n>from_shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>from_shape</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=n>from_shape</span><span class=p>[</span><span class=mi>3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                                <span class=mi>1</span><span class=p>],</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>bias</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>broadcast_ones</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                     <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>bias</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>),</span> <span class=n>transpose_b</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span>
</span></span><span class=line><span class=cl>    <span class=c1># masked positions, this operation will create a tensor which is 0.0 for</span>
</span></span><span class=line><span class=cl>    <span class=c1># positions we want to attend and -10000.0 for masked positions.</span>
</span></span><span class=line><span class=cl>    <span class=n>adder</span> <span class=o>=</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>bias</span><span class=p>)</span> <span class=o>*</span> <span class=o>-</span><span class=mf>10000.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Since we are adding it to the raw scores before the softmax, this is</span>
</span></span><span class=line><span class=cl>    <span class=c1># effectively the same as removing these entirely.</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>+=</span> <span class=n>adder</span>
</span></span><span class=line><span class=cl>  <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>adder</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>attention_probs</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;attention_probs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>attention_probs</span> <span class=o>=</span> <span class=n>dropout</span><span class=p>(</span><span class=n>attention_probs</span><span class=p>,</span> <span class=n>dropout_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attention_probs</span><span class=p>,</span> <span class=n>v</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用pytorch版本的教程中提供的范例</span>
</span></span><span class=line><span class=cl><span class=c1># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;Compute &#39;Scaled Dot Product Attention&#39;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>d_k</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> \
</span></span><span class=line><span class=cl>           <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>p_attn</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># (Dropout described below)</span>
</span></span><span class=line><span class=cl>  <span class=n>p_attn</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>p_attn</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>p_attn</span><span class=p>,</span> <span class=n>value</span><span class=p>),</span> <span class=n>p_attn</span>
</span></span></code></pre></div><p>这只是注意力的一种形式，还有其他比如query跟key的运算方式是拼接后再内积一个参数向量，权重也不一定要归一化，等等。</p><h3 id=self-attention-sa>Self-Attention (SA)<a hidden class=anchor aria-hidden=true href=#self-attention-sa>#</a></h3><p>在实际的应用中, 不同的场景的$Q,K,V$是不一样的, 如果是SQuAD的话，$Q$是文章的向量序列，$K=V$为问题的向量序列，输出就是Aligned Question Embedding。</p><p>Google所说的自注意力(SA), 就是$Attention(\mathbb{X},\mathbb{X},\mathbb{X})$, 通过在序列自身做Attention，寻找序列自身内部的联系。Google论文的主要贡献之一是它表明了SA在序列编码部分是相当重要的，甚至可以替代传统的RNN(LSTM), CNN, 而之前关于Seq2Seq的研究基本都是关注如何把注意力机制用在解码部分。</p><p>编码时，自注意力层处理来自相同位置的输入$queries, keys, value$，即编码器前一层的输出。编码器中的每个位置都可以关注前一层的所有位置.</p><p>在解码器中，SA层使每个位置能够关注解码器中当前及之前的所有位置。为了保持 auto-regressive 属性，需要阻止解码器中的向左信息流, 所以要在scaled dot-product attention层中屏蔽（设置为-∞）softmax输入中与非法连接相对应的所有值.</p><p>作者使用SA层而不是CNN或RNN层的动机是:</p><ol><li>最小化每层的总计算复杂度: SA层通过$O(1)$数量的序列操作连接所有位置. ($O(n)$ in RNN)</li><li>最大化可并行化计算：对于序列长度$n$ &lt; representation dimensionality $d$（对于SOTA序列表达模型，如word-piece, byte-pair）。对于非常长的序列$n > d$, SA可以仅考虑以相应输出位置为中心的输入序列中的某个大小$r$的邻域，从而将最大路径长度增加到$O(n/r)$</li><li>最小化由不同类型层组成的网络中任意两个输入和输出位置之间的最大路径长度。任何输入和输出序列中的位置组合之间的路径越短，越容易学习长距离依赖。</li></ol><h3 id=multi-head-attention>Multi-head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h3><p>Transformer的SA将关联输入和输出序列中的（特别是远程）位置的计算量减少到$O(1)$。然而，这是以降低有效分辨率为代价的，因为注意力加权位置被平均了。为了弥补这种损失, 文章提出了 Multi-head Attention:<img loading=lazy src=/images/multi_head_attention.png title="Multi-Head Attention consists of h attention layers running in parallel. image from https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#positional-encoding-pe"></p><ul><li>$h=8$ attention layers (“heads”): 将key $K$ 和 query $Q$ 线性投影到 $d_k$ 维度, 将value $V$ 投影到$d_v$维度, (线性投影的目的是减少维度) $$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\dots,h$$ 投影是参数矩阵$W^Q_i, W^K_i\in\mathbb{R}^{d_{model}\times d_k}, W^V_i\in\mathbb{R}^{d_{model}\times d_v}$ $d_k=d_v=d_{model}/h = 64$</li><li>每层并行地应用 scaled-dot attention(用不同的线性变换), 得到$d_v$维度的输出</li><li>把每一层的输出拼接在一起 $Concat(head_1,\dots,head_h)$</li><li>再线性变换上一步的拼接向量$MultiHeadAttention(Q,K,V) = Concat(head_1,\dots,head_h) W^O$, where $W^0\in\mathbb{R}^{d_{hd_v}\times d_{model}}$</li></ul><p>因为Transformer只是把原来$d_{model}$维度的注意力函数计算并行分割为$h$个独立的$d_{model}/h$维度的head, 所以计算量相差不大.</p><p>BERT/ALBERT中的multi-head attention层实现:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>attention_layer</span><span class=p>(</span><span class=n>from_tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>to_tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>num_attention_heads</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>query_act</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>key_act</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>value_act</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>attention_probs_dropout_prob</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>initializer_range</span><span class=o>=</span><span class=mf>0.02</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>batch_size</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>from_seq_length</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>to_seq_length</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Performs multi-headed attention from `from_tensor` to `to_tensor`.
</span></span></span><span class=line><span class=cl><span class=s2>  Args:
</span></span></span><span class=line><span class=cl><span class=s2>    from_tensor: float Tensor of shape [batch_size, from_seq_length,
</span></span></span><span class=line><span class=cl><span class=s2>      from_width].
</span></span></span><span class=line><span class=cl><span class=s2>    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].
</span></span></span><span class=line><span class=cl><span class=s2>    attention_mask: (optional) int32 Tensor of shape [batch_size,
</span></span></span><span class=line><span class=cl><span class=s2>      from_seq_length, to_seq_length]. The values should be 1 or 0. The
</span></span></span><span class=line><span class=cl><span class=s2>      attention scores will effectively be set to -infinity for any positions in
</span></span></span><span class=line><span class=cl><span class=s2>      the mask that are 0, and will be unchanged for positions that are 1.
</span></span></span><span class=line><span class=cl><span class=s2>    num_attention_heads: int. Number of attention heads.
</span></span></span><span class=line><span class=cl><span class=s2>    query_act: (optional) Activation function for the query transform.
</span></span></span><span class=line><span class=cl><span class=s2>    key_act: (optional) Activation function for the key transform.
</span></span></span><span class=line><span class=cl><span class=s2>    value_act: (optional) Activation function for the value transform.
</span></span></span><span class=line><span class=cl><span class=s2>    attention_probs_dropout_prob: (optional) float. Dropout probability of the
</span></span></span><span class=line><span class=cl><span class=s2>      attention probabilities.
</span></span></span><span class=line><span class=cl><span class=s2>    initializer_range: float. Range of the weight initializer.
</span></span></span><span class=line><span class=cl><span class=s2>    batch_size: (Optional) int. If the input is 2D, this might be the batch size
</span></span></span><span class=line><span class=cl><span class=s2>      of the 3D version of the `from_tensor` and `to_tensor`.
</span></span></span><span class=line><span class=cl><span class=s2>    from_seq_length: (Optional) If the input is 2D, this might be the seq length
</span></span></span><span class=line><span class=cl><span class=s2>      of the 3D version of the `from_tensor`.
</span></span></span><span class=line><span class=cl><span class=s2>    to_seq_length: (Optional) If the input is 2D, this might be the seq length
</span></span></span><span class=line><span class=cl><span class=s2>      of the 3D version of the `to_tensor`.
</span></span></span><span class=line><span class=cl><span class=s2>  Returns:
</span></span></span><span class=line><span class=cl><span class=s2>    float Tensor of shape [batch_size, from_seq_length, num_attention_heads,
</span></span></span><span class=line><span class=cl><span class=s2>      size_per_head].
</span></span></span><span class=line><span class=cl><span class=s2>  Raises:
</span></span></span><span class=line><span class=cl><span class=s2>    ValueError: Any of the arguments or tensor shapes are invalid.
</span></span></span><span class=line><span class=cl><span class=s2>  &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>from_shape</span> <span class=o>=</span> <span class=n>get_shape_list</span><span class=p>(</span><span class=n>from_tensor</span><span class=p>,</span> <span class=n>expected_rank</span><span class=o>=</span><span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>to_shape</span> <span class=o>=</span> <span class=n>get_shape_list</span><span class=p>(</span><span class=n>to_tensor</span><span class=p>,</span> <span class=n>expected_rank</span><span class=o>=</span><span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>size_per_head</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>from_shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>/</span><span class=n>num_attention_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>from_shape</span><span class=p>)</span> <span class=o>!=</span> <span class=nb>len</span><span class=p>(</span><span class=n>to_shape</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;The rank of `from_tensor` must match the rank of `to_tensor`.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>from_shape</span><span class=p>)</span> <span class=o>==</span> <span class=mi>3</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=n>from_shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>from_seq_length</span> <span class=o>=</span> <span class=n>from_shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>to_seq_length</span> <span class=o>=</span> <span class=n>to_shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>elif</span> <span class=nb>len</span><span class=p>(</span><span class=n>from_shape</span><span class=p>)</span> <span class=o>==</span> <span class=mi>2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>batch_size</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=n>from_seq_length</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=n>to_seq_length</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=s2>&#34;When passing in rank 2 tensors to attention_layer, the values &#34;</span>
</span></span><span class=line><span class=cl>          <span class=s2>&#34;for `batch_size`, `from_seq_length`, and `to_seq_length` &#34;</span>
</span></span><span class=line><span class=cl>          <span class=s2>&#34;must all be specified.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Scalar dimensions referenced here:</span>
</span></span><span class=line><span class=cl>  <span class=c1>#   B = batch size (number of sequences)</span>
</span></span><span class=line><span class=cl>  <span class=c1>#   F = `from_tensor` sequence length</span>
</span></span><span class=line><span class=cl>  <span class=c1>#   T = `to_tensor` sequence length</span>
</span></span><span class=line><span class=cl>  <span class=c1>#   N = `num_attention_heads`</span>
</span></span><span class=line><span class=cl>  <span class=c1>#   H = `size_per_head`</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># `query_layer` = [B, F, N, H]</span>
</span></span><span class=line><span class=cl>  <span class=n>q</span> <span class=o>=</span> <span class=n>dense_layer_3d</span><span class=p>(</span><span class=n>from_tensor</span><span class=p>,</span> <span class=n>num_attention_heads</span><span class=p>,</span> <span class=n>size_per_head</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                     <span class=n>create_initializer</span><span class=p>(</span><span class=n>initializer_range</span><span class=p>),</span> <span class=n>query_act</span><span class=p>,</span> <span class=s2>&#34;query&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># `key_layer` = [B, T, N, H]</span>
</span></span><span class=line><span class=cl>  <span class=n>k</span> <span class=o>=</span> <span class=n>dense_layer_3d</span><span class=p>(</span><span class=n>to_tensor</span><span class=p>,</span> <span class=n>num_attention_heads</span><span class=p>,</span> <span class=n>size_per_head</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                     <span class=n>create_initializer</span><span class=p>(</span><span class=n>initializer_range</span><span class=p>),</span> <span class=n>key_act</span><span class=p>,</span> <span class=s2>&#34;key&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># `value_layer` = [B, T, N, H]</span>
</span></span><span class=line><span class=cl>  <span class=n>v</span> <span class=o>=</span> <span class=n>dense_layer_3d</span><span class=p>(</span><span class=n>to_tensor</span><span class=p>,</span> <span class=n>num_attention_heads</span><span class=p>,</span> <span class=n>size_per_head</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                     <span class=n>create_initializer</span><span class=p>(</span><span class=n>initializer_range</span><span class=p>),</span> <span class=n>value_act</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>q</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>k</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>v</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>v</span><span class=p>,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_mask</span><span class=p>,</span> <span class=p>[</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>to_seq_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># &#39;new_embeddings = [B, N, F, H]&#39;</span>
</span></span><span class=line><span class=cl>  <span class=n>new_embeddings</span> <span class=o>=</span> <span class=n>dot_product_attention</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                         <span class=n>attention_probs_dropout_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>new_embeddings</span><span class=p>,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span></code></pre></div><p>可以看到<code>k</code>和<code>v</code>都是<code>to_tensor</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用pytorch版本的教程中提供的范例</span>
</span></span><span class=line><span class=cl><span class=c1># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadedAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Take in model size and number of heads.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>(</span><span class=n>MultiHeadedAttention</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>d_model</span> <span class=o>%</span> <span class=n>h</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=c1># We assume d_v always equals d_k</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>h</span> <span class=o>=</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>p</span> <span class=o>=</span> <span class=n>dropout</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>linears</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>),</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>attn</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=c1># Same mask applied to all h heads.</span>
</span></span><span class=line><span class=cl>      <span class=n>mask</span> <span class=o>=</span> <span class=n>mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>nbatches</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span> <span class=o>=</span> <span class=p>[</span><span class=n>l</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>nbatches</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>h</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                         <span class=k>for</span> <span class=n>l</span><span class=p>,</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linears</span><span class=p>,</span> <span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2) Apply attention on all the projected vectors in batch.</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn</span> <span class=o>=</span> <span class=n>attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3) &#34;Concat&#34; using a view and apply a final linear.</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>nbatches</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>h</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>linears</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><p>NMT中Transformer以三种不同的方式使用Multi-head Attention：</p><ol><li>在<code>encoder-decoder attention</code>层中，<code>queries</code>来自前一层decoder层，并且 memory keys and values 来自encoder的输出。这让decoder的每个位置都可以注意到输入序列的所有位置。这其实还原了典型的seq2seq模型里常用的编码器 - 解码器注意力机制（例如<a href=https://arxiv.org/abs/1409.0473>Bahdanau et al., 2014</a>或Conv2S2）。</li><li>编码器本身也包含了self-attention layers。在self-attention layers中，所有 keys, values and queries 来自相同的位置，在这里是编码器中前一层的输出。这样，编码器的每个位置都可以注意到前一层的所有位置。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s2>&#34;attention_1&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s2>&#34;self&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>attention_output</span> <span class=o>=</span> <span class=n>attention_layer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=n>from_tensor</span><span class=o>=</span><span class=n>layer_input</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>to_tensor</span><span class=o>=</span><span class=n>layer_input</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>num_attention_heads</span><span class=o>=</span><span class=n>num_attention_heads</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>attention_probs_dropout_prob</span><span class=o>=</span><span class=n>attention_probs_dropout_prob</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>initializer_range</span><span class=o>=</span><span class=n>initializer_range</span><span class=p>)</span>
</span></span></code></pre></div><ol start=4><li>类似地，解码器中的 self-attention layers 允许解码器的每个位置注意到解码器中包括该位置在内的所有前面的位置（有mask屏蔽了后面的位置）。需要阻止解码器中的向左信息流以保持<code>自回归</code>属性(auto-regressive 可以简单理解为时序序列的特性, 只能从左到右, 从过去到未来)。我们通过在scaled dot-product attention层中屏蔽（设置为-∞）softmax输入中与非法连接相对应的所有值来维持该特性。</li></ol><h3 id=position-wise-feed-forward-networks>Position-wise Feed-Forward Networks<a hidden class=anchor aria-hidden=true href=#position-wise-feed-forward-networks>#</a></h3><p>在编码器和解码器中，每个层都包含一个全连接的前馈网络(FFN)，FFN 分别应用于每个位置，使用相同的两个线性变换和一个ReLU</p>$$FFN(x) = max(0, xW_1+b_1) W_2 + b_2$$<p>虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。它的工作方式类似于两个内核大小为1的卷积层. 输入/输出维度是$d_{model}=512$, 内层的维度$d_{ff} = 2048$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用pytorch版本的教程中提供的范例</span>
</span></span><span class=line><span class=cl><span class=c1># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionwiseFeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;Implements FFN equation.&#34;</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>(</span><span class=n>PositionwiseFeedForward</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1># Torch linears have a `b` by default.</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>w_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>w_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_ff</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>w_2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>w_1</span><span class=p>(</span><span class=n>x</span><span class=p>))))</span>
</span></span></code></pre></div><h3 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h3><p>在解码时序信息时，LSTM模型通过时间步的概念以输入/输出流一次一个的形式编码的. 而Transformer选择把时序编码为正弦波。这些信号作为额外的信息加入到输入和输出中以表达时序信息.</p><p>这种编码使模型能够感知到当前正在处理的是输入（或输出）序列的哪个部分。位置编码可以学习或者使用固定参数。作者进行了测试（PPL，BLEU），显示两种方式表现相似。文中作者选择使用固定的位置编码参数:</p>$$ \begin{eqnarray} PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \end{eqnarray} $$<p></p>$$ \begin{eqnarray} PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\end{eqnarray} $$<p>其中$pos$是位置，$i$是维度。</p><p>也就是说，位置编码的每个维度对应于正弦余弦曲线的拼接。波长形成从2π到10000⋅2π的几何级数。选择这个函数，是因为假设它能让模型容易地学习相对位置，因为对于任意固定偏移$k$，$PE_{pos + k}$可以表示为$PE_{pos}$的线性函数。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用pytorch版本的教程中提供的范例</span>
</span></span><span class=line><span class=cl><span class=c1># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionalEncoding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;Implement the PE function.&#34;</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>max_len</span><span class=o>=</span><span class=mi>5000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>(</span><span class=n>PositionalEncoding</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Compute the positional encodings once in log space.</span>
</span></span><span class=line><span class=cl>    <span class=n>pe</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>max_len</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>                         <span class=o>-</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>pe</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>pe</span> <span class=o>=</span> <span class=n>pe</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;pe&#39;</span><span class=p>,</span> <span class=n>pe</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>Variable</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>pe</span><span class=p>[:,</span> <span class=p>:</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><p>位置编码将根据位置添加正弦余弦波。每个维度的波的频率和偏移是不同的。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>pe</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>pe</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>Variable</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>20</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>100</span><span class=p>),</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=p>:,</span> <span class=mi>4</span><span class=p>:</span><span class=mi>8</span><span class=p>]</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>numpy</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>([</span><span class=s2>&#34;dim </span><span class=si>%d</span><span class=s2>&#34;</span><span class=o>%</span><span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span><span class=mi>5</span><span class=p>,</span><span class=mi>6</span><span class=p>,</span><span class=mi>7</span><span class=p>]])</span>
</span></span></code></pre></div><p><img loading=lazy src=http://nlp.seas.harvard.edu/images/the-annotated-transformer_49_0.png>
直观的理解是，将这些值添加到embedding中，一旦它们被投影到$Q / K / V$向量和dot product attention中，就给embedding向量之间提供了有意义的相对距离。</p><p><img loading=lazy src=https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png title="A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors. image from: https://jalammar.github.io/illustrated-transformer/">
<img loading=lazy src=https://jalammar.github.io/images/t/transformer_positional_encoding_example.png title="A real example of positional encoding with a toy embedding size of 4, image from: https://jalammar.github.io/illustrated-transformer/"></p><h3 id=shared-weight-embeddings-and-softmax>Shared-Weight Embeddings and Softmax<a hidden class=anchor aria-hidden=true href=#shared-weight-embeddings-and-softmax>#</a></h3><p>与其他序列转导模型类似，使用可学习的Embeddings将 input tokens and output tokens 转换为维度$d_{model}$的向量。通过线性变换和softmax函数将解码器的输出向量转换为预测的token概率。在Transformer模型中，两个嵌入层和pre-softmax线性变换之间共享相同的权重矩阵，在Embeddings层中，将权重乘以$\sqrt{d_{\text{model}}}$. 这些都是当前主流的操作。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用pytorch版本的教程中提供的范例</span>
</span></span><span class=line><span class=cl><span class=c1># http://nlp.seas.harvard.edu/2018/04/03/attention.html</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Embeddings</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>vocab</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>(</span><span class=n>Embeddings</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>lut</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>lut</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>*</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=启发>启发<a hidden class=anchor aria-hidden=true href=#启发>#</a></h3><p>作者已经进行了一系列测试（论文表3），其中他们讨论N = 6层的建议，模型大小为512，基于h = 8个heads，键值维度为64，使用100K步。</p><p>还指出，由于模型质量随着$d_k$（行B）的减小而降低，因此可以进一步优化点积兼容性功能。</p><p>其声称提出的固定正弦位置编码，与学习到的位置编码相比，产生几乎相等的分数。</p><h3 id=算法适合哪些类型的问题>算法适合哪些类型的问题？<a hidden class=anchor aria-hidden=true href=#算法适合哪些类型的问题>#</a></h3><ul><li>序列转导（语言翻译）</li><li>语法选区解析的经典语言分析任务 syntactic constituency parsing</li><li>共指消解 coreference resolution</li></ul><h3 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h3><p><a href=https://research.googleblog.com/2017/08/transformer-novel-neural-network.html>https://research.googleblog.com/2017/08/transformer-novel-neural-network.html</a>
<a href=https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html>https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html</a>
<a href=https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/>https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/</a>
<a href=http://nlp.seas.harvard.edu/2018/04/03/attention.html>http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/attention/>Attention</a></li><li><a href=https://congchan.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/%E5%88%A9%E7%94%A8bert%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/><span class=title>« Prev</span><br><span>利用bert进行迁移学习</span>
</a><a class=next href=https://congchan.github.io/posts/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/><span class=title>Next »</span><br><span>概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer & Self-Attention (多头)自注意力编码 on x" href="https://x.com/intent/tweet/?text=Transformer%20%26%20Self-Attention%20%28%e5%a4%9a%e5%a4%b4%29%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bc%96%e7%a0%81&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2ftransformer-self-attention-%25E5%25A4%259A%25E5%25A4%25B4%25E8%2587%25AA%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E7%25BC%2596%25E7%25A0%2581%2f&amp;hashtags=Attention%2cNLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer & Self-Attention (多头)自注意力编码 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2ftransformer-self-attention-%25E5%25A4%259A%25E5%25A4%25B4%25E8%2587%25AA%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E7%25BC%2596%25E7%25A0%2581%2f&amp;title=Transformer%20%26%20Self-Attention%20%28%e5%a4%9a%e5%a4%b4%29%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bc%96%e7%a0%81&amp;summary=Transformer%20%26%20Self-Attention%20%28%e5%a4%9a%e5%a4%b4%29%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bc%96%e7%a0%81&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2ftransformer-self-attention-%25E5%25A4%259A%25E5%25A4%25B4%25E8%2587%25AA%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E7%25BC%2596%25E7%25A0%2581%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer & Self-Attention (多头)自注意力编码 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2ftransformer-self-attention-%25E5%25A4%259A%25E5%25A4%25B4%25E8%2587%25AA%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E7%25BC%2596%25E7%25A0%2581%2f&title=Transformer%20%26%20Self-Attention%20%28%e5%a4%9a%e5%a4%b4%29%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bc%96%e7%a0%81"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer & Self-Attention (多头)自注意力编码 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2ftransformer-self-attention-%25E5%25A4%259A%25E5%25A4%25B4%25E8%2587%25AA%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E7%25BC%2596%25E7%25A0%2581%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer & Self-Attention (多头)自注意力编码 on whatsapp" href="https://api.whatsapp.com/send?text=Transformer%20%26%20Self-Attention%20%28%e5%a4%9a%e5%a4%b4%29%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bc%96%e7%a0%81%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2ftransformer-self-attention-%25E5%25A4%259A%25E5%25A4%25B4%25E8%2587%25AA%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E7%25BC%2596%25E7%25A0%2581%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer & Self-Attention (多头)自注意力编码 on telegram" href="https://telegram.me/share/url?text=Transformer%20%26%20Self-Attention%20%28%e5%a4%9a%e5%a4%b4%29%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bc%96%e7%a0%81&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2ftransformer-self-attention-%25E5%25A4%259A%25E5%25A4%25B4%25E8%2587%25AA%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E7%25BC%2596%25E7%25A0%2581%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer & Self-Attention (多头)自注意力编码 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Transformer%20%26%20Self-Attention%20%28%e5%a4%9a%e5%a4%b4%29%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bc%96%e7%a0%81&u=https%3a%2f%2fcongchan.github.io%2fposts%2ftransformer-self-attention-%25E5%25A4%259A%25E5%25A4%25B4%25E8%2587%25AA%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E7%25BC%2596%25E7%25A0%2581%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>