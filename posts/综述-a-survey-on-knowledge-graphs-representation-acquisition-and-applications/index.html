<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications | Cong's Log</title><meta name=keywords content="NLP,2021,IEEE,Knowledge Graphs"><meta name=description content="Survey: https://arxiv.org/abs/2002.00388v4

A knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.

Entities can be real-world objects and abstract concepts,
Relationships represent the relation between entities,
Semantic descriptions of entities and their relationships contain types and properties with a well-defined meaning

G: A knowledge graph
F: A set of facts
(h, r, t): A triple of head, relation and tail
$(\mathbf{h}, \mathbf{r}, \mathbf{t})$: Embedding of head, relation and tail"><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/%E7%BB%BC%E8%BF%B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/%E7%BB%BC%E8%BF%B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/%E7%BB%BC%E8%BF%B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications"><meta property="og:description" content="Survey: https://arxiv.org/abs/2002.00388v4
A knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.
Entities can be real-world objects and abstract concepts, Relationships represent the relation between entities, Semantic descriptions of entities and their relationships contain types and properties with a well-defined meaning G: A knowledge graph F: A set of facts (h, r, t): A triple of head, relation and tail $(\mathbf{h}, \mathbf{r}, \mathbf{t})$: Embedding of head, relation and tail"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-02-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-02-01T00:00:00+00:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="2021"><meta property="article:tag" content="IEEE"><meta property="article:tag" content="Knowledge Graphs"><meta name=twitter:card content="summary"><meta name=twitter:title content="综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications"><meta name=twitter:description content="Survey: https://arxiv.org/abs/2002.00388v4

A knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.

Entities can be real-world objects and abstract concepts,
Relationships represent the relation between entities,
Semantic descriptions of entities and their relationships contain types and properties with a well-defined meaning

G: A knowledge graph
F: A set of facts
(h, r, t): A triple of head, relation and tail
$(\mathbf{h}, \mathbf{r}, \mathbf{t})$: Embedding of head, relation and tail"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications","item":"https://congchan.github.io/posts/%E7%BB%BC%E8%BF%B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications","name":"综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications","description":"Survey: https://arxiv.org/abs/2002.00388v4\nA knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.\nEntities can be real-world objects and abstract concepts, Relationships represent the relation between entities, Semantic descriptions of entities and their relationships contain types and properties with a well-defined meaning G: A knowledge graph F: A set of facts (h, r, t): A triple of head, relation and tail $(\\mathbf{h}, \\mathbf{r}, \\mathbf{t})$: Embedding of head, relation and tail\n","keywords":["NLP","2021","IEEE","Knowledge Graphs"],"articleBody":"Survey: https://arxiv.org/abs/2002.00388v4\nA knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.\nEntities can be real-world objects and abstract concepts, Relationships represent the relation between entities, Semantic descriptions of entities and their relationships contain types and properties with a well-defined meaning G: A knowledge graph F: A set of facts (h, r, t): A triple of head, relation and tail $(\\mathbf{h}, \\mathbf{r}, \\mathbf{t})$: Embedding of head, relation and tail\nDefinition 1 (Ehrlinger and W¨oß [7]). A knowledge graph acquires and integrates information into an ontology and applies a reasoner to derive new knowledge.\nDefinition 2 (Wang et al. [8]). A knowledge graph is a multi- relational graph composed of entities and relations which are regarded as nodes and different types of edges, respectively.\nKB vs KG The term of knowledge graph is synonymous with knowledge base with a minor difference. A knowledge graph can be viewed as a graph when considering its graph structure. When it involves formal semantics, it can be taken as a knowledge base for interpretation and inference over facts\nData Structure Resource Description Framework (RDF)：(head, relation, tail) or (subject, predicate, object) Web Ontology Language (OWL) Directed Graph：with nodes as entities and edges as relations KNOWLEDGE-AWARE APPLICATIONS Question Answering knowledge-graph-based question answering (KG-QA) an- swers natural language questions with facts from knowledge graphs. Neural\nSingle-fact QA Taking knowledge graph as an external intellectual source, simple factoid QA or single-fact QA is to answer simple question involving with a single knowledge graph fact Multi-hop Reasoning Recommender Systems Integrating knowledge graphs as external information enables recommendation systems to have the ability of commonsense reasoning.\nBy injecting knowledge-graph-based side information such as entities, relations, and attributes, many efforts work on embedding-based regularization to improve recommendation.\nknowledge representation learning (KRL) focus on knowledge representation learning (KRL) or knowledge graph embedding (KGE) by mapping entities and relations into low-dimensional vectors while capturing their semantic meanings\nrepresentation space in which the relations and entities are represented; scoring function for measuring the plausibility of factual triples; encoding models for representing and learning relational interactions; auxiliary information to be incorporated into the embedding methods. Representation 3.1 Representation Space 3.1.1 Point-Wise Space Point-wise TransE: represents entities and relations in d-dimension vector space\nNTN: models entities across multiple dimensions by a bilinear tensor neural layer.\n3.1.2 ComplexVector Space Instead 3.1.3 Gaussian Distribution Inspired by Gaussian word embedding, the density-based embedding model KG2E [21] introduces Gaussian distribution to deal with the (un)certainties of entities and relations.\n3.1.4 Manifold and Group A manifold is a topological space which could be defined as a set of points with neighborhoods by the set theory, while the group is algebraic structures defined in abstract algebra.\n3.2 Scoring Function Distance-based scoring function measures the plausibility of facts by calculating the distance between entities, where addictive translation with relations as h + r ≈ t is widely used\nSemantic similarity based scoring measures the plausibility of facts by semantic matching, which usually adopts multiplicative formulation $\\mathbf{h}^{\\top} \\mathbf{M}_{r} \\approx \\mathbf{t}^{\\top}$\n3.2.1 Distance-based Scoring Function calculate the Euclidean distance between the relational projection of entities.\nStructural Embedding (SE) :\nA more intensively used principle is the translation-based scoring function that aims to learn embeddings by representing relations as translations from head to tail entities.\nTransE:\nTransH:\nTransR:\nTransD:\nTransA:\nTransF:\nKG2E:\n3.2.2 Semantic Matching DistMult: By restricting relation matrixMr to be diagonal for multi-relational representation learning\n3.3 Encoding Models 3.3.1 Linear/Bilinear Models applying linear operation $g_{r}(\\mathbf{h}, \\mathbf{t})=\\mathbf{M}_{r}^{T}\\left(\\begin{array}{c} \\mathbf{h} \\\\ \\mathbf{t} \\end{array}\\right)$ bilinear transformation operations $f_{r}(h, t)=\\mathbf{h}^{\\top} \\mathbf{M}_{r} \\mathbf{t}$ Y. Wang, R. Gemulla, and H. Li, “On multi-relational link prediction with bilinear models,” showed that the ensembles of multiple linear models can improve the prediction performance through experiments\n3.3.2 Factorization Models Factorization methods formulated KRL models as three-way tensor $X$ decomposition. A general principle of tensor factorization can be denoted as $X_{h r t} \\approx \\mathbf{h}^{\\top} \\mathbf{M}_{r} \\mathbf{t}$\n3.3.3 Neural Networks Encoding models with linear/bilinear blocks can also be modeled using neural networks.\nGenerally, they take entities and/or relations into deep neural networks and compute a semantic matching score.\nConvolutional Neural Networks\nRSN: a recurrent skip mechanism to enhance\nTransformers: KG-BERT\nGraph Neural Networks: GNNs for learning connectivity structure under an encoder-decoder framework.\n3.4 Embedding with Auxiliary Information 3.4.1 Textual Description The challenge of KRL with textual description is to embed both structured knowledge and unstructured textual information in the same space. Wang\n3.4.2 Type Information Entities are represented with hierarchical classes or types, and consequently, relations with semantic types\n3.4.3 Visual Information knowledge acquisition tasks knowledge graph completion (KGC): expanding existing knowledge graphs embedding-based ranking, relation path reasoning, rule-based reasoning meta relational learning entity discovery recognition, disambiguation, typing alignment Relation extraction triple classification, 4.1 Knowledge Graph Completion Knowledge graph completion completes missing links between existing entities or infers entities given entity and relation queries, to add new triples to a knowledge graph. Typical subtasks include link prediction, entity prediction and relation prediction.\nembedding-based methods: failed to capture multi-step relationships relation path inference: explore multi-step relation paths rule-based reasoning: incorporate logical rules 4.1.1 Embedding-based Models learn embedding vectors based on existing triples, then replace tail entity or head entity with each entity e ∈ E to calculate scores of all the candidate entities and rank the top k entities. 4.1.2 Relation Path Reasoning Random walk inference has been widely investigated, for example, the Path-Ranking Algorithm (PRA) [69] chooses relational path under a combination of path constraints, and conducts maximum-likelihood classification\n4.1.3 RL-basedPath Finding Deep reinforcement learning (RL) is introduced for multi- hop reasoning by formulating path-finding between entity pairs as sequential decision making, specifically a Markov decision process (MDP). The policy-based RL agent learns to find a step of relation to extend the reasoning paths via the interaction between the knowledge graph environment, where the policy gradient is utilized for training RL agents.\n4.1.4 Rule-based Reasoning logical rule learning\n4.1.5 Meta Relational Learning The long-tail phenomena exist in the relations of knowledge graphs. Meanwhile, the real-world scenario of knowledge is dynamic, where unseen triples are usually acquired.\nmeta relational learning or few-shot relational learning, requires models to predict new relational facts with only a very few samples.\n4.1.6 Triple Classification Triple classification is to determine whether facts are correct in testing data, which is typically regarded as a binary classification problem.\n4.2 Entity Discovery 4.2.1 Entity Recognition 4.2.2 Entity Typing Entity typing includes coarse and fine-grained types, while the latter one uses a tree-structured type category and is typically regarded as multi-class and multi-label classifi- cation.\n4.2.3 Entity Disambiguation or entity linking is a unified task which links entity mentions to the corresponding entities in a knowledge graph.\n4.2.4 Entity Alignment aims to fuse knowledge among heterogeneous knowledge graphs. In practice, a small set of alignment seeds (i.e., synonymous entities appear in different knowledge graphs) is given to start the alignment process.\nEmbedding-based alignment calculates the similarity between embeddings of a pair of entities.\n4.3 Relation Extraction distant supervision, also referred as weak supervision or self supervision, uses heuristic matching to create training data by assuming that sentences containing the same entity mentions may express the same relation under the supervision of a relational database.\nTemporal Knowledge Graphs incorporate temporal information for representation learning.\ntemporal embedding, entity dynamics, temporal relational dependency, temporal logical reasoning. Knowledge-aware Applications include natural language understanding (NLU), question answering, recommendation systems, and miscellaneous real-world tasks, which inject knowledge to improve representation learning.\nOpen Knowledge Bases or Ontologies WordNet, DBpedia, YAGO, and Freebase\n","wordCount":"1245","inLanguage":"en","datePublished":"2020-02-01T00:00:00Z","dateModified":"2020-02-01T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/%E7%BB%BC%E8%BF%B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications</h1><div class=post-meta><span title='2020-02-01 00:00:00 +0000 UTC'>2020-02-01</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/paper-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#kb-vs-kg aria-label="KB vs KG">KB vs KG</a></li><li><a href=#data-structure aria-label="Data Structure">Data Structure</a></li><li><a href=#knowledge-aware-applications aria-label="KNOWLEDGE-AWARE APPLICATIONS">KNOWLEDGE-AWARE APPLICATIONS</a><ul><li><a href=#question-answering aria-label="Question Answering">Question Answering</a></li><li><a href=#recommender-systems aria-label="Recommender Systems">Recommender Systems</a></li></ul></li><li><a href=#knowledge-representation-learning-krl aria-label="knowledge representation learning (KRL)">knowledge representation learning (KRL)</a><ul><li><a href=#31-representation-space aria-label="3.1 Representation Space">3.1 Representation Space</a><ul><li><a href=#311-point-wise-space-point-wise aria-label="3.1.1 Point-Wise Space Point-wise">3.1.1 Point-Wise Space Point-wise</a></li><li><a href=#312-complexvector-space-instead aria-label="3.1.2 ComplexVector Space Instead">3.1.2 ComplexVector Space Instead</a></li><li><a href=#313-gaussian-distribution aria-label="3.1.3 Gaussian Distribution">3.1.3 Gaussian Distribution</a></li><li><a href=#314-manifold-and-group aria-label="3.1.4 Manifold and Group">3.1.4 Manifold and Group</a></li></ul></li><li><a href=#32-scoring-function aria-label="3.2 Scoring Function">3.2 Scoring Function</a><ul><li><a href=#321-distance-based-scoring-function aria-label="3.2.1 Distance-based Scoring Function">3.2.1 Distance-based Scoring Function</a></li><li><a href=#322-semantic-matching aria-label="3.2.2 Semantic Matching">3.2.2 Semantic Matching</a></li></ul></li><li><a href=#33-encoding-models aria-label="3.3 Encoding Models">3.3 Encoding Models</a><ul><li><a href=#331-linearbilinear-models aria-label="3.3.1 Linear/Bilinear Models">3.3.1 Linear/Bilinear Models</a></li><li><a href=#332-factorization-models aria-label="3.3.2 Factorization Models">3.3.2 Factorization Models</a></li><li><a href=#333-neural-networks aria-label="3.3.3 Neural Networks">3.3.3 Neural Networks</a></li></ul></li><li><a href=#34-embedding-with-auxiliary-information aria-label="3.4 Embedding with Auxiliary Information">3.4 Embedding with Auxiliary Information</a><ul><li><a href=#341-textual-description aria-label="3.4.1 Textual Description">3.4.1 Textual Description</a></li><li><a href=#342-type-information aria-label="3.4.2 Type Information">3.4.2 Type Information</a></li><li><a href=#343-visual-information aria-label="3.4.3 Visual Information">3.4.3 Visual Information</a></li></ul></li></ul></li><li><a href=#knowledge-acquisition-tasks aria-label="knowledge acquisition tasks">knowledge acquisition tasks</a><ul><li><a href=#41-knowledge-graph-completion aria-label="4.1 Knowledge Graph Completion">4.1 Knowledge Graph Completion</a><ul><li><a href=#411-embedding-based-models aria-label="4.1.1 Embedding-based Models">4.1.1 Embedding-based Models</a></li><li><a href=#412-relation-path-reasoning aria-label="4.1.2 Relation Path Reasoning">4.1.2 Relation Path Reasoning</a></li><li><a href=#413-rl-basedpath-finding aria-label="4.1.3 RL-basedPath Finding">4.1.3 RL-basedPath Finding</a></li><li><a href=#414-rule-based-reasoning aria-label="4.1.4 Rule-based Reasoning">4.1.4 Rule-based Reasoning</a></li><li><a href=#415-meta-relational-learning aria-label="4.1.5 Meta Relational Learning">4.1.5 Meta Relational Learning</a></li><li><a href=#416-triple-classification aria-label="4.1.6 Triple Classification">4.1.6 Triple Classification</a></li></ul></li><li><a href=#42-entity-discovery aria-label="4.2 Entity Discovery">4.2 Entity Discovery</a><ul><li><a href=#421-entity-recognition aria-label="4.2.1 Entity Recognition">4.2.1 Entity Recognition</a></li><li><a href=#422-entity-typing aria-label="4.2.2 Entity Typing">4.2.2 Entity Typing</a></li><li><a href=#423-entity-disambiguation aria-label="4.2.3 Entity Disambiguation">4.2.3 Entity Disambiguation</a></li><li><a href=#424-entity-alignment aria-label="4.2.4 Entity Alignment">4.2.4 Entity Alignment</a></li></ul></li><li><a href=#43-relation-extraction aria-label="4.3 Relation Extraction">4.3 Relation Extraction</a></li></ul></li><li><a href=#temporal-knowledge-graphs aria-label="Temporal Knowledge Graphs">Temporal Knowledge Graphs</a></li><li><a href=#knowledge-aware-applications-1 aria-label="Knowledge-aware Applications">Knowledge-aware Applications</a></li><li><a href=#open-knowledge-bases-or-ontologies aria-label="Open Knowledge Bases or Ontologies">Open Knowledge Bases or Ontologies</a></li></ul></div></details></div><div class=post-content><p>Survey: <a href=https://arxiv.org/abs/2002.00388v4>https://arxiv.org/abs/2002.00388v4</a></p><p>A knowledge graph is a structured representation of facts, consisting of entities, relationships and semantic descriptions.</p><ul><li><strong>Entities</strong> can be real-world objects and abstract concepts,</li><li><strong>Relationships</strong> represent the relation between entities,</li><li><strong>Semantic descriptions</strong> of entities and their relationships contain types and properties with a well-defined meaning</li></ul><p>G: A knowledge graph
F: A set of facts
(h, r, t): A triple of head, relation and tail
$(\mathbf{h}, \mathbf{r}, \mathbf{t})$: Embedding of head, relation and tail</p><p>Definition 1 (Ehrlinger and W¨oß [7]). A knowledge graph acquires and integrates information into an ontology and applies a reasoner to derive new knowledge.</p><p>Definition 2 (Wang et al. [8]). A knowledge graph is a multi- relational graph composed of entities and relations which are regarded as nodes and different types of edges, respectively.</p><p><img loading=lazy src=/images/papers/paper10.png></p><p><img alt=/images/papers/paper10-1.png loading=lazy src=/images/papers/paper10-1.png></p><h1 id=kb-vs-kg>KB vs KG<a hidden class=anchor aria-hidden=true href=#kb-vs-kg>#</a></h1><p>The term of knowledge graph is synonymous with knowledge base with a minor difference. A knowledge graph can be viewed as a graph when considering its graph structure. When it involves formal semantics, it can be taken as a knowledge base for interpretation and inference over facts</p><p><img alt=/images/papers/paper10-2.png loading=lazy src=/images/papers/paper10-2.png></p><h1 id=data-structure>Data Structure<a hidden class=anchor aria-hidden=true href=#data-structure>#</a></h1><ul><li>Resource Description Framework (RDF)：(head, relation, tail) or (subject, predicate, object)</li><li>Web Ontology Language (OWL)</li><li>Directed Graph：with nodes as entities and edges as relations</li></ul><h1 id=knowledge-aware-applications>KNOWLEDGE-AWARE APPLICATIONS<a hidden class=anchor aria-hidden=true href=#knowledge-aware-applications>#</a></h1><h2 id=question-answering>Question Answering<a hidden class=anchor aria-hidden=true href=#question-answering>#</a></h2><p>knowledge-graph-based question answering (KG-QA) an- swers natural language questions with facts from knowledge graphs. Neural</p><ul><li>Single-fact QA Taking knowledge graph as an external intellectual source, simple factoid QA or single-fact QA is to answer simple question involving with a single knowledge graph fact</li><li>Multi-hop Reasoning</li></ul><h2 id=recommender-systems>Recommender Systems<a hidden class=anchor aria-hidden=true href=#recommender-systems>#</a></h2><p>Integrating knowledge graphs as external information enables recommendation systems to have the ability of commonsense reasoning.</p><p>By injecting knowledge-graph-based side information such as entities, relations, and attributes, many efforts work on embedding-based regularization to improve recommendation.</p><h1 id=knowledge-representation-learning-krl>knowledge representation learning (KRL)<a hidden class=anchor aria-hidden=true href=#knowledge-representation-learning-krl>#</a></h1><p>focus on knowledge representation learning (KRL) or knowledge graph embedding (KGE) by mapping entities and relations into low-dimensional vectors while capturing their semantic meanings</p><ol><li><strong>representation space</strong> in which the relations and entities are represented;</li><li><strong>scoring function</strong> for measuring the plausibility of factual triples;</li><li><strong>encoding models</strong> for representing and learning relational interactions;</li><li><strong>auxiliary information</strong> to be incorporated into the embedding methods.
Representation</li></ol><h2 id=31-representation-space>3.1 Representation Space<a hidden class=anchor aria-hidden=true href=#31-representation-space>#</a></h2><h3 id=311-point-wise-space-point-wise>3.1.1 Point-Wise Space Point-wise<a hidden class=anchor aria-hidden=true href=#311-point-wise-space-point-wise>#</a></h3><p>TransE: represents entities and relations in d-dimension vector space</p><p><img alt=/images/papers/paper10-3.png loading=lazy src=/images/papers/paper10-3.png></p><p>NTN: models entities across multiple dimensions by a bilinear tensor neural layer.</p><p><img alt=/images/papers/paper10-4.png loading=lazy src=/images/papers/paper10-4.png></p><h3 id=312-complexvector-space-instead>3.1.2 ComplexVector Space Instead<a hidden class=anchor aria-hidden=true href=#312-complexvector-space-instead>#</a></h3><h3 id=313-gaussian-distribution>3.1.3 Gaussian Distribution<a hidden class=anchor aria-hidden=true href=#313-gaussian-distribution>#</a></h3><p>Inspired by Gaussian word embedding, the density-based embedding model KG2E [21] introduces Gaussian distribution to deal with the (un)certainties of entities and relations.</p><h3 id=314-manifold-and-group>3.1.4 Manifold and Group<a hidden class=anchor aria-hidden=true href=#314-manifold-and-group>#</a></h3><p>A manifold is a topological space which could be defined as a set of points with neighborhoods by the set theory, while the group is algebraic structures defined in abstract algebra.</p><h2 id=32-scoring-function>3.2 Scoring Function<a hidden class=anchor aria-hidden=true href=#32-scoring-function>#</a></h2><p>Distance-based scoring function measures the plausibility of facts by calculating the distance between entities, where addictive translation with relations as <code>h + r ≈ t</code> is widely used</p><p>Semantic similarity based scoring measures the plausibility of facts by semantic matching, which usually adopts multiplicative formulation $\mathbf{h}^{\top} \mathbf{M}_{r} \approx \mathbf{t}^{\top}$</p><p><img alt=/images/papers/paper10-5.png loading=lazy src=/images/papers/paper10-5.png></p><h3 id=321-distance-based-scoring-function>3.2.1 Distance-based Scoring Function<a hidden class=anchor aria-hidden=true href=#321-distance-based-scoring-function>#</a></h3><ul><li><p>calculate the Euclidean distance between the relational projection of entities.</p><p>Structural Embedding (SE) :</p><p><img alt=/images/papers/paper10-6.png loading=lazy src=/images/papers/paper10-6.png></p></li><li><p>A more intensively used principle is the translation-based scoring function that aims to learn embeddings by representing relations as translations from head to tail entities.</p><p>TransE:</p><p><img alt=/images/papers/paper10-7.png loading=lazy src=/images/papers/paper10-7.png></p><p>TransH:</p><p><img alt=/images/papers/paper10-8.png loading=lazy src=/images/papers/paper10-8.png></p><p>TransR:</p><p><img alt=/images/papers/paper10-9.png loading=lazy src=/images/papers/paper10-9.png></p><p>TransD:</p><p><img alt=/images/papers/paper10-10.png loading=lazy src=/images/papers/paper10-10.png></p><p>TransA:</p><p><img alt=/images/papers/paper10-11.png loading=lazy src=/images/papers/paper10-11.png></p><p>TransF:</p><p><img alt=/images/papers/paper10-12.png loading=lazy src=/images/papers/paper10-12.png></p><p>KG2E:</p><p><img alt=/images/papers/paper10-13.png loading=lazy src=/images/papers/paper10-13.png></p></li></ul><h3 id=322-semantic-matching>3.2.2 Semantic Matching<a hidden class=anchor aria-hidden=true href=#322-semantic-matching>#</a></h3><p>DistMult: By restricting relation matrixMr to be diagonal for multi-relational representation learning</p><p><img alt=/images/papers/paper10-14.png loading=lazy src=/images/papers/paper10-14.png></p><h2 id=33-encoding-models>3.3 Encoding Models<a hidden class=anchor aria-hidden=true href=#33-encoding-models>#</a></h2><h3 id=331-linearbilinear-models>3.3.1 Linear/Bilinear Models<a hidden class=anchor aria-hidden=true href=#331-linearbilinear-models>#</a></h3><ul><li>applying linear operation $g_{r}(\mathbf{h}, \mathbf{t})=\mathbf{M}_{r}^{T}\left(\begin{array}{c}
\mathbf{h} \\
\mathbf{t}
\end{array}\right)$</li><li>bilinear transformation operations $f_{r}(h, t)=\mathbf{h}^{\top} \mathbf{M}_{r} \mathbf{t}$</li></ul><p>Y. Wang, R. Gemulla, and H. Li, “On multi-relational link prediction with bilinear models,” showed that the ensembles of multiple linear models can improve the prediction performance through experiments</p><h3 id=332-factorization-models>3.3.2 Factorization Models<a hidden class=anchor aria-hidden=true href=#332-factorization-models>#</a></h3><p>Factorization methods formulated KRL models as three-way tensor $X$ decomposition. A general principle of tensor factorization can be denoted as $X_{h r t} \approx \mathbf{h}^{\top} \mathbf{M}_{r} \mathbf{t}$</p><h3 id=333-neural-networks>3.3.3 Neural Networks<a hidden class=anchor aria-hidden=true href=#333-neural-networks>#</a></h3><p><img alt=/images/papers/paper10-15.png loading=lazy src=/images/papers/paper10-15.png></p><p>Encoding models with linear/bilinear blocks can also be modeled using neural networks.</p><p>Generally, they take entities and/or relations into deep neural networks and compute a semantic matching score.</p><p><img alt=/images/papers/paper10-16.png loading=lazy src=/images/papers/paper10-16.png></p><p>Convolutional Neural Networks</p><p><img alt=/images/papers/paper10-17.png loading=lazy src=/images/papers/paper10-17.png></p><p>RSN: a recurrent skip mechanism to enhance</p><p><img alt=/images/papers/paper10-18.png loading=lazy src=/images/papers/paper10-18.png></p><p>Transformers: KG-BERT</p><p>Graph Neural Networks: GNNs for learning connectivity structure under an encoder-decoder framework.</p><h2 id=34-embedding-with-auxiliary-information>3.4 Embedding with Auxiliary Information<a hidden class=anchor aria-hidden=true href=#34-embedding-with-auxiliary-information>#</a></h2><h3 id=341-textual-description>3.4.1 Textual Description<a hidden class=anchor aria-hidden=true href=#341-textual-description>#</a></h3><p>The challenge of KRL with textual description is to embed both structured knowledge and unstructured textual information in the same space. Wang</p><h3 id=342-type-information>3.4.2 Type Information<a hidden class=anchor aria-hidden=true href=#342-type-information>#</a></h3><p>Entities are represented with hierarchical classes or types, and consequently, relations with semantic types</p><h3 id=343-visual-information>3.4.3 Visual Information<a hidden class=anchor aria-hidden=true href=#343-visual-information>#</a></h3><h1 id=knowledge-acquisition-tasks>knowledge acquisition tasks<a hidden class=anchor aria-hidden=true href=#knowledge-acquisition-tasks>#</a></h1><ul><li>knowledge graph completion (KGC): expanding existing knowledge graphs<ul><li>embedding-based ranking,</li><li>relation path reasoning,</li><li>rule-based reasoning</li><li>meta relational learning</li></ul></li><li>entity discovery<ul><li>recognition,</li><li>disambiguation,</li><li>typing</li><li>alignment</li></ul></li><li>Relation extraction</li><li>triple classification,</li></ul><h2 id=41-knowledge-graph-completion>4.1 Knowledge Graph Completion<a hidden class=anchor aria-hidden=true href=#41-knowledge-graph-completion>#</a></h2><p>Knowledge graph completion completes missing links between existing entities or infers entities given entity and relation queries, to add new triples to a knowledge graph. Typical subtasks include link prediction, entity prediction and relation prediction.</p><ul><li>embedding-based methods: failed to capture multi-step relationships</li><li>relation path inference: explore multi-step relation paths</li><li>rule-based reasoning: incorporate logical rules</li></ul><p><img alt=/images/papers/paper10-19.png loading=lazy src=/images/papers/paper10-19.png></p><h3 id=411-embedding-based-models>4.1.1 Embedding-based Models<a hidden class=anchor aria-hidden=true href=#411-embedding-based-models>#</a></h3><ul><li>learn embedding vectors based on existing triples,</li><li>then replace tail entity or head entity with each entity e ∈ E to calculate scores of all the candidate entities and rank the top k entities.</li></ul><h3 id=412-relation-path-reasoning>4.1.2 Relation Path Reasoning<a hidden class=anchor aria-hidden=true href=#412-relation-path-reasoning>#</a></h3><p>Random walk inference has been widely investigated, for example, the Path-Ranking Algorithm (PRA) [69] chooses relational path under a combination of path constraints, and conducts maximum-likelihood classification</p><h3 id=413-rl-basedpath-finding>4.1.3 RL-basedPath Finding<a hidden class=anchor aria-hidden=true href=#413-rl-basedpath-finding>#</a></h3><p>Deep reinforcement learning (RL) is introduced for multi- hop reasoning by formulating path-finding between entity pairs as sequential decision making, specifically a Markov decision process (MDP). The policy-based RL agent learns to find a step of relation to extend the reasoning paths via the interaction between the knowledge graph environment, where the policy gradient is utilized for training RL agents.</p><h3 id=414-rule-based-reasoning>4.1.4 Rule-based Reasoning<a hidden class=anchor aria-hidden=true href=#414-rule-based-reasoning>#</a></h3><p>logical rule learning</p><h3 id=415-meta-relational-learning>4.1.5 Meta Relational Learning<a hidden class=anchor aria-hidden=true href=#415-meta-relational-learning>#</a></h3><p>The long-tail phenomena exist in the relations of knowledge graphs. Meanwhile, the real-world scenario of knowledge is dynamic, where unseen triples are usually acquired.</p><p>meta relational learning or few-shot relational learning, requires models to predict new relational facts with only a very few samples.</p><h3 id=416-triple-classification>4.1.6 Triple Classification<a hidden class=anchor aria-hidden=true href=#416-triple-classification>#</a></h3><p>Triple classification is to determine whether facts are correct in testing data, which is typically regarded as a binary classification problem.</p><h2 id=42-entity-discovery>4.2 Entity Discovery<a hidden class=anchor aria-hidden=true href=#42-entity-discovery>#</a></h2><h3 id=421-entity-recognition>4.2.1 Entity Recognition<a hidden class=anchor aria-hidden=true href=#421-entity-recognition>#</a></h3><h3 id=422-entity-typing>4.2.2 Entity Typing<a hidden class=anchor aria-hidden=true href=#422-entity-typing>#</a></h3><p>Entity typing includes coarse and fine-grained types, while the latter one uses a tree-structured type category and is typically regarded as multi-class and multi-label classifi- cation.</p><h3 id=423-entity-disambiguation>4.2.3 Entity Disambiguation<a hidden class=anchor aria-hidden=true href=#423-entity-disambiguation>#</a></h3><p>or entity linking is a unified task which links entity mentions to the corresponding entities in a knowledge graph.</p><h3 id=424-entity-alignment>4.2.4 Entity Alignment<a hidden class=anchor aria-hidden=true href=#424-entity-alignment>#</a></h3><p>aims to fuse knowledge among heterogeneous knowledge graphs. In practice, a small set of alignment seeds (i.e., synonymous entities appear in different knowledge graphs) is given to start the alignment process.</p><p>Embedding-based alignment calculates the similarity between embeddings of a pair of entities.</p><h2 id=43-relation-extraction>4.3 Relation Extraction<a hidden class=anchor aria-hidden=true href=#43-relation-extraction>#</a></h2><p><strong>distant supervision</strong>, also referred as weak supervision or self supervision, uses heuristic matching to create training data by assuming that sentences containing the same entity mentions may express the same relation under the supervision of a relational database.</p><p><img alt=/images/papers/paper10-20.png loading=lazy src=/images/papers/paper10-20.png></p><h1 id=temporal-knowledge-graphs>Temporal Knowledge Graphs<a hidden class=anchor aria-hidden=true href=#temporal-knowledge-graphs>#</a></h1><p>incorporate temporal information for representation learning.</p><ul><li>temporal embedding,</li><li>entity dynamics,</li><li>temporal relational dependency,</li><li>temporal logical reasoning.</li></ul><h1 id=knowledge-aware-applications-1>Knowledge-aware Applications<a hidden class=anchor aria-hidden=true href=#knowledge-aware-applications-1>#</a></h1><p>include natural language understanding (NLU), question answering, recommendation systems, and miscellaneous real-world tasks, which inject knowledge to improve representation learning.</p><h1 id=open-knowledge-bases-or-ontologies>Open Knowledge Bases or Ontologies<a hidden class=anchor aria-hidden=true href=#open-knowledge-bases-or-ontologies>#</a></h1><p>WordNet, DBpedia, YAGO, and Freebase</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/nlp/>NLP</a></li><li><a href=https://congchan.github.io/tags/2021/>2021</a></li><li><a href=https://congchan.github.io/tags/ieee/>IEEE</a></li><li><a href=https://congchan.github.io/tags/knowledge-graphs/>Knowledge Graphs</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/knowledge-graph-embedding%E7%9A%84translate%E6%97%8Ftransetranshtransrtransd/><span class=title>« Prev</span><br><span>Knowledge-Graph-Embedding的Translate族（TransE，TransH，TransR，TransD）</span>
</a><a class=next href=https://congchan.github.io/posts/open-domain-targeted-sentiment-analysis-via-span-based-extraction-and-classification/><span class=title>Next »</span><br><span>Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications on x" href="https://x.com/intent/tweet/?text=%e7%bb%bc%e8%bf%b0%20A%20Survey%20on%20Knowledge%20Graphs%20-%20Representation%2c%20Acquisition%20and%20Applications&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E7%25BB%25BC%25E8%25BF%25B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications%2f&amp;hashtags=NLP%2c2021%2cIEEE%2cKnowledgeGraphs"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E7%25BB%25BC%25E8%25BF%25B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications%2f&amp;title=%e7%bb%bc%e8%bf%b0%20A%20Survey%20on%20Knowledge%20Graphs%20-%20Representation%2c%20Acquisition%20and%20Applications&amp;summary=%e7%bb%bc%e8%bf%b0%20A%20Survey%20on%20Knowledge%20Graphs%20-%20Representation%2c%20Acquisition%20and%20Applications&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E7%25BB%25BC%25E8%25BF%25B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E7%25BB%25BC%25E8%25BF%25B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications%2f&title=%e7%bb%bc%e8%bf%b0%20A%20Survey%20on%20Knowledge%20Graphs%20-%20Representation%2c%20Acquisition%20and%20Applications"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E7%25BB%25BC%25E8%25BF%25B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications on whatsapp" href="https://api.whatsapp.com/send?text=%e7%bb%bc%e8%bf%b0%20A%20Survey%20on%20Knowledge%20Graphs%20-%20Representation%2c%20Acquisition%20and%20Applications%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2f%25E7%25BB%25BC%25E8%25BF%25B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications on telegram" href="https://telegram.me/share/url?text=%e7%bb%bc%e8%bf%b0%20A%20Survey%20on%20Knowledge%20Graphs%20-%20Representation%2c%20Acquisition%20and%20Applications&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E7%25BB%25BC%25E8%25BF%25B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 综述 A Survey on Knowledge Graphs - Representation, Acquisition and Applications on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e7%bb%bc%e8%bf%b0%20A%20Survey%20on%20Knowledge%20Graphs%20-%20Representation%2c%20Acquisition%20and%20Applications&u=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E7%25BB%25BC%25E8%25BF%25B0-a-survey-on-knowledge-graphs-representation-acquisition-and-applications%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>