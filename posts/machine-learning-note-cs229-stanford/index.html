<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Machine Learning Note - cs229 - Stanford | Cong's Log</title><meta name=keywords content="Machine Learning"><meta name=description content="参考
CS229: Machine Learning, Stanford
什么是机器学习？目前有两个定义。
亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。
汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义：
E：经验，即历史的数据集。
T：某类任务。
P：任务的绩效衡量。
若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习
“如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。
例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。

Supervised Learning
Linear Regression

Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y
Intercept term: to simplify notation, introduce the convention of letting x0 = 1
Cost function J(θ):   a function that measures, for each value of the θ’s, how close the h(x(i))’s are to the corresponding y(i)’s
Purpose: to choose θ so as to minimize J(θ).
Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ).

LMS(least mean squares) algorithm:

gradient descent
learning rate
error term
batch gradient descent：looks at every example in the entire training set on every step
stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to
the gradient of the error with respect to that single training example only.
particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent.

The normal equations
performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θj’s, and setting them to zero.
To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices"><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/machine-learning-note-cs229-stanford/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/machine-learning-note-cs229-stanford/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/machine-learning-note-cs229-stanford/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Machine Learning Note - cs229 - Stanford"><meta property="og:description" content="参考 CS229: Machine Learning, Stanford
什么是机器学习？目前有两个定义。
亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。
汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义： E：经验，即历史的数据集。 T：某类任务。 P：任务的绩效衡量。 若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习 “如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。 例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。
Supervised Learning Linear Regression Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y Intercept term: to simplify notation, introduce the convention of letting x0 = 1 Cost function J(θ): a function that measures, for each value of the θ’s, how close the h(x(i))’s are to the corresponding y(i)’s Purpose: to choose θ so as to minimize J(θ). Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ). LMS(least mean squares) algorithm: gradient descent learning rate error term batch gradient descent：looks at every example in the entire training set on every step stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only. particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent. The normal equations performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θj’s, and setting them to zero. To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-12-05T00:00:00+00:00"><meta property="article:modified_time" content="2017-12-05T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning Note - cs229 - Stanford"><meta name=twitter:description content="参考
CS229: Machine Learning, Stanford
什么是机器学习？目前有两个定义。
亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。
汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义：
E：经验，即历史的数据集。
T：某类任务。
P：任务的绩效衡量。
若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习
“如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。
例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。

Supervised Learning
Linear Regression

Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y
Intercept term: to simplify notation, introduce the convention of letting x0 = 1
Cost function J(θ):   a function that measures, for each value of the θ’s, how close the h(x(i))’s are to the corresponding y(i)’s
Purpose: to choose θ so as to minimize J(θ).
Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ).

LMS(least mean squares) algorithm:

gradient descent
learning rate
error term
batch gradient descent：looks at every example in the entire training set on every step
stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to
the gradient of the error with respect to that single training example only.
particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent.

The normal equations
performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θj’s, and setting them to zero.
To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Machine Learning Note - cs229 - Stanford","item":"https://congchan.github.io/posts/machine-learning-note-cs229-stanford/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Machine Learning Note - cs229 - Stanford","name":"Machine Learning Note - cs229 - Stanford","description":"参考 CS229: Machine Learning, Stanford\n什么是机器学习？目前有两个定义。\n亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。\n汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义： E：经验，即历史的数据集。 T：某类任务。 P：任务的绩效衡量。 若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习 “如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。 例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。\nSupervised Learning Linear Regression Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y Intercept term: to simplify notation, introduce the convention of letting x0 = 1 Cost function J(θ): a function that measures, for each value of the θ’s, how close the h(x(i))’s are to the corresponding y(i)’s Purpose: to choose θ so as to minimize J(θ). Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ). LMS(least mean squares) algorithm: gradient descent learning rate error term batch gradient descent：looks at every example in the entire training set on every step stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only. particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent. The normal equations performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θj’s, and setting them to zero. To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices\n","keywords":["Machine Learning"],"articleBody":"参考 CS229: Machine Learning, Stanford\n什么是机器学习？目前有两个定义。\n亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。\n汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义： E：经验，即历史的数据集。 T：某类任务。 P：任务的绩效衡量。 若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习 “如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。 例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。\nSupervised Learning Linear Regression Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y Intercept term: to simplify notation, introduce the convention of letting x0 = 1 Cost function J(θ): a function that measures, for each value of the θ’s, how close the h(x(i))’s are to the corresponding y(i)’s Purpose: to choose θ so as to minimize J(θ). Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ). LMS(least mean squares) algorithm: gradient descent learning rate error term batch gradient descent：looks at every example in the entire training set on every step stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only. particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent. The normal equations performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θj’s, and setting them to zero. To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices\nMatrix derivatives: the gradient ∇Af(A) is itself an m-by-n matrix, whose (i, j)-element is ∂f/∂Aij Least squares revisited: Given a training set, define the design matrix X to be the m-by-n matrix (actually m-by-n + 1, if we include the intercept term) that contains the training examples’ input values in its rows, let y be the m-dimensional vector containing all the target values from the training set, used the fact that the trace of a real number is just the real number( trace operator, written “tr.” For an n-by-n matrix A, the trace of A is defined to be the sum of its diagonal entries: trA = ΣAii To minimize J, find its derivatives with respect to θ: ∇θJ(θ) = XTXθ − XTy To minimize J, we set its derivatives to zero, and obtain the normal equations: XTXθ = XTy Thus the value of θ that minimizes J(θ) is given in closed form by the equation: θ = (XTX)-1XTy Probabilistic interpretation why the least-squares cost function J is a reasonable choice? With a set a probabilistic assumptions, under which least-squares regression is derived as a very natural algorithm.\nLocally weighted linear regression (LWR) algorithm assuming there is sufficient training data, makes the choice of features less critical.\nIn the original linear regression algorithm, to make a prediction at a query point x (i.e., to evaluate h(x)), we would: Fit θ to minimize Σi(y(i) − θTx(i))2. Output θTx. The locally weighted linear regression algorithm does the following: Fit θ to minimize Σiw(i)(y(i) − θTx(i))2. Output θTx. Here, the w(i)’s are non-negative valued weights Intuitively, if w(i) is large for a particular value of i, then in picking θ, we’ll try hard to make (y(i) − θTx(i))2 small. If w(i) is small, then the error term will be pretty much ignored in the fit. A fairly standard choice for the weights is w(i) = exp(-(x(i)-x)2 / 2τ2 ) if |x(i)-x| is small, then w(i) ≈ 1; if large, then w(i) is small. Hence, θ is chosen giving a much higher “weight” to the (errors on) training examples close to the query point x. The parameter τ controls how quickly the weight of a training example falls off with distance of its x(i), from the query point x; τ is called the bandwidth parameter Classification and logistic regression Logistic regression logistic function or the sigmoid function: g(z) = (1 + e−z)-1. g(z) tends towards 1 as z → ∞, and g(z) tends towards 0 as z → −∞. derivative of the sigmoid function: g(z)’ = g(z)(1 - g(z)) endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likelihood: Similar to our derivation in the case of linear regression, we can use gradient ascent to maximize the likelihood. updates will therefore be given by θ := θ + α∇θℓ(θ). (Note the positive rather than negative sign in the update formula, since we’re maximizing,rather than minimizing, a function now.) This therefore gives us the stochastic gradient ascent rule: θj := θj + α(y(i)− hθ(x(i)))x(i)j If we compare this to the LMS update rule, we see that it looks identical; but this is not the same algorithm, because hθ(x(i)) is now defined as a non-linear function of θTx(i). * Nonetheless, it’s a little surprising that we end up with the same update rule for a rather different algorithm and learning problem. Is this coincidence, or is there a deeper reason behind this? Check GLM models. Generalized Linear Models The exponential family Bernoulli distributions Gaussianexponential distributions multinomial Poisson (for modelling count-data) beta and the Dirichlet (for distributions over probabilities) Constructing GLMs Ordinary Least Squares Logistic Regression Softmax Regression Softmax Regression Consider a classification problem in which the response variable y can take on any one of k values, so y ∈ {1, 2, . . . , k}. We will thus model it as distributed according to a multinomial distribution.\nparameterize the multinomial with only k − 1 parameters, φ1, . . . , φk−1, where φi = p(y = i; φ), and p(y = k; φ) = 1 − Σki=1φi. To express the multinomial as an exponential family distribution, we will definee T(y) ∈ Rk-1： * η = [log(φ1/φk),…,log(φk-1/φk)], the ηi’s are linearly related to the x’s. * softmax function: a mapping from the η’s to the φ’s: φi = eηi / Σkj=1eηi softmax regression: the model, which applies to classification problems where y ∈ {1, . . . , k}: p(y = i|x; θ) = φi = eθTi x / Σkj=1eθTi x This hypothesis will output the estimated probability that p(y = i|x; θ), for every value of i = 1, . . . , k. parameter fitting: obtain the maximum likelihood estimate of the parameters by maximizing ℓ(θ) in terms of θ, using a method such as gradient ascent or Newton’s method. Naive Bayes classification 朴素贝叶斯 以二元分类为例: 根据A和B各自的先验概率和条件概率, 算出针对某一特征事件的后验概率, 然后正则化(正则化后两个后验概率之和为1, 但不影响对事件的触发对象是A或B的判断)\nWhy naïve: 忽略了事件发生的顺序, 故称之为\"朴素\" Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了 详见加速自然语言处理-朴素贝叶斯 Problem: how to deal with continuous values features? Use Gaussian Naive Bayes. Gaussian Naive Bayes With real-valued inputs, we can calculate the mean and standard deviation of input values (x) for each class to summarize the distribution. This means that in addition to the probabilities for each class, we also store the mean μ and standard deviations σ of each feature for each class.\nThe class conditional probability P(x|c) is estimated by probability density of the normal distribution : Algorithm – continuous Xi (but still discrete Y) Train Naïve Bayes (examples) for each class value yk: estimate P(Yk) for each attribute Xi: estimate class conditional mean, variance Classify(xnew): Ynew \u003c- argmax(k) ∏P(xi|Yk)P(Yk) Short: classes with the same distribution Missing data instances in NB Ignore attribute in instance where its value is missing compute likelihood based on observed attribtues no need to “fill in” or explicitly model missing values based on conditional independence between attributes Generative and Discriminative Algorithm: Generative classifiers learn a model of the joint probability, p(x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(y|x), and then picking the most likely label y. Discriminative classifiers model the posterior p(y|x) directly, or learn a direct map(hypothesis/functions) from inputs x to the class labels. Generative models advantage: Can be good with missing data, naive Bayes handles missing data good for detecting outliers to generate likely input (x,y). Decision trees 决策树 Algorithm: ID3 algorithm Decision trees with continuous attributes: Create split based on threshold ID3 algorithm Recursive Split( node, {examples} ): 1. A \u003c- the best attribute for splitting the {examples} 2. For each value of A, create new child node 3. Split training {examples} to child nodes 4. For each child node, subset: * If subset is pure - stop * Else: split(child_node, {subset} ) How to decide which attribute is the best to split on: Entropy\nEntropy Use log2 here is to represent concepts of information - on average how many bits needed to tell X split purity To represent two classes, need one bit “0, 1”, to represent 4 classes, need 2 bits “00, 01, 10, 11” If x is pure(one class only), entropy is 0. Information Gain: Expected drop in entropy after split, Gain( P, C) = Entropy(parent) - Σw*Entropy(children), w is weighted average matrix., A is the split attribute Problems: tend to pick attributes with lots of values, could not generalize well on new data. use GainRation: for attribute A with many different values V, the SplitEntropy will be large, Overfitting in Decision Trees the tree split too deep to try to classify almost every single sample. As a result the model could not predict new data well.\nSub-tree replacement pruning For each node: Pretend remove node + all children from the tree Measure performance on validation set Remove node that results in greatest improvement Repeat until further pruning is harmful Decision boundary Logistic Regression and trees differ in the way that they generate decision boundaries\nDecision Trees bisect the space into smaller and smaller regions, Logistic Regression fits a single line/hyperplane to divide the space exactly into two. Random Decision forest Grow K different decision trees: pick a random subset Sr of training examples grow a full ID3 tree (no prunning): When splitting: pick from d«D random attributes Computing gain based on Sr instead of full set repeat for r =1…K Given a new data point X: classify X using each of the trees T1 …. Tk use majority vote: class predicted most often SVM Intuition: Suppose there is a good hyperplane to seperate data set, h(x)=g(wTx+b), (relation with fully connected layer and activation funciton in DNN). Want functional margin of hyperplane to be large: for dataset (xi,yi), functional margin γi = yi(wTxi+b), if yi=1, need wTxi+b»0, if yi=-1, need wTxi+b«0. Thus γi\u003e0 means the classification is correct. Geometric margins: Define the hyperplane as wTx+b=0, the normal of the hyperplane is w/||w||, thus a point A(xi)’s, which represents the input x(i) of some training example with label y(i) = 1, projection on the hyperplane is point B = xi - γi·w/||w||, where γi is xi’s distance to the decision boundary. Thus wT(xi - γi·w/||w||) + b=0 =\u003e γi = (w/||w||)Txi+ b/||w||. More generally, the geometric margin of (w, b) with respect to a training example (xi, yi) is γi = yi· (w/||w||)Txi+ b/||w|| If ||w|| = 1, then the functional margin equals the geometric margin The optimal margin classifier: Given a training set, a natural desideratum is to try to find a decision boundary that maximizes the minimum (geometric) margin, i.e want min(γi) as large as possible. Via some transformation, the object turns to minimize ||w||2, subject to y(i)·(wTxi+b) ≥ 1, Lagrange duality: solving constrained optimization problems. w = Σαiyixi, αi is Lagrange multipliers. Support vector: The points with the smallest margins. The number of support vectors can be much smaller than the size the training set Training: fit our model’s parameters to a training set, and now wish to make a prediction at a new point input x. We would then calculate wTx + b, and predict y = 1 if and only if this quantity is bigger than zero. . In order to make a prediction, we have to calculate it which depends only on the inner product between x and the points in the training set. Moreover, αi’s will all be zero except for the support vectors. Thus, many of the terms in the sum above will be zero, and we need to find only the inner products between x and the support vectors (of which there is often only a small number) in order to make our prediction. The inner product could be replaced by kernel k(xi,x) Kernels Define the “original” input value x as the input attributes of a problem. When that is mapped to some new set of quantities that are then passed to the learning algorithm, we’ll call those new quantities the input features. φ denote the feature mapping, which maps from the attributes to the features. E.g. φ(x) = [x, x^2, x^3] given a feature mapping φ, we define the corresponding Kernel to be K(x, z) = φ(x)Tφ(z) Often, φ(x) itself may be very expensive to calculate (perhaps because it is an extremely high dimensional vector, require memory), K(x, z) may be very inexpensive to calculate. We can get SVMs to learn in the high dimensional feature space given by φ, but without ever having to explicitly find or represent vectors φ(x). E.g. Based on Mercer’s Theorem, you can either explicitly map the data with a φ and take the dot product, or you can take any kernel and use it right away, without knowing nor caring what φ looks like Keep in mind however that the idea of kernels has significantly broader applicability than SVMs. Specifically, if you have any learning algorithm that you can write in terms of only inner products between input attribute vectors, then by replacing this with K(x, z) where K is a kernel, you can allow your algorithm to work efficiently in the high dimensional feature space corresponding to K. SVM vs. Logistic regression Logistic regression focuses on maximizing the probability of the data. The further the data lies from the separating hyperplane (on the correct side), the happier LR is. An SVM don’t care about getting the right probability, i.e the right P(y=1|x), but only care about P(y=1|x)/P(y=0|x)≥ c. It tries to find the separating hyperplane that maximizes the distance of the closest points to the margin (the support vectors). If a point is not a support vector, it doesn’t really matter. P(y=1|x)/P(y=0|x) \u003e c, if c=1, that means P(y=1|x) \u003e P(y=0|x), thus y=1, take log of both side, and plug in P(y=1|x) = sigmoid(wTx + b), P(y=0|x)=1-P(y=1|x), recall the sigmoid, we get wTx + b \u003e 0 Underlying basic idea of linear prediction is the same, but error functions differ, the r = P(y=1|x)/P(y=0|x) = exp(wTx + b), different classifiers assigns different cost to r If cost(r)=log(1 + 1/r), this is logistic regression If cost(r)=max(0, 1-log(r))=max(0, 1-(wTx + b)), then SVM Logistic regression (non-sparse) vs SVM (hinge loss, sparse solution) Linear regression (squared error) vs SVM (ϵ insensitive error) K Nearest Neighbour Intuition: predict based on nearby/similar training data.\nAlgorithm: for a test data compute its distance to every training example xi select k closest training instances prediction: For Classification: predict as the most frequent label among the k instances. For regression: predict as the mean of label among the k instances. Choose k large k: everything classified as the most probable class small k: highly variable, unstable decision boundaries affects “smoothness” of the boundary Use train-validation to choose k Distance meansures: Euclidian: symmetric, spherical, treats all dimensions equally, but sensitive to extreme differences in single attribtue Hamming: number of attribtues that differ Resolve ties: random prior: pick class with greater prior nearest: use 1-NN classifier to decide Missing values: have to fill in the missing values, otherwise cannot compute distance. Pro and cons: Almost no assumptions about data easy to update in online setting: just add new item to training set Need to handle missing data: fill-in or create a special distance Sensitive to outliers Sensitve to lots of irrelevant aeributes (affect distance) Computationally expensive: need to compute distance to all examples O(nd) - Vectorization Faster knn: K-D Trees, Inverted lists, Locality-sensitive hashing K-D Trees low-dimensional, real-valued data\nA kd-tree is a binary tree data structure for storing a fi\fnite set of points from a k-dimensional space. Build the tree: Pick random dimension, Find median, Split data Nearest neighbor search: Traverse the whole tree, BUT make two modifications to prune to search space: Keep variable of closest point C found so far. Prune subtrees once their bounding boxes say that they can’t contain any point closer than C Search the subtrees in order that maximizes the chance for pruning Inverted lists high-dimensional, discrete data, sparse\nApplication: text classification, most attribute values are zero (sparseness), training: list all training examples that contain particular attribute Testing: merge inverted list for attribtues presented in the test set, and choose those instances in the new inverted list as the neighbours Locality-sensitive hashing high-d, discrete or real-valued\nUnsupervised learning 无监督学习 Clustering K-means split data into a specified number of populations\nInput: K (number of clusters in the data) Training set {x1, x2, x3 …, xn) Algorithm: Randomly initialize K cluster centroids as {μ1, μ2, μ3 … μK}, now centroid could represent cluster. Repeat until converge: Inner loop 1: repeatedly sets the c(i) variable to be the index of the closes variable of cluster centroid closes to xi, i.e. take ith example, measure squared distance to each cluster centroid, assign c(i)to the closest cluster(centroid) Inner loop 2: For each cluster j, new centroid c(j) = average mean of all the points assigned to the cluster j in previous step. Target (Distortion) function: J(c,μ)=Σ|| xi-μi ||^2, coordinate ascent, decrease monotonically, thus guarantee to converge. What if there’s a centroid with no data: Remove that centroid, so end up with K-1 classes, Or, randomly reinitialize it, not sure when though… How to choose cluster numbers: scree plot to find the best k. Hierarchical K-means A Top-down approach run k-means algorithm on the original dataset for each of the resulting clusters, recursively run k-means Pro cons: Fast nearby points may end up in different clusters Agglomerative Clustering A bottom up algorithm:\n1. starts with a collections of singleton clusters 2. repeat until only one cluster is left: 1. Find a pair of clusters that is closest 2. Merge the pair of clusters into one new cluster 3. Remove the old pair of clusters Need to define a distance metric over clusters Produce a dendrogram: Hierarchical tree of clusters slow Gaussian Mixtures For non-Gaussian distribution data, assume it is a mixture of several(k) Gaussians.\nAlgorithm: EM EM algorithm strategy will be to repeatedly construct a lower-bound on ℓ(E-step) based on Jensen’s inequality, and then optimize that lower-bound(M-step).\nE step: For each i, let Qi be some distribution over the z’s (ΣzQi(z) = 1, Qi(z) ≥ 0). z(i) indicating which of the k Gaussians each x(i) had come from, get P(Z)=φ, then compute the conditional probability wj as P(x|Z) via Gaussian Naive Bayes: M step: maximize, with respect to our parameters φ, µ, Σ, the quantity, by updating parameter(φ, µ, σ) 举例：start with two randomly placed Gaussians (μa, σa), (μb, σb), assume a uniform prior (P(a)=P(b)=0.5), iterate until convergence: E-step: for each point: P(b|xi), P(a|xi)=1-P(b|xi) , does it look like it came from b or a? M-step: adjust (μa, σa) and (μb, σb) to fit points soft assigned to them, The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the “hard” cluster assignments c, we instead have the “soft” assignments w. Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea. How to pick k: cannot discover K, likelihood keeps growing with K K-means vs. EM Dimensionality Reduction Pros: reflects human intuitions about the data allows estimating probabilities in highadimensional data: no need to assume independence etc. dramatic reduction in size of data: faster processing (as long as reduction is fast), smaller storage Cons too expensive for many applications (Twitter, web) disastrous for tasks with fine-grained classes understand assumptions behind the methods (linearity etc.): there may be better ways to deal with sparseness Factor analysis If the features n ≫ m, or n≈m, in such a problem, it might be difficult to model the data even with a single Gaussian, 更别提高斯混合了. Because the variance matrix Σ becomes singular - non invertable.\nPrincipal Components Analysis PCA, automatically detect and reduce data to lower dimension k, k « n, preserve dimenson that affects class separability most.\nAlgorithm: Pre-process: data normalization to 0 mean and unit variance, Steps (3-4) may be omitted if we had apriori knowledge that the different attributes are all on the same scale to project data into a k-dimensional subspace (k \u003c n), we should choose e1,… ek to be the top k eigenvectors of Σ. The e’s now form a new, orthogonal basis for the data. To represent a training data point x with d dimension into this basis (k dimension), e1Tx,…ekTx The vectors u1,…, uk are called the first k principal components of the data. Eigenvalue λi = variance along ei. Pick ei that explain the most variance by sorting eigenvectors s.t. λ1 ≥ λ2 ≥…≥ λn pick first k eigenvectors which explain 90% or 95% of the total variance Σλ(i). Maximize the variance of projection of x onto a unit vector u, Application: eigenfaces Linear Discriminant Analysis LDA\nIdea: pick a new dimension that gives maximum separation between means of projected classes minimum variance within each projected class How: eigenvectors based on between-class and within-class covariance matrices LDA not guaranteed to be better for Classification assumes classes are unimodal Gaussians fails when discriminatory information is not in the mean, but in the variance of the data Singular Value Decomposition Generalization and evaluation Receiver Operating Characteristic ROC, plot TPR(Sensitivity) vs. FPR(Specificity) as t varies from ∞ to -∞, shows performance of system across all possible thresholds A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner. Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test AUC: area under ROC curve, popular alternative to Accuracy Confidence interval tell us how closed our estimation\nE = probability that misclassify a random instance: Take a random set of n instances, how many misclassified? Equal to Binomial distribution with mean = nE, variance = nE(1-E) Efuture: the next instance’s probability of misclassified = average #misclassifed = variance / n = mean E= E(1-E)/n, small variance means big confidence interval, a Gaussian distribution with one variance distance extend from mean will cover 2/3 future test sets p% Confidence interval for future error, 95% confidence interval needs about 2 variance extends from mean. .\n","wordCount":"3775","inLanguage":"en","datePublished":"2017-12-05T00:00:00Z","dateModified":"2017-12-05T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/machine-learning-note-cs229-stanford/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Machine Learning Note - cs229 - Stanford</h1><div class=post-meta><span title='2017-12-05 00:00:00 +0000 UTC'>2017-12-05</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/machine-learning.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#supervised-learning aria-label="Supervised Learning">Supervised Learning</a><ul><li><a href=#linear-regression aria-label="Linear Regression">Linear Regression</a><ul><li><a href=#lmsleast-mean-squares-algorithm aria-label="LMS(least mean squares) algorithm:">LMS(least mean squares) algorithm:</a></li><li><a href=#the-normal-equations aria-label="The normal equations">The normal equations</a></li><li><a href=#probabilistic-interpretation aria-label="Probabilistic interpretation">Probabilistic interpretation</a></li><li><a href=#locally-weighted-linear-regression-lwr-algorithm aria-label="Locally weighted linear regression (LWR) algorithm">Locally weighted linear regression (LWR) algorithm</a></li></ul></li><li><a href=#classification-and-logistic-regression aria-label="Classification and logistic regression">Classification and logistic regression</a><ul><li><a href=#logistic-regression aria-label="Logistic regression">Logistic regression</a></li></ul></li><li><a href=#generalized-linear-models aria-label="Generalized Linear Models">Generalized Linear Models</a><ul><li><a href=#the-exponential-family aria-label="The exponential family">The exponential family</a></li><li><a href=#constructing-glms aria-label="Constructing GLMs">Constructing GLMs</a><ul><li><a href=#softmax-regression aria-label="Softmax Regression">Softmax Regression</a></li></ul></li></ul></li><li><a href=#naive-bayes-classification-%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af aria-label="Naive Bayes classification 朴素贝叶斯">Naive Bayes classification 朴素贝叶斯</a><ul><li><a href=#gaussian-naive-bayes aria-label="Gaussian Naive Bayes">Gaussian Naive Bayes</a></li><li><a href=#missing-data-instances-in-nb aria-label="Missing data instances in NB">Missing data instances in NB</a></li></ul></li><li><a href=#generative-and-discriminative-algorithm aria-label="Generative and Discriminative Algorithm:">Generative and Discriminative Algorithm:</a></li><li><a href=#decision-trees-%e5%86%b3%e7%ad%96%e6%a0%91 aria-label="Decision trees 决策树">Decision trees 决策树</a><ul><li><a href=#id3-algorithm aria-label="ID3 algorithm">ID3 algorithm</a></li><li><a href=#entropy aria-label=Entropy>Entropy</a></li><li><a href=#overfitting-in-decision-trees aria-label="Overfitting in Decision Trees">Overfitting in Decision Trees</a></li><li><a href=#decision-boundary aria-label="Decision boundary">Decision boundary</a></li><li><a href=#random-decision-forest aria-label="Random Decision forest">Random Decision forest</a></li></ul></li><li><a href=#svm aria-label=SVM>SVM</a><ul><li><a href=#kernels aria-label=Kernels>Kernels</a></li><li><a href=#svm-vs-logistic-regression aria-label="SVM vs. Logistic regression">SVM vs. Logistic regression</a></li></ul></li><li><a href=#k-nearest-neighbour aria-label="K Nearest Neighbour">K Nearest Neighbour</a><ul><li><a href=#k-d-trees aria-label="K-D Trees">K-D Trees</a></li><li><a href=#inverted-lists aria-label="Inverted lists">Inverted lists</a></li><li><a href=#locality-sensitive-hashing aria-label="Locality-sensitive hashing">Locality-sensitive hashing</a></li></ul></li></ul></li><li><a href=#unsupervised-learning-%e6%97%a0%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0 aria-label="Unsupervised learning 无监督学习">Unsupervised learning 无监督学习</a><ul><li><a href=#clustering aria-label=Clustering>Clustering</a><ul><li><a href=#k-means aria-label=K-means>K-means</a></li><li><a href=#hierarchical-k-means aria-label="Hierarchical K-means">Hierarchical K-means</a></li><li><a href=#agglomerative-clustering aria-label="Agglomerative Clustering">Agglomerative Clustering</a></li><li><a href=#gaussian-mixtures aria-label="Gaussian Mixtures">Gaussian Mixtures</a></li><li><a href=#em-algorithm aria-label="EM algorithm">EM algorithm</a></li><li><a href=#k-means-vs-em aria-label="K-means vs. EM">K-means vs. EM</a></li></ul></li><li><a href=#dimensionality-reduction aria-label="Dimensionality Reduction">Dimensionality Reduction</a><ul><li><a href=#factor-analysis aria-label="Factor analysis">Factor analysis</a></li><li><a href=#principal-components-analysis aria-label="Principal Components Analysis">Principal Components Analysis</a></li><li><a href=#linear-discriminant-analysis aria-label="Linear Discriminant Analysis">Linear Discriminant Analysis</a></li><li><a href=#singular-value-decomposition aria-label="Singular Value Decomposition">Singular Value Decomposition</a></li></ul></li></ul></li><li><a href=#generalization-and-evaluation aria-label="Generalization and evaluation">Generalization and evaluation</a><ul><li><a href=#receiver-operating-characteristic aria-label="Receiver Operating Characteristic">Receiver Operating Characteristic</a></li><li><a href=#confidence-interval aria-label="Confidence interval">Confidence interval</a></li></ul></li></ul></div></details></div><div class=post-content><p>参考
<a href=http://cs229.stanford.edu/notes>CS229: Machine Learning, Stanford</a></p><p>什么是机器学习？目前有两个定义。</p><p>亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。</p><p>汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义：
E：经验，即历史的数据集。
T：某类任务。
P：任务的绩效衡量。
若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习
“如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。
例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。</p><h2 id=supervised-learning><a href=http://cs229.stanford.edu/notes/cs229-notes1.pdf>Supervised Learning</a><a hidden class=anchor aria-hidden=true href=#supervised-learning>#</a></h2><h3 id=linear-regression>Linear Regression<a hidden class=anchor aria-hidden=true href=#linear-regression>#</a></h3><ul><li>Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y</li><li>Intercept term: to simplify notation, introduce the convention of letting x<sub>0</sub> = 1</li><li>Cost function J(θ): <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/linearR_cost.png> a function that measures, for each value of the θ’s, how close the h(x<sup>(i)</sup>)’s are to the corresponding y<sup>(i)</sup>’s</li><li>Purpose: to choose θ so as to minimize J(θ).</li><li>Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ).</li></ul><h4 id=lmsleast-mean-squares-algorithm>LMS(least mean squares) algorithm:<a hidden class=anchor aria-hidden=true href=#lmsleast-mean-squares-algorithm>#</a></h4><ul><li>gradient descent</li><li>learning rate</li><li>error term</li><li>batch gradient descent：looks at every example in the entire training set on every step</li><li>stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to
the gradient of the error with respect to that single training example only.</li><li>particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent.</li></ul><h4 id=the-normal-equations>The normal equations<a hidden class=anchor aria-hidden=true href=#the-normal-equations>#</a></h4><p>performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θ<sub>j</sub>’s, and setting them to zero.
To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices</p><ul><li>Matrix derivatives: the gradient ∇<sub>A</sub>f(A) is itself an m-by-n matrix, whose (i, j)-element is ∂f/∂A<sub>ij</sub></li><li>Least squares revisited: Given a training set,<ul><li>define the design matrix X to be the m-by-n matrix (actually m-by-n + 1, if we include the intercept term) that contains the training examples’ input values in its rows,</li><li>let y be the m-dimensional vector containing all the target values from the training set,</li><li>used the fact that the trace of a real number is just the real number( trace operator, written “tr.” For an n-by-n matrix A, the trace of A is defined to be the sum of its diagonal entries: trA = ΣA<sub>ii</sub></li><li>To minimize J, find its derivatives with respect to θ: ∇<sub>θ</sub>J(θ) = X<sup>T</sup>Xθ − X<sup>T</sup>y</li><li>To minimize J, we set its derivatives to zero, and obtain the normal equations: X<sup>T</sup>Xθ = X<sup>T</sup>y</li><li>Thus the value of θ that minimizes J(θ) is given in closed form by the equation: θ = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y</li></ul></li></ul><h4 id=probabilistic-interpretation>Probabilistic interpretation<a hidden class=anchor aria-hidden=true href=#probabilistic-interpretation>#</a></h4><p>why the least-squares cost function J is a reasonable choice? With a set a probabilistic assumptions, under which least-squares regression is derived as a very natural algorithm.</p><h4 id=locally-weighted-linear-regression-lwr-algorithm>Locally weighted linear regression (LWR) algorithm<a hidden class=anchor aria-hidden=true href=#locally-weighted-linear-regression-lwr-algorithm>#</a></h4><p>assuming there is sufficient training data, makes the choice of features less critical.</p><ul><li>In the original linear regression algorithm, to make a prediction at a query point x (i.e., to evaluate h(x)), we would:<ol><li>Fit θ to minimize Σ<sub>i</sub>(y<sup>(i)</sup> − θ<sup>T</sup>x<sup>(i)</sup>)<sup>2</sup>.</li><li>Output θ<sup>T</sup>x.</li></ol></li><li>The locally weighted linear regression algorithm does the following:<ol><li>Fit θ to minimize Σ<sub>i</sub>w<sup>(i)</sup>(y<sup>(i)</sup> − θ<sup>T</sup>x<sup>(i)</sup>)<sup>2</sup>.</li><li>Output θ<sup>T</sup>x.</li></ol></li><li>Here, the w<sup>(i)</sup>’s are non-negative valued <strong>weights</strong></li><li>Intuitively, if w<sup>(i)</sup> is large for a particular value of i, then in picking θ, we’ll try hard to make (y<sup>(i)</sup> − θ<sup>T</sup>x<sup>(i)</sup>)<sup>2</sup> small. If w<sup>(i)</sup> is small, then the error term will be pretty much ignored in the fit.</li><li>A fairly standard choice for the weights is w<sup>(i)</sup> = exp(-(x<sup>(i)</sup>-x)<sup>2</sup> / 2τ<sup>2</sup> )</li><li>if |x<sup>(i)</sup>-x| is small, then w<sup>(i)</sup> ≈ 1; if large, then w<sup>(i)</sup> is small. Hence, θ is chosen giving a much higher “weight” to the (errors on) training examples close to the query point x.</li><li>The parameter τ controls how quickly the weight of a training example falls off with distance of its x<sup>(i)</sup>, from the query point x; τ is called the <strong>bandwidth</strong> parameter</li></ul><h3 id=classification-and-logistic-regression>Classification and logistic regression<a hidden class=anchor aria-hidden=true href=#classification-and-logistic-regression>#</a></h3><h4 id=logistic-regression>Logistic regression<a hidden class=anchor aria-hidden=true href=#logistic-regression>#</a></h4><ul><li>logistic function or the <strong>sigmoid function</strong>: g(z) = (1 + e<sup>−z</sup>)<sup>-1</sup>. <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/lr.png>
g(z) tends towards 1 as z → ∞, and g(z) tends towards 0 as z → −∞. <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/sigmoid.png></li><li>derivative of the sigmoid function: g(z)<sup>&rsquo;</sup> = g(z)(1 - g(z))</li><li>endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likelihood:<ul><li>Similar to our derivation in the case of linear regression, we can use gradient ascent to maximize the likelihood.</li><li>updates will therefore be given by θ := θ + α∇<sub>θ</sub>ℓ(θ). (Note the positive rather than negative sign in the update formula, since we’re maximizing,rather than minimizing, a function now.)</li><li>This therefore gives us the stochastic gradient ascent rule: θ<sub>j</sub> := θ<sub>j</sub> + α(y<sup>(i)</sup>− h<sub>θ</sub>(x<sup>(i)</sup>))x<sup>(i)</sup><sub>j</sub></li><li>If we compare this to the LMS update rule, we see that it looks identical; but this is not the same algorithm, because h<sub>θ</sub>(x<sup>(i)</sup>) is now defined as a non-linear function of θ<sup>T</sup>x<sup>(i)</sup>.
     * Nonetheless, it’s a little surprising that we end up with the same update rule for a rather different algorithm and learning problem. Is this coincidence, or is there a deeper reason behind this? Check <a href=/posts/machine-learning-note-cs229-stanford/#generalized-linear-models>GLM models</a>.  </li></ul></li></ul><h3 id=generalized-linear-models>Generalized Linear Models<a hidden class=anchor aria-hidden=true href=#generalized-linear-models>#</a></h3><h4 id=the-exponential-family>The exponential family<a hidden class=anchor aria-hidden=true href=#the-exponential-family>#</a></h4><ul><li>Bernoulli distributions</li><li>Gaussianexponential distributions</li><li>multinomial</li><li>Poisson (for modelling count-data)</li><li>beta and the Dirichlet (for distributions over probabilities)</li></ul><h4 id=constructing-glms>Constructing GLMs<a hidden class=anchor aria-hidden=true href=#constructing-glms>#</a></h4><ul><li>Ordinary Least Squares</li><li>Logistic Regression</li><li>Softmax Regression</li></ul><h5 id=softmax-regression>Softmax Regression<a hidden class=anchor aria-hidden=true href=#softmax-regression>#</a></h5><p>Consider a classification problem in which the response variable y can take on any one of k values, so y ∈ {1, 2, . . . , k}. We will thus model it as distributed according to a multinomial distribution.</p><ul><li>parameterize the multinomial with only k − 1 parameters, φ<sub>1</sub>, . . . , φ<sub>k−1</sub>, where φ<sub>i</sub> = p(y = i; φ), and p(y = k; φ) = 1 − Σ<sup>k</sup><sub>i=1</sub>φ<sub>i</sub>.</li><li>To express the multinomial as an exponential family distribution, we will definee T(y) ∈ R<sup>k-1</sup>：
<img alt=T(y) loading=lazy src=/image/cs229-notes1-pic01.png>
  * η = [log(φ<sub>1</sub>/φ<sub>k</sub>),&mldr;,log(φ<sub>k-1</sub>/φ<sub>k</sub>)], the η<sub>i</sub>’s are linearly related to the x’s.
  * softmax function: a mapping from the η’s to the φ’s: φ<sub>i</sub> = e<sup>ηi</sup> / Σ<sup>k</sup><sub>j=1</sub>e<sup>ηi</sup></li><li>softmax regression: the model, which applies to classification problems where y ∈ {1, . . . , k}: p(y = i|x; θ) = φ<sub>i</sub> = e<sup>θ<sup>T</sup><sub>i</sub> x</sup> / Σ<sup>k</sup><sub>j=1</sub>e<sup>θ<sup>T</sup><sub>i</sub> x</sup>  </li><li>This hypothesis will output the estimated probability that p(y = i|x; θ), for every value of i = 1, . . . , k.</li><li>parameter fitting: obtain the maximum likelihood estimate of the parameters by maximizing ℓ(θ) in terms of θ, using a method such as gradient ascent or Newton’s method.</li></ul><h3 id=naive-bayes-classification-朴素贝叶斯>Naive Bayes classification 朴素贝叶斯<a hidden class=anchor aria-hidden=true href=#naive-bayes-classification-朴素贝叶斯>#</a></h3><p>以二元分类为例: 根据A和B各自的先验概率和条件概率, 算出针对某一特征事件的后验概率, 然后正则化(正则化后两个后验概率之和为1, 但不影响对事件的触发对象是A或B的判断)</p><ul><li>Why naïve: 忽略了事件发生的顺序, 故称之为"朴素"</li><li>Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了</li><li>详见<a href=https://github.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/blob/master/%E5%8A%A0%E9%80%9F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.md#naive-bayes-classifier>加速自然语言处理-朴素贝叶斯</a></li><li>Problem: how to deal with continuous values features? Use Gaussian Naive Bayes.</li></ul><h4 id=gaussian-naive-bayes>Gaussian Naive Bayes<a hidden class=anchor aria-hidden=true href=#gaussian-naive-bayes>#</a></h4><p>With real-valued inputs, we can calculate the mean and standard deviation of input values (x) for each class to summarize the distribution. This means that in addition to the probabilities for each class, we also store the mean μ and standard deviations σ of each feature for each class.</p><ul><li>The class conditional probability P(x|c) is estimated by probability density of the normal distribution <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/probability_density.png>:</li><li>Algorithm – continuous Xi (but still discrete Y)<ul><li>Train Naïve Bayes (examples)</li></ul><pre tabindex=0><code>for each class value yk:
    estimate P(Yk)
    for each attribute Xi:
        estimate class conditional mean, variance
</code></pre><ul><li>Classify(xnew): <code>Ynew &lt;- argmax(k) ∏P(xi|Yk)P(Yk)</code></li></ul></li><li>Short: classes with the same distribution</li></ul><h4 id=missing-data-instances-in-nb>Missing data instances in NB<a hidden class=anchor aria-hidden=true href=#missing-data-instances-in-nb>#</a></h4><ul><li>Ignore attribute in instance where its value is missing</li><li>compute likelihood based on observed attribtues</li><li>no need to “fill in” or explicitly model missing values</li><li>based on conditional independence between attributes</li></ul><h3 id=generative-and-discriminative-algorithm>Generative and Discriminative Algorithm:<a hidden class=anchor aria-hidden=true href=#generative-and-discriminative-algorithm>#</a></h3><ul><li>Generative classifiers learn a model of the joint probability, p(x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(y|x), and then picking the most likely label y.</li><li>Discriminative classifiers model the posterior p(y|x) directly, or learn a direct map(hypothesis/functions) from inputs x to the class labels.</li><li>Generative models advantage:<ul><li>Can be good with missing data, naive Bayes handles missing data</li><li>good for detecting outliers</li><li>to generate likely input (x,y).</li></ul></li></ul><h3 id=decision-trees-决策树>Decision trees 决策树<a hidden class=anchor aria-hidden=true href=#decision-trees-决策树>#</a></h3><ul><li>Algorithm: ID3 algorithm</li><li>Decision trees with continuous attributes: Create split based on threshold</li></ul><h4 id=id3-algorithm>ID3 algorithm<a hidden class=anchor aria-hidden=true href=#id3-algorithm>#</a></h4><p>Recursive Split( node, {examples} ):
1. A &lt;- the best attribute for splitting the {examples}
2. For each value of A, create new child node
3. Split training {examples} to child nodes
4. For each child node, subset:
* If subset is pure - stop
* Else: split(child_node, {subset} )
How to decide which attribute is the best to split on: Entropy</p><h4 id=entropy>Entropy<a hidden class=anchor aria-hidden=true href=#entropy>#</a></h4><ul><li><img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/entropy.png></li><li>Use log2 here is to represent concepts of information - on average how many bits needed to tell X split purity<ul><li>To represent two classes, need one bit &ldquo;0, 1&rdquo;, to represent 4 classes, need 2 bits &ldquo;00, 01, 10, 11&rdquo;</li><li>If x is pure(one class only), entropy is 0.</li></ul></li><li>Information Gain: Expected drop in entropy after split, Gain( P, C) = Entropy(parent) - Σw*Entropy(children), w is weighted average matrix.<img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/infogain.png>, A is the split attribute<ul><li>Problems: tend to pick attributes with lots of values, could not generalize well on new data.</li><li>use GainRation: for attribute A with many different values V, the SplitEntropy will be large, <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/SplitEntropy.png></li></ul></li></ul><h4 id=overfitting-in-decision-trees>Overfitting in Decision Trees<a hidden class=anchor aria-hidden=true href=#overfitting-in-decision-trees>#</a></h4><p>the tree split too deep to try to classify almost every single sample. As a result the model could not predict new data well.</p><ul><li>Sub-tree replacement pruning<ol><li>For each node:<ul><li>Pretend remove node + all children from the tree</li><li>Measure performance on validation set</li></ul></li><li>Remove node that results in greatest improvement</li><li>Repeat until further pruning is harmful</li></ol></li></ul><h4 id=decision-boundary>Decision boundary<a hidden class=anchor aria-hidden=true href=#decision-boundary>#</a></h4><p>Logistic Regression and trees differ in the way that they generate decision boundaries</p><ul><li>Decision Trees bisect the space into smaller and smaller regions,</li><li>Logistic Regression fits a single line/hyperplane to divide the space exactly into two.</li></ul><h4 id=random-decision-forest>Random Decision forest<a hidden class=anchor aria-hidden=true href=#random-decision-forest>#</a></h4><ul><li>Grow K different decision trees:<ul><li>pick a random subset Sr of training examples</li><li>grow a full ID3 tree (no prunning):<ul><li>When splitting: pick from d&#171;D random attributes</li><li>Computing gain based on Sr instead of full set</li></ul></li><li>repeat for r =1…K</li></ul></li><li>Given a new data point X:<ul><li>classify X using each of the trees T1 …. Tk</li><li>use majority vote: class predicted most often</li></ul></li></ul><h3 id=svm>SVM<a hidden class=anchor aria-hidden=true href=#svm>#</a></h3><ul><li>Intuition: Suppose there is a good hyperplane to seperate data set, h(x)=g(w<sup>T</sup>x+b), (relation with fully connected layer and activation funciton in DNN).<ul><li>Want functional margin of hyperplane to be large: for dataset (xi,yi), functional margin γi = yi(w<sup>T</sup>xi+b), if yi=1, need w<sup>T</sup>xi+b&#187;0, if yi=-1, need w<sup>T</sup>xi+b&#171;0. Thus γi>0 means the classification is correct.</li><li>Geometric margins: Define the hyperplane as w<sup>T</sup>x+b=0, the normal of the hyperplane is w/||w||, thus a point A(xi)&rsquo;s, which represents the input x(i) of some training example with label y(i) = 1, projection on the hyperplane is point B = xi - γi·w/||w||, where γi is xi&rsquo;s distance to the decision boundary. Thus w<sup>T</sup>(xi - γi·w/||w||) + b=0 => γi = (w/||w||)<sup>T</sup>xi+ b/||w||. More generally, the geometric margin of (w, b) with respect to a training example (xi, yi) is γi = yi· (w/||w||)<sup>T</sup>xi+ b/||w||</li><li>If ||w|| = 1, then the functional margin equals the geometric margin</li></ul></li><li>The optimal margin classifier: Given a training set, a natural desideratum is to try to find a decision boundary that maximizes the minimum (geometric) margin, i.e want min(γi) as large as possible. Via some <a href=http://cs229.stanford.edu/notes/cs229-notes3.pdf>transformation</a>, the object turns to minimize ||w||<sup>2</sup>, subject to y(i)·(w<sup>T</sup>xi+b) ≥ 1,</li><li><a href=http://cs229.stanford.edu/notes/cs229-notes3.pdf>Lagrange duality</a>: solving constrained optimization problems. w = Σαiyixi, αi is Lagrange multipliers.</li><li>Support vector: The points with the smallest margins. The number of support vectors can be much smaller than the size the training set</li><li>Training: fit our model’s parameters to a training set, and now wish to make a prediction at a new point input x. We would then calculate w<sup>T</sup>x + b, and predict y = 1 if and only if this quantity is bigger than zero. <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/svm.png>.<ul><li>In order to make a prediction, we have to calculate it which depends only on the inner product between x and the points in the training set.</li><li>Moreover, αi’s will all be zero except for the support vectors. Thus, many of the terms in the sum above will be zero, and we need to find only the inner products between x and the support vectors (of which there is often only a small number) in order to make our prediction.</li><li>The inner product &lt;xi,x> could be replaced by kernel k(xi,x)</li></ul></li></ul><h4 id=kernels>Kernels<a hidden class=anchor aria-hidden=true href=#kernels>#</a></h4><ul><li>Define the “original” input value x as the input attributes of a problem. When that is mapped to some new set of quantities that are then passed to the learning algorithm, we’ll call those new quantities the input features.</li><li>φ denote the feature mapping, which maps from the attributes to the features. E.g. φ(x) = [x, x^2, x^3]</li><li>given a feature mapping φ, we define the corresponding Kernel to be K(x, z) = φ(x)<sup>T</sup>φ(z)</li><li>Often, φ(x) itself may be very expensive to calculate (perhaps because it is an extremely high dimensional vector, require memory), K(x, z) may be very inexpensive to calculate.</li><li>We can get SVMs to learn in the high dimensional feature space given by φ, but without ever having to explicitly find or represent vectors φ(x). E.g.<img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/kernel1.png> <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/kernel2.png></li><li>Based on <a href=https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf>Mercer’s Theorem</a>, you can either explicitly map the data with a φ and take the dot product, or you can take any kernel and use it right away, without knowing nor caring what φ looks like</li><li>Keep in mind however that the idea of kernels has significantly broader applicability than SVMs. Specifically, if you have any learning algorithm that you can write in terms of only inner products &lt;x, z> between input attribute vectors, then by replacing this with K(x, z) where K is a kernel, you can allow your algorithm to work efficiently in the high dimensional feature space corresponding to K.</li></ul><h4 id=svm-vs-logistic-regression>SVM vs. Logistic regression<a hidden class=anchor aria-hidden=true href=#svm-vs-logistic-regression>#</a></h4><ul><li>Logistic regression focuses on maximizing the probability of the data. The further the data lies from the separating hyperplane (on the correct side), the happier LR is.</li><li>An SVM don’t care about getting the right probability, i.e the right P(y=1|x), but only care about P(y=1|x)/P(y=0|x)≥ c. It tries to find the separating hyperplane that maximizes the distance of the closest points to the margin (the support vectors). If a point is not a support vector, it doesn’t really matter.</li><li>P(y=1|x)/P(y=0|x) > c, if c=1, that means P(y=1|x) > P(y=0|x), thus y=1, take log of both side, and plug in P(y=1|x) = sigmoid(w<sup>T</sup>x + b), P(y=0|x)=1-P(y=1|x), recall the <a href=/posts/machine-learning-note-cs229-stanford/#logistic-regression>sigmoid</a>, we get w<sup>T</sup>x + b > 0</li><li>Underlying basic idea of linear prediction is the same, but error functions differ, the r = P(y=1|x)/P(y=0|x) = exp(w<sup>T</sup>x + b), different classifiers assigns different cost to r<ul><li>If cost(r)=log(1 + 1/r), this is logistic regression</li><li>If cost(r)=max(0, 1-log(r))=max(0, 1-(w<sup>T</sup>x + b)), then SVM</li><li>Logistic regression (non-sparse) vs SVM (hinge loss, sparse solution)</li><li>Linear regression (squared error) vs SVM (ϵ insensitive error)</li></ul></li></ul><h3 id=k-nearest-neighbour>K Nearest Neighbour<a hidden class=anchor aria-hidden=true href=#k-nearest-neighbour>#</a></h3><p>Intuition: predict based on nearby/similar training data.</p><ul><li>Algorithm: for a test data<ol><li>compute its distance to every training example xi</li><li>select k closest training instances</li><li>prediction:<ul><li>For Classification: predict as the most frequent label among the k instances.</li><li>For regression: predict as the mean of label among the k instances.</li></ul></li></ol></li><li>Choose k<ul><li>large k: everything classified as the most probable class</li><li>small k: highly variable, unstable decision boundaries</li><li>affects “smoothness” of the boundary</li><li>Use train-validation to choose k</li></ul></li><li>Distance meansures:<ul><li>Euclidian: symmetric, spherical, treats all dimensions equally, but sensitive to extreme differences in single attribtue</li><li>Hamming: number of attribtues that differ</li></ul></li><li>Resolve ties:<ul><li>random</li><li>prior: pick class with greater prior</li><li>nearest: use 1-NN classifier to decide</li></ul></li><li>Missing values: have to fill in the missing values, otherwise cannot compute distance.</li><li>Pro and cons:<ul><li>Almost no assumptions about data</li><li>easy to update in online setting: just add new item to training set</li><li>Need to handle missing data: fill-in or create a special distance</li><li>Sensitive to outliers</li><li>Sensitve to lots of irrelevant aeributes (affect distance)</li><li>Computationally expensive: need to compute distance to all examples O(nd) - <a href=http://cs229.stanford.edu/section/vec_demo/Vectorization_Section.pdf>Vectorization</a> <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/knn.png></li></ul></li><li>Faster knn: K-D Trees, Inverted lists, Locality-sensitive hashing</li></ul><h4 id=k-d-trees>K-D Trees<a hidden class=anchor aria-hidden=true href=#k-d-trees>#</a></h4><p>low-dimensional, real-valued data</p><ul><li>A kd-tree is a binary tree data structure for storing a fi nite set of points from a k-dimensional space.</li><li>Build the tree: Pick random dimension, Find median, Split data</li><li>Nearest neighbor search: Traverse the whole tree, BUT make two modifications to prune to search space:<ul><li>Keep variable of closest point C found so far. Prune subtrees once their bounding boxes say that they can’t contain any point closer than C</li><li>Search the subtrees in order that maximizes the chance for pruning</li></ul></li></ul><h4 id=inverted-lists>Inverted lists<a hidden class=anchor aria-hidden=true href=#inverted-lists>#</a></h4><p>high-dimensional, discrete data, sparse</p><ul><li>Application: text classification, most attribute values are zero (sparseness),</li><li>training: list all training examples that contain particular attribute</li><li>Testing: merge inverted list for attribtues presented in the test set, and choose those instances in the new inverted list as the neighbours</li></ul><h4 id=locality-sensitive-hashing>Locality-sensitive hashing<a hidden class=anchor aria-hidden=true href=#locality-sensitive-hashing>#</a></h4><p>high-d, discrete or real-valued</p><h2 id=unsupervised-learning-无监督学习>Unsupervised learning 无监督学习<a hidden class=anchor aria-hidden=true href=#unsupervised-learning-无监督学习>#</a></h2><h3 id=clustering>Clustering<a hidden class=anchor aria-hidden=true href=#clustering>#</a></h3><h4 id=k-means>K-means<a hidden class=anchor aria-hidden=true href=#k-means>#</a></h4><p>split data into a specified number of populations</p><ul><li>Input: <ul><li>K (number of clusters in the data)</li><li>Training set {x1, x2, x3 &mldr;, xn) </li></ul></li><li>Algorithm:<ul><li>Randomly initialize K cluster centroids as {μ1, μ2, μ3 &mldr; μK}, now centroid could represent cluster.</li><li>Repeat until converge:<ul><li>Inner loop 1: repeatedly sets the c(i) variable to be the index of the closes variable of cluster centroid closes to xi, i.e. take ith example, measure squared distance to each cluster centroid, assign c(i)to the closest cluster(centroid)</li><li>Inner loop 2: For each cluster j, new centroid c(j) = average mean of all the points assigned to the cluster j in previous step.</li></ul></li></ul></li><li>Target (Distortion) function: J(c,μ)=Σ|| xi-μi ||^2, coordinate ascent, decrease monotonically, thus guarantee to converge.</li><li>What if there&rsquo;s a centroid with no data:<ul><li>Remove that centroid, so end up with K-1 classes,</li><li>Or, randomly reinitialize it, not sure when though&mldr;</li></ul></li><li>How to choose cluster numbers: scree plot to find the best k.</li></ul><h4 id=hierarchical-k-means>Hierarchical K-means<a hidden class=anchor aria-hidden=true href=#hierarchical-k-means>#</a></h4><ul><li>A Top-down approach<ol><li>run k-means algorithm on the original dataset</li><li>for each of the resulting clusters, recursively run k-means</li></ol></li><li>Pro cons:<ul><li>Fast</li><li>nearby points may end up in different clusters</li></ul></li></ul><h4 id=agglomerative-clustering>Agglomerative Clustering<a hidden class=anchor aria-hidden=true href=#agglomerative-clustering>#</a></h4><p>A bottom up algorithm:</p><pre tabindex=0><code>1. starts with a collections of singleton clusters
2. repeat until only one cluster is left:
    1. Find a pair of clusters that is closest
    2. Merge the pair of clusters into one new cluster
    3. Remove the old pair of clusters
</code></pre><ul><li>Need to define a distance metric over clusters
<img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/cluster_dist.png></li><li>Produce a dendrogram: Hierarchical tree of clusters
<img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Dendrogram.png></li><li>slow</li></ul><h4 id=gaussian-mixtures>Gaussian Mixtures<a hidden class=anchor aria-hidden=true href=#gaussian-mixtures>#</a></h4><p>For non-Gaussian distribution data, assume it is a mixture of several(k) Gaussians.</p><ul><li>Algorithm: EM</li></ul><h4 id=em-algorithm>EM algorithm<a hidden class=anchor aria-hidden=true href=#em-algorithm>#</a></h4><p>strategy will be to repeatedly construct a lower-bound on ℓ(E-step) based on <a href=http://cs229.stanford.edu/notes/cs229-notes8.pdf>Jensen’s inequality</a>, and then optimize that lower-bound(M-step).</p><ul><li>E step: For each i, let Qi be some distribution over the z’s (Σ<sub>z</sub>Qi(z) = 1, Qi(z) ≥ 0). z(i) indicating which of the k Gaussians each x(i) had come from, get P(Z)=φ, then compute the conditional probability wj as P(x|Z) via <a href=/posts/machine-learning-note-cs229-stanford/#gaussian-naive-bayes>Gaussian Naive Bayes</a>: <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Estep.png></li><li>M step: maximize, with respect to our parameters φ, µ, Σ, the quantity<img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Mstep1.png>,
by updating parameter(φ, µ, σ) <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Mstep2.png></li><li>举例：start with two randomly placed Gaussians (μa, σa), (μb, σb), assume a uniform prior (P(a)=P(b)=0.5), iterate until convergence:<ul><li>E-step: for each point: P(b|xi), P(a|xi)=1-P(b|xi) <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_e.png>, does it look like it came from b or a?</li><li>M-step: adjust (μa, σa) and (μb, σb) to fit points <strong>soft</strong> assigned to them, <img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_m1.png>
<img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_m2.png></li></ul></li><li>The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the “hard” cluster assignments c, we instead have the “soft” assignments w. Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea.</li><li>How to pick k: cannot discover K, likelihood keeps growing with K</li></ul><h4 id=k-means-vs-em>K-means vs. EM<a hidden class=anchor aria-hidden=true href=#k-means-vs-em>#</a></h4><p><img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/km&em.jpg></p><h3 id=dimensionality-reduction>Dimensionality Reduction<a hidden class=anchor aria-hidden=true href=#dimensionality-reduction>#</a></h3><ul><li>Pros:<ul><li>reflects human intuitions about the data</li><li>allows estimating probabilities in highadimensional data: no need to assume independence etc.</li><li>dramatic reduction in size of data: faster processing (as long as reduction is fast), smaller storage</li></ul></li><li>Cons<ul><li>too expensive for many applications (Twitter, web)</li><li>disastrous for tasks with fine-grained classes</li><li>understand assumptions behind the methods (linearity etc.): there may be better ways to deal with sparseness</li></ul></li></ul><h4 id=factor-analysis>Factor analysis<a hidden class=anchor aria-hidden=true href=#factor-analysis>#</a></h4><p>If the features n ≫ m, or n≈m, in such a problem, it might be difficult to model the data even with a single Gaussian, 更别提高斯混合了. Because the variance matrix Σ becomes singular - <a href=http://cs229.stanford.edu/notes/cs229-notes9.pdf>non invertable</a>.</p><h4 id=principal-components-analysis>Principal Components Analysis<a hidden class=anchor aria-hidden=true href=#principal-components-analysis>#</a></h4><p>PCA, automatically detect and reduce data to lower dimension k, k &#171; n, preserve dimenson that affects class separability most.</p><ul><li>Algorithm:<ul><li>Pre-process: data normalization to 0 mean and unit variance<img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/pca_norm.png>,
Steps (3-4) may be omitted if we had apriori knowledge that the different attributes are all on the same scale</li><li>to project data into a k-dimensional subspace (k &lt; n), we should choose e1,&mldr; ek to be the top k eigenvectors of Σ. The e’s now form a new, orthogonal basis for the data.</li><li>To represent a training data point x with d dimension into this basis (k dimension), e1<sup>T</sup>x,&mldr;ek<sup>T</sup>x</li></ul></li><li>The vectors u1,&mldr;, uk are called the first k principal components of the data.</li><li>Eigenvalue λi = variance along ei.<ul><li>Pick ei that explain the most variance by sorting eigenvectors s.t. λ1 ≥ λ2 ≥…≥ λn</li><li>pick first k eigenvectors which explain 90% or 95% of the total variance Σλ(i).</li></ul></li><li>Maximize the variance of projection of x onto a unit vector u,</li><li>Application: eigenfaces</li></ul><h4 id=linear-discriminant-analysis>Linear Discriminant Analysis<a hidden class=anchor aria-hidden=true href=#linear-discriminant-analysis>#</a></h4><p>LDA</p><ul><li>Idea: pick a new dimension that gives<ul><li>maximum separation between means of projected classes</li><li>minimum variance within each projected class</li></ul></li><li>How: eigenvectors based on between-class and within-class covariance matrices</li><li>LDA not guaranteed to be better for Classification<ul><li>assumes classes are unimodal Gaussians</li><li>fails when discriminatory information is not in the mean, but in the variance of the data</li></ul></li></ul><h4 id=singular-value-decomposition>Singular Value Decomposition<a hidden class=anchor aria-hidden=true href=#singular-value-decomposition>#</a></h4><h2 id=generalization-and-evaluation>Generalization and evaluation<a hidden class=anchor aria-hidden=true href=#generalization-and-evaluation>#</a></h2><h3 id=receiver-operating-characteristic>Receiver Operating Characteristic<a hidden class=anchor aria-hidden=true href=#receiver-operating-characteristic>#</a></h3><p>ROC, plot TPR(Sensitivity) vs. FPR(Specificity) as t varies from ∞ to -∞, shows performance of system across all possible thresholds
<img loading=lazy src=https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/roc.png></p><ul><li>A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner. Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test</li><li>AUC: area under ROC curve, popular alternative to Accuracy</li></ul><h3 id=confidence-interval>Confidence interval<a hidden class=anchor aria-hidden=true href=#confidence-interval>#</a></h3><p>tell us how closed our estimation</p><ul><li>E = probability that misclassify a random instance: Take a random set of n instances, how many misclassified? Equal to Binomial distribution with mean = nE, variance = nE(1-E)</li><li>Efuture: the next instance&rsquo;s probability of misclassified = average #misclassifed = variance / n = mean E= E(1-E)/n, small variance means big confidence interval, a Gaussian distribution with one variance distance extend from mean will cover 2/3 future test sets</li><li>p% Confidence interval for future error, 95% confidence interval needs about 2 variance extends from mean.</li></ul><p>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/topic-modelling-%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1%E4%BB%A5%E5%8F%8A%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B/><span class=title>« Prev</span><br><span>Topic Modelling - 主题建模以及隐变量模型</span>
</a><a class=next href=https://congchan.github.io/posts/machine-learning-with-scikit-learn-sklearn-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/><span class=title>Next »</span><br><span>Machine Learning with Scikit-learn (Sklearn) 机器学习实践</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Note - cs229 - Stanford on x" href="https://x.com/intent/tweet/?text=Machine%20Learning%20Note%20-%20cs229%20-%20Stanford&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fmachine-learning-note-cs229-stanford%2f&amp;hashtags=MachineLearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Note - cs229 - Stanford on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fmachine-learning-note-cs229-stanford%2f&amp;title=Machine%20Learning%20Note%20-%20cs229%20-%20Stanford&amp;summary=Machine%20Learning%20Note%20-%20cs229%20-%20Stanford&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fmachine-learning-note-cs229-stanford%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Note - cs229 - Stanford on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fmachine-learning-note-cs229-stanford%2f&title=Machine%20Learning%20Note%20-%20cs229%20-%20Stanford"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Note - cs229 - Stanford on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fmachine-learning-note-cs229-stanford%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Note - cs229 - Stanford on whatsapp" href="https://api.whatsapp.com/send?text=Machine%20Learning%20Note%20-%20cs229%20-%20Stanford%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fmachine-learning-note-cs229-stanford%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Note - cs229 - Stanford on telegram" href="https://telegram.me/share/url?text=Machine%20Learning%20Note%20-%20cs229%20-%20Stanford&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fmachine-learning-note-cs229-stanford%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Note - cs229 - Stanford on ycombinator" href="https://news.ycombinator.com/submitlink?t=Machine%20Learning%20Note%20-%20cs229%20-%20Stanford&u=https%3a%2f%2fcongchan.github.io%2fposts%2fmachine-learning-note-cs229-stanford%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>