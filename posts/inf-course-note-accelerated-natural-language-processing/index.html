<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Inf Course Note - Accelerated Natural Language Processing | Cong's Log</title><meta name=keywords content="NLP,Inf Course Note"><meta name=description content="爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh
References:
Accelerated natural language processing
ANLP revision guide
Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning

概率模型 Probability Model
概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率
估算概率（Probability Estimation）一般使用最大似然估计（MLE，相关频率）：
$$p(x_i) = \frac{Count(x_i)}{\sum_{i=0}^nCount(x_i)}$$平滑Smoothing
一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。
Language modeling
To compute the probability of sentence /sequence of words $P(w_1, w_2, w_3...)$, or to predict upcomming words $P(w|w_1, w_2, w_3...)$&mldr; a language model is also a probability model."><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/inf-course-note-accelerated-natural-language-processing/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/inf-course-note-accelerated-natural-language-processing/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/inf-course-note-accelerated-natural-language-processing/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Inf Course Note - Accelerated Natural Language Processing"><meta property="og:description" content="爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh
References: Accelerated natural language processing ANLP revision guide Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning
概率模型 Probability Model 概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率
估算概率（Probability Estimation）一般使用最大似然估计（MLE，相关频率）：
$$p(x_i) = \frac{Count(x_i)}{\sum_{i=0}^nCount(x_i)}$$平滑Smoothing 一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。
Language modeling To compute the probability of sentence /sequence of words $P(w_1, w_2, w_3...)$, or to predict upcomming words $P(w|w_1, w_2, w_3...)$… a language model is also a probability model."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-06-30T00:00:00+00:00"><meta property="article:modified_time" content="2018-06-30T00:00:00+00:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Inf Course Note"><meta name=twitter:card content="summary"><meta name=twitter:title content="Inf Course Note - Accelerated Natural Language Processing"><meta name=twitter:description content="爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh
References:
Accelerated natural language processing
ANLP revision guide
Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning

概率模型 Probability Model
概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率
估算概率（Probability Estimation）一般使用最大似然估计（MLE，相关频率）：
$$p(x_i) = \frac{Count(x_i)}{\sum_{i=0}^nCount(x_i)}$$平滑Smoothing
一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。
Language modeling
To compute the probability of sentence /sequence of words $P(w_1, w_2, w_3...)$, or to predict upcomming words $P(w|w_1, w_2, w_3...)$&mldr; a language model is also a probability model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Inf Course Note - Accelerated Natural Language Processing","item":"https://congchan.github.io/posts/inf-course-note-accelerated-natural-language-processing/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Inf Course Note - Accelerated Natural Language Processing","name":"Inf Course Note - Accelerated Natural Language Processing","description":"爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh\nReferences: Accelerated natural language processing ANLP revision guide Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning\n概率模型 Probability Model 概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率\n估算概率（Probability Estimation）一般使用最大似然估计（MLE，相关频率）：\n$$p(x_i) = \\frac{Count(x_i)}{\\sum_{i=0}^nCount(x_i)}$$平滑Smoothing 一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。\nLanguage modeling To compute the probability of sentence /sequence of words $P(w_1, w_2, w_3...)$, or to predict upcomming words $P(w|w_1, w_2, w_3...)$\u0026hellip; a language model is also a probability model.\n","keywords":["NLP","Inf Course Note"],"articleBody":"爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh\nReferences: Accelerated natural language processing ANLP revision guide Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning\n概率模型 Probability Model 概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率\n估算概率（Probability Estimation）一般使用最大似然估计（MLE，相关频率）：\n$$p(x_i) = \\frac{Count(x_i)}{\\sum_{i=0}^nCount(x_i)}$$平滑Smoothing 一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。\nLanguage modeling To compute the probability of sentence /sequence of words $P(w_1, w_2, w_3...)$, or to predict upcomming words $P(w|w_1, w_2, w_3...)$… a language model is also a probability model.\nProbability computation makes use of chain rule of probability, the products of a sequence of conditional probability.\n$$P(w_{1:n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})P(w_4|w_{1:3})...P(w_n|w_{1:n-1})$$But the last term based on the entire sentence is very difficult to compute. So it is simplified by Markov Assumption: approximate the conditional probability by only accounting several prefixes, a one-order Markov assumption simplifies as P(the| water is so transparent that) ≈ P(the| that) $$\\begin{align} P(w_{1:n}) \u0026= \\prod_{i=1}^n P(w_i | w_1, ..., w_{i-1}) \\\\\\\\ \u0026\\propto \\prod_{i=1}^n P(w_i | w_{i-k}, ..., w_{i-1}) \\end{align}$$ Evaluation: Perplexity\nPerplexity Intuition based on Shannon game: The best language model is one that best predicts an unseen test set(e.g. next word), gives the highest $P(sentence)$ to the word that actually occurs.\nDefinition: Perplexity is the inverse probability of the test set, normalized by the number of words(lie between 0-1). Normalize the log probability of all the test sentences: $$\\frac{1}{M} \\log_2 \\prod_{i=1}^m p(x^{(i)}) = \\frac{1}{M} \\sum_{i=1}^m \\log_2 p(x^{(i)})$$ Then transform to perplexity: $$Perplexity = 2^{-\\frac{1}{M} \\sum_{i=1}^m \\log_2 p(x^{(i)})}$$ So minimizing perplexity is the same as maximizing probability\nBad approximation: unless the test data looks just like the training data, so generally only useful in pilot experiments.\nN-Gram Language Model N-Gram语言模型是基于N-1阶马尔可夫假设且由MLE估算出的LM。N-GramLM 预测下一个单词出现概率仅条件于前面的(N-1)个单词, 以The students opened their books为例:\nBi-gram: 统计$P(w_{i}=m|w_{i-1})$, P(students | the), P(opened | students), …, 属于马尔可夫一阶模型, 即当前t时间步的状态仅跟t-1相关. Tri-gram: P(students | The), P(opened | The students), 马尔可夫二阶模型 Four-gram: 依此类推 特殊的Uni-gram: 统计$P(w_i)$, P(the), P(students), …, 此时整个模型退化为词袋模型, 不再属于马尔可夫模型, 而是基于贝叶斯假设, 即各个单词是条件独立的. 所以一般N-gram是指N\u003e1的.\nHow to estimate theparameter? Maximum likelyhood estimate：\n$$P(w_{i}=m|w_{i-n:i-1}) = \\frac{Count(w_{i-n:i})}{Count(w_{i-n:i-1})}$$In practice, use log space to avoid underflow, and adding is faster than multiplying.\nInsufficient: To catch long-distance dependencies, the n has to be very large, that asks for very large memory requirement N-grams only work well for word prediction if the test corpus looks like the training corpus. Sparsity: Zero count of gram, means zero probability? No. To deal with 0 probability, commonly use Kneser-Ney smoothing, for very large N-grams like web, use stupid backoff. Add Alpha Smoothing Assign equal probability to all unseen events. Applied in text classification, or domains where zeros probability is not common. Backoff Smoothing Use information from lower order N-grams (shorter histories) Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram. Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. Interpolation Smoothing Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts Simple interpolation: $P(w_3|w_1, w_2) = \\lambda_1 P(w_3|w_1, w_2) + \\lambda_2 P(w_3|w_2) + \\lambda_3 P(w_3), \\sum \\lambda = 1$. λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data Kneser-Ney Smoothing Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams. Use a better estimate for probabilities of lower-order unigrams, the continuation probability, $P_{continuatin}(w)$ is how likely is w to appear as a novel continutaion. For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w. Every bigram type was a novel continuation the first time it was seen. normalized by the total number of word bigram types. To lower the probability of some fix bigram like “San Franscio” For general N-gram, Naive Bayes Classifier Application: Text classification, to classify a text, we calculate each class probability given the test sequence, and choose the biggest one. Evaluation: precision, recall, F-measure Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了 Text Classification Or text categorization, method is not limited to NB, see lab7. Spam email, gender/authorship/language identification, sentiments analysis,(opinion extraction, subjectivity analysis)…\nSentiments Analysis For sentiment(or other text classification), word occurrence may matter more than word frequency. Thus it often improves performance to clip the word counts in each document at 1. This variant binary NB is called binary multinominal naive Bayes or binary NB. Remove duplicates in each data sample - bag of words representation, boolean features. Binarized seems to work better than full word counts. Deal with negation: like, not like, A very simple baseline that is commonly used in sentiment to deal with negation is during text normalization to prepend the prefix NOT_ to every word after a token of logical negation Sentiment lexicons: lists of words that are preannotated with positive or negative sentiment. To deal with insufficient labeled training data. A common way to use lexicons in the classifier is to use as one feature the total count of occurrences of any words in the positive lexicon, and as a second feature the total count of occurrences of words in the negative lexicon. Using just two features results in classifiers that are much less sparse to small amounts of training data, and may generalize better. See lab8. Naive Bayes Assumptions Bags of words: a set of unordered words/features with its frequency in the documents, their order was ignored. Conditional independence: the probabilities $P(w|C)$ are independence given the class, thus a sequence of words(w1,w2,w3…) probability coculd be estimate via prducts of each $P(w_i|C)$ by walking through every pisition of the sequence, noted that the orders in the sequence does not matter. Naive Bayes Training Each classes’ prior probability P(C) is the percentage of the classes in the training set. For the test set, its probability as a class j, is the products of its sequence probability $P(w_1, w_2, w_3...|C_j)$ and $P(C_j)$, normalized by the sequence probability $P(w_1, w_2, w_3...)$, which could be calculated by summing all $P(w_1, w_2, w_3...|C_j)\\*P(C_j)$. The joint features probability $P(w_1, w_2, w_3...|C)$ of each class is calculated by naively multiplying each word’s MLE given that class. In practice, to deal with 0 probability, we dun use MLE, instead we use add alpha smoothing. Why 0 probability matters? Because it makes the whole sequence probability $P(w_1, w_2, w_3...|C)$ 0, then all the other features as evidence for the class are eliminated too. How: first extract all the vocabulary V in the training set. Then, for each feature/word k, its add alpha smoothing probability estimation within a class j is $(Njk + \\alpha)/(N_j+V\\*\\alpha)$. This is not the actual probability, but just the numerator. Naive Bayes Relationship to Language Modelling When using all of the words as features for naive bayes, then each class in naive bayes is a unigram languange model. For each word, assign probability $P(word|C)$, For each sentence, assign probability $P(S|C) = P(w_1, w_2, w_3...|C)$ Running multiple languange models(classes) to assign probabilities, and pick out the highest language model. Hidden Markov Model The HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), they compute a probability distribution over possible sequences of labels and choose the best label sequence.\nHMM参数$λ= (Y, X, π, A, B)$ :\nY是隐状态（输出变量）的集合 X是观察值（输入）集合 Initial probability π Transition probability matrix A, $P(Tag_{i+1} | Tag_{i})$ Emission probability B, $P(Word | Tag)$ Application: part-of-speech tagging, name entity recognition(NEr), parse tree, speech recognition\nHidden?: these tags, trees or words is not observed(hidden). b比如在POS任务中, X就是观察到的句子, Y就是待推导的标注序列, 因为词性待求的, 所以人们称之为隐含状态.\nThe three fundamental problems of HMM:\ndecoding: discover the best hidden state sequence via Viterbi algorithm. Probability of the observation: Given an HMM with know parameters λ and an observation sequence O, determine the likelihood $P(O| \\lambda)$ (a language model regardless of tags) via Forward algorithm Learning (training): Given only the observed sequence, learn the best(MLE) HMM parameters λ via forward-backward algorithm, thus training a HMM is an unsupervised learning task. 算法:\n前向算法和后向算法解决如何计算似然$P(O| \\lambda)$的问题 Viterbi算法解决HMM 解码问题. 这些算法都是动态规划算法 HMM的缺陷是其基于观察序列中的每个元素都相互条件独立的假设。即在任何时刻观察值仅仅与状态（即要标注的标签）有关。对于简单的数据集，这个假设倒是合理。但大多数现实世界中的真实观察序列是由多个相互作用的特征和观察序列中较长范围内的元素之间的依赖而形成的。而条件随机场(conditional random fiel, CRF)恰恰就弥补了这个缺陷.\n同时, 由于生成模型定义的是联合概率，必须列举所有观察序列的可能值，这对多数领域来说是比较困难的。\nPart-of-speech Tagging Part-of-speech(POS), word classes, or syntactic categories, a description of eight parts-of-speech: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner. noun 名詞 (代號 n. ) pronoun 代名詞 (代號 pron. ) verb 動詞 (代號 v. ) adjective 形容詞 (代號 adj. ) adverb 副詞 (代號 adv. ) preposition 介系詞 (代號 prep. ) conjunction 連接詞 (代號 conj. ) interjection 感歎詞 (代號 int. ) Motivation: Use model to find the best tag sequence T for an untagged sentence S: argmax $P(T|S)$ -\u003e argmax $P(S|T)\\*P(T)$, where P(T) is the transition (prior) probabilities, $P(S|T)$ is the emission (likelihood) probabilities. Parts-of-speech can be divided into two broad supercategories: closed class types and open class types Search for the best tag sequence: Viterbi algorithm evaluation: tag accuracy 使用HMM处理POS代码\nTransition Probability Matrix Tags or states Each (i,j) represent the probability of moving from state i to j When estimated from sequences, should include beginning and end markers. Tag transition probability matrix: the probability of tag i followed by j Emission Probability Also called observation likelihoods, each expressing the probability of an observation j being generated from a states i. Word/symbol Penn Treebank Forward Algorithm Compute the likelihood of a particular observation sequence. Implementation is almost the same as Viterbi. Yet Viterbi takes the max over the previous path probabilities whereas the forward algorithm takes the sum. Viterbi Algorithm Decoding task: the task of determining which sequence of variables is the underlying source of some sequence of observations.\nViterbi的实现参考HMM POS Tagging\nIntuition: The probability of words $w_1$ followed by $w_2$ with tag/state i and j (i,j is index of all Tags), is the chain rule of the probability of i followed by j and the probability of i output $w_i$ $P(w_1 | i)$ and $P(w_2 |j)$, then choose the maximum from all the possible i j. Then using chain rule to multiply the whole sequence of words.\nThe value of each cell $Vt(j)$ is computed by recursively taking the most probable path that could lead us to this cell from left columns to right. See exampls in tutorial 2 Since HMM based on Markov Assumptions, so the present column $V_t$ is only related with the nearby left column $V_{t-1}$. HMM Training 给定观察序列$X = x_1, x_2, ..., x_t$ ，训练调整模型参数λ, 使$p(X | \\lambda)$最大: Baum-Welch算法 (Forward-backward algorithm)\ninputs: just the observed sequence output: the converged λ(A,B). For each interation k until λ converged: Compute expected counts using λ(k-1) Set λ(k) using MLE on the expected counts. 经常会得到局部最优解.\nContext-free Grammar CFG(phrase-structure grammar) consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered toLexicon gether, and a lexicon of words and symbols.\nConstituency Phrase structure, organizes words into nested constituents. Groups of words behaving as a single units, or constituents.\nNoun phrase(NP), a sequence of words surrounding at least one noun. While the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase Preposed or Postposed constructions. While the entire phrase can be placed differently, the individual words making up the phrase cannot be. Fallback: In languages with free word order, phrase structure (constituency) grammars don’t make as much sense. Headed phrase structure: many phrase has head, VP-\u003eVB, NP-\u003eNN, the other symbols excepct the head is modifyer. Probabilistic Context-free Grammar PCFG(Stochastic Context-Free Grammar SCFG (SCFG)), a probabilistic augmentation of context-free grammars in which each rule is associated with a probability.\nG = (T,N,S,R,P) T, N: Terminal and Non-terminal S: starts symbol R: Derive rule/grammar, N -\u003e N/C P: a probability function, for a given N, ΣP(N-\u003eNi/Ci)=1. Normally P(S-\u003eNP VP)=1, because this is the only rule for S. PCFG could generates a sentence/tree, thus it is a language model, assigns a probability to the string of words constituting a sentence The probability of a tree t is the product of the probabilities of the rules used to generate it. The probability of the string s is the sum of the probabilities of the trees/parses which have that string as their yield. The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees for the sentence. Application: Probabilistic parsing Shortage: lack the lexicalization of a trigram model, i.e only a small fraction of the rules contains information about words. To solve this problem, use lexicalized PCFGs Lexicalization of PCFGs The head word of phrase gives a good representation of the phrase’s structure and meaning Puts the properties of words back into a PCFG Word to word affinities are useful for certain ambiguities, because we know the probability of rule with words and words now, e.g. PP attachment ambiguity Recursive Descent Parsing It is a top-down, depth-first parser:\nBlindly expand nonterminals until reaching a terminal (word). If multiple options available, choose one but store current state as a backtrack point (in a stack to ensure depth-first.) If terminal matches next input word, continue; else, backtrack Can be massively inefficient (exponential in sentence length) if faced with local ambiguity Can fall into infinite loop CKY Parsing Well-formed substring table: For parsing, subproblems are analyses of substrings, memoized in well-formed substring table(WFST, chart).\nChart entries are indexed by start and end positions in the sentence, and correspond to: either a complete constituent (sub-tree) spanning those positions (if working bottom-up), or a prediction about what complete constituent might be found (if working top-down). The chart is a matrix where cell [i, j] holds information about the word span from position i to position j: The root node of any constituent(s) spanning those words Pointers to its sub-constituents (Depending on parsing method,) predictions about what constituents might follow the substring. Probability CKY parsing: Dependency Parsing Motivation: context-free parsing algorithms base their decisions on adjacency; in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective); we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present). Approach: Transition-based dependency parsing Dependency Syntax Dependency structure shows which words depend on (modify or are arguments of) which other words.\nA fully lexicalized formalism without phrasal constituents and phrase-structure rules: binary, asymmetric grammatical relations between words. More specific, head-dependent relations, with edges point from heads to their dependents. Motivation: In languages with free word order, phrase structure (constituency) grammars don’t make as much sense. E.g. we may need both S → NP VP and S → VP NP, but could not tell too much information simply looking at the rule. Dependencies: Identifies syntactic relations directly. The syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words. Relation between phrase structure and dependency structure Convert phrase structure annotations to dependencies via head rules. (Convenient if we already have a phrase structure treebank.): For a given lexicalized constituency parse(CFG tree), remove the phrasal categories, remove the (duplicated) terminals, and collapse chains of duplicates. The closure of dependencies give constituency from a dependency tree Transition-based Dependency Parsing transition-based systems use supervised machine learning methods to train classifiers that play the role of the oracle. Given appropriate training data, these methods learn a function that maps from configurations to transition operators(actions).\nBottom up Like shift-reduce parsing, but the ‘reduce’ actions are specialized to create dependencies with head on left or right. configuration：consists of a stack, an input buffer of words or tokens, and a set of relations/arcs, a set of actions. How to choose the next action: each action is predicted by a discriminative classifier(often SVM, could be maxent) over each legal move. features: a sequence of the correct (configuration, action) pairs f(c ; x). Evaluation: accuracy (# correct dependencies with or ignore label)). Dependency Tree Dependencies from a CFG tree using heads, must be projective: There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words. But dependency theory normally does allow non-projective structures to account for displaced constituents. Bounded and Unbounded Dependencies Unbounded dependency could be considered as long distance dependency\nLong-distance dependencies: contained in wh-non-subject-question, “What flights do you have from Burbank to Tacoma Washington?”, the Wh-NP what flights is far away from the predicate that it is semantically related to, the main verb have in the VP. Noisy Channel Model: The intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel. a probability model using Bayesian inference, input -\u003e noisy/errorful encoding -\u003e output, see an observation x (a misspelled word) and our job is to find the word w that generated this misspelled word. $P(w|x) = P(x|w)\\*P(w)/P(x)$ Noisy channel model of spelling using naive bayes\nThe noisy channel model is to maximize the product of likelihood(probability estimation) P(s|w) and the prior probability of correct words P(w). Intuitively it is modleing the noisy channel that turn a correct word ‘w’ to the misspelling. The likelihood(probability estimation) P(s|w) is called the the channel/error model, telling if it was the word ‘w’, how likely it was to generate this exact error. The P(w) is called the language model Generative vs. Discriminative Models Generative(joint) models palce probabilities $p(c, d)$ over both observed data d and the hidden variables c (generate the obersved data from hidden stuff).\nDiscriminative(conditional) models take the data as given, and put a probability over hidden structure given the data, $p(c | d)$.\n在朴素贝叶斯与Logistic Regression, 以及HMM和CRF之间, 有生成式和判别式的区别. 生成式模型描述标签向量y如何有概率地生成特征向量x, 即尝试构建x和y的联合分布$p(y, x)$, 典型的模型有N-Gram语言模型, 朴素贝叶斯模型（Naive Bayes）， 隐马尔科夫模型（HMM）, MRF。\n而判别模型直接描述如何根据特征向量x判断其标签y, 即尝试构建$p(y | x)$的条件概率分布, 典型模型如如LR, SVM，CRF，MEMM等.\nExponential Models It is a family, includes Log-linear, MaxEnt, Logistic Regression models.\nMake probability model from the linear combination of weights λ and features f as votes, normalized by the total votes. It is a probabilistic distribution: it estimates a probability for each class/label, aka Softmax. It is a classifier, deciding how to weight features, given data. choose the highest probability label. Application: dependency parsing actions prediction, text classification, Word sense disambiguation Training Discriminative Model Features in NLP are more general, they specify indicator function(a yes/no[0,1] boolean matching function) of properties of the input and each class. Weights: low possibility features will associate with low/negative weight, vise versa. Define features: Pick sets of data points d which are distinctive enough to deserve model parameters: related words, words contians #, words end with ing, etc. Regularization in Discriminative Model The issue of scale:\nLots of features sparsity: easily overfitting: need smoothing Many features seen in training never occur again in test Optimization problem: feature weights can be infinite, and iterative solvers can take a long time to get to those infinities. See tutorial 4. Solution: Early stopping Smooth the parameter via L2 regularization. Smooth the data, like the add alpha smoothing, but hard to know what artificial data to create Morphology 构词学（英语言学分科学名：morphology，“组织与形态”)，又称形态学，是语言学的一个分支，研究单词（word）的内部结构和其形成方式。如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher。构词学正是研究这种单字间组成的关系，并试着整理出其组成的规则。\nMorphemes: The way words are built up from smaller meaning-bearing units.\nLemma:\nLexeme, refers to the set of all the forms that have the same meaning, lemma: refers to the particular form that is chosen by convention to represent the lexeme. E.g: run, runs, ran, running are forms of the same lexeme, with run as the lemma. Affixes: Adding additional meanings of various kinds. “+ed, un+”\nsuffix : follow the stem Plural of nouns ‘cat+s’ Comparative and superlative of adjectives ‘small+er’ Formation of adverbs ‘great+ly’ Verb tenses ‘walk+ed’ All inflectional morphology in English uses suffixes Prefix: precede the stem In English: these typically change the meaning Adjectives ‘un+friendly’, ‘dis+interested’ Verbs ’re+consider’ Some language use prefixing much more widely Infix: inserted inside the stem Circumfix: do both(follow, precede) Root, stem and base are all terms used in the literature to designate that part of a word that remains when all affixes have been removed.\nThe root word is the primary lexical unit of a word, and of a word family (this root is then called the base word), which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents. E.g: In the form ‘untouchables’ the root is ‘touch’, to which first the suffix ‘-able’, then the prefix ‘un-‘ and finally the suffix ‘-s’ have been added. In a compound word like ‘wheelchair’ there are two roots, ‘wheel’ and ‘chair’. Stem is of concern only when dealing with inflectional morphology\nStemming: reduce terms to their stems in info retrieval, E.g: In the form ‘untouchables’ the stem is ‘untouchable’, ‘touched’ -\u003e ‘touch’; ‘wheelchairs’ -\u003e ‘wheelchair’. Morphological Parsing Use Finite-state transducers, FST, a transducer maps between one representation and another; It is a kind of FSA which maps between two sets of symbols.\nInflectional vs. Derivational Morphology Inflectional · nouns for count (plural: +s) and for possessive case (+’s) · verbs for tense (+ed, +ing) and a special 3rd person singular present form (+s) · adjectives in comparative (+er) and superlative (+est) forms.\nDerivational · Changing the part of speech, e.g. noun to verb: ‘word → wordify’ · Changing the verb back to a noun · Nominalization: formation of new nouns, often verbs or adjectives\nInflectional Derivational does not change basic meaning or part of speech may change the part of speech or meaning of a word expresses grammatical features or relations between words not driven by syntactic relations outside the word applies to all words of the same part of speech, inflection occurs at word edges: govern+ment+s, centr+al+ize+d applies closer to the stem Challenge of Rich Morphology For a morphologically rich language, many issues would arise because of the morphological complexity.\nThese productive word-formation processes result in a large vocabulary for these languages Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages For POS, augmentations become necessary when dealing with highly inflected or agglutinative languages with rich morphology like Czech, Hungarian and Turkish., part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a single primitive tag. Dependency grammar is better than constituency in dealing with languages that are morphologically rich。 Linguistic and Representational Concepts Parsing Parsing is a combination of recognizing an input string and assigning a correct linguistic structure/tree to it based on a grammar. The Syntactic, Statistical parsing are constituent-based representations(context-free grammars). The Dependency Parsing are based on dependency structure(dependency grammars). Syntactic Parsing Syntactic parsing, is the task of recognizing a sentence and assigning a correct syntactic structure to it.\nSyntactic parsing can be viewed as a search search space: all possible trees generated by the grammar search guided by the structure of the space and the input. search direction top-down: start with root category (S), choose expansions, build down to words. bottom-up: build subtrees over words, build up to S. Search algorithm/strategy: DFS, BFS, Recursive descent parsing, CKY Parsing Challenge: Structual Ambiguity Statistical Parsing Or probabilistic parsing, Build probabilistic models of syntactic knowledge and use some of this probabilistic knowledge to build efficient probabilistic parsers.\nmotivation: to solve the problem of disambiguation algorithm: probability CKY parsing evaluation: Compare the output constituency parser with golden standard tree, a constituent(part of the output parser) marked as correct if it spans the same sentence positions with the corresponding constituent in golder standard tree. Then we get the precision, recall and F1 measure. constituency: S-(0:10), NP-(0:2), VP-(0:9)… Precission = (# correct constituents)/(# in parser output), recall = (# correct constituents)/(# in gold standard) Not a good evaluation, because it higher order constituent is marked wrong simply it contains a lower level wrong constituent. Dependency Parsing Constituency Dependency Morphology Ambiguity Structural ambiguity: Occurs when the grammar can assign more than one parse to a sentence. Attachment ambiguity: A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place. Coordination ambiguity: different sets of phrases can be conjoined by a conjunction like and. E.g green egg and bread. Coordination: The major phrase types discussed here can be conjoined with conjunctions like and, or, and but to form larger constructions of the same type. Global and local ambiguity global ambiguity: multiple analyses for a full sentence, like I saw the man with the telescope local ambiguity: multiple analyses for parts of sentence. the dog bit the child: first three words could be NP (but aren’t). Building useless partial structures wastes time. Open-class Closed-class Closed classes are those with relatively fixed membership\nprepositions: on, under, over, near, by, at, from, to, with determiners: a, an, the pronouns: she, who, I, others conjunctions: and, but, or, as, if, when auxiliary verbs: can, may, should, are particles: up, down, on, off, in, out, at, by numerals: one, two, three, first, second, third Open-class\nNouns, verbs, adjectives, adverbs Word Sense A discrete representation of an aspect of a word’s meaning. How: Distributional semantic models\nCollocation: A sequence of words or terms that co-occur more often than would be expected by chance.\nSynonym: 代名词, When two senses of two different words (lemmas) are identical, or nearly identical, the two senses are synonyms. E.g. couch/sofa vomit/throw up filbert/hazelnut car/automobile\nSimilarity: Or distance, a looser metric than synonymy. Two ways to measure similarity:\nThesaurus词库-based: are words nearby in hypernym hierarchy? Do words have similar definitions? Distributional: do words have similar distributional contexts Hyponym: 下义词, One sense is a hyponym of another sense if the first sense is more specific, denoting a subclass of the other. E.g. car is a hyponym of vehicle; dog is a hyponym of animal, and mango is a hyponym of fruit.\nHypernym: Superordinate, 上位词, vehicle is a hypernym of car, and animal is a hypernym of dog.\nWord Sense Disambiguation WSD, The task of selecting the correct sense for a word, formulated as a classification task.\nChose features: Directly neighboring words, content words, syntactically related words, topic of the text, part-of-speech tag, surrounding part-of-speech tags, etc … Distributional Semantic Models Vector semantics(embeddings): The meaning of a word is represented as a vector.\nTwo words are similar if they have similar word contexts vector. Term-context matrix(Co-occurrence\tMatrices): a word/term is defined by a vector over counts of context words. The row represent words, columns contexts. Problem: simple frequency isn’t the best measure of association between words. One problem is that raw frequency is very skewed and not very discriminative. “the” and “of” are very frequent, but maybe not the most discriminative. Sulution: use Pointwise mutual information. Then the Co-occurrence\tMatrices is filled with PPMI, instead of raw counts. Measuring vectors similarity based on PPMI: Dot product(inner product): More frequent words will have higher dot products, which cause similarity sensitive to word frequency. Cosine: normalized dot product , Raw frequency or PPMI is non-negative, so cosine range [0,1]. Evaluation of similarity Intrinsic: correlation between algorithm and human word similarity ratings. Check if there is correlation between similarity measures and word frequency. Application: sentiment analysis, see lab8 Pointwise Mutual Information PMI: do events x and y co-occur more than if they were independent?\nPMI between two words: $$PMI(w, c) = \\log_2 \\frac{P(w,c)}{P(W)P(c)}$$ Compute PMI on a term-context matrix(using counts): $$PMI(x, y) = log_2 \\frac{N \\times count(x, y)}{Count(x) Count(y)}$$ p(w=information, c=data) = 6/19 p(w=information) = 11/19 p(c=data) = 7/19 PMI(information,data) = log2(6*19/(11*7)) PMI is biased towards infrequent events, solution: Add-one smoothing PPMI: Positive PMI, could better handle low frequencies PPMI = max(PMI,0)\nT-test The t-test statistic, like PMI, can be used to measure how much more frequent the association is than chance.\nThe t-test statistic computes the difference between observed and expected means, normalized by the variance. The higher the value of t, the greater the likelihood that we can reject the null hypothesis. Null hypothesis: the two words are independent, and hence P(a,b) = P(a)P(b) correctly models the relationship between the two words.$$t\\textrm{-}test(a,b) = \\frac{P(a,b) - P(a)P(b)}{\\sqrt{P(a)P(b)}}$$ Minimum Edit Distance the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. Algorithm: searching the shortest path, use Dynamic programming to avoid repeating, (use BFS to search the shortest path?)\nWordNet A hierarchically organizesd lexical database, resource for English sense relations\nSynset: The set of near-synonyms for a WordNet sense (for synonym set) Word2Vec Sentence Meaning Representation 我们假设语言表达具有意义表征，这些表征由用于表示常识的类型相同的东西组成。而创建这种表征并将其分配给输入的语言的任务，称为语义分析（Semantic Analysis）。The symbols in our meaning representations language (MRL) correspond to objects, properties, and relations in the world. ![](/images/meaning_representation.png “A list of symbols, two directed graphs, and a record structure: a sampler of meaning representations for “I have a car”. image from: Speech and Language Processing”) 上图展示了使用四种常用的MRL表达“I have a car”，第一行是First order logic，有向图和其文字信息是 Abstract Meaning Representation (AMR)，其余两种是Frame-Based 和 Slot-Filler。\nQualifications of MRL:\nCanonical form: sentences with the same (literal) meaning should have the same MR. Compositional: The meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined. Verifiable: Can use the MR of a sentence to determine the truth of the sentence with respect to some given model(knowledge base) of the world. Unambiguous: an MR should have exactly one interpretation. Inference and Variables: we should be able to verify sentences not only directly, but also by drawing conclusions based on the input MR and facts in the knowledge base. Expressiveness: the MRL should allow us to handle a wide range of meanings and express appropriate relationships between the words in a sentence. Lexical semantics: the meaning of individual words.\nLexical semantic relationships: Relations between word senses\n模型论 从仅是正式的陈述到能够告诉我们世界某些事态的陈述，我们期望 meaning representations 弥合这种差距。而提供这种保证的依据就是模型。模型是一种正式的结构，可以代表真是世界的特定事态。\n意义表达的词汇表包含两部分：\n非逻辑词汇表，由构成我们试图表达的世界的对象，属性和关系的开放式名称组成。如 谓语predicates, nodes, labels on links, or labels in slots in frames。 逻辑词汇表，由一组封闭的符号，运算符，量词，链接等组成，它们提供了用给定意义表示语言编写表达式的形式化方法。 所有非逻辑词汇的元素都需要在模型中有一个表示（属于模型的固定的且定义明确的一部分）。 • 对象 Objects denote elements of the domain • 属性 Properties denote sets of elements of the domain • 关系 Relations denote sets of **tuples of elements of the domain\nFirst-order Logic FOL, Predicate logic, meets all of the MRL qualifications except compositionality.\nTerm: represent objects. Expressions are constructed from terms in three ways: Constants in FOL refer to specific objects in the world being described. FOL constants refer to exactly one object. Objects can, however, have multiple constants that refer to them. Functions in FOL correspond to concepts that are often expressed in English as genitives(所有格) 如 “Frasca’s location”, 一般表达为LocationOf(Frasca). Functions provide a convenient way to refer to specific objects without having to associate a named constant with them. Variables, 允许我们对对象做出断言和推理，而不必引用任何特定的命名对象。Make statements about anonymous objects: making statements about a particular unknown object and making statements about all the objects in some arbitrary world of objects. Predicate(谓语, 谓词, 宾词, 述语): symbols that represent properties of entities and relations between entities.\nTerms can be combined into predicate-argument structures. Restaurant(Maharani) 指明Maharani的属性是Restaurant. Predicates with multiple arguments represent relations between entities: member-of(UK, EU) /N to indicate that a predicate takes N arguments: member-of/2 Logical connectives: create larger representations by conjoining logical formulas using one of three operators. ∨(or), ∧(and), ¬(not), ⇒(implies). “I only have five dollars and I don’t have a lot of time.”, Have(Speaker,FiveDollars) ∧ ¬Have(Speaker,LotOfTime)\nVariables and Quantifiers:\nExistential Quantifiers: (“there exists”), “a restaurant that serves Mexican food near ICSI” - ∃xRestaurant(x) ∧ Serves(x, MexicanFood) ∧ Near((LocationOf(x), LocationOf(ICSI)), 头部的∃告诉我们如何解读句中的变量x: 要让句子为真, 那么对于变量x至少存在一个对象。 Universal Quantifier:∀(“for all”). “All vegetarian restaurants serve vegetarian food.” - ∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood). A predicate with a variable among its arguments only has a truth value if it is bound by a quantifier: ∀x.likes(x, Gim) has an interpretation as either true or false.\nLambda Notation Extend FOL, to work with ‘partially constructed’ formula, with this form λx.P(x).\nλ-reduction: 应用于逻辑 term 以产生新的FOL表达式, 其中形参变量绑定到指定的term, 形式为λx.P(x)(A) -\u003e P(A). E.g.：λx.sleep(x)(Marie) -\u003e sleep(Marie)\n嵌套使用, Verbal (event) MRs：λx.λy.Near(x,y)(Bacaro) -\u003e λy.Near(Bacaro,y), λz. λy. λx. Giving1(x,y,z) (book)(Mary)(John) -\u003e Giving1(John, Mary, book) -\u003e John gave Mary a book Problem: fixed arguments Requires separate Giving predicate for each syntactic subcategorisation frame(number/type/position of arguments). Separate predicates have no logical relation: if Giving3(a, b, c, d, e) is true, what about Giving2(a, b, c, d) and Giving1(a, b, c). Solution: Reification of events 事件具象化 Inference 推断的两种思路, forward chaining 和 backward chaining.\nforward chaining systems: Modus ponens(if - then) states that if the left-hand side(antecedent) of an implication rule is true, then the right-hand side(consequent) of the rule can be inferred. VegetarianRestaurant(Leaf) ∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood) then Serves(Leaf ,VegetarianFood) 随着单个事实被添加到知识库中，modus ponens用于触发所有适用的implication rules。优点是事实可以在被在需要时才在知识库中呈现，因为在某种意义上来说所有推断都是预先执行的。这可以大大减少后续的queries所需的时间，因为都应该是简单的查找。但缺点是那些永远用不到的事实也可能被推断和存储。\nbackward chaining:\n第一步是通过查看query公式是否存在知识库中, 来确认query是否为真。比如查询Serves(Leaf ,VegetarianFood). 如果没有，则搜索知识库中存在的适用implication rules。对涉及到的antecedent递归运行backward chaining. 比如发动搜索适用规则，从而找到规则∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood), 对于term Leaf而言, 对应的antecedent是VegetarianRestaurant(Leaf), 存在于知识库中. Prolog 就是采用backward chaining 推断策略的编程语言.\n虽然forward和backward推理是合理的，但两者都不完备。这意味着单独使用这些方法的系统无法找到有效的推论。完备的推理是解析 resolution, 但计算成本很高. 在实践中，大多数系统使用某种形式的chaining并把负担压到知识库开发人员去解码知识，以支持必要inference可以推论.\nReification of Events John gave Mary a book -\u003e ∃e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z)\nReify: to “make real” or concrete, i.e., give events the same status as entities. In practice, introduce variables for events, which we can quantify over Entailment relations: automatically gives us logical entailment relations between events [John gave Mary a book on Tuesday] -\u003e [John gave Mary a book] ∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) ∧ Time(e, Tuesday) -\u003e ∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) Semantic Parsing Aka semantic analysis. Systems for mapping from a text string to any logical form.\nMotivation: deriving a meaning representation from a sentence. Application: question answering Method: Syntax driven semantic analysis with semantic attachments Syntax Driven Semantic Analysis Principle of compositionality: the construction of constituent meaning is derived from/composed of the meaning of the constituents/words within that constituent, guided by word order and syntactic relations. Build up the MR by augmenting CFG rules with semantic composition rules. Add semantic attachments to CFG rules. Problem: encounter invalide FOL for some (base-form) MR, need type-raise. Training Semantic Attachments E.g\nVP → Verb NP : {Verb.sem(NP.sem)} Verb.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) NP.sem = Meat -\u003e VP.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) (Meat) = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) The MR for VP, is computed by applying the MR function to VP’s children.\nComplete the rule:\nS → NP VP : {VP.sem(NP.sem)} VP.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) NP.sem = AyCaramba -\u003e S.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) (AyCa.) = ∃e. Serving(e) ∧ Server(e, AyCaramba) ∧ Served(e, Meat) Abstract Meaning Representation AMR是Rooted，带标签的digraph，易于人们阅读，易于程序处理。扩展了PropBank的Frames集合。AMR 的特点是将句子中的词抽象为概念，因而使最终的语义表达式与原始的句子没有直接的对应关系，对相同意思的不同句子能够抽象出相同的表达。\nKnowledge-driven AMRL\nThe Alexa Meaning Representation Language World Knowledge for Abstract Meaning Representation Parsing：依赖于wordnet，而中文本身就是字组词， Knowledge-driven Abstract Meaning Representation AMR is bias towards English.\nSlot-fillers/Template-filling/Frame 在文档中找到此类情况并填写模板位置。这些空位填充符可以包含直接从文本中提取的文本段，也可以包括通过额外处理从文本元素中推断出的诸如时间，数量或本体实体之类的概念\n许多文本包含事件的报告，以及可能的事件序列，这些报告通常对应于世界上相当普遍的刻板印象。这些抽象的情况或story，和 script（Schank and Abelson，1975）有关，由子事件，参与者及其角色的原型序列组成。类似的定义还有 Frame ( Minsky（1974），Hymes（1974）和Goffman（1974）大约在同一时间提出的一系列相关概念) 和 schemata（Bobrow and Norman，1975）。\nTopic Modelling 假如知道有什么主题，或者对主题的数量和分布做出先验假设，此时可以使用监督学习, 如朴素贝叶斯分类, 把文章处理成词袋(bag of words). 但假如不知道这些先验呢?\n就要依靠无监督学习, 比如聚类：Instead of using supervised topic classification – rather not fix topics in advance nor do manual annotation, Use clustering to teases out the topics. Only the number of topics is specified in advance.\n这就是主题建模(Topic Modelling), 一种常用的文本挖掘方法，用于发现文本中的隐藏语义结构。此时主题数量就是一个超参数, 通过主题建模，构建了单词的clusters而不是文本的clusters。因此，文本被表达为多个主题的混合，每个主题都有一定的权重。\n因为主题建模不再是用词频来表达, 而是用主题权重{Topic_i: weight(Topic_i, T) for Topic_i in Topics}, 所以主题建模也是一种 Dimensionality Reduction.\n主题建模也可以理解为文本主题的tagging任务, 只是无监督罢了.\n主题建模的算法:\n(p)LSA: (Probabilistic) Latent Semantic Analysis – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra. SVD假设了Gaussian distributed. LDA: latent Dirichlet allocation, 假设了multimonial distribution。 LDA LDA是pLSA的generalization, LDA的hyperparameter设为特定值的时候，就specialize成 pLSA 了。从工程应用价值的角度看，这个数学方法的generalization，允许我们用一个训练好的模型解释任何一段文本中的语义。而pLSA只能理解训练文本中的语义。（虽然也有ad hoc的方法让pLSA理解新文本的语义，但是大都效率低，并且并不符合pLSA的数学定义。）这就让继续研究pLSA价值不明显了。\nLatent Dirichlet allocation(LDA): each document may be viewed as a mixture of various topics where each document is generated by LDA. A topic is a distribution over words generate document: Randomly choose a distribution over topics For each word in the document randomly choose a topic from the distribution over topics randomly choose a word from the corresponding topic (distribution over the vocabulary) training: repeat until converge assign each word in each document to one of T topics. For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t) Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d) Inference: LDA没法做精确inference，只有近似算法，比如variational inference。 LDA模型代码\nEvaluation Extrinsic Evaluation Use something external to measure the model. End-to-end evaluation, the best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves.\nPut each model in a task: spelling corrector, speech recognizer, MT system Run the task, get an accuracy for A and for B How many misspelled words corrected properly How many words translated correctly Compare accuracy for A and B Unfortunately, running big NLP systems end-to-end is often very expensive.\nIntrinsic Evaluation Measures independenly to any application. Train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. Which means whichever model assigns a higher probability to the test set\nHuman Evaluation E.g to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to gold labels match. We will refer to these human labels as the gold labels.\nPrecision, Recall, F-measure To deal with unbalanced lables Application: text classification, parsing. Evaluation in text classification: the 2 by 2 contingency table, golden lable is true or false, the classifier output is positive or negative. Precision: Percentage of positive items that are golden correct, from the view of classifier\nRecall: Percentage of golden correct items that are positive, from the view of test set.\nF-measure\nMotivation: there is tradeoff between precision and recall, so we need a combined meeasure that assesses the P/R tradeoff. The b parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of b \u003e 1 favor recall, while values of b \u003c 1 favor precision. Balanced F1 measure with beta =1, F = 2PR/(P+R) Confusion Matrix Recalled that confusion matrix’s row represent golden label, column represent the classifier’s output, to anwser the quesion：for any pair of classes(c1,c2), how many test sample from c1 were incorrectly assigned to c2\nRecall: Fraction of samples in $c_1$ classified correctly, $\\frac{CM(c_1, c_1)}{\\sum_jCM(c_1, j)}$ Precision: fraction of samples assigned $c_1$ that are actually $c_1$, $\\frac{CM(c_1, c_1)}{\\sum_iCM(i, c_1)}$ Accuracy: $\\frac{\\sum diagnal}{all}$ Correlation When two sets of data are strongly linked together we say they have a High Correlation. Correlation is Positive when the values increase together, and Correlation is Negative when one value decreases as the other increases.\nPearson correlation: covariance of the two variables divided by the product of their standard deviations.$$r = \\frac{\\sum_{i=1}^n(x_i - \\overrightarrow{x})(y_i - \\overrightarrow{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\overrightarrow{x})^2} \\sqrt{\\sum_{i=1}^n(y_i - \\overrightarrow{y})^2}}$$ Spearman correlation: the Pearson correlation between the rank values of the two variables Basic Text Processing Regular Expressions NLP工作必备技能(考试不需要).\n一些练习Regular Expressions的有趣网站: https://alf.nu/RegexGolf https://regexr.com/\nWord Tokenization NLP task needs to do text normalization:\nSegmenting/tokenizing words in running text Normalizing word formats Segmenting sentences in running text they lay back on the San Francisco grass and looked at the stars and their\nType: an element of the vocabulary. Token: an instance of that type in the actual text. 英文比较简单. 中文有一个难点, 需要分词.\n","wordCount":"6425","inLanguage":"en","datePublished":"2018-06-30T00:00:00Z","dateModified":"2018-06-30T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/inf-course-note-accelerated-natural-language-processing/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Inf Course Note - Accelerated Natural Language Processing</h1><div class=post-meta><span title='2018-06-30 00:00:00 +0000 UTC'>2018-06-30</span>&nbsp;·&nbsp;31 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/UoE-anlp.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%a6%82%e7%8e%87%e6%a8%a1%e5%9e%8b-probability-model aria-label="概率模型 Probability Model">概率模型 Probability Model</a><ul><li><a href=#%e5%b9%b3%e6%bb%91smoothing aria-label=平滑Smoothing>平滑Smoothing</a></li></ul></li><li><a href=#language-modeling aria-label="Language modeling">Language modeling</a><ul><li><a href=#perplexity aria-label=Perplexity>Perplexity</a></li></ul></li><li><a href=#n-gram-language-model aria-label="N-Gram Language Model">N-Gram Language Model</a><ul><li><a href=#add-alpha-smoothing aria-label="Add Alpha Smoothing">Add Alpha Smoothing</a></li><li><a href=#backoff-smoothing aria-label="Backoff Smoothing">Backoff Smoothing</a></li><li><a href=#interpolation-smoothing aria-label="Interpolation Smoothing">Interpolation Smoothing</a></li><li><a href=#kneser-ney-smoothing aria-label="Kneser-Ney Smoothing">Kneser-Ney Smoothing</a></li></ul></li><li><a href=#naive-bayes-classifier aria-label="Naive Bayes Classifier">Naive Bayes Classifier</a><ul><li><a href=#text-classification aria-label="Text Classification">Text Classification</a></li><li><a href=#sentiments-analysis aria-label="Sentiments Analysis">Sentiments Analysis</a></li><li><a href=#naive-bayes-assumptions aria-label="Naive Bayes Assumptions">Naive Bayes Assumptions</a></li><li><a href=#naive-bayes-training aria-label="Naive Bayes Training">Naive Bayes Training</a></li><li><a href=#naive-bayes-relationship-to-language-modelling aria-label="Naive Bayes Relationship to Language Modelling">Naive Bayes Relationship to Language Modelling</a></li></ul></li><li><a href=#hidden-markov-model aria-label="Hidden Markov Model">Hidden Markov Model</a><ul><li><a href=#part-of-speech-tagging aria-label="Part-of-speech Tagging">Part-of-speech Tagging</a></li><li><a href=#transition-probability-matrix aria-label="Transition Probability Matrix">Transition Probability Matrix</a></li><li><a href=#emission-probability aria-label="Emission Probability">Emission Probability</a></li><li><a href=#penn-treebank aria-label="Penn Treebank">Penn Treebank</a></li><li><a href=#forward-algorithm aria-label="Forward Algorithm">Forward Algorithm</a></li><li><a href=#viterbi-algorithm aria-label="Viterbi Algorithm">Viterbi Algorithm</a></li><li><a href=#hmm-training aria-label="HMM Training">HMM Training</a></li></ul></li><li><a href=#context-free-grammar aria-label="Context-free Grammar">Context-free Grammar</a><ul><li><a href=#constituency aria-label=Constituency>Constituency</a></li><li><a href=#probabilistic-context-free-grammar aria-label="Probabilistic Context-free Grammar">Probabilistic Context-free Grammar</a></li><li><a href=#lexicalization-of-pcfgs aria-label="Lexicalization of PCFGs">Lexicalization of PCFGs</a></li><li><a href=#recursive-descent-parsing aria-label="Recursive Descent Parsing">Recursive Descent Parsing</a></li><li><a href=#cky-parsing aria-label="CKY Parsing">CKY Parsing</a></li></ul></li><li><a href=#dependency-parsing aria-label="Dependency Parsing">Dependency Parsing</a><ul><li><a href=#dependency-syntax aria-label="Dependency Syntax">Dependency Syntax</a></li><li><a href=#transition-based-dependency-parsing aria-label="Transition-based Dependency Parsing">Transition-based Dependency Parsing</a></li><li><a href=#dependency-tree aria-label="Dependency Tree">Dependency Tree</a></li><li><a href=#bounded-and-unbounded-dependencies aria-label="Bounded and Unbounded Dependencies">Bounded and Unbounded Dependencies</a></li></ul></li><li><a href=#noisy-channel-model aria-label="Noisy Channel Model:">Noisy Channel Model:</a></li><li><a href=#generative-vs-discriminative-models aria-label="Generative vs. Discriminative Models">Generative vs. Discriminative Models</a><ul><li><a href=#exponential-models aria-label="Exponential Models">Exponential Models</a></li><li><a href=#training-discriminative-model aria-label="Training Discriminative Model">Training Discriminative Model</a></li><li><a href=#regularization-in-discriminative-model aria-label="Regularization in Discriminative Model">Regularization in Discriminative Model</a></li></ul></li><li><a href=#morphology aria-label=Morphology>Morphology</a><ul><li><a href=#morphological-parsing aria-label="Morphological Parsing">Morphological Parsing</a></li><li><a href=#inflectional-vs-derivationalmorphology aria-label="Inflectional vs. Derivational Morphology">Inflectional vs. Derivational Morphology</a></li><li><a href=#challenge-of-rich-morphology aria-label="Challenge of Rich Morphology">Challenge of Rich Morphology</a></li></ul></li><li><a href=#linguistic-and-representational-concepts aria-label="Linguistic and Representational Concepts">Linguistic and Representational Concepts</a><ul><li><a href=#parsing aria-label=Parsing>Parsing</a></li><li><a href=#syntactic-parsing aria-label="Syntactic Parsing">Syntactic Parsing</a></li><li><a href=#statistical-parsing aria-label="Statistical Parsing">Statistical Parsing</a></li><li><a href=#dependency-parsing-1 aria-label="Dependency Parsing">Dependency Parsing</a></li><li><a href=#constituency-1 aria-label=Constituency>Constituency</a></li><li><a href=#dependency aria-label=Dependency>Dependency</a></li><li><a href=#morphology-1 aria-label=Morphology>Morphology</a></li><li><a href=#ambiguity aria-label=Ambiguity>Ambiguity</a></li><li><a href=#open-class-closed-class aria-label="Open-class Closed-class">Open-class Closed-class</a></li></ul></li><li><a href=#word-sense aria-label="Word Sense">Word Sense</a><ul><li><a href=#word-sense-disambiguation aria-label="Word Sense Disambiguation">Word Sense Disambiguation</a></li><li><a href=#distributional-semantic-models aria-label="Distributional Semantic Models">Distributional Semantic Models</a></li><li><a href=#pointwise-mutual-information aria-label="Pointwise Mutual Information">Pointwise Mutual Information</a></li><li><a href=#t-test aria-label=T-test>T-test</a></li><li><a href=#minimum-edit-distance aria-label="Minimum Edit Distance">Minimum Edit Distance</a></li><li><a href=#wordnet aria-label=WordNet>WordNet</a></li><li><a href=#word2vec aria-label=Word2Vec>Word2Vec</a></li></ul></li><li><a href=#sentence-meaning-representation aria-label="Sentence Meaning Representation">Sentence Meaning Representation</a><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e8%ae%ba aria-label=模型论>模型论</a></li><li><a href=#first-order-logic aria-label="First-order Logic">First-order Logic</a></li><li><a href=#lambda-notation aria-label="Lambda Notation">Lambda Notation</a></li><li><a href=#inference aria-label=Inference>Inference</a></li><li><a href=#reification-of-events aria-label="Reification of Events">Reification of Events</a></li><li><a href=#semantic-parsing aria-label="Semantic Parsing">Semantic Parsing</a></li><li><a href=#syntax-driven-semantic-analysis aria-label="Syntax Driven Semantic Analysis">Syntax Driven Semantic Analysis</a></li><li><a href=#semantic-attachments aria-label="Semantic Attachments">Semantic Attachments</a></li><li><a href=#abstract-meaning-representation aria-label="Abstract Meaning Representation">Abstract Meaning Representation</a></li><li><a href=#slot-fillerstemplate-fillingframe aria-label=Slot-fillers/Template-filling/Frame>Slot-fillers/Template-filling/Frame</a></li></ul></li><li><a href=#topic-modelling aria-label="Topic Modelling">Topic Modelling</a><ul><li><a href=#lda aria-label=LDA>LDA</a></li></ul></li><li><a href=#evaluation aria-label=Evaluation>Evaluation</a><ul><li><a href=#extrinsic-evaluation aria-label="Extrinsic Evaluation">Extrinsic Evaluation</a></li><li><a href=#intrinsic-evaluation aria-label="Intrinsic Evaluation">Intrinsic Evaluation</a></li><li><a href=#human-evaluation aria-label="Human Evaluation">Human Evaluation</a></li><li><a href=#precision-recall-f-measure aria-label="Precision, Recall, F-measure">Precision, Recall, F-measure</a></li><li><a href=#confusion-matrix aria-label="Confusion Matrix">Confusion Matrix</a></li><li><a href=#correlation aria-label=Correlation>Correlation</a></li></ul></li><li><a href=#basic-text-processing aria-label="Basic Text Processing">Basic Text Processing</a><ul><li><a href=#regular-expressions aria-label="Regular Expressions">Regular Expressions</a></li><li><a href=#word-tokenization aria-label="Word Tokenization">Word Tokenization</a></li></ul></li></ul></div></details></div><div class=post-content><p>爱丁堡大学信息学院课程笔记 Accelerated Natural Language Processing, Informatics, University of Edinburgh</p><p>References:
<a href=http://www.inf.ed.ac.uk/teaching/courses/anlp/>Accelerated natural language processing</a>
<a href=https://www.inf.ed.ac.uk/teaching/courses/anlp/review/review_ay17.html>ANLP revision guide</a>
<a href=https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html>Lecture Slides from the Stanford Coursera course Natural Language Processing, by Dan Jurafsky and Christopher Manning</a></p><h2 id=概率模型-probability-model>概率模型 Probability Model<a hidden class=anchor aria-hidden=true href=#概率模型-probability-model>#</a></h2><p>概率模型是随机现象的数学表示，由样本空间，样本空间内的事件以及与每个事件相关的概率定义。目标是模拟给一个事件发生的概率</p><p>估算概率（Probability Estimation）一般使用最大似然估计（MLE，相关频率）：</p>$$p(x_i) = \frac{Count(x_i)}{\sum_{i=0}^nCount(x_i)}$$<h3 id=平滑smoothing>平滑Smoothing<a hidden class=anchor aria-hidden=true href=#平滑smoothing>#</a></h3><p>一般用于处理0概率的问题，比如在训练集中看不到, 但出现在测试集中的词。</p><h2 id=language-modeling>Language modeling<a hidden class=anchor aria-hidden=true href=#language-modeling>#</a></h2><p>To compute the probability of sentence /sequence of words $P(w_1, w_2, w_3...)$, or to predict upcomming words $P(w|w_1, w_2, w_3...)$&mldr; a language model is also a probability model.</p><p>Probability computation makes use of chain rule of probability, the products of a sequence of <strong>conditional</strong> probability.</p>$$P(w_{1:n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})P(w_4|w_{1:3})...P(w_n|w_{1:n-1})$$<p>But the last term based on the entire sentence is very difficult to compute. So it is simplified by <strong>Markov Assumption</strong>: approximate the conditional probability by only accounting several prefixes, a one-order Markov assumption simplifies as <code>P(the| water is so transparent that) ≈ P(the| that)</code></p>$$\begin{align}
P(w_{1:n}) &= \prod_{i=1}^n P(w_i | w_1, ..., w_{i-1}) \\\\
&\propto \prod_{i=1}^n P(w_i | w_{i-k}, ..., w_{i-1}) \end{align}$$<p>Evaluation: Perplexity</p><h3 id=perplexity>Perplexity<a hidden class=anchor aria-hidden=true href=#perplexity>#</a></h3><p>Intuition based on Shannon game: The best language model is one that best predicts an unseen test set(e.g. next word), gives the highest $P(sentence)$ to the word that actually occurs.</p><ul><li>Definition: Perplexity is the inverse probability of the test set, normalized by the number of words(lie between 0-1).</li></ul><p>Normalize the log probability of all the test sentences:</p>$$\frac{1}{M} \log_2 \prod_{i=1}^m p(x^{(i)}) = \frac{1}{M} \sum_{i=1}^m \log_2 p(x^{(i)})$$<p>Then transform to perplexity:</p>$$Perplexity = 2^{-\frac{1}{M} \sum_{i=1}^m \log_2 p(x^{(i)})}$$<p>So minimizing perplexity is the same as maximizing probability</p><p>Bad approximation: unless the test data looks just like the training data, so generally only useful in pilot experiments.</p><h2 id=n-gram-language-model>N-Gram Language Model<a hidden class=anchor aria-hidden=true href=#n-gram-language-model>#</a></h2><p><code>N-Gram</code>语言模型是基于<code>N-1</code>阶马尔可夫假设且由MLE估算出的LM。<code>N-Gram</code>LM 预测下一个单词出现概率仅条件于前面的<code>(N-1)</code>个单词, 以<code>The students opened their books</code>为例:</p><ul><li><code>Bi-gram</code>: 统计$P(w_{i}=m|w_{i-1})$, <code>P(students | the)</code>, <code>P(opened | students)</code>, &mldr;, 属于<code>马尔可夫一阶模型</code>, 即当前<code>t</code>时间步的状态仅跟<code>t-1</code>相关.</li><li><code>Tri-gram</code>: <code>P(students | &lt;/s> The)</code>, <code>P(opened | The students)</code>, <code>马尔可夫二阶模型</code></li><li><code>Four-gram</code>: 依此类推</li></ul><p>特殊的<code>Uni-gram</code>: 统计$P(w_i)$, <code>P(the)</code>, <code>P(students)</code>, &mldr;, 此时整个模型退化为词袋模型, 不再属于马尔可夫模型, 而是基于贝叶斯假设, 即各个单词是条件独立的. 所以一般<code>N-gram</code>是指<code>N>1</code>的.</p><p>How to estimate theparameter?
Maximum likelyhood estimate：</p>$$P(w_{i}=m|w_{i-n:i-1}) = \frac{Count(w_{i-n:i})}{Count(w_{i-n:i-1})}$$<p>In practice, use log space to avoid underflow, and adding is faster than multiplying.</p><ul><li>Insufficient:<ul><li>To catch long-distance dependencies, the <code>n</code> has to be very large, that asks for very large memory requirement</li><li>N-grams only work well for word prediction if the test corpus looks like the training corpus.</li></ul></li><li>Sparsity:<ul><li>Zero count of gram, means zero probability? No. To deal with 0 probability, commonly use <a href=/posts/inf-course-note-accelerated-natural-language-processing/#kneser-ney-smoothing>Kneser-Ney smoothing</a>, for very large N-grams like web, use stupid backoff.</li></ul></li></ul><h3 id=add-alpha-smoothing>Add Alpha Smoothing<a hidden class=anchor aria-hidden=true href=#add-alpha-smoothing>#</a></h3><ul><li>Assign equal probability to all unseen events.</li><li>Applied in text classification, or domains where zeros probability is not common.</li></ul><h3 id=backoff-smoothing>Backoff Smoothing<a hidden class=anchor aria-hidden=true href=#backoff-smoothing>#</a></h3><ul><li>Use information from lower order N-grams (shorter histories)</li><li>Back off to a lower-order N-gram if we have zero evidence for a higher-order interpolation N-gram.</li><li>Discount: In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams.</li></ul><h3 id=interpolation-smoothing>Interpolation Smoothing<a hidden class=anchor aria-hidden=true href=#interpolation-smoothing>#</a></h3><ul><li>Interpolation: mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts</li><li>Simple interpolation: $P(w_3|w_1, w_2) = \lambda_1 P(w_3|w_1, w_2) + \lambda_2 P(w_3|w_2) + \lambda_3 P(w_3), \sum \lambda = 1$.</li><li>λ could be trianed/conditioned on training set/contest, choose λ that maximie the probability of held-out data</li></ul><h3 id=kneser-ney-smoothing>Kneser-Ney Smoothing<a hidden class=anchor aria-hidden=true href=#kneser-ney-smoothing>#</a></h3><ul><li>Combine absolute discounting and interpolation: Extending interpolatation with an absolute discounting 0.75 for high order grams.</li><li>Use a better estimate for probabilities of lower-order unigrams, the continuation probability, $P_{continuatin}(w)$ is how likely is w to appear as a novel continutaion.<ul><li>For each word w, count the number of bigram types it completes. Or count the number of word types seen to precede w.</li><li>Every bigram type was a novel continuation the first time it was seen.</li><li>normalized by the total number of word bigram types.</li></ul></li><li>To lower the probability of some fix bigram like &ldquo;San Franscio&rdquo;</li><li>For general N-gram, <img loading=lazy src=/images/kneser-ney.png></li></ul><h2 id=naive-bayes-classifier>Naive Bayes Classifier<a hidden class=anchor aria-hidden=true href=#naive-bayes-classifier>#</a></h2><ul><li>Application: <a href=/posts/inf-course-note-accelerated-natural-language-processing/#text-classification>Text classification</a>, to classify a text, we calculate each class probability given the test sequence, and choose the biggest one.</li><li>Evaluation: <a href=/posts/inf-course-note-accelerated-natural-language-processing/#precision-recall-f-measure>precision, recall, F-measure</a></li><li>Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了</li></ul><h3 id=text-classification>Text Classification<a hidden class=anchor aria-hidden=true href=#text-classification>#</a></h3><p>Or text categorization, method is not limited to NB, see <a href=http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab7.html>lab7</a>.
Spam email, gender/authorship/language identification, sentiments analysis,(opinion extraction, subjectivity analysis)&mldr;</p><h3 id=sentiments-analysis>Sentiments Analysis<a hidden class=anchor aria-hidden=true href=#sentiments-analysis>#</a></h3><ul><li>For sentiment(or other text classification), word occurrence may matter more than word frequency. Thus it often improves performance to clip the word counts in each document at 1.<ul><li>This variant binary NB is called binary multinominal naive Bayes or binary NB.</li><li>Remove duplicates in each data sample - bag of words representation, boolean features. Binarized seems to work better than full word counts.</li></ul></li><li>Deal with negation: <code>like, not like</code>, A very simple baseline that is commonly used in sentiment to deal with negation is during text normalization to prepend the prefix NOT_ to every word after a token of logical negation</li><li>Sentiment lexicons: lists of words that are preannotated with positive or negative sentiment. To deal with insufficient labeled training data. A common way to use lexicons in the classifier is to use as one feature the total
count of occurrences of any words in the positive lexicon, and as a second feature the total count of occurrences of words in the negative lexicon. Using just two features results in classifiers that are much less sparse to small amounts of training data, and may generalize better. See <a href=http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab8.html>lab8</a>.</li></ul><h3 id=naive-bayes-assumptions>Naive Bayes Assumptions<a hidden class=anchor aria-hidden=true href=#naive-bayes-assumptions>#</a></h3><ul><li>Bags of words: a set of unordered words/features with its frequency in the documents, their order was ignored.</li><li>Conditional independence: the probabilities $P(w|C)$ are independence given the class, thus a sequence of words(w1,w2,w3&mldr;) probability coculd be estimate via prducts of each $P(w_i|C)$ by walking through every pisition of the sequence, noted that the orders in the sequence does not matter.</li></ul><h3 id=naive-bayes-training>Naive Bayes Training<a hidden class=anchor aria-hidden=true href=#naive-bayes-training>#</a></h3><ul><li>Each classes&rsquo; prior probability P(C) is the percentage of the classes in the training set.</li><li>For the test set, its probability as a class j, is the products of its sequence probability $P(w_1, w_2, w_3...|C_j)$ and $P(C_j)$, normalized by the sequence probability $P(w_1, w_2, w_3...)$, which could be calculated by summing all $P(w_1, w_2, w_3...|C_j)\*P(C_j)$.</li><li>The joint features probability $P(w_1, w_2, w_3...|C)$ of each class is calculated by naively multiplying each word&rsquo;s MLE given that class.</li><li>In practice, to deal with 0 probability, we dun use MLE, instead we use add alpha smoothing.<ul><li>Why 0 probability matters? Because it makes the whole sequence probability $P(w_1, w_2, w_3...|C)$ 0, then all the other features as evidence for the class are eliminated too.</li><li>How: first extract all the vocabulary V in the training set.</li><li>Then, for each feature/word k, its add alpha smoothing probability estimation within a class j is $(Njk + \alpha)/(N_j+V\*\alpha)$.</li><li>This is not the actual probability, but just the numerator.</li></ul></li></ul><h3 id=naive-bayes-relationship-to-language-modelling>Naive Bayes Relationship to Language Modelling<a hidden class=anchor aria-hidden=true href=#naive-bayes-relationship-to-language-modelling>#</a></h3><ul><li>When using all of the words as features for naive bayes, then each class in naive bayes is a unigram languange model.</li><li>For each word, assign probability $P(word|C)$,</li><li>For each sentence, assign probability $P(S|C) = P(w_1, w_2, w_3...|C)$</li><li>Running multiple languange models(classes) to assign probabilities, and pick out the highest language model.</li></ul><h2 id=hidden-markov-model>Hidden Markov Model<a hidden class=anchor aria-hidden=true href=#hidden-markov-model>#</a></h2><p>The HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), they compute a probability distribution over possible sequences of labels and choose the best label sequence.</p><p>HMM参数$λ= (Y, X, π, A, B)$ :</p><ul><li>Y是隐状态（输出变量）的集合</li><li>X是观察值（输入）集合</li><li>Initial probability π</li><li>Transition probability matrix A, $P(Tag_{i+1} | Tag_{i})$</li><li>Emission probability B, $P(Word | Tag)$</li></ul><p>Application: part-of-speech tagging, name entity recognition(NEr), parse tree, speech recognition</p><p><strong>Hidden?</strong>: these tags, trees or words is not observed(hidden). b比如在POS任务中, X就是观察到的句子, Y就是待推导的标注序列, 因为词性待求的, 所以人们称之为<strong>隐含状态</strong>.</p><p>The three fundamental problems of HMM:</p><ul><li>decoding: discover the best hidden state sequence via <a href=/posts/inf-course-note-accelerated-natural-language-processing/#viterbi-algorithm>Viterbi algorithm</a>.</li><li>Probability of the observation: Given an HMM with know parameters λ and an observation sequence O, determine the likelihood $P(O| \lambda)$ (a language model regardless of tags) via <a href=/posts/inf-course-note-accelerated-natural-language-processing/#forward-algorithm>Forward algorithm</a></li><li>Learning (training): Given only the observed sequence, learn the best(MLE) HMM parameters λ via <a href=/posts/inf-course-note-accelerated-natural-language-processing/#forward-backward-algorithm>forward-backward algorithm</a>, thus <a href=/posts/inf-course-note-accelerated-natural-language-processing/#hmm-training>training a HMM</a> is an unsupervised learning task.</li></ul><p>算法:</p><ul><li>前向算法和后向算法解决如何计算似然$P(O| \lambda)$的问题</li><li>Viterbi算法解决HMM 解码问题.</li><li>这些算法都是动态规划算法</li></ul><p>HMM的缺陷是其基于观察序列中的每个元素都相互条件独立的假设。即在任何时刻观察值仅仅与状态（即要标注的标签）有关。对于简单的数据集，这个假设倒是合理。但大多数现实世界中的真实观察序列是由多个相互作用的特征和观察序列中较长范围内的元素之间的依赖而形成的。而条件随机场(conditional random fiel, CRF)恰恰就弥补了这个缺陷.</p><p>同时, 由于生成模型定义的是联合概率，必须列举所有观察序列的可能值，这对多数领域来说是比较困难的。</p><h3 id=part-of-speech-tagging>Part-of-speech Tagging<a hidden class=anchor aria-hidden=true href=#part-of-speech-tagging>#</a></h3><ul><li>Part-of-speech(POS), word classes, or syntactic categories, a description of eight parts-of-speech: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner.<ol><li>noun 名詞 (代號 n. )</li><li>pronoun 代名詞 (代號 pron. )</li><li>verb 動詞 (代號 v. )</li><li>adjective 形容詞 (代號 adj. )</li><li>adverb 副詞 (代號 adv. )</li><li>preposition 介系詞 (代號 prep. )</li><li>conjunction 連接詞 (代號 conj. )</li><li>interjection 感歎詞 (代號 int. )</li></ol></li><li>Motivation: Use model to find the best tag sequence T for an untagged sentence S: argmax $P(T|S)$ -> argmax $P(S|T)\*P(T)$, where P(T) is the transition (prior) probabilities, $P(S|T)$ is the emission (likelihood) probabilities.</li><li>Parts-of-speech can be divided into two broad supercategories: <a href=/posts/inf-course-note-accelerated-natural-language-processing/#open-class-closed-class>closed class types and open class types</a></li><li>Search for the best tag sequence: <a href=/posts/inf-course-note-accelerated-natural-language-processing/#Viterbi-Algorithm>Viterbi algorithm</a></li><li>evaluation: tag accuracy</li></ul><p><a href=https://github.com/congchan/Chinese-nlp/blob/master/hmm_pos_tag.ipynb>使用HMM处理POS代码</a></p><h3 id=transition-probability-matrix>Transition Probability Matrix<a hidden class=anchor aria-hidden=true href=#transition-probability-matrix>#</a></h3><ul><li>Tags or states</li><li>Each (i,j) represent the probability of moving from state i to j</li><li>When estimated from sequences, should include beginning <code>&lt;s></code> and end <code>&lt;/s></code> markers.</li><li>Tag transition probability matrix: the probability of tag i followed by j</li></ul><h3 id=emission-probability>Emission Probability<a hidden class=anchor aria-hidden=true href=#emission-probability>#</a></h3><ul><li>Also called observation likelihoods, each expressing the probability of an observation j being generated from a states i.</li><li>Word/symbol</li></ul><h3 id=penn-treebank>Penn Treebank<a hidden class=anchor aria-hidden=true href=#penn-treebank>#</a></h3><p><img loading=lazy src=/images/Treebank.png></p><h3 id=forward-algorithm>Forward Algorithm<a hidden class=anchor aria-hidden=true href=#forward-algorithm>#</a></h3><ul><li>Compute the likelihood of a particular observation sequence.</li><li>Implementation is almost the same as Viterbi.</li><li>Yet Viterbi takes the max over the previous path probabilities whereas the forward algorithm takes the sum.</li></ul><h3 id=viterbi-algorithm>Viterbi Algorithm<a hidden class=anchor aria-hidden=true href=#viterbi-algorithm>#</a></h3><p>Decoding task: the task of determining which sequence of variables is the underlying source of some sequence of observations.</p><p>Viterbi的实现参考<a href=https://github.com/congchan/Chinese-nlp/blob/master/hmm_pos_tag.ipynb>HMM POS Tagging</a></p><p>Intuition: The probability of words $w_1$ followed by $w_2$ with tag/state i and j (i,j is index of all Tags), is the chain rule of the probability of i followed by j and the probability of i output $w_i$ $P(w_1 | i)$ and $P(w_2 |j)$, then choose the maximum from all the possible i j. Then using chain rule to multiply the whole sequence of words.</p><p>The value of each cell $Vt(j)$ is computed by recursively taking the most probable path that could lead us to this cell from left columns to right. See exampls in <a href=http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab_solutions.html#Tutorial_2>tutorial 2</a>
<img loading=lazy src=/images/viterbi.png></p><ul><li>Since HMM based on Markov Assumptions, so the present column $V_t$ is only related with the nearby left column $V_{t-1}$.</li></ul><h3 id=hmm-training>HMM Training<a hidden class=anchor aria-hidden=true href=#hmm-training>#</a></h3><p>给定观察序列$X = x_1, x_2, ..., x_t$ ，训练调整模型参数λ, 使$p(X | \lambda)$最大: Baum-Welch算法 (Forward-backward algorithm)</p><ul><li>inputs: just the observed sequence</li><li>output: the converged <code>λ(A,B)</code>.</li><li>For each interation k until λ converged:<ul><li>Compute expected counts using <code>λ(k-1)</code></li><li>Set <code>λ(k)</code> using MLE on the expected counts.</li></ul></li></ul><p>经常会得到局部最优解.</p><h2 id=context-free-grammar>Context-free Grammar<a hidden class=anchor aria-hidden=true href=#context-free-grammar>#</a></h2><p>CFG(phrase-structure grammar) consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered toLexicon gether, and a lexicon of words and symbols.</p><h3 id=constituency>Constituency<a hidden class=anchor aria-hidden=true href=#constituency>#</a></h3><p>Phrase structure, organizes words into nested constituents. Groups of words behaving as a single units, or constituents.</p><ul><li>Noun phrase(NP), a sequence of words surrounding at least one noun. While the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase</li><li>Preposed or Postposed constructions. While the entire phrase can be placed differently, the individual words making up the phrase cannot be.</li><li>Fallback: In languages with free word order, phrase structure
(constituency) grammars don’t make as much sense.</li><li>Headed phrase structure: many phrase has head, VP->VB, NP->NN, the other symbols excepct the head is modifyer.</li></ul><h3 id=probabilistic-context-free-grammar>Probabilistic Context-free Grammar<a hidden class=anchor aria-hidden=true href=#probabilistic-context-free-grammar>#</a></h3><p>PCFG(Stochastic Context-Free Grammar SCFG (SCFG)), a probabilistic augmentation of context-free grammars in which each rule is associated with a probability.</p><ul><li>G = (T,N,S,R,P)<ul><li>T, N: Terminal and Non-terminal</li><li>S: starts symbol</li><li>R: Derive rule/grammar, N -> N/C</li><li>P: a probability function, for a given N, ΣP(N->Ni/Ci)=1. Normally P(S->NP VP)=1, because this is the only rule for S.</li></ul></li><li>PCFG could generates a sentence/tree,<ul><li>thus it is a language model, assigns a probability to the string of words constituting a sentence</li><li>The probability of a tree t is the product of the probabilities of the rules used to generate it.</li><li>The probability of the string s is the sum of the probabilities of the trees/parses which have that string as their yield.</li><li>The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees for the sentence.</li></ul></li><li>Application: Probabilistic parsing</li><li>Shortage: lack the lexicalization of a trigram model, i.e only a small fraction of the rules contains information about words. To solve this problem, use <a href=/posts/inf-course-note-accelerated-natural-language-processing/#lexicalization-of-pcfgs>lexicalized PCFGs</a></li></ul><h3 id=lexicalization-of-pcfgs>Lexicalization of PCFGs<a hidden class=anchor aria-hidden=true href=#lexicalization-of-pcfgs>#</a></h3><ul><li>The head word of phrase gives a good representation of the phrase&rsquo;s structure and meaning</li><li>Puts the properties of words back into a PCFG</li><li>Word to word affinities are useful for certain ambiguities, because we know the probability of rule with words and words now, e.g. PP attachment ambiguity</li></ul><h3 id=recursive-descent-parsing>Recursive Descent Parsing<a hidden class=anchor aria-hidden=true href=#recursive-descent-parsing>#</a></h3><p>It is a top-down, depth-first parser:</p><ol><li>Blindly expand nonterminals until reaching a terminal (word).</li><li>If multiple options available, choose one but store current state
as a backtrack point (in a stack to ensure depth-first.)</li><li>If terminal matches next input word, continue; else, backtrack <img loading=lazy src=/images/recursive_descent.png></li><li>Can be massively inefficient (exponential in sentence length) if faced with local ambiguity</li><li>Can fall into infinite loop</li></ol><h3 id=cky-parsing>CKY Parsing<a hidden class=anchor aria-hidden=true href=#cky-parsing>#</a></h3><p>Well-formed substring table: For parsing, subproblems are analyses of substrings, memoized in well-formed substring table(WFST, chart).</p><ul><li>Chart entries are indexed by start and end positions in the sentence, and correspond to:<ul><li>either a complete constituent (sub-tree) spanning those positions (if working bottom-up),</li><li>or a prediction about what complete constituent might be found (if working top-down).</li></ul></li><li>The chart is a matrix where cell <code>[i, j]</code> holds information about the word span from position i to position j:<ul><li>The root node of any constituent(s) spanning those words</li><li>Pointers to its sub-constituents</li><li>(Depending on parsing method,) predictions about what
constituents might follow the substring.</li></ul></li></ul><p>Probability CKY parsing:
<img loading=lazy src=/images/CKY_proba1.png>
<img loading=lazy src=/images/CKY_proba2.png></p><h2 id=dependency-parsing>Dependency Parsing<a hidden class=anchor aria-hidden=true href=#dependency-parsing>#</a></h2><ul><li>Motivation: context-free parsing algorithms base their decisions on adjacency; in a dependency structure, a dependent need not be adjacent to its head (even if the structure is projective); we need new parsing algorithms to deal with non-adjacency (and with non-projectivity if present).</li><li>Approach: Transition-based dependency parsing</li></ul><h3 id=dependency-syntax>Dependency Syntax<a hidden class=anchor aria-hidden=true href=#dependency-syntax>#</a></h3><p>Dependency structure shows which words depend on (modify or are arguments of) which other words.</p><ul><li>A fully lexicalized formalism without phrasal constituents and phrase-structure rules: binary, asymmetric grammatical relations between words.</li><li>More specific, head-dependent relations, with edges point from heads to their dependents.</li><li>Motivation: In languages with free word order, phrase structure (constituency) grammars don’t make as much sense. E.g. we may need both S → NP VP and S → VP NP, but could not tell too much information simply looking at the rule.</li><li>Dependencies: Identifies syntactic relations directly. The syntactic structure of a sentence is described solely in terms of the words (or lemmas) in a sentence and an associated set of directed binary grammatical relations that hold among the words.</li><li>Relation between phrase structure and dependency structure<ul><li>Convert phrase structure annotations to dependencies via head rules. (Convenient if we already have a phrase structure treebank.): For a given lexicalized constituency parse(CFG tree), remove the phrasal categories, remove the (duplicated) terminals, and collapse chains of duplicates.</li><li>The closure of dependencies give constituency from a dependency tree</li></ul></li></ul><p><img loading=lazy src=/images/Dependency_Relations.png></p><h3 id=transition-based-dependency-parsing>Transition-based Dependency Parsing<a hidden class=anchor aria-hidden=true href=#transition-based-dependency-parsing>#</a></h3><p>transition-based systems use supervised machine learning methods to train classifiers that play the role of the oracle. Given appropriate training data, these methods learn a function that maps from configurations to transition operators(actions).</p><ul><li>Bottom up</li><li>Like shift-reduce parsing, but the &lsquo;reduce&rsquo; actions are specialized to create dependencies with head on left or right.</li><li>configuration：consists of a stack, an input buffer of words or tokens, and a set of relations/arcs, a set of actions.</li><li>How to choose the next action: each action is predicted by a <a href=/posts/inf-course-note-accelerated-natural-language-processing/#discriminative-probability-models>discriminative classifier</a>(often SVM, could be maxent) over each legal move.<ul><li>features: a sequence of the correct (configuration, action) pairs <code>f(c ; x)</code>.</li></ul></li><li>Evaluation: accuracy (# correct dependencies with or ignore label)).</li></ul><h3 id=dependency-tree>Dependency Tree<a hidden class=anchor aria-hidden=true href=#dependency-tree>#</a></h3><ul><li>Dependencies from a CFG tree using heads, must be projective: There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words.</li><li>But dependency theory normally does allow non-projective structures to account for displaced constituents.</li></ul><h3 id=bounded-and-unbounded-dependencies>Bounded and Unbounded Dependencies<a hidden class=anchor aria-hidden=true href=#bounded-and-unbounded-dependencies>#</a></h3><p>Unbounded dependency could be considered as long distance dependency</p><ul><li>Long-distance dependencies: contained in wh-non-subject-question, &ldquo;What flights do you have from Burbank to Tacoma Washington?&rdquo;, the Wh-NP <code>what flights</code> is far away from the predicate that it is semantically related to, the main verb <code>have</code> in the VP.</li></ul><h2 id=noisy-channel-model>Noisy Channel Model:<a hidden class=anchor aria-hidden=true href=#noisy-channel-model>#</a></h2><ul><li>The intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel.</li><li>a probability model using Bayesian inference, input -> noisy/errorful encoding -> output, see an observation x (a misspelled word) and our job is to find the word w that generated this misspelled word.</li><li>$P(w|x) = P(x|w)\*P(w)/P(x)$</li></ul><p>Noisy channel model of spelling using <a href=/posts/inf-course-note-accelerated-natural-language-processing/#nb-training>naive bayes</a></p><ul><li>The noisy channel model is to maximize the product of likelihood(probability estimation) P(s|w) and the prior probability of correct words P(w). Intuitively it is modleing the noisy channel that turn a correct word &lsquo;w&rsquo; to the misspelling.</li><li>The likelihood(probability estimation) P(s|w) is called the the channel/error model, telling if it was the word &lsquo;w&rsquo;, how likely it was to generate this exact error.</li><li>The P(w) is called the language model</li></ul><h2 id=generative-vs-discriminative-models>Generative vs. Discriminative Models<a hidden class=anchor aria-hidden=true href=#generative-vs-discriminative-models>#</a></h2><blockquote><p>Generative(joint) models palce probabilities $p(c, d)$ over both observed data d and the hidden variables c (generate the obersved data from hidden stuff).</p></blockquote><blockquote><p>Discriminative(conditional) models take the data as given, and put a probability over hidden structure given the data, $p(c | d)$.</p></blockquote><p>在朴素贝叶斯与Logistic Regression, 以及HMM和CRF之间, 有生成式和判别式的区别.
<img loading=lazy src=/images/relationship_nbs_hmm_lr_crf.png title="Diagram of the relationship between naive Bayes, logistic regression, HMMs, linear-chain CRFs, generative models, and general CRFs. image from: An Introduction to Conditional Random Fields, by Charles Sutton and Andrew McCallum"></p><p>生成式模型描述标签向量y如何有概率地<strong>生成</strong>特征向量x, 即尝试构建x和y的联合分布$p(y, x)$, 典型的模型有N-Gram语言模型, 朴素贝叶斯模型（Naive Bayes）， 隐马尔科夫模型（HMM）, MRF。</p><p>而判别模型直接描述如何根据特征向量x判断其标签y, 即尝试构建$p(y | x)$的条件概率分布, 典型模型如如LR, SVM，CRF，MEMM等.</p><h3 id=exponential-models>Exponential Models<a hidden class=anchor aria-hidden=true href=#exponential-models>#</a></h3><p>It is a family, includes Log-linear, MaxEnt, Logistic Regression models.</p><p>Make probability model from the linear combination of weights λ and features f as votes, normalized by the total votes. <img loading=lazy src=/images/softmax.png></p><ul><li>It is a probabilistic distribution: it estimates a probability for each class/label, aka Softmax.</li><li>It is a classifier, deciding how to weight features, given data. choose the highest probability label.</li><li>Application: dependency parsing actions prediction, text classification, <a href=/posts/inf-course-note-accelerated-natural-language-processing/#word-sense-disambiguation>Word sense disambiguation</a>
<img loading=lazy src=/images/discriminative.png></li></ul><h3 id=training-discriminative-model>Training Discriminative Model<a hidden class=anchor aria-hidden=true href=#training-discriminative-model>#</a></h3><ul><li>Features in NLP are more general, they specify indicator function(a yes/no<code>[0,1]</code> boolean matching function) of properties of the input and each class.</li><li>Weights: low possibility features will associate with low/negative weight, vise versa.</li><li>Define features: Pick sets of data points d which are distinctive enough to deserve model parameters: related words, words contians #, words end with ing, etc.</li></ul><h3 id=regularization-in-discriminative-model>Regularization in Discriminative Model<a hidden class=anchor aria-hidden=true href=#regularization-in-discriminative-model>#</a></h3><p>The issue of scale:</p><ul><li>Lots of features</li><li>sparsity:<ul><li>easily overfitting: need smoothing</li><li>Many features seen in training never occur again in test</li></ul></li><li>Optimization problem: feature weights can be infinite, and iterative solvers can take a long time to get to those infinities. See <a href=http://www.inf.ed.ac.uk/teaching/courses/anlp/tutorials/anlp_t04-sol.pdf>tutorial 4</a>.</li><li>Solution:<ul><li>Early stopping</li><li>Smooth the parameter via L2 regularization.</li><li>Smooth the data, like the add alpha smoothing, but hard to know what artificial data to create</li></ul></li></ul><h2 id=morphology>Morphology<a hidden class=anchor aria-hidden=true href=#morphology>#</a></h2><blockquote><p>构词学（英语言学分科学名：morphology，“组织与形态”)，又称形态学，是语言学的一个分支，研究单词（word）的内部结构和其形成方式。如英语的dog、dogs和dog-catcher有相当的关系，英语使用者能够利用他们的背景知识来判断此关系，对他们来说，dog和dogs的关系就如同cat和cats，dog和dog-catcher就如同dish和dishwasher。构词学正是研究这种单字间组成的关系，并试着整理出其组成的规则。</p></blockquote><p><strong>Morphemes</strong>: The way words are built up from smaller meaning-bearing units.</p><p><strong>Lemma</strong>:</p><ul><li>Lexeme, refers to the set of all the forms that have the same meaning,</li><li>lemma: refers to the particular form that is chosen by convention to represent the lexeme.</li><li>E.g: <code>run, runs, ran, running</code> are forms of the same lexeme, with run as the lemma.</li></ul><p><strong>Affixes</strong>: Adding additional meanings of various kinds. &ldquo;+ed, un+&rdquo;</p><ul><li>suffix : follow the stem<ul><li>Plural of nouns &lsquo;cat+s&rsquo;</li></ul><ol start=2><li>Comparative and superlative of adjectives &lsquo;small+er&rsquo; </li><li>Formation of adverbs &lsquo;great+ly&rsquo;</li><li>Verb tenses &lsquo;walk+ed&rsquo; </li><li>All inflectional morphology in English uses suffixes</li></ol></li><li>Prefix: precede the stem<ul><li>In English: these typically change the meaning </li></ul><ol start=2><li>Adjectives &lsquo;un+friendly&rsquo;, &lsquo;dis+interested&rsquo;</li><li>Verbs &rsquo;re+consider&rsquo;</li><li>Some language use prefixing much more widely</li></ol></li><li>Infix: inserted inside the stem</li><li>Circumfix: do both(follow, precede)</li></ul><p><strong>Root</strong>, <strong>stem</strong> and <strong>base</strong> are all terms used in the literature to designate that part of a word that remains when all affixes have been removed.</p><ul><li>The root word is the primary lexical unit of a word, and of a word family (this root is then called the base word), which carries the most significant aspects of semantic content and cannot be reduced into smaller constituents.</li><li>E.g: In the form ‘untouchables’ the root is ‘touch’, to which first the suffix ‘-able’, then the prefix ‘un-‘ and finally the suffix ‘-s’ have been added. In a compound word like ‘wheelchair’ there are two roots, ‘wheel’ and ‘chair’.</li></ul><p>Stem is of concern only when dealing with inflectional morphology</p><ul><li>Stemming: reduce terms to their stems in info retrieval,</li><li>E.g: In the form ‘untouchables’ the stem is ‘untouchable’, ‘touched’ -> ‘touch’; ‘wheelchairs’ -> ‘wheelchair’.</li></ul><h3 id=morphological-parsing>Morphological Parsing<a hidden class=anchor aria-hidden=true href=#morphological-parsing>#</a></h3><p>Use Finite-state transducers, FST, a transducer maps between one representation and another; It is a kind of FSA which maps between two sets of symbols.</p><h3 id=inflectional-vs-derivationalmorphology>Inflectional vs. Derivational Morphology<a hidden class=anchor aria-hidden=true href=#inflectional-vs-derivationalmorphology>#</a></h3><p>Inflectional
· nouns for count (plural: +s) and for possessive case (+’s) 
· verbs for tense (+ed, +ing) and a special 3rd person singular present form (+s) 
· adjectives in comparative (+er) and superlative (+est) forms.</p><p>Derivational
· Changing the part of speech, e.g. noun to verb: &lsquo;word → wordify&rsquo;
· Changing the verb back to a noun
· Nominalization: formation of new nouns, often verbs or adjectives</p><table><thead><tr><th>Inflectional</th><th>Derivational</th></tr></thead><tbody><tr><td>does not change basic meaning or part of speech</td><td>may change the part of speech or meaning of a word</td></tr><tr><td>expresses grammatical features or relations between words</td><td>not driven by syntactic relations outside the word</td></tr><tr><td>applies to all words of the same part of speech, inflection occurs at word edges: govern+ment+s, centr+al+ize+d</td><td>applies closer to the stem</td></tr></tbody></table><h3 id=challenge-of-rich-morphology>Challenge of Rich Morphology<a hidden class=anchor aria-hidden=true href=#challenge-of-rich-morphology>#</a></h3><p>For a morphologically rich language, many issues would arise because of the morphological complexity.</p><ul><li>These productive word-formation processes result in a large vocabulary for these languages</li><li>Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages</li><li>For POS, augmentations become necessary when dealing with highly inflected or agglutinative languages with rich morphology like Czech, Hungarian and Turkish., part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a
single primitive tag.</li><li>Dependency grammar is better than constituency in dealing with languages that are morphologically rich。</li></ul><h2 id=linguistic-and-representational-concepts>Linguistic and Representational Concepts<a hidden class=anchor aria-hidden=true href=#linguistic-and-representational-concepts>#</a></h2><h3 id=parsing>Parsing<a hidden class=anchor aria-hidden=true href=#parsing>#</a></h3><ul><li>Parsing is a combination of recognizing an input string and assigning a <strong>correct</strong> linguistic structure/tree to it based on a grammar.</li><li>The Syntactic, Statistical parsing are constituent-based representations(context-free grammars).</li><li>The Dependency Parsing are based on dependency structure(dependency grammars).</li></ul><h3 id=syntactic-parsing>Syntactic Parsing<a hidden class=anchor aria-hidden=true href=#syntactic-parsing>#</a></h3><p>Syntactic parsing, is the task of recognizing a sentence and assigning a correct syntactic structure to it.</p><ul><li>Syntactic parsing can be viewed as a search<ul><li>search space: all possible trees generated by the grammar</li><li>search guided by the structure of the space and the input.</li><li>search direction<ul><li>top-down: start with root category (S), choose expansions, build down to words.</li><li>bottom-up: build subtrees over words, build up to S.</li></ul></li><li>Search algorithm/strategy: DFS, BFS, Recursive descent parsing, CKY Parsing</li></ul></li><li>Challenge: Structual <a href=/posts/inf-course-note-accelerated-natural-language-processing/#ambiguity>Ambiguity</a></li></ul><h3 id=statistical-parsing>Statistical Parsing<a hidden class=anchor aria-hidden=true href=#statistical-parsing>#</a></h3><p>Or probabilistic parsing, Build probabilistic models of syntactic knowledge and use some of this probabilistic knowledge to build efficient probabilistic parsers.</p><ul><li>motivation: to solve the problem of disambiguation</li><li>algorithm: <a href=/posts/inf-course-note-accelerated-natural-language-processing/#probability-cky-parsing>probability CKY parsing</a></li><li>evaluation: Compare the output <strong>constituency</strong> parser with golden standard tree, a constituent(part of the output parser) marked as correct if it spans the same sentence positions with the corresponding constituent in golder standard tree. Then we get the <a href=/posts/inf-course-note-accelerated-natural-language-processing/#precision-recall-f-measure>precision, recall and F1 measure</a>.<ul><li>constituency: S-(0:10), NP-(0:2), VP-(0:9)&mldr;</li><li>Precission = (# correct constituents)/(# in parser output), recall = (# correct constituents)/(# in gold standard)</li><li>Not a good evaluation, because it higher order constituent is marked wrong simply it contains a lower level wrong constituent.</li></ul></li></ul><h3 id=dependency-parsing-1><a href=/posts/inf-course-note-accelerated-natural-language-processing/#dependency-parsing>Dependency Parsing</a><a hidden class=anchor aria-hidden=true href=#dependency-parsing-1>#</a></h3><h3 id=constituency-1><a href=/posts/inf-course-note-accelerated-natural-language-processing/#constituency>Constituency</a><a hidden class=anchor aria-hidden=true href=#constituency-1>#</a></h3><h3 id=dependency><a href=/posts/inf-course-note-accelerated-natural-language-processing/#dependency-syntax>Dependency</a><a hidden class=anchor aria-hidden=true href=#dependency>#</a></h3><h3 id=morphology-1><a href=/posts/inf-course-note-accelerated-natural-language-processing/#morphology>Morphology</a><a hidden class=anchor aria-hidden=true href=#morphology-1>#</a></h3><h3 id=ambiguity>Ambiguity<a hidden class=anchor aria-hidden=true href=#ambiguity>#</a></h3><ul><li>Structural ambiguity: Occurs when the grammar can assign more than one parse to a sentence.</li><li>Attachment ambiguity: A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place.</li><li>Coordination ambiguity: different sets of phrases can be conjoined by a conjunction like and. E.g <code>green egg and bread</code>.<ul><li>Coordination: The major phrase types discussed here can be conjoined with conjunctions like <code>and, or, and but</code> to form larger constructions of the same type.</li></ul></li><li>Global and local ambiguity<ul><li>global ambiguity: multiple analyses for a full sentence, like <code>I saw the man with the telescope</code></li><li>local ambiguity: multiple analyses for parts of sentence.<ul><li><code>the dog bit the child</code>: first three words could be NP (but aren’t).</li><li>Building useless partial structures wastes time.</li></ul></li></ul></li></ul><h3 id=open-class-closed-class>Open-class Closed-class<a hidden class=anchor aria-hidden=true href=#open-class-closed-class>#</a></h3><p>Closed classes are those with relatively fixed membership</p><ul><li>prepositions: on, under, over, near, by, at, from, to, with</li><li>determiners: a, an, the</li><li>pronouns: she, who, I, others</li><li>conjunctions: and, but, or, as, if, when</li><li>auxiliary verbs: can, may, should, are</li><li>particles: up, down, on, off, in, out, at, by</li><li>numerals: one, two, three, first, second, third</li></ul><p>Open-class</p><ul><li>Nouns, verbs, adjectives, adverbs</li></ul><h2 id=word-sense>Word Sense<a hidden class=anchor aria-hidden=true href=#word-sense>#</a></h2><p>A discrete representation of an aspect of a word&rsquo;s meaning.
How: <a href=/posts/inf-course-note-accelerated-natural-language-processing/#distributional-semantic-models>Distributional semantic models</a></p><p><strong>Collocation</strong>: A sequence of words or terms that co-occur more often than would be expected by chance.</p><p><strong>Synonym</strong>: 代名词, When two senses of two different words (lemmas) are identical, or nearly identical, the two senses are synonyms. E.g. couch/sofa vomit/throw up filbert/hazelnut car/automobile</p><p><strong>Similarity</strong>: Or distance, a looser metric than synonymy.
Two ways to measure similarity:</p><ul><li>Thesaurus词库-based: are words nearby in hypernym hierarchy? Do words have similar definitions?</li><li>Distributional: do words have similar distributional contexts</li></ul><p><strong>Hyponym</strong>: 下义词, One sense is a hyponym of another sense if the first sense is more specific, denoting a subclass of the other. E.g. car is a hyponym of vehicle; dog is a hyponym of animal, and mango is a hyponym of fruit.</p><p><strong>Hypernym</strong>: Superordinate, 上位词, vehicle is a hypernym of car, and animal is a hypernym of dog.</p><h3 id=word-sense-disambiguation>Word Sense Disambiguation<a hidden class=anchor aria-hidden=true href=#word-sense-disambiguation>#</a></h3><p>WSD, The task of selecting the correct sense for a word, formulated as a classification task.</p><ul><li>Chose features: Directly neighboring words, content words, syntactically related words, topic of the text, part-of-speech tag, surrounding part-of-speech tags, etc &mldr;</li></ul><h3 id=distributional-semantic-models>Distributional Semantic Models<a hidden class=anchor aria-hidden=true href=#distributional-semantic-models>#</a></h3><p>Vector semantics(embeddings): The meaning of a word is represented as a vector.</p><ul><li>Two words are similar if they have similar word contexts vector.</li><li>Term-context matrix(Co-occurrence Matrices): a word/term is defined by a vector over counts of context words. The row represent words, columns contexts.<ul><li>Problem: simple frequency isn&rsquo;t the best measure of association between words. One problem is that raw frequency is very skewed and not very discriminative. “the” and “of” are very frequent, but maybe not the most discriminative.</li><li>Sulution: use <a href=/posts/inf-course-note-accelerated-natural-language-processing/#pointwise-mutual-information>Pointwise mutual information</a>. Then the Co-occurrence Matrices is filled with PPMI, instead of raw counts.</li></ul></li><li>Measuring vectors similarity based on PPMI:<ul><li>Dot product(inner product): More frequent words will have higher dot products, which cause similarity sensitive to word frequency.</li><li>Cosine: normalized dot product <img alt=Cosine loading=lazy src=/images/cos.png>, Raw frequency or PPMI is non-negative, so cosine range <code>[0,1]</code>.</li></ul></li><li>Evaluation of similarity<ul><li>Intrinsic: <a href=/posts/inf-course-note-accelerated-natural-language-processing/#correlation>correlation</a> between algorithm and human word similarity ratings.</li><li>Check if there is <a href=/posts/inf-course-note-accelerated-natural-language-processing/#correlation>correlation</a> between similarity measures and word frequency.</li></ul></li><li>Application: sentiment analysis, see <a href=http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab8.html>lab8</a></li></ul><h3 id=pointwise-mutual-information>Pointwise Mutual Information<a hidden class=anchor aria-hidden=true href=#pointwise-mutual-information>#</a></h3><p>PMI: do events x and y co-occur more than if they were independent?</p><ul><li>PMI between two words: $$PMI(w, c) = \log_2 \frac{P(w,c)}{P(W)P(c)}$$</li><li>Compute PMI on a term-context matrix(using counts): $$PMI(x, y) = log_2 \frac{N \times count(x, y)}{Count(x) Count(y)}$$</li></ul><pre tabindex=0><code>p(w=information, c=data) = 6/19
p(w=information) = 11/19
p(c=data) = 7/19
PMI(information,data) = log2(6*19/(11*7))
</code></pre><p><img alt=PMI loading=lazy src=/images/PMI_counts.png></p><ul><li>PMI is biased towards infrequent events, solution:<ul><li>Add-one smoothing</li></ul></li></ul><p><strong>PPMI</strong>: Positive PMI, could better handle low frequencies
<code>PPMI = max(PMI,0)</code></p><h3 id=t-test>T-test<a hidden class=anchor aria-hidden=true href=#t-test>#</a></h3><p>The t-test statistic, like PMI, can be used to measure how much
more frequent the association is than chance.</p><ul><li>The t-test statistic computes the difference between observed and expected means, normalized by the variance.</li><li>The higher the value of t, the greater the likelihood that we can reject the null hypothesis.</li><li>Null hypothesis: the two words are independent, and hence P(a,b) = P(a)P(b) correctly models the relationship between the two words.$$t\textrm{-}test(a,b) = \frac{P(a,b) - P(a)P(b)}{\sqrt{P(a)P(b)}}$$</li></ul><h3 id=minimum-edit-distance>Minimum Edit Distance<a hidden class=anchor aria-hidden=true href=#minimum-edit-distance>#</a></h3><p>the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.
Algorithm: searching the shortest path, use Dynamic programming to avoid repeating, (use BFS to search the shortest path?)</p><h3 id=wordnet><a href=http://wordnetweb.princeton.edu/perl/webwn>WordNet</a><a hidden class=anchor aria-hidden=true href=#wordnet>#</a></h3><p>A hierarchically organizesd lexical database, resource for English sense relations</p><ul><li>Synset: The set of near-synonyms for a WordNet sense (for synonym set)</li></ul><h3 id=word2vec>Word2Vec<a hidden class=anchor aria-hidden=true href=#word2vec>#</a></h3><h2 id=sentence-meaning-representation>Sentence Meaning Representation<a hidden class=anchor aria-hidden=true href=#sentence-meaning-representation>#</a></h2><p>我们假设语言表达具有意义表征，这些表征由用于表示常识的类型相同的东西组成。而创建这种表征并将其分配给输入的语言的任务，称为语义分析（Semantic Analysis）。The symbols in our meaning representations language (MRL) correspond to objects, properties, and relations in the world. ![](/images/meaning_representation.png &ldquo;A list of symbols, two directed graphs, and a record structure: a sampler of meaning representations for &ldquo;I have a car&rdquo;. image from: Speech and Language Processing&rdquo;)
上图展示了使用四种常用的MRL表达“I have a car”，第一行是First order logic，有向图和其文字信息是 Abstract Meaning Representation (AMR)，其余两种是Frame-Based 和 Slot-Filler。</p><p>Qualifications of MRL:</p><ul><li><strong>Canonical form</strong>: sentences with the same (literal) meaning should have the same MR.</li><li>Compositional: The meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined.</li><li><strong>Verifiable</strong>: Can use the MR of a sentence to determine the <strong>truth</strong> of the sentence with respect to some given <strong>model</strong>(knowledge base) of the world.</li><li><strong>Unambiguous</strong>: an MR should have exactly one interpretation.</li><li><strong>Inference and Variables</strong>: we should be able to verify sentences not only directly, but also by drawing conclusions based on the input MR and facts in the knowledge base.</li><li><strong>Expressiveness</strong>: the MRL should allow us to handle a wide range of meanings and express appropriate relationships between the words in a sentence.</li></ul><p><strong>Lexical semantics</strong>: the meaning of individual words.</p><p><strong>Lexical semantic relationships</strong>: Relations between word senses</p><h3 id=模型论>模型论<a hidden class=anchor aria-hidden=true href=#模型论>#</a></h3><p>从仅是正式的陈述到能够告诉我们世界某些事态的陈述，我们期望 meaning representations 弥合这种差距。而提供这种保证的依据就是<strong>模型</strong>。模型是一种正式的结构，可以代表真是世界的特定事态。</p><p>意义表达的词汇表包含两部分：</p><ol><li>非逻辑词汇表，由构成我们试图表达的世界的对象，属性和关系的开放式名称组成。如 谓语predicates, nodes, labels on links, or labels in slots in frames。</li><li>逻辑词汇表，由一组封闭的符号，运算符，量词，链接等组成，它们提供了用给定意义表示语言编写表达式的形式化方法。</li></ol><p>所有非逻辑词汇的元素都需要在模型中有一个表示（属于模型的固定的且定义明确的一部分）。
• <strong>对象 Objects</strong> denote elements of the domain
• <strong>属性 Properties</strong> denote sets of elements of the domain
• <strong>关系 Relations</strong> denote sets of **tuples of elements of the domain</p><h3 id=first-order-logic>First-order Logic<a hidden class=anchor aria-hidden=true href=#first-order-logic>#</a></h3><p>FOL, Predicate logic, meets all of the MRL qualifications <strong>except compositionality</strong>.<img loading=lazy src=/images/first_order_logic.png title="A context-free grammar specification of the syntax of First-Order Logic representations. image from: Speech and Language Processing"></p><ul><li>Term: represent objects.</li><li>Expressions are constructed from terms in three ways:<ul><li><strong>Constants</strong> in FOL refer to specific objects in the world being described. FOL constants refer to exactly one object. Objects can, however, have multiple constants that refer to them.</li><li><strong>Functions</strong> in FOL correspond to concepts that are often expressed in English as genitives(所有格) 如 &ldquo;Frasca’s location&rdquo;, 一般表达为<code>LocationOf(Frasca)</code>. Functions provide a convenient way to refer to specific objects without having to associate a named constant with them.</li><li><strong>Variables</strong>, 允许我们对对象做出断言和推理，而不必引用任何特定的命名对象。Make statements about anonymous objects: making statements about a particular unknown object and making statements about all the objects in some arbitrary world of objects.</li></ul></li></ul><p><strong>Predicate</strong>(谓语, 谓词, 宾词, 述语): symbols that represent properties of entities and relations between entities.</p><ul><li>Terms can be combined into predicate-argument structures. <code>Restaurant(Maharani)</code> 指明<code>Maharani</code>的属性是<code>Restaurant</code>.</li><li>Predicates with multiple arguments represent relations between entities: <code>member-of(UK, EU)</code></li><li><code>/N</code> to indicate that a predicate takes N arguments: <code>member-of/2</code></li></ul><p><strong>Logical connectives</strong>: create larger representations by conjoining logical formulas using one of three operators. <code>∨</code>(or), <code>∧</code>(and), <code>¬</code>(not), <code>⇒</code>(implies). &ldquo;I only have five dollars and I don’t have a lot of time.&rdquo;, <code>Have(Speaker,FiveDollars) ∧ ¬Have(Speaker,LotOfTime)</code></p><p>Variables and Quantifiers:</p><ul><li>Existential Quantifiers: (“there exists”), &ldquo;a restaurant that serves Mexican food near ICSI&rdquo; - <code>∃xRestaurant(x) ∧ Serves(x, MexicanFood) ∧ Near((LocationOf(x), LocationOf(ICSI))</code>, 头部的<code>∃</code>告诉我们如何解读句中的变量<code>x</code>: 要让句子为真, 那么对于变量<code>x</code>至少存在一个对象。</li><li>Universal Quantifier:<code>∀</code>(“for all”). &ldquo;All vegetarian restaurants serve vegetarian food.&rdquo; - <code>∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood)</code>.</li></ul><p>A predicate with a variable among its arguments only has a truth value if it is bound by a quantifier: ∀x.likes(x, Gim) has an interpretation as either true or false.</p><h3 id=lambda-notation>Lambda Notation<a hidden class=anchor aria-hidden=true href=#lambda-notation>#</a></h3><p>Extend FOL, to work with ‘partially constructed’ formula, with this form <code>λx.P(x)</code>.</p><p>λ-reduction: 应用于逻辑 term 以产生新的FOL表达式, 其中形参变量绑定到指定的term, 形式为<code>λx.P(x)(A)</code> -> <code>P(A)</code>. E.g.：<code>λx.sleep(x)(Marie)</code> -> <code>sleep(Marie)</code></p><ul><li>嵌套使用, Verbal (event) MRs：<code>λx.λy.Near(x,y)(Bacaro)</code> -> <code>λy.Near(Bacaro,y)</code>, <code>λz. λy. λx. Giving1(x,y,z) (book)(Mary)(John)</code> -> <code>Giving1(John, Mary, book)</code> -> <code>John gave Mary a book</code></li><li>Problem:<ul><li>fixed arguments</li><li>Requires separate <code>Giving</code> predicate for each syntactic subcategorisation frame(number/type/position of arguments).</li><li>Separate predicates have no logical relation: if <code>Giving3(a, b, c, d, e)</code> is true, what about <code>Giving2(a, b, c, d)</code> and <code>Giving1(a, b, c)</code>.</li></ul></li><li>Solution: Reification of events 事件具象化</li></ul><h3 id=inference>Inference<a hidden class=anchor aria-hidden=true href=#inference>#</a></h3><p>推断的两种思路, forward chaining 和 backward chaining.</p><p>forward chaining systems:
<strong>Modus ponens</strong>(if - then) states that if the left-hand side(antecedent) of an implication rule is true, then the right-hand side(consequent) of the rule can be inferred.
<code>VegetarianRestaurant(Leaf)</code>
<code>∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood)</code>
then
<code>Serves(Leaf ,VegetarianFood)</code>
随着单个事实被添加到知识库中，modus ponens用于触发所有适用的implication rules。优点是事实可以在被在需要时才在知识库中呈现，因为在某种意义上来说所有推断都是预先执行的。这可以大大减少后续的queries所需的时间，因为都应该是简单的查找。但缺点是那些永远用不到的事实也可能被推断和存储。</p><p>backward chaining:</p><ol><li>第一步是通过查看query公式是否存在知识库中, 来确认query是否为真。比如查询<code>Serves(Leaf ,VegetarianFood)</code>.</li><li>如果没有，则搜索知识库中存在的适用implication rules。对涉及到的antecedent递归运行backward chaining. 比如发动搜索适用规则，从而找到规则<code>∀xVegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood)</code>, 对于term <code>Leaf</code>而言, 对应的antecedent是<code>VegetarianRestaurant(Leaf)</code>, 存在于知识库中.</li></ol><p>Prolog 就是采用backward chaining 推断策略的编程语言.</p><p>虽然forward和backward推理是合理的，但两者都不完备。这意味着单独使用这些方法的系统无法找到有效的推论。完备的推理是<code>解析 resolution</code>, 但计算成本很高. 在实践中，大多数系统使用某种形式的chaining并把负担压到知识库开发人员去解码知识，以支持必要inference可以推论.</p><h3 id=reification-of-events>Reification of Events<a hidden class=anchor aria-hidden=true href=#reification-of-events>#</a></h3><p><code>John gave Mary a book -> ∃e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z)</code></p><ul><li>Reify: to “make real” or concrete, i.e., give events the same status as entities.</li><li>In practice, introduce variables for events, which we can quantify over</li><li>Entailment relations: automatically gives us logical entailment relations between events</li></ul><pre tabindex=0><code>[John gave Mary a book on Tuesday] -&gt; [John gave Mary a book]
∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z) ∧ Time(e, Tuesday)
-&gt;
∃ e, z. Giving(e) ∧ Giver(e, John) ∧ Givee(e, Mary) ∧ Given(e,z) ∧ Book(z)
</code></pre><h3 id=semantic-parsing>Semantic Parsing<a hidden class=anchor aria-hidden=true href=#semantic-parsing>#</a></h3><p>Aka semantic analysis. Systems for mapping from a text string to any logical form.</p><ul><li>Motivation: deriving a meaning representation from a sentence.</li><li>Application: question answering</li><li>Method: Syntax driven semantic analysis with semantic attachments</li></ul><h3 id=syntax-driven-semantic-analysis>Syntax Driven Semantic Analysis<a hidden class=anchor aria-hidden=true href=#syntax-driven-semantic-analysis>#</a></h3><ul><li>Principle of compositionality: the construction of constituent meaning is derived from/composed of the meaning of the constituents/words within that constituent, guided by word order and syntactic relations.</li><li>Build up the MR by augmenting CFG rules with semantic composition rules. Add semantic attachments to CFG rules.</li><li>Problem: encounter invalide FOL for some (base-form) MR, need type-raise.</li><li>Training</li></ul><h3 id=semantic-attachments>Semantic Attachments<a hidden class=anchor aria-hidden=true href=#semantic-attachments>#</a></h3><p>E.g</p><pre tabindex=0><code>VP → Verb NP : {Verb.sem(NP.sem)}
Verb.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y)
NP.sem = Meat
-&gt;
VP.sem = λy. λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, y) (Meat)
= λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat)
</code></pre><p>The MR for VP, is computed by applying the MR function to VP&rsquo;s children.</p><p>Complete the rule:</p><pre tabindex=0><code>S → NP VP : {VP.sem(NP.sem)}
VP.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat)
NP.sem = AyCaramba
-&gt;
S.sem = λx. ∃e. Serving(e) ∧ Server(e, x) ∧ Served(e, Meat) (AyCa.)
= ∃e. Serving(e) ∧ Server(e, AyCaramba) ∧ Served(e, Meat)
</code></pre><h3 id=abstract-meaning-representation>Abstract Meaning Representation<a hidden class=anchor aria-hidden=true href=#abstract-meaning-representation>#</a></h3><p>AMR是Rooted，带标签的digraph，易于人们阅读，易于程序处理。扩展了PropBank的Frames集合。AMR 的特点是将句子中的词抽象为概念，因而使最终的语义表达式与原始的句子没有直接的对应关系，对相同意思的不同句子能够抽象出相同的表达。</p><p>Knowledge-driven AMRL</p><ol><li>The Alexa Meaning Representation Language</li><li>World Knowledge for Abstract Meaning Representation Parsing：依赖于wordnet，而中文本身就是字组词，</li><li>Knowledge-driven Abstract Meaning Representation</li></ol><p>AMR is bias towards English.</p><h3 id=slot-fillerstemplate-fillingframe>Slot-fillers/Template-filling/Frame<a hidden class=anchor aria-hidden=true href=#slot-fillerstemplate-fillingframe>#</a></h3><p>在文档中找到此类情况并填写模板位置。这些空位填充符可以包含直接从文本中提取的文本段，也可以包括通过额外处理从文本元素中推断出的诸如时间，数量或本体实体之类的概念</p><p>许多文本包含事件的报告，以及可能的事件序列，这些报告通常对应于世界上相当普遍的刻板印象。这些抽象的情况或story，和 script（Schank and Abelson，1975）有关，由子事件，参与者及其角色的原型序列组成。类似的定义还有 Frame ( Minsky（1974），Hymes（1974）和Goffman（1974）大约在同一时间提出的一系列相关概念) 和 schemata（Bobrow and Norman，1975）。</p><h2 id=topic-modelling>Topic Modelling<a hidden class=anchor aria-hidden=true href=#topic-modelling>#</a></h2><p>假如知道有什么主题，或者对主题的数量和分布做出先验假设，此时可以使用监督学习, 如朴素贝叶斯分类, 把文章处理成词袋(bag of words). 但假如不知道这些先验呢?</p><p>就要依靠无监督学习, 比如聚类：Instead of using supervised topic classification – rather not fix topics in advance nor do manual annotation, Use clustering to teases out the topics. Only the number of topics is specified in advance.</p><p>这就是主题建模(Topic Modelling), 一种常用的文本挖掘方法，用于发现文本中的隐藏语义结构。此时主题数量就是一个超参数, 通过主题建模，构建了单词的clusters而不是文本的clusters。因此，文本被表达为多个主题的混合，每个主题都有一定的权重。</p><p>因为主题建模不再是用词频来表达, 而是用主题权重<code>{Topic_i: weight(Topic_i, T) for Topic_i in Topics}</code>, 所以主题建模也是一种 Dimensionality Reduction.</p><p>主题建模也可以理解为文本主题的tagging任务, 只是无监督罢了.</p><p>主题建模的算法:</p><ol><li>(p)LSA: (Probabilistic) Latent Semantic Analysis – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra. SVD假设了Gaussian distributed.</li><li>LDA: latent Dirichlet allocation, 假设了multimonial distribution。</li></ol><h3 id=lda>LDA<a hidden class=anchor aria-hidden=true href=#lda>#</a></h3><blockquote><p>LDA是pLSA的generalization, LDA的hyperparameter设为特定值的时候，就specialize成 pLSA 了。从工程应用价值的角度看，这个数学方法的generalization，允许我们用一个训练好的模型解释任何一段文本中的语义。而pLSA只能理解训练文本中的语义。（虽然也有ad hoc的方法让pLSA理解新文本的语义，但是大都效率低，并且并不符合pLSA的数学定义。）这就让继续研究pLSA价值不明显了。</p></blockquote><ul><li>Latent Dirichlet allocation(LDA): each document may be viewed as a mixture of various topics where each document is generated by LDA.</li><li>A topic is a distribution over words</li><li>generate document:<ol><li>Randomly choose a distribution over topics</li><li>For each word in the document<ol><li>randomly choose a topic from the distribution over topics</li><li>randomly choose a word from the corresponding topic (distribution over the vocabulary)</li></ol></li></ol></li><li>training: repeat until converge<ol><li>assign each word in each document to one of T topics.</li><li>For each document d, go through each word w in d and for each topic t, compute: p(t|d), P(w|t)</li><li>Reassign w to a new topic, where we choose topic t with probability P(w|t)xP(t|d)</li></ol></li><li>Inference: LDA没法做精确inference，只有近似算法，比如variational inference。</li></ul><p><a href=https://github.com/congchan/Chinese-nlp/blob/master/latent-dirichlet-allocation-topic-model.ipynb>LDA模型代码</a></p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><h3 id=extrinsic-evaluation>Extrinsic Evaluation<a hidden class=anchor aria-hidden=true href=#extrinsic-evaluation>#</a></h3><p>Use something external to measure the model. End-to-end evaluation, the best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves.</p><ol><li>Put each model in a task: spelling corrector, speech recognizer, MT system</li><li>Run the task, get an accuracy for A and for B<ul><li>How many misspelled words corrected properly</li><li>How many words translated correctly</li></ul></li><li>Compare accuracy for A and B</li></ol><p>Unfortunately, running big NLP systems end-to-end is often very expensive.</p><h3 id=intrinsic-evaluation>Intrinsic Evaluation<a hidden class=anchor aria-hidden=true href=#intrinsic-evaluation>#</a></h3><p>Measures independenly to any application. Train the parameters of both models on the training set, and then compare how well the two trained models fit the test set. Which means whichever model assigns a higher probability to the test set</p><h3 id=human-evaluation>Human Evaluation<a hidden class=anchor aria-hidden=true href=#human-evaluation>#</a></h3><p>E.g to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to
gold labels match. We will refer to these human labels as the <strong>gold labels</strong>.</p><h3 id=precision-recall-f-measure>Precision, Recall, F-measure<a hidden class=anchor aria-hidden=true href=#precision-recall-f-measure>#</a></h3><ul><li>To deal with unbalanced lables</li><li>Application: <a href=/posts/inf-course-note-accelerated-natural-language-processing/#text-classification>text classification</a>, parsing.</li><li>Evaluation in text classification: the 2 by 2 contingency table<img alt="contingency table" loading=lazy src=/images/Contingency.png>, golden lable is true or false, the classifier output is positive or negative.</li></ul><p>Precision: Percentage of positive items that are golden correct, from the view of classifier</p><p>Recall: Percentage of golden correct items that are positive, from the view of test set.</p><p>F-measure</p><ul><li>Motivation: there is tradeoff between precision and recall, so we need a combined meeasure that assesses the P/R tradeoff.</li><li>The b parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of b > 1 favor recall, while values of b &lt; 1 favor precision.</li><li>Balanced F1 measure with beta =1, F = 2PR/(P+R)</li></ul><h3 id=confusion-matrix>Confusion Matrix<a hidden class=anchor aria-hidden=true href=#confusion-matrix>#</a></h3><p>Recalled that confusion matrix&rsquo;s row represent golden label, column represent the classifier&rsquo;s output, to anwser the quesion：for any pair of classes(c1,c2), how many test sample from c1 were incorrectly assigned to c2</p><ul><li>Recall: Fraction of samples in $c_1$ classified correctly, $\frac{CM(c_1, c_1)}{\sum_jCM(c_1, j)}$</li><li>Precision: fraction of samples assigned $c_1$ that are actually $c_1$, $\frac{CM(c_1, c_1)}{\sum_iCM(i, c_1)}$</li><li>Accuracy: $\frac{\sum diagnal}{all}$</li></ul><h3 id=correlation>Correlation<a hidden class=anchor aria-hidden=true href=#correlation>#</a></h3><p>When two sets of data are strongly linked together we say they have a High Correlation.
Correlation is Positive when the values increase together, and Correlation is Negative when one value decreases as the other increases.</p><ul><li>Pearson correlation: covariance of the two variables divided by the product of their standard deviations.$$r = \frac{\sum_{i=1}^n(x_i - \overrightarrow{x})(y_i - \overrightarrow{y})}{\sqrt{\sum_{i=1}^n(x_i - \overrightarrow{x})^2} \sqrt{\sum_{i=1}^n(y_i - \overrightarrow{y})^2}}$$</li><li>Spearman correlation: the Pearson correlation between the rank values of the two variables</li></ul><h2 id=basic-text-processing>Basic Text Processing<a hidden class=anchor aria-hidden=true href=#basic-text-processing>#</a></h2><h3 id=regular-expressions>Regular Expressions<a hidden class=anchor aria-hidden=true href=#regular-expressions>#</a></h3><p>NLP工作必备技能(考试不需要).</p><p>一些练习Regular Expressions的有趣网站:
<a href=https://alf.nu/RegexGolf>https://alf.nu/RegexGolf</a>
<a href=https://regexr.com/>https://regexr.com/</a></p><h3 id=word-tokenization>Word Tokenization<a hidden class=anchor aria-hidden=true href=#word-tokenization>#</a></h3><p>NLP task needs to do text normalization:</p><ol><li>Segmenting/tokenizing words in running text</li><li>Normalizing word formats</li><li>Segmenting sentences in running text</li></ol><p><code>they lay back on the San Francisco grass and looked at the stars and their</code></p><ul><li>Type: an element of the vocabulary.</li><li>Token: an instance of that type in the actual text.</li></ul><p>英文比较简单.
中文有一个难点, 需要分词.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/nlp/>NLP</a></li><li><a href=https://congchan.github.io/tags/inf-course-note/>Inf Course Note</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/percolations-problem/><span class=title>« Prev</span><br><span>Percolations problem</span>
</a><a class=next href=https://congchan.github.io/posts/inf-course-note-natural-language-understanding/><span class=title>Next »</span><br><span>Inf Course Note - Natural Language Understanding</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Accelerated Natural Language Processing on x" href="https://x.com/intent/tweet/?text=Inf%20Course%20Note%20-%20Accelerated%20Natural%20Language%20Processing&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-accelerated-natural-language-processing%2f&amp;hashtags=NLP%2cInfCourseNote"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Accelerated Natural Language Processing on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-accelerated-natural-language-processing%2f&amp;title=Inf%20Course%20Note%20-%20Accelerated%20Natural%20Language%20Processing&amp;summary=Inf%20Course%20Note%20-%20Accelerated%20Natural%20Language%20Processing&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-accelerated-natural-language-processing%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Accelerated Natural Language Processing on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-accelerated-natural-language-processing%2f&title=Inf%20Course%20Note%20-%20Accelerated%20Natural%20Language%20Processing"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Accelerated Natural Language Processing on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-accelerated-natural-language-processing%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Accelerated Natural Language Processing on whatsapp" href="https://api.whatsapp.com/send?text=Inf%20Course%20Note%20-%20Accelerated%20Natural%20Language%20Processing%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-accelerated-natural-language-processing%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Accelerated Natural Language Processing on telegram" href="https://telegram.me/share/url?text=Inf%20Course%20Note%20-%20Accelerated%20Natural%20Language%20Processing&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-accelerated-natural-language-processing%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Accelerated Natural Language Processing on ycombinator" href="https://news.ycombinator.com/submitlink?t=Inf%20Course%20Note%20-%20Accelerated%20Natural%20Language%20Processing&u=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-accelerated-natural-language-processing%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>