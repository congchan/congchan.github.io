<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>BERT的Adam Weight Decay | Cong's Log</title><meta name=keywords content="Machine Learning"><meta name=description content="Adam Weight Decay in BERT
在看BERT(Devlin et al., 2019)的源码中优化器部分的实现时，发现有这么一段话
# Just adding the square of the weights to the loss function is *not*
# the correct way of using L2 regularization/weight decay with Adam,
# since that will interact with the m and v parameters in strange ways.
#
# Instead we want ot decay the weights in a manner that doesn't interact
# with the m/v parameters. This is equivalent to adding the square
# of the weights to the loss with plain (non-momentum) SGD.

其针对性地指出一些传统的Adam weight decay实现是错误的."><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/bert%E7%9A%84adam-weight-decay/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/bert%E7%9A%84adam-weight-decay/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/bert%E7%9A%84adam-weight-decay/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="BERT的Adam Weight Decay"><meta property="og:description" content="Adam Weight Decay in BERT 在看BERT(Devlin et al., 2019)的源码中优化器部分的实现时，发现有这么一段话
# Just adding the square of the weights to the loss function is *not* # the correct way of using L2 regularization/weight decay with Adam, # since that will interact with the m and v parameters in strange ways. # # Instead we want ot decay the weights in a manner that doesn't interact # with the m/v parameters. This is equivalent to adding the square # of the weights to the loss with plain (non-momentum) SGD. 其针对性地指出一些传统的Adam weight decay实现是错误的."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-03-03T00:00:00+00:00"><meta property="article:modified_time" content="2019-03-03T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="BERT的Adam Weight Decay"><meta name=twitter:description content="Adam Weight Decay in BERT
在看BERT(Devlin et al., 2019)的源码中优化器部分的实现时，发现有这么一段话
# Just adding the square of the weights to the loss function is *not*
# the correct way of using L2 regularization/weight decay with Adam,
# since that will interact with the m and v parameters in strange ways.
#
# Instead we want ot decay the weights in a manner that doesn't interact
# with the m/v parameters. This is equivalent to adding the square
# of the weights to the loss with plain (non-momentum) SGD.

其针对性地指出一些传统的Adam weight decay实现是错误的."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"BERT的Adam Weight Decay","item":"https://congchan.github.io/posts/bert%E7%9A%84adam-weight-decay/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"BERT的Adam Weight Decay","name":"BERT的Adam Weight Decay","description":"Adam Weight Decay in BERT 在看BERT(Devlin et al., 2019)的源码中优化器部分的实现时，发现有这么一段话\n# Just adding the square of the weights to the loss function is *not* # the correct way of using L2 regularization/weight decay with Adam, # since that will interact with the m and v parameters in strange ways. # # Instead we want ot decay the weights in a manner that doesn\u0026#39;t interact # with the m/v parameters. This is equivalent to adding the square # of the weights to the loss with plain (non-momentum) SGD. 其针对性地指出一些传统的Adam weight decay实现是错误的.\n","keywords":["Machine Learning"],"articleBody":"Adam Weight Decay in BERT 在看BERT(Devlin et al., 2019)的源码中优化器部分的实现时，发现有这么一段话\n# Just adding the square of the weights to the loss function is *not* # the correct way of using L2 regularization/weight decay with Adam, # since that will interact with the m and v parameters in strange ways. # # Instead we want ot decay the weights in a manner that doesn't interact # with the m/v parameters. This is equivalent to adding the square # of the weights to the loss with plain (non-momentum) SGD. 其针对性地指出一些传统的Adam weight decay实现是错误的.\n优化器回顾 先回顾一下几个优化器.\nSGD和动量更新 SGD在所有参数上均采用全局且均等的学习率。\n# Vanilla update x += - learning_rate * dx 加入动量更新Momentum update一般都能得到更好的收敛速。动量更新可以从优化问题的物理角度出发来理解。损失函数可以解释为丘陵地形的高度（因此也可以解释为势能，U = mgh , 势能正比于高度）。\n随机数初始化参数等效于在某个位置将初始速度设置为零。优化过程就等同于模拟参数矢量（即粒子）在损失函数的丘陵地形上滚动的过程。\n由于作用在粒子上的力与势能的梯度有关（即$F = - \\nabla U$），因此粒子所感受到的力正好是损失函数的（负）梯度。此外$F = ma$，因此（负）梯度在这个视角下和中与粒子的加速度成比例。因此梯度直接影响的是速度，由速度来影响位置.\n# Momentum update v = mu * v - learning_rate * dx # integrate velocity x += v # integrate position 动量mu（一般取0.9）虽然叫动量，但其物理意义更像是摩擦系数. 它会衰减速度并降低系统的动能，避免粒子一直在山底震荡无法停止. 也就是在梯度方向有所改变的维度上的衰减速度. 同时可以在梯度方向不变的维度上维持速度，这样就可以加快收敛并减小震荡。\nAdaGrad, RMSprop和Adam 我们希望优化器算法可以对每个参数自适应地调整学习率. AdaGrad(Duchi et al.)独立地适应模型的每个参数:\n# Assume the gradient dx and parameter vector x cache += dx**2 x += - learning_rate * dx / (np.sqrt(cache) + eps) 变量cache跟踪每个参数的梯度平方和。然后，将其用于element-wise地正则化参数更新。接收高梯度的权重将降低其有效学习率，而接收较小或不经常更新的权重将提高其有效学习率。 每个参数的学习率会缩放各参数反比于其历史梯度平方值总和的平方根.\nRMSprop(Tieleman \u0026 Hinton, 2012)优化器也是一种自适应学习率方法, 不过没发表, 都是引用 slide 29 of Lecture 6 of Geoff Hinton’s Coursera class.\nRMSProp对Adagrad进行如下调整:\ncache = decay_rate * cache + (1 - decay_rate) * dx**2 x += - learning_rate * dx / (np.sqrt(cache) + eps) 使用了梯度平方的移动平均值, 避免激进的单调递减的学习率。 decay_rate一般取[0.9, 0.99, 0.999].\nAdam (Kingma \u0026 Ba, 2014)可以看做动量法和RMSprop的结合, 结合了AdaGrad处理稀疏梯度的能力和RMSProp处理不平稳目标函数的能力。简化的实现:\nm = beta1*m + (1-beta1)*dx v = beta2*v + (1-beta2)*(dx**2) x += - learning_rate * m / (np.sqrt(v) + eps) 看起来与RMSProp更新完全相同，只是使用了渐变m的“平滑”版本而不是原始（且可能是嘈杂的）梯度dx。文章建议值为eps = 1e-8, beta1 = 0.9, beta2 = 0.999\n在MNIST数据上做的简单对比实验: 引用cs231的图: Adam Weight Decay 和 L2正则化 以前在训练语言模型时, 发现精调的SGD比Adam得到的最终效果更好. 可见Adam的优势并不如原来文章所言. 在2017年的论文《Fixing Weight Decay Regularization in Adam》(后来更新第三版为Decoupled Weight Decay Regularization, Loshchilov 2017)[#refer]中提出了Adam Weight Decay的方法用于修复Adam的权重衰减错误。问题在于目前大多数DL框架的L2 regularization实现用的是weight decay的方式，而weight decay在与Adam共同使用的时候有互相耦合。\nL2 regularization: 给参数加上一个L2惩罚 $$ f_{t}^{r e g}(\\mathbb{\\theta})=f_{t}(\\mathbb{\\theta})+\\frac{\\lambda^{\\prime}}{2}\\|\\mathbb{\\theta}\\|_{2}^{2} $$ 用程序表达是:\nfinal_loss = loss + weight_decay_r * all_weights.pow(2).sum() / 2 Hanson \u0026 Pratt (1988)的Weight decay让weight $\\theta$以$\\lambda$的速率指数衰减: $$ \\theta_{t+1}=(1-\\lambda) \\theta_{t}-\\alpha \\nabla f_{t}\\left(\\theta_{t}\\right), $$ 在vanilla SGD中用程序表达是:\nw = w - lr * w.grad - lr * weight_decay_r * w 大部分库都使用第一个实现。不过实际上几乎总是通过在梯度上添加 weight_decay_r * w来实现，而不是实际更改损失函数。）\n在标准SGD的情况下，通过对衰减系数做变换，令$\\lambda^{\\prime}=\\frac{\\lambda}{\\alpha}$, L2正则则等价于Weight Decay. 但是其他情况下, 比如增加了momentum后, L2正则化和权重衰减并不等价。\nboth mechanisms push weights closer to zero, at the same rate\nfast ai的代码解释是, 在momentum SGD中使用L2正则就需要把weight_decay_r * w加到梯度中. 但是梯度不是直接在weights中减去, 而是要通过移动平均\nmoving_avg = alpha * moving_avg + (1-alpha) * (w.grad + weight_decay_r*w) 该移动平均值再乘以学习率，然后从weights中减去.\n而权重衰减则是:\nmoving_avg = alpha * moving_avg + (1-alpha) * w.grad w = w - lr * moving_avg - lr * wd * w 很明显二者会不同的.\n在自适应优化器Adam中情况类似, 主要体现在以下二者:\nthe sums of the gradient of the loss function the gradient of the regularizer (i.e., the L2 norm of the weights) 红色是Adam+L2 regularization的方式，梯度$g_t$的移动平均 $m_t$ 与梯度平方的移动平均 $v_t$ 都加入了$\\lambda \\theta_{t- 1}$\n如何解释这种不同? 直接引用文章原文:\nwith decoupled weight decay, only the gradients of the loss function are adapted (with the weight decay step separated from the adaptive gradient mechanism)\nWith L2 regularization both types of gradients are normalized by their typical (summed) magnitudes, and therefore weights x with large typical gradient magnitude s are regularized by a smaller relative amount than other weights.\ndecoupled weight decay regularizes all weights with the same rate λ, effectively regularizing weights x with large s more than standard L2 regularization\nBERT源码中的apply_gradients给出了修正方法:\ndef apply_gradients(self, grads_and_vars, global_step=None, name=None): \"\"\"See base class.\"\"\" assignments = [] for (grad, param) in grads_and_vars: if grad is None or param is None: continue param_name = self._get_variable_name(param.name) m = tf.get_variable( name=param_name + \"/adam_m\", shape=param.shape.as_list(), dtype=tf.float32, trainable=False, initializer=tf.zeros_initializer()) v = tf.get_variable( name=param_name + \"/adam_v\", shape=param.shape.as_list(), dtype=tf.float32, trainable=False, initializer=tf.zeros_initializer()) # Standard Adam update. next_m = ( tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad)) next_v = ( tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2, tf.square(grad))) update = next_m / (tf.sqrt(next_v) + self.epsilon) # Just adding the square of the weights to the loss function is *not* # the correct way of using L2 regularization/weight decay with Adam, # since that will interact with the m and v parameters in strange ways. # # Instead we want ot decay the weights in a manner that doesn't interact # with the m/v parameters. This is equivalent to adding the square # of the weights to the loss with plain (non-momentum) SGD. if self._do_use_weight_decay(param_name): update += self.weight_decay_rate * param update_with_lr = self.learning_rate * update next_param = param - update_with_lr assignments.extend( [param.assign(next_param), m.assign(next_m), v.assign(next_v)]) return tf.group(*assignments, name=name) tensorflow v1 加入了修正, 但是后续的tf2就是很混乱找不到了.\nAdamWOptimizer = tf.contrib.opt.extend_with_decoupled_weight_decay(tf.train.AdamOptimizer) optimizer = AdamWOptimizer(weight_decay=weight_decay, learning_rate=deep_learning_rate) 参考资料 Devlin et al., 2019: https://github.com/google-research/BERT Duchi et al.: http://jmlr.org/papers/v12/duchi11a.html Tieleman \u0026 Hinton, 2012: csc321 Kingma \u0026 Ba, 2014: Adam: A Method for Stochastic Optimization cs231: https://cs231n.github.io/neural-networks-3/#sgd Wilson et al. (2017): Loshchilov 2017: Decoupled Weight Decay Regularization Hanson \u0026 Pratt (1988): Comparing biases for minimal network construction with back-propagation fast ai: https://www.fast.ai/2018/07/02/adam-weight-decay/ ","wordCount":"743","inLanguage":"en","datePublished":"2019-03-03T00:00:00Z","dateModified":"2019-03-03T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/bert%E7%9A%84adam-weight-decay/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">BERT的Adam Weight Decay</h1><div class=post-meta><span title='2019-03-03 00:00:00 +0000 UTC'>2019-03-03</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/ML-adam-weight-decay.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#adam-weight-decay-in-bert aria-label="Adam Weight Decay in BERT">Adam Weight Decay in BERT</a><ul><li><a href=#%e4%bc%98%e5%8c%96%e5%99%a8%e5%9b%9e%e9%a1%be aria-label=优化器回顾>优化器回顾</a><ul><li><a href=#sgd%e5%92%8c%e5%8a%a8%e9%87%8f%e6%9b%b4%e6%96%b0 aria-label=SGD和动量更新>SGD和动量更新</a></li><li><a href=#adagrad-rmsprop%e5%92%8cadam aria-label="AdaGrad, RMSprop和Adam">AdaGrad, RMSprop和Adam</a></li></ul></li></ul></li><li><a href=#adam-weight-decay-%e5%92%8c-l2%e6%ad%a3%e5%88%99%e5%8c%96 aria-label="Adam Weight Decay 和 L2正则化">Adam Weight Decay 和 L2正则化</a><ul><ul><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label=参考资料>参考资料</a></li></ul></ul></li></ul></div></details></div><div class=post-content><h1 id=adam-weight-decay-in-bert>Adam Weight Decay in BERT<a hidden class=anchor aria-hidden=true href=#adam-weight-decay-in-bert>#</a></h1><p>在看BERT(<a href=/posts/bert%E7%9A%84adam-weight-decay/#refer>Devlin et al., 2019</a>)的源码中优化器部分的实现时，发现有这么一段话</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Just adding the square of the weights to the loss function is *not*</span>
</span></span><span class=line><span class=cl><span class=c1># the correct way of using L2 regularization/weight decay with Adam,</span>
</span></span><span class=line><span class=cl><span class=c1># since that will interact with the m and v parameters in strange ways.</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Instead we want ot decay the weights in a manner that doesn&#39;t interact</span>
</span></span><span class=line><span class=cl><span class=c1># with the m/v parameters. This is equivalent to adding the square</span>
</span></span><span class=line><span class=cl><span class=c1># of the weights to the loss with plain (non-momentum) SGD.</span>
</span></span></code></pre></div><p>其针对性地指出一些传统的Adam weight decay实现是错误的.</p><h2 id=优化器回顾>优化器回顾<a hidden class=anchor aria-hidden=true href=#优化器回顾>#</a></h2><p>先回顾一下几个优化器.</p><h3 id=sgd和动量更新>SGD和动量更新<a hidden class=anchor aria-hidden=true href=#sgd和动量更新>#</a></h3><p>SGD在所有参数上均采用全局且均等的学习率。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Vanilla update</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>+=</span> <span class=o>-</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>dx</span>
</span></span></code></pre></div><p>加入<strong>动量更新Momentum update</strong>一般都能得到更好的收敛速。动量更新可以从优化问题的物理角度出发来理解。损失函数可以解释为丘陵地形的高度（因此也可以解释为势能，<code>U = mgh</code> , 势能正比于高度）。</p><p>随机数初始化参数等效于在某个位置将初始速度设置为零。优化过程就等同于模拟参数矢量（即粒子）在损失函数的丘陵地形上滚动的过程。</p><p>由于作用在粒子上的力与势能的梯度有关（即$F = - \nabla U$），因此粒子所感受到的力正好是损失函数的（负）梯度。此外$F = ma$，因此（负）梯度在这个视角下和中与粒子的加速度成比例。因此梯度直接影响的是速度，由速度来影响位置.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Momentum update</span>
</span></span><span class=line><span class=cl><span class=n>v</span> <span class=o>=</span> <span class=n>mu</span> <span class=o>*</span> <span class=n>v</span> <span class=o>-</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>dx</span> <span class=c1># integrate velocity</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>+=</span> <span class=n>v</span> <span class=c1># integrate position</span>
</span></span></code></pre></div><p>动量<code>mu</code>（一般取0.9）虽然叫动量，但其物理意义更像是摩擦系数. 它会衰减速度并降低系统的动能，避免粒子一直在山底震荡无法停止. 也就是在梯度方向有所改变的维度上的衰减速度. 同时可以在梯度方向不变的维度上维持速度，这样就可以加快收敛并减小震荡。</p><h3 id=adagrad-rmsprop和adam>AdaGrad, RMSprop和Adam<a hidden class=anchor aria-hidden=true href=#adagrad-rmsprop和adam>#</a></h3><p>我们希望优化器算法可以对每个参数自适应地调整学习率. AdaGrad(<a href=/posts/bert%E7%9A%84adam-weight-decay/#refer>Duchi et al.</a>)独立地适应模型的每个参数:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Assume the gradient dx and parameter vector x</span>
</span></span><span class=line><span class=cl><span class=n>cache</span> <span class=o>+=</span> <span class=n>dx</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>+=</span> <span class=o>-</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>dx</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>cache</span><span class=p>)</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span></code></pre></div><p>变量cache跟踪每个参数的梯度平方和。然后，将其用于element-wise地正则化参数更新。接收高梯度的权重将降低其有效学习率，而接收较小或不经常更新的权重将提高其有效学习率。
每个参数的学习率会缩放各参数反比于其历史梯度平方值总和的平方根.</p><p>RMSprop(<a href=/posts/bert%E7%9A%84adam-weight-decay/#refer>Tieleman & Hinton, 2012</a>)优化器也是一种自适应学习率方法, 不过没发表, 都是引用 slide 29 of Lecture 6 of Geoff Hinton’s Coursera class.</p><p>RMSProp对Adagrad进行如下调整:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>cache</span> <span class=o>=</span> <span class=n>decay_rate</span> <span class=o>*</span> <span class=n>cache</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>decay_rate</span><span class=p>)</span> <span class=o>*</span> <span class=n>dx</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>+=</span> <span class=o>-</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>dx</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>cache</span><span class=p>)</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span></code></pre></div><p>使用了梯度平方的移动平均值, 避免激进的单调递减的学习率。 <code>decay_rate</code>一般取<code>[0.9, 0.99, 0.999]</code>.</p><p><a href=/posts/bert%E7%9A%84adam-weight-decay/#refer>Adam (Kingma & Ba, 2014)</a>可以看做动量法和RMSprop的结合, 结合了AdaGrad处理稀疏梯度的能力和RMSProp处理不平稳目标函数的能力。简化的实现:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>beta1</span><span class=o>*</span><span class=n>m</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>beta1</span><span class=p>)</span><span class=o>*</span><span class=n>dx</span>
</span></span><span class=line><span class=cl><span class=n>v</span> <span class=o>=</span> <span class=n>beta2</span><span class=o>*</span><span class=n>v</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>beta2</span><span class=p>)</span><span class=o>*</span><span class=p>(</span><span class=n>dx</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>+=</span> <span class=o>-</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>m</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>v</span><span class=p>)</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span></code></pre></div><p>看起来与RMSProp更新完全相同，只是使用了渐变m的“平滑”版本而不是原始（且可能是嘈杂的）梯度dx。文章建议值为<code>eps = 1e-8, beta1 = 0.9, beta2 = 0.999</code></p><p>在MNIST数据上做的简单对比实验:
<img loading=lazy src=/images/optimizers1.png></p><p>引用<a href=/posts/bert%E7%9A%84adam-weight-decay/#refer>cs231</a>的图:
<img loading=lazy src=/images/optimizers.gif></p><h1 id=adam-weight-decay-和-l2正则化>Adam Weight Decay 和 L2正则化<a hidden class=anchor aria-hidden=true href=#adam-weight-decay-和-l2正则化>#</a></h1><p>以前在训练语言模型时, 发现精调的SGD比Adam得到的最终效果更好. 可见Adam的优势并不如原来文章所言. 在2017年的论文《Fixing Weight Decay Regularization in Adam》(后来更新第三版为Decoupled Weight Decay Regularization, Loshchilov 2017)[#refer]中提出了Adam Weight Decay的方法用于修复Adam的权重衰减错误。问题在于目前大多数DL框架的L2 regularization实现用的是weight decay的方式，而weight decay在与Adam共同使用的时候有互相耦合。</p><p>L2 regularization: 给参数加上一个L2惩罚</p>$$
f_{t}^{r e g}(\mathbb{\theta})=f_{t}(\mathbb{\theta})+\frac{\lambda^{\prime}}{2}\|\mathbb{\theta}\|_{2}^{2}
$$<p>用程序表达是:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>final_loss</span> <span class=o>=</span> <span class=n>loss</span> <span class=o>+</span> <span class=n>weight_decay_r</span> <span class=o>*</span> <span class=n>all_weights</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>/</span> <span class=mi>2</span>
</span></span></code></pre></div><p><a href=/posts/bert%E7%9A%84adam-weight-decay/#refer>Hanson & Pratt (1988)</a>的Weight decay让weight $\theta$以$\lambda$的速率指数衰减:</p>$$
\theta_{t+1}=(1-\lambda) \theta_{t}-\alpha \nabla f_{t}\left(\theta_{t}\right),
$$<p>在vanilla SGD中用程序表达是:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>w</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>w</span><span class=o>.</span><span class=n>grad</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>weight_decay_r</span> <span class=o>*</span> <span class=n>w</span>
</span></span></code></pre></div><p>大部分库都使用第一个实现。不过实际上几乎总是通过在梯度上添加 <code>weight_decay_r * w</code>来实现，而不是实际更改损失函数。）</p><p>在标准SGD的情况下，通过对衰减系数做变换，令$\lambda^{\prime}=\frac{\lambda}{\alpha}$, L2正则则等价于Weight Decay. 但是其他情况下, 比如增加了momentum后, L2正则化和权重衰减并不等价。</p><blockquote><p>both mechanisms push weights closer to zero, <strong>at the same rate</strong></p></blockquote><p><a href=/posts/bert%E7%9A%84adam-weight-decay/#refer>fast ai</a>的代码解释是, 在momentum SGD中使用L2正则就需要把<code>weight_decay_r * w</code>加到梯度中. 但是梯度不是直接在weights中减去, 而是要通过移动平均</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>moving_avg</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>moving_avg</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>w</span><span class=o>.</span><span class=n>grad</span> <span class=o>+</span> <span class=n>weight_decay_r</span><span class=o>*</span><span class=n>w</span><span class=p>)</span>
</span></span></code></pre></div><p>该移动平均值再乘以学习率，然后从weights中减去.</p><p>而权重衰减则是:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>moving_avg</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>moving_avg</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=n>w</span><span class=o>.</span><span class=n>grad</span> 
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>w</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>moving_avg</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>wd</span> <span class=o>*</span> <span class=n>w</span>
</span></span></code></pre></div><p>很明显二者会不同的.</p><p>在自适应优化器Adam中情况类似, 主要体现在以下二者:</p><ol><li>the sums of the gradient of the loss function</li><li>the gradient of the regularizer (i.e., the L2 norm of the weights)</li></ol><p><img loading=lazy src=/images/adam_with_l2.png></p><p>红色是Adam+L2 regularization的方式，梯度$g_t$的移动平均 $m_t$ 与梯度平方的移动平均 $v_t$ 都加入了$\lambda \theta_{t- 1}$</p><p>如何解释这种不同? 直接引用文章原文:</p><blockquote><p>with decoupled weight decay, <strong>only the gradients of the loss function are adapted</strong> (with the weight decay step separated from the adaptive gradient mechanism)</p><p>With L2 regularization <strong>both types of gradients are normalized by their typical (summed) magnitudes</strong>, and therefore weights x with large typical gradient magnitude s are regularized by a smaller relative amount than other weights.</p><p>decoupled weight decay regularizes all weights with the same rate λ, effectively regularizing weights x with large s more than standard L2 regularization</p></blockquote><p>BERT源码中的<code>apply_gradients</code>给出了修正方法:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>apply_gradients</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>grads_and_vars</span><span class=p>,</span> <span class=n>global_step</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;See base class.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>assignments</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=n>grad</span><span class=p>,</span> <span class=n>param</span><span class=p>)</span> <span class=ow>in</span> <span class=n>grads_and_vars</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>grad</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=n>param</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=k>continue</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>param_name</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_variable_name</span><span class=p>(</span><span class=n>param</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>m</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=n>param_name</span> <span class=o>+</span> <span class=s2>&#34;/adam_m&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>shape</span><span class=o>=</span><span class=n>param</span><span class=o>.</span><span class=n>shape</span><span class=o>.</span><span class=n>as_list</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>trainable</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>initializer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>zeros_initializer</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=n>v</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=n>param_name</span> <span class=o>+</span> <span class=s2>&#34;/adam_v&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>shape</span><span class=o>=</span><span class=n>param</span><span class=o>.</span><span class=n>shape</span><span class=o>.</span><span class=n>as_list</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>trainable</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>initializer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>zeros_initializer</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Standard Adam update.</span>
</span></span><span class=line><span class=cl>    <span class=n>next_m</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>beta_1</span><span class=p>,</span> <span class=n>m</span><span class=p>)</span> <span class=o>+</span> <span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta_1</span><span class=p>,</span> <span class=n>grad</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>next_v</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>beta_2</span><span class=p>,</span> <span class=n>v</span><span class=p>)</span> <span class=o>+</span> <span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta_2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                  <span class=n>tf</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>grad</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>update</span> <span class=o>=</span> <span class=n>next_m</span> <span class=o>/</span> <span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>next_v</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Just adding the square of the weights to the loss function is *not*</span>
</span></span><span class=line><span class=cl>    <span class=c1># the correct way of using L2 regularization/weight decay with Adam,</span>
</span></span><span class=line><span class=cl>    <span class=c1># since that will interact with the m and v parameters in strange ways.</span>
</span></span><span class=line><span class=cl>    <span class=c1>#</span>
</span></span><span class=line><span class=cl>    <span class=c1># Instead we want ot decay the weights in a manner that doesn&#39;t interact</span>
</span></span><span class=line><span class=cl>    <span class=c1># with the m/v parameters. This is equivalent to adding the square</span>
</span></span><span class=line><span class=cl>    <span class=c1># of the weights to the loss with plain (non-momentum) SGD.</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_do_use_weight_decay</span><span class=p>(</span><span class=n>param_name</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>update</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_decay_rate</span> <span class=o>*</span> <span class=n>param</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>update_with_lr</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>learning_rate</span> <span class=o>*</span> <span class=n>update</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>next_param</span> <span class=o>=</span> <span class=n>param</span> <span class=o>-</span> <span class=n>update_with_lr</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>assignments</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=n>param</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>next_param</span><span class=p>),</span>
</span></span><span class=line><span class=cl>          <span class=n>m</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>next_m</span><span class=p>),</span>
</span></span><span class=line><span class=cl>          <span class=n>v</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>next_v</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=o>*</span><span class=n>assignments</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=n>name</span><span class=p>)</span>
</span></span></code></pre></div><p>tensorflow v1 加入了修正, 但是后续的tf2就是很混乱找不到了.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>AdamWOptimizer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>opt</span><span class=o>.</span><span class=n>extend_with_decoupled_weight_decay</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>AdamOptimizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamWOptimizer</span><span class=p>(</span><span class=n>weight_decay</span><span class=o>=</span><span class=n>weight_decay</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=n>deep_learning_rate</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h3><div id=refer></div><ul><li>Devlin et al., 2019: <a href=https://github.com/google-research/BERT>https://github.com/google-research/BERT</a></li><li>Duchi et al.: <a href=http://jmlr.org/papers/v12/duchi11a.html>http://jmlr.org/papers/v12/duchi11a.html</a></li><li>Tieleman & Hinton, 2012: <a href=http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>csc321</a></li><li>Kingma & Ba, 2014: <a href=http://arxiv.org/abs/1412.6980>Adam: A Method for Stochastic Optimization</a></li><li>cs231: <a href=https://cs231n.github.io/neural-networks-3/#sgd>https://cs231n.github.io/neural-networks-3/#sgd</a></li><li>Wilson et al. (2017):</li><li>Loshchilov 2017: <a href=https://arxiv.org/abs/1711.05101v3>Decoupled Weight Decay Regularization</a></li><li>Hanson & Pratt (1988): Comparing biases for minimal network construction with back-propagation</li><li>fast ai: <a href=https://www.fast.ai/2018/07/02/adam-weight-decay/>https://www.fast.ai/2018/07/02/adam-weight-decay/</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/deep-q-networks/><span class=title>« Prev</span><br><span>Deep Q Networks</span>
</a><a class=next href=https://congchan.github.io/posts/word-lattice/><span class=title>Next »</span><br><span>Word Lattice</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT的Adam Weight Decay on x" href="https://x.com/intent/tweet/?text=BERT%e7%9a%84Adam%20Weight%20Decay&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fbert%25E7%259A%2584adam-weight-decay%2f&amp;hashtags=MachineLearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT的Adam Weight Decay on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fbert%25E7%259A%2584adam-weight-decay%2f&amp;title=BERT%e7%9a%84Adam%20Weight%20Decay&amp;summary=BERT%e7%9a%84Adam%20Weight%20Decay&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fbert%25E7%259A%2584adam-weight-decay%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT的Adam Weight Decay on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fbert%25E7%259A%2584adam-weight-decay%2f&title=BERT%e7%9a%84Adam%20Weight%20Decay"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT的Adam Weight Decay on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fbert%25E7%259A%2584adam-weight-decay%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT的Adam Weight Decay on whatsapp" href="https://api.whatsapp.com/send?text=BERT%e7%9a%84Adam%20Weight%20Decay%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fbert%25E7%259A%2584adam-weight-decay%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT的Adam Weight Decay on telegram" href="https://telegram.me/share/url?text=BERT%e7%9a%84Adam%20Weight%20Decay&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fbert%25E7%259A%2584adam-weight-decay%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT的Adam Weight Decay on ycombinator" href="https://news.ycombinator.com/submitlink?t=BERT%e7%9a%84Adam%20Weight%20Decay&u=https%3a%2f%2fcongchan.github.io%2fposts%2fbert%25E7%259A%2584adam-weight-decay%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>