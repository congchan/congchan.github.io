<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>深入理解word2vec | Cong's Log</title><meta name=keywords content="Python,Programming Language"><meta name=description content="Word2vec Mikolov et al.

How to represent meanings?
如何在数学上表达词义？
Vector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上语义相似的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于Distributional Hypothesis, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, Latent Semantic Analysis))和Predictive models(例如 neural net language models (NNLM))。
具体的区别详见Baroni et al.. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。
Word2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.).
从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words ('the cat sits on the')预测目标单词(例如&#34;mat&#34;); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。"><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3word2vec/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3word2vec/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3word2vec/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="深入理解word2vec"><meta property="og:description" content="Word2vec Mikolov et al.
How to represent meanings? 如何在数学上表达词义？
Vector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上语义相似的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于Distributional Hypothesis, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, Latent Semantic Analysis))和Predictive models(例如 neural net language models (NNLM))。
具体的区别详见Baroni et al.. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。
Word2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.).
从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words ('the cat sits on the')预测目标单词(例如&#34;mat&#34;); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-06-22T00:00:00+00:00"><meta property="article:modified_time" content="2018-06-22T00:00:00+00:00"><meta property="article:tag" content="Python"><meta property="article:tag" content="Programming Language"><meta name=twitter:card content="summary"><meta name=twitter:title content="深入理解word2vec"><meta name=twitter:description content="Word2vec Mikolov et al.

How to represent meanings?
如何在数学上表达词义？
Vector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上语义相似的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于Distributional Hypothesis, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, Latent Semantic Analysis))和Predictive models(例如 neural net language models (NNLM))。
具体的区别详见Baroni et al.. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。
Word2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.).
从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words ('the cat sits on the')预测目标单词(例如&#34;mat&#34;); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"深入理解word2vec","item":"https://congchan.github.io/posts/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3word2vec/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"深入理解word2vec","name":"深入理解word2vec","description":"Word2vec Mikolov et al.\nHow to represent meanings? 如何在数学上表达词义？\nVector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上语义相似的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于Distributional Hypothesis, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, Latent Semantic Analysis))和Predictive models(例如 neural net language models (NNLM))。\n具体的区别详见Baroni et al.. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。\nWord2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.).\n从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words ('the cat sits on the')预测目标单词(例如\u0026quot;mat\u0026quot;); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。\n","keywords":["Python","Programming Language"],"articleBody":"Word2vec Mikolov et al.\nHow to represent meanings? 如何在数学上表达词义？\nVector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上语义相似的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于Distributional Hypothesis, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, Latent Semantic Analysis))和Predictive models(例如 neural net language models (NNLM))。\n具体的区别详见Baroni et al.. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。\nWord2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.).\n从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words ('the cat sits on the')预测目标单词(例如\"mat\"); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。\nWord2vec的算法流程是：\nStart with a sentence: “the quick brown fox jumps.” Use a sliding window across the sentence to create (context, target) pairs, where the target is the center word, and the context is the surrounding words: ([the, brown], quick) ([quick, fox], brown) ([brown, jumps], fox) Use a lookup embedding layer to convert the context words into vectors, and average them to get a single input vector that represents the full context. Using the context vector as input into a fully connected Neural Network layer with a Softmax transformation. This results in a probability for every word in your entire vocabulary for being the correct target given the current context. Minimize the cross-entropy loss where label = 1 for the correct target word, and label = 0 for all other words. 优化目标函数 NNLM 的训练是利用 最大似然 maximum likelihood (ML) 原则来最大化给定上文单词\\(h\\) (for “history”) 预测下一个词的概率 \\(w_t\\) (for “target”)。\n$$ \\begin{align} P(w_t | h) \u0026= \\text{softmax}(\\text{score}(w_t, h)) \\\\\\\\ \u0026= \\frac{\\exp \\{ \\text{score}(w_t, h) \\} } {\\sum_\\text{Word w' in Vocab} \\exp \\{ \\text{score}(w', h) \\} } \\end{align} $$其中 \\(\\text{score}(w_t, h)\\) 计算 word \\(w_t\\) 和 context \\(h\\) 的相关性 (一般用点乘).\n训练时，最大化\n$$ \\begin{align} J_\\text{ML} \u0026= \\log P(w_t | h) \\\\\\\\ \u0026= \\text{score}(w_t, h) - \\log \\left( \\sum_\\text{Word w' in Vocab} \\exp \\{ \\text{score}(w', h) \\} \\right). \\end{align} $$这么计算成本很高， 因为在每一训练步，需要为词汇表 \\(V\\) 中的每一个词汇 \\(w’\\) 计算在当前上下文 \\(h\\) 的分数概率。\nNegative sampling 但是，word2vec的目的是特征学习，而不是学习完整的概率语言模型。所以word2vec（CBOW和Skip gram一样）的训练目标函数其实是一个二分类模型(logistic regression)，给定一个上下文，在 \\(k\\) 个噪声词（根据算法选出）和一个真正的目标词汇\\(w_t\\)中识别出目标词\\(w_t\\)。如下图(以CBOW为例, Skip gram方向反过来)\n目标函数变为最大化: $$ J_\\text{NEG} = \\log Q_\\theta(D=1 |w_t, h) + k \\mathop{\\mathbb{E}}_{\\tilde w \\sim P_n} \\left[ \\log Q_\\theta(D = 0 |\\tilde w, h) \\right] $$where \\(Q_\\theta(D=1 | w, h)\\) is the binary logistic regression probability under the model of seeing the word \\(w\\) in the context \\(h\\) in the dataset \\(D\\), calculated in terms of the learned embedding vectors \\(\\theta\\). In practice we approximate the expectation by drawing \\(k\\) contrastive words from the noise distribution (i.e. we compute a Monte Carlo average).\n负采样是指每个训练样本仅更新模型权重的一小部分：\n负采样的选择是基于 unigram 分布 $f(w_i)$: 一个词作为负面样本被选择的概率与其出现的频率有关，更频繁的词更可能被选作负面样本。 $$ P(w_i) = \\frac{ {f(w_i)}^{3/4} }{\\sum_{j=0}^{n}\\left( {f(w_j)}^{3/4} \\right) } $$ 负采样带来的好处是\n训练速度不再受限于 vocabulary size 能够并行实现 模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。 模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。 通过这种方式把学习P分布的无监督学习任务改造为监督学习。 每个词由两个向量表示：\n$v_w$, 表示这个词作为中心词 (Focus Word) 时的样子。 $u_w$, 表示它作为另一个中心词的上下文 (Context Word) 时的样子。 这样, 对于一个中心词 $c$ 和外围词$o$: $$ P(o|c) = \\frac{exp(u^T_o v_c)}{\\sum_{w \\in V} \\left( {exp(u^T_w v_c)} \\right)} $$在C语言的源代码里，这些向量由两个数组 (Array) 分别负责： syn0数组，负责某个词作为中心词时的向量。是随机初始化的。\n// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369 for (a = 0; a \u003c vocab_size; a++) for (b = 0; b \u003c layer1_size; b++) { next_random = next_random * (unsigned long long)25214903917 + 11; syn0[a * layer1_size + b] = (((next_random \u0026 0xFFFF) / (real)65536) - 0.5) / layer1_size; } syn1neg数组，负责这个词作为上下文时的向量。是零初始化的。\n// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L365 for (a = 0; a \u003c vocab_size; a++) for (b = 0; b \u003c layer1_size; b++) syn1neg[a * layer1_size + b] = 0; 训练时，先选出一个中心词。在正、负样本训练的时候，这个中心词就保持不变 (Constant) 了。\n# https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L226 # Negative sampling. sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler( true_classes=labels_matrix, num_true=1, num_sampled=opts.num_samples, unique=True, range_max=opts.vocab_size, distortion=0.75, unigrams=opts.vocab_counts.tolist())) Noise Contrastive Estimation Noise Contrastive Estimation （NCE）是一种通过比较数据分布和定义的噪声分布来学习数据分布的方法。在实现上与负采样非常相似，但它增加了一些理论上的依据。NCE方法中的精髓在于把难以计算的高维Sofmax多分类损失函数转化成为了更容易计算的二分类损失函数。\n使用Logistic Regression (LogReg) model对输入来自一个类而不是另一类的log-odds赔率比率进行建模： $$ logit=\\log \\left(\\frac{p_{1}}{p_{2}}\\right)=\\log \\left(\\frac{p_{1}}{1-p_{1}}\\right) $$如果替换为正样例P和负样例Q，将试图学习的数据分布与噪声分布进行比较，这就是NCE $$ \\operatorname{logit}=\\log \\left(\\frac{\\mathrm{P}}{\\mathrm{Q}}\\right)=\\log (\\mathrm{P})-\\log (\\mathrm{Q}) $$ 我们不知道真正的分布P，但我们可以自由地指定Q分布来生成负样本。例如，以相等的概率对所有词汇进行采样，或者以一种考虑到一个单词在训练数据中频率的方式进行采样。总之Q是由我们决定的，所以计算log(Q)部分是可以直接统计出来的。随机从我们指定的分布Q中抽取负样例。直接通过神经网络预测正样本为正，负样本为负的方式训练网络，仅计算正目标词的网络输出值以及我们从噪声分布中随机采样的单词，仅更新对应的权重。\n具体步骤前三步和上面一样 create the same (context, target) pairs, and average the embeddings of the context words to get a context vector.\nIn Step 4, you do the same thing as in Negative Sampling: use the context embedding vector as input to the neural network, and then gather the output for the target word and a random sample of k negative samples from the noise distribution, Q. For the network output of each of the selected words, $z_i$, subtract $log(Q)$, $y_i = z_i - log(Q_i)$ Instead of a Softmax transformation, apply a sigmoid transformation, as in Logistic Regression: $$ \\hat{p}_i=\\sigma\\left(y_i\\right)=\\frac{1}{1-e^{-y_i}} $$ Label the correct target word with label=1 and the negative samples with label=0. Use these as training samples for a Logistic Regression, and minimize the Binary Cross Entropy Loss: $$ BCE = \\frac{1}{N} \\sum_i^N l_i \\log (\\hat{p}_i) + (1 - l_i) \\log (1 - \\hat{p}_i) $$ $N = k+1$ (number of negative samples plus the target word), $l_i$ are the labels for if it’s the target or a negative sample, and Equation are the outputs of the sigmoid as defined above. 图表征学习 参考漫谈表征学习 - 张相於的文章 - 知乎\n互联网场景下常见的ID类特征有大量信息冗余。ID类特征本身含有的信息是非常少的，每一个维度就是一个ID，而每一个ID上本身是一个取值只有0或1的二元特征，并没有什么提炼汇总的空间，它的冗余主要体现在大量ID之间存在或强或弱的相关性。这种情况下，要相对这海量的ID进行降维有两大类思路：第一类思路是将ID进行归类，即将个体ID降维到类别ID，这里的典型代表是LDA这类的主题聚类方法；第二类是不直接进行ID归类，而是将ID投射到一个新的低维空间中，在这个空间中ID间可计算相似度，拥有更丰富的相似度空间，这类方法的典型代表是word2vec等序列学习的方法。\n实践中常用的图表征学习方法基本上都可以溯源到word2vec中的词向量训练方法，其中又以SGNS为主。\nSGNS方法通过序列构造训练样本的方式，通过负采样完成模型的高效求解。\n这套方法中模型只在样本中指明了不同节点之间应该是什么关系，应该亲密还是疏远，同时给出一组特征用来进行这种亲密度的判断，但有趣的地方在于就在这组特征：这里给的是一组完全随机初始化的，每个维度没有什么明确含义的特征，这和其他常用模型有着两点本质区别：每一维特征没有既定输入的特征值每一维特征没有明确的含义作为对比，我们在电商CTR模型中可能会用一维特征叫做价格，同时这个特征在每条样本上也会有明确的取值，但在SGNS这套方法中这两点都被打破了。\n如果说传统机器学习算法是给定特征值，学习特征权重，那么图表征学习就是在同时学习特征值和特征权重。但仔细一想，事实也并非完全如此，样本中也并非完全没有输入特征值，其实每条样本都有一个信息量高度聚集的输入特征值，那就是节点本身，或者说是节点ID，所以从某种角度来看，整个图表征学习的过程就是把节点ID这一信息量大，但却稀疏性高，缺乏节点间共享能力的特征，分解到了一组向量上面，使得这组向量能够还原原始节点ID所持有的信息量，而原始节点ID的信息全部通过样本中两两节点的关系来体现，所以学习得到的向量也能够体现原始节点的两两关系。更重要的是，这样分解后的向量在不同节点有了大量共享重合的维度，具有了很好的泛化能力。从这个角度来看，图表征学习就像是一个“打碎信息”的过程：将原本高度聚集在一个ID上的信息打碎之后分配在一个向量上，使得向量中分散的信息通过内积运算仍然能够最大程度还原原始ID的信息量。\nSGNS方法很好地解决了样本构造和海量softmax计算这两个图表征学习中最重要的问题，因此对后面的其他图表征学习算法都产生了很大的影响，像DeepWalk以及node2vec等方法，基本都保留SGNS方法的整体框架，只是在样本构造方式上做了优化。此外，SGNS算法还有比较强的调优空间，这主要体现在损失函数的构造上，negative sampling本质上是nce方法（noise contrastive estimate）的一种具体实现，而nce方法中的精髓在于他把难以计算的标准化多分类损失函数转化成为了更容易计算的二分类损失函数，这一精髓思想也在很多后续工作中得到了发扬.\n例如Airbnb发表在KDD 2018上的工作中，就把nce loss中负样本的部分进行了扩展:\nSGNS中标准的正负样本loss 另外两个项则是使用类似负采样的思想构造出来的，指明了想要把什么样的样本和什么样的样本区分开来。 指出用户点击过的房子；应该和最终预订的房子比较像， 用户点击的房子要和同区域内曝光未点击的房子比较不像。 相比于LDA用于表征学习\n以SGNS为代表的表征学习方法有着一些比较明显的优势。\n首先是与深度学习架构的原生兼容性。SGNS本质上就是一个特殊的浅层神经网络，其优化过程也是基于反向梯度传播的，所以SGNS得到的结果可以很方便地作为输入嫁接到一个更复杂的深度网络中继续训练，在之前结果上继续细粒度调优。\n其次是图表征学习方法捕捉到的信息要比LDA更加丰富，或者说加入了很多LDA这类方法没有包含的信息，最为典型的就是节点间的顺序信息以及局部图结构信息：LDA看待文档中的词是没有顺序和局部性的，只有文档和词的相互对应关系，但是图表征学习却非常看重这一信息，能够学习到局部顺序信息，这一点对于一些对于信息要求高的下游应用是很有用的。\n再次，以node2vec和deepwalk为代表的方法可以同时在同一个图结构中学到多种不同类型物品的表征，且这些物品在同一个语义空间中。例如可以同时将用户、物品甚至物品的属性放在一起进行学习，只要他们能够成一个图结构即可，物品和用户的表征是由于在训练过程中是无差别的，因此是在同一个语义空间中的，可以进行内积运算直接计算相似度。但LDA算法的理论基础决定了它一次只适合描述一种类型的物品，或者最多同时学习出文档和主题这两种类型的表示，而且即使这样，这两种表示也不在同一语义空间中，因此无法直接进行相关性运算，也无法当做同一组特征嵌入到其他模型中，在应用面和易用性方面相对受限。\n最后就是工具层面的影响，在计算机行业，最流行的方法不一定是理论上最先进的，但一定是工具最方便的，古今中外，概莫能外。在TensorFlow、Pytorch等工具流行之前，基本都是每个算法一个专用工具，典型的例如libsvm、liblinear等，一个算法如果没有一个高效易用的实现是很难被广泛应用的，包括word2vec的广泛应用也有很大一部分要归功于其工具的简单高效，而tf这些可微分编程工具的出现，使得新算法的开发和优化变得更加简单，那么像LDA这样没有落入到可微分编程范畴内的方法自然就相对受到冷落了。上面这些原因的存在，使得图表征学习成为了事实标准以及未来趋势，但LDA为代表的生成式表征学习方法仍然会在合适的场合继续发挥作用，并且也有可能会和可微分编程框架发生融合，焕发第二春。\n用TensorFlow实现word2vec Negative Sampling 可以利用原理近似的 noise-contrastive estimation (NCE) loss, 已经在TF的tf.nn.nce_loss()实现了.\nBuilding the graph 初始化一个在-1: 1之间随机均匀分布的矩阵\nembeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) NCE loss 依附于 logistic regression 模型。为此, 我们需要定义词汇中每个单词的weights和bias。\nnce_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) nce_biases = tf.Variable(tf.zeros([vocabulary_size])) 参数已经就位, 接下来定义模型图.\nThe skip-gram model 有两个输入. 一个是以word indice表达的一个batch的context words, 另一个是目标单词。为这些输入创建placeholder节点, 以便后续馈送数据。\n# Placeholders for inputs train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) 利用embedding_lookup来高效查找word indice对应的vector.\nembed = tf.nn.embedding_lookup(embeddings, train_inputs) 使用NCE作为训练目标函数来预测target word:\n# Compute the NCE loss, using a sample of the negative labels each time. loss = tf.reduce_mean( tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels=train_labels, inputs=embed, num_sampled=num_sampled, num_classes=vocabulary_size)) 然后添加计算梯度和更新参数等所需的节点。\n# We use the SGD optimizer. optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss) Training the model 使用feed_dict推送数据到placeholders, 调用tf.Session.run\nfor inputs, labels in generate_batch(...): feed_dict = {train_inputs: inputs, train_labels: labels} _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict) Evaluating the model Embeddings 对于 NLP 中的各种下游预测任务非常有用。可以利用analogical reasoning, 也就是预测句法和语义关系来简单而直观地评估embeddings, 如king is to queen as father is to ?\n# https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L292 def build_eval_graph(self): \"\"\"Build the eval graph.\"\"\" # Eval graph # Each analogy task is to predict the 4th word (d) given three # words: a, b, c. E.g., a=italy, b=rome, c=france, we should # predict d=paris. # The eval feeds three vectors of word ids for a, b, c, each of # which is of size N, where N is the number of analogies we want to # evaluate in one batch. analogy_a = tf.placeholder(dtype=tf.int32) # [N] analogy_b = tf.placeholder(dtype=tf.int32) # [N] analogy_c = tf.placeholder(dtype=tf.int32) # [N] # Normalized word embeddings of shape [vocab_size, emb_dim]. nemb = tf.nn.l2_normalize(self._emb, 1) # Each row of a_emb, b_emb, c_emb is a word's embedding vector. # They all have the shape [N, emb_dim] a_emb = tf.gather(nemb, analogy_a) # a's embs b_emb = tf.gather(nemb, analogy_b) # b's embs c_emb = tf.gather(nemb, analogy_c) # c's embs # We expect that d's embedding vectors on the unit hyper-sphere is # near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim]. target = c_emb + (b_emb - a_emb) # Compute cosine distance between each pair of target and vocab. # dist has shape [N, vocab_size]. dist = tf.matmul(target, nemb, transpose_b=True) # For each question (row in dist), find the top 4 words. _, pred_idx = tf.nn.top_k(dist, 4) # Nodes for computing neighbors for a given word according to # their cosine distance. nearby_word = tf.placeholder(dtype=tf.int32) # word id nearby_emb = tf.gather(nemb, nearby_word) nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True) nearby_val, nearby_idx = tf.nn.top_k(nearby_dist, min(1000, self._options.vocab_size)) Reference https://tensorflow.google.cn/tutorials/representation/word2vec Learning word embeddings efficiently with noise-contrastive estimation Learning word embeddings efficiently with noise-contrastive estimation http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ A Gentle Introduction to Noise Contrastive Estimation ","wordCount":"1181","inLanguage":"en","datePublished":"2018-06-22T00:00:00Z","dateModified":"2018-06-22T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3word2vec/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">深入理解word2vec</h1><div class=post-meta><span title='2018-06-22 00:00:00 +0000 UTC'>2018-06-22</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/word2vec.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#how-to-represent-meanings aria-label="How to represent meanings?">How to represent meanings?</a><ul><li><a href=#%e4%bc%98%e5%8c%96%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0 aria-label=优化目标函数>优化目标函数</a></li><li><a href=#negative-sampling aria-label="Negative sampling">Negative sampling</a></li><li><a href=#noise-contrastive-estimation aria-label="Noise Contrastive Estimation">Noise Contrastive Estimation</a></li></ul></li><li><a href=#%e5%9b%be%e8%a1%a8%e5%be%81%e5%ad%a6%e4%b9%a0 aria-label=图表征学习>图表征学习</a></li><li><a href=#%e7%94%a8tensorflow%e5%ae%9e%e7%8e%b0word2vec aria-label=用TensorFlow实现word2vec>用TensorFlow实现word2vec</a><ul><li><a href=#building-the-graph aria-label="Building the graph">Building the graph</a></li><li><a href=#training-the-model aria-label="Training the model">Training the model</a></li><li><a href=#evaluating-the-model aria-label="Evaluating the model">Evaluating the model</a></li></ul></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><p>Word2vec <a href=https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>Mikolov et al.</a></p><h2 id=how-to-represent-meanings>How to represent meanings?<a hidden class=anchor aria-hidden=true href=#how-to-represent-meanings>#</a></h2><p>如何在数学上表达词义？</p><p>Vector space models (VSMs) 表示把单词映射到(嵌入)连续的矢量空间, 而且理论上<strong>语义相似</strong>的单词会映射到空间中临近的位置。VSMs是一个历史悠久的NLP理论，但所有实现方法都不同程度依赖于<a href=https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis>Distributional Hypothesis</a>, 即出现在相同（相似）的上下文中的单词具有相同（相似）的语义意义。利用此原则的方法大致可以分为两类: Count-based methods (例如, <a href=https://en.wikipedia.org/wiki/Latent_semantic_analysis>Latent Semantic Analysis</a>))和Predictive models(例如 <a href=http://www.scholarpedia.org/article/Neural_net_language_models>neural net language models (NNLM)</a>)。</p><p>具体的区别详见<a href=http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf>Baroni et al.</a>. 但总的来说，Count-based methods 统计词汇间的共现频率，然后把co-occurs matrix 映射到向量空间中；而Predictive models直接通过上下文预测单词的方式来学习向量空间（也就是模型参数空间）。</p><p>Word2vec 是一种计算特别高效的predictive model, 用于从文本中学习word embeddings。它有两种方案, Continuous Bag-of-Words model (CBOW) 和 Skip-Gram model (Section 3.1 and 3.2 in <a href=https://arxiv.org/pdf/1301.3781.pdf>Mikolov et al.</a>).</p><p>从算法上讲, 两种方案是相似的, 只不过 CBOW 会从source context-words (<code>'the cat sits on the'</code>)预测目标单词(例如<code>"mat"</code>); 而skip-gram则相反, 预测目标单词的source context-words。Skip-gram这种做法可能看起来有点随意. 但从统计上看, CBOW 会平滑大量分布信息(通过将整个上下文视为一个观测值), 在大多数情况下, 这对较小的数据集是很有用的。但是, Skip-gram将每个context-target pair视为新的观测值, 当数据集较大时, 这往往带来更好的效果。</p><p>Word2vec的算法流程是：</p><ol><li>Start with a sentence: “the quick brown fox jumps.”</li><li>Use a sliding window across the sentence to create (context, target) pairs, where the target is the center word, and the context is the surrounding words:</li></ol><pre tabindex=0><code>([the, brown], quick)
([quick, fox], brown)
([brown, jumps], fox)
</code></pre><ol start=3><li>Use a lookup embedding layer to convert the context words into vectors, and average them to get a single input vector that represents the full context.</li><li>Using the context vector as input into a fully connected Neural Network layer with a Softmax transformation. This results in a probability for every word in your entire vocabulary for being the correct target given the current context.</li><li>Minimize the cross-entropy loss where label = 1 for the correct target word, and label = 0 for all other words.</li></ol><h3 id=优化目标函数>优化目标函数<a hidden class=anchor aria-hidden=true href=#优化目标函数>#</a></h3><p>NNLM 的训练是利用 <a href=https://en.wikipedia.org/wiki/Maximum_likelihood>最大似然 maximum likelihood</a> (ML) 原则来最大化给定上文单词\(h\) (for &ldquo;history&rdquo;) 预测下一个词的概率 \(w_t\) (for &ldquo;target&rdquo;)。</p>$$
\begin{align}
P(w_t | h) &= \text{softmax}(\text{score}(w_t, h)) \\\\
&= \frac{\exp \{ \text{score}(w_t, h) \} }
{\sum_\text{Word w' in Vocab} \exp \{ \text{score}(w', h) \} }
\end{align}
$$<p>其中 \(\text{score}(w_t, h)\) 计算 word \(w_t\) 和 context \(h\) 的相关性 (一般用点乘).</p><p>训练时，最大化</p>$$
\begin{align}
J_\text{ML} &= \log P(w_t | h) \\\\
&= \text{score}(w_t, h) -
\log \left( \sum_\text{Word w' in Vocab} \exp \{ \text{score}(w', h) \} \right).
\end{align}
$$<p>这么计算成本很高， 因为在每一训练步，需要为词汇表 \(V\) 中的每一个词汇 \(w&rsquo;\) 计算在当前上下文 \(h\) 的分数概率。</p><p><img loading=lazy src=https://tensorflow.google.cn/images/softmax-nplm.png></p><h3 id=negative-sampling>Negative sampling<a hidden class=anchor aria-hidden=true href=#negative-sampling>#</a></h3><p>但是，word2vec的目的是特征学习，而不是学习完整的概率语言模型。所以word2vec（CBOW和Skip gram一样）的训练目标函数其实是一个二分类模型(<a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>)，给定一个上下文，在 \(k\) 个噪声词（根据算法选出）和一个真正的目标词汇\(w_t\)中识别出目标词\(w_t\)。如下图(以CBOW为例, Skip gram方向反过来)</p><p><img loading=lazy src=https://tensorflow.google.cn/images/nce-nplm.png></p><p>目标函数变为最大化:</p>$$
J_\text{NEG} = \log Q_\theta(D=1 |w_t, h) + k \mathop{\mathbb{E}}_{\tilde w \sim P_n}
\left[ \log Q_\theta(D = 0 |\tilde w, h) \right]
$$<p>where \(Q_\theta(D=1 | w, h)\) is the binary logistic regression probability
under the model of seeing the word \(w\) in the context \(h\) in the dataset
\(D\), calculated in terms of the learned embedding vectors \(\theta\). In
practice we approximate the expectation by drawing \(k\) contrastive words
from the noise distribution (i.e. we compute a
<a href=https://en.wikipedia.org/wiki/Monte_Carlo_integration>Monte Carlo average</a>).</p><p>负采样是指每个训练样本仅更新模型权重的一小部分：</p><p>负采样的选择是基于 unigram 分布 $f(w_i)$: 一个词作为负面样本被选择的概率与其出现的频率有关，更频繁的词更可能被选作负面样本。</p>$$
P(w_i) = \frac{ {f(w_i)}^{3/4} }{\sum_{j=0}^{n}\left( {f(w_j)}^{3/4} \right) }
$$<p>负采样带来的好处是</p><ol><li>训练速度不再受限于 vocabulary size</li><li>能够并行实现</li><li>模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。
模型的表现更好。因为负采样契合NLP的稀疏性质，大部分情况下，虽然语料库很大，但是每一个词只跟很小部分词由关联，大部分词之间是毫无关联的，从无关联的两个词之间也别指望能学到什么有用的信息，不如直接忽略。</li><li>通过这种方式把学习P分布的无监督学习任务改造为监督学习。</li></ol><p>每个词由两个向量表示：</p><ol><li>$v_w$, 表示这个词作为中心词 (Focus Word) 时的样子。</li><li>$u_w$, 表示它作为另一个中心词的上下文 (Context Word) 时的样子。</li></ol><p>这样, 对于一个中心词 $c$ 和外围词$o$:</p>$$
P(o|c) = \frac{exp(u^T_o v_c)}{\sum_{w \in V} \left( {exp(u^T_w v_c)} \right)}
$$<p>在C语言的源代码里，这些向量由两个数组 (Array) 分别负责：
<code>syn0</code>数组，负责某个词作为中心词时的向量。是<strong>随机初始化</strong>的。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=n>a</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>a</span> <span class=o>&lt;</span> <span class=n>vocab_size</span><span class=p>;</span> <span class=n>a</span><span class=o>++</span><span class=p>)</span> <span class=k>for</span> <span class=p>(</span><span class=n>b</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>b</span> <span class=o>&lt;</span> <span class=n>layer1_size</span><span class=p>;</span> <span class=n>b</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>     <span class=n>next_random</span> <span class=o>=</span> <span class=n>next_random</span> <span class=o>*</span> <span class=p>(</span><span class=kt>unsigned</span> <span class=kt>long</span> <span class=kt>long</span><span class=p>)</span><span class=mi>25214903917</span> <span class=o>+</span> <span class=mi>11</span><span class=p>;</span>
</span></span><span class=line><span class=cl>     <span class=n>syn0</span><span class=p>[</span><span class=n>a</span> <span class=o>*</span> <span class=n>layer1_size</span> <span class=o>+</span> <span class=n>b</span><span class=p>]</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=p>(((</span><span class=n>next_random</span> <span class=o>&amp;</span> <span class=mh>0xFFFF</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>real</span><span class=p>)</span><span class=mi>65536</span><span class=p>)</span> <span class=o>-</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>/</span> <span class=n>layer1_size</span><span class=p>;</span>
</span></span><span class=line><span class=cl>   <span class=p>}</span>
</span></span></code></pre></div><p><code>syn1neg</code>数组，负责这个词作为上下文时的向量。是<strong>零初始化</strong>的。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L365
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=n>a</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>a</span> <span class=o>&lt;</span> <span class=n>vocab_size</span><span class=p>;</span> <span class=n>a</span><span class=o>++</span><span class=p>)</span> <span class=k>for</span> <span class=p>(</span><span class=n>b</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>b</span> <span class=o>&lt;</span> <span class=n>layer1_size</span><span class=p>;</span> <span class=n>b</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>syn1neg</span><span class=p>[</span><span class=n>a</span> <span class=o>*</span> <span class=n>layer1_size</span> <span class=o>+</span> <span class=n>b</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span></code></pre></div><p>训练时，先选出一个中心词。在正、负样本训练的时候，这个中心词就保持不变 (Constant) 了。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L226</span>
</span></span><span class=line><span class=cl><span class=c1># Negative sampling.</span>
</span></span><span class=line><span class=cl><span class=n>sampled_ids</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>fixed_unigram_candidate_sampler</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>true_classes</span><span class=o>=</span><span class=n>labels_matrix</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_true</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_sampled</span><span class=o>=</span><span class=n>opts</span><span class=o>.</span><span class=n>num_samples</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>unique</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>range_max</span><span class=o>=</span><span class=n>opts</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>distortion</span><span class=o>=</span><span class=mf>0.75</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>unigrams</span><span class=o>=</span><span class=n>opts</span><span class=o>.</span><span class=n>vocab_counts</span><span class=o>.</span><span class=n>tolist</span><span class=p>()))</span>
</span></span></code></pre></div><h3 id=noise-contrastive-estimation>Noise Contrastive Estimation<a hidden class=anchor aria-hidden=true href=#noise-contrastive-estimation>#</a></h3><p>Noise Contrastive Estimation （NCE）是一种通过比较数据分布和定义的噪声分布来学习数据分布的方法。在实现上与负采样非常相似，但它增加了一些理论上的依据。NCE方法中的精髓在于把难以计算的高维Sofmax多分类损失函数转化成为了更容易计算的二分类损失函数。</p><p>使用Logistic Regression (LogReg) model对输入来自一个类而不是另一类的log-odds赔率比率进行建模：</p>$$
logit=\log \left(\frac{p_{1}}{p_{2}}\right)=\log \left(\frac{p_{1}}{1-p_{1}}\right)
$$<p>如果替换为正样例P和负样例Q，将试图学习的数据分布与噪声分布进行比较，这就是NCE</p>$$
\operatorname{logit}=\log \left(\frac{\mathrm{P}}{\mathrm{Q}}\right)=\log (\mathrm{P})-\log (\mathrm{Q})
$$<p>我们不知道真正的分布P，但我们可以自由地指定Q分布来生成负样本。例如，以相等的概率对所有词汇进行采样，或者以一种考虑到一个单词在训练数据中频率的方式进行采样。总之Q是由我们决定的，所以计算log(Q)部分是可以直接统计出来的。随机从我们指定的分布Q中抽取负样例。直接通过神经网络预测正样本为正，负样本为负的方式训练网络，仅计算正目标词的网络输出值以及我们从噪声分布中随机采样的单词，仅更新对应的权重。</p><p>具体步骤前三步和上面一样 create the same (context, target) pairs, and average the embeddings of the context words to get a context vector.</p><ol><li>In Step 4, you do the same thing as in Negative Sampling: use the context embedding vector as input to the neural network, and then gather the output for the target word and a random sample of k negative samples from the noise distribution, Q.</li><li>For the network output of each of the selected words, $z_i$, subtract $log(Q)$, $y_i = z_i - log(Q_i)$</li><li>Instead of a Softmax transformation, apply a sigmoid transformation, as in Logistic Regression:
$$
\hat{p}_i=\sigma\left(y_i\right)=\frac{1}{1-e^{-y_i}}
$$</li><li>Label the correct target word with <code>label=1</code> and the negative samples with <code>label=0</code>.</li><li>Use these as training samples for a Logistic Regression, and minimize the Binary Cross Entropy Loss:
$$
BCE = \frac{1}{N} \sum_i^N l_i \log (\hat{p}_i) + (1 - l_i) \log (1 - \hat{p}_i)
$$
$N = k+1$ (number of negative samples plus the target word), $l_i$ are the labels for if it’s the target or a negative sample, and Equation are the outputs of the sigmoid as defined above.</li></ol><h2 id=图表征学习>图表征学习<a hidden class=anchor aria-hidden=true href=#图表征学习>#</a></h2><p>参考<a href=https://zhuanlan.zhihu.com/p/115072666>漫谈表征学习 - 张相於的文章 - 知乎</a></p><blockquote><p>互联网场景下常见的ID类特征有大量信息冗余。ID类特征本身含有的信息是非常少的，每一个维度就是一个ID，而每一个ID上本身是一个取值只有0或1的二元特征，并没有什么提炼汇总的空间，它的冗余主要体现在大量ID之间存在或强或弱的相关性。这种情况下，要相对这海量的ID进行降维有两大类思路：第一类思路是将ID进行归类，即将个体ID降维到类别ID，这里的典型代表是LDA这类的主题聚类方法；第二类是不直接进行ID归类，而是将ID投射到一个新的低维空间中，在这个空间中ID间可计算相似度，拥有更丰富的相似度空间，这类方法的典型代表是word2vec等序列学习的方法。</p></blockquote><p>实践中常用的图表征学习方法基本上都可以溯源到word2vec中的词向量训练方法，其中又以SGNS为主。</p><p>SGNS方法通过序列构造训练样本的方式，通过负采样完成模型的高效求解。</p><blockquote><p>这套方法中模型只在样本中指明了不同节点之间应该是什么关系，应该亲密还是疏远，同时给出一组特征用来进行这种亲密度的判断，但有趣的地方在于就在这组特征：这里给的是一组完全随机初始化的，每个维度没有什么明确含义的特征，这和其他常用模型有着两点本质区别：每一维特征没有既定输入的特征值每一维特征没有明确的含义作为对比，我们在电商CTR模型中可能会用一维特征叫做价格，同时这个特征在每条样本上也会有明确的取值，但在SGNS这套方法中这两点都被打破了。</p><p>如果说传统机器学习算法是给定特征值，学习特征权重，那么图表征学习就是在同时学习特征值和特征权重。但仔细一想，事实也并非完全如此，样本中也并非完全没有输入特征值，其实每条样本都有一个信息量高度聚集的输入特征值，那就是节点本身，或者说是节点ID，所以从某种角度来看，整个图表征学习的过程就是把节点ID这一信息量大，但却稀疏性高，缺乏节点间共享能力的特征，分解到了一组向量上面，使得这组向量能够还原原始节点ID所持有的信息量，而原始节点ID的信息全部通过样本中两两节点的关系来体现，所以学习得到的向量也能够体现原始节点的两两关系。更重要的是，这样分解后的向量在不同节点有了大量共享重合的维度，具有了很好的泛化能力。从这个角度来看，图表征学习就像是一个“打碎信息”的过程：将原本高度聚集在一个ID上的信息打碎之后分配在一个向量上，使得向量中分散的信息通过内积运算仍然能够最大程度还原原始ID的信息量。</p><p>SGNS方法很好地解决了样本构造和海量softmax计算这两个图表征学习中最重要的问题，因此对后面的其他图表征学习算法都产生了很大的影响，像DeepWalk以及node2vec等方法，基本都保留SGNS方法的整体框架，只是在样本构造方式上做了优化。此外，SGNS算法还有比较强的调优空间，这主要体现在损失函数的构造上，negative sampling本质上是nce方法（noise contrastive estimate）的一种具体实现，而nce方法中的精髓在于他把难以计算的标准化多分类损失函数转化成为了更容易计算的二分类损失函数，这一精髓思想也在很多后续工作中得到了发扬.</p></blockquote><p>例如Airbnb发表在KDD 2018上的工作中，就把nce loss中负样本的部分进行了扩展:</p><ol><li>SGNS中标准的正负样本loss</li><li>另外两个项则是使用类似负采样的思想构造出来的，指明了想要把什么样的样本和什么样的样本区分开来。<ol><li>指出用户点击过的房子；应该和最终预订的房子比较像，</li><li>用户点击的房子要和同区域内曝光未点击的房子比较不像。</li></ol></li></ol><p>相比于LDA用于表征学习</p><blockquote><p>以SGNS为代表的表征学习方法有着一些比较明显的优势。</p><p>首先是与深度学习架构的原生兼容性。SGNS本质上就是一个特殊的浅层神经网络，其优化过程也是基于反向梯度传播的，所以SGNS得到的结果可以很方便地作为输入嫁接到一个更复杂的深度网络中继续训练，在之前结果上继续细粒度调优。</p><p>其次是图表征学习方法捕捉到的信息要比LDA更加丰富，或者说加入了很多LDA这类方法没有包含的信息，最为典型的就是节点间的顺序信息以及局部图结构信息：LDA看待文档中的词是没有顺序和局部性的，只有文档和词的相互对应关系，但是图表征学习却非常看重这一信息，能够学习到局部顺序信息，这一点对于一些对于信息要求高的下游应用是很有用的。</p><p>再次，以node2vec和deepwalk为代表的方法可以同时在同一个图结构中学到多种不同类型物品的表征，且这些物品在同一个语义空间中。例如可以同时将用户、物品甚至物品的属性放在一起进行学习，只要他们能够成一个图结构即可，物品和用户的表征是由于在训练过程中是无差别的，因此是在同一个语义空间中的，可以进行内积运算直接计算相似度。但LDA算法的理论基础决定了它一次只适合描述一种类型的物品，或者最多同时学习出文档和主题这两种类型的表示，而且即使这样，这两种表示也不在同一语义空间中，因此无法直接进行相关性运算，也无法当做同一组特征嵌入到其他模型中，在应用面和易用性方面相对受限。</p><p>最后就是工具层面的影响，在计算机行业，最流行的方法不一定是理论上最先进的，但一定是工具最方便的，古今中外，概莫能外。在TensorFlow、Pytorch等工具流行之前，基本都是每个算法一个专用工具，典型的例如libsvm、liblinear等，一个算法如果没有一个高效易用的实现是很难被广泛应用的，包括word2vec的广泛应用也有很大一部分要归功于其工具的简单高效，而tf这些可微分编程工具的出现，使得新算法的开发和优化变得更加简单，那么像LDA这样没有落入到可微分编程范畴内的方法自然就相对受到冷落了。上面这些原因的存在，使得图表征学习成为了事实标准以及未来趋势，但LDA为代表的生成式表征学习方法仍然会在合适的场合继续发挥作用，并且也有可能会和可微分编程框架发生融合，焕发第二春。</p></blockquote><h2 id=用tensorflow实现word2vec>用TensorFlow实现word2vec<a hidden class=anchor aria-hidden=true href=#用tensorflow实现word2vec>#</a></h2><p>Negative Sampling 可以利用原理近似的 <a href=https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf>noise-contrastive estimation (NCE) loss</a>, 已经在TF的<a href=https://tensorflow.google.cn/api_docs/python/tf/nn/nce_loss>tf.nn.nce_loss()</a>实现了.</p><h3 id=building-the-graph>Building the graph<a hidden class=anchor aria-hidden=true href=#building-the-graph>#</a></h3><p>初始化一个在<code>-1: 1</code>之间随机均匀分布的矩阵</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>tf</span><span class=o>.</span><span class=n>random_uniform</span><span class=p>([</span><span class=n>vocabulary_size</span><span class=p>,</span> <span class=n>embedding_size</span><span class=p>],</span> <span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>))</span>
</span></span></code></pre></div><p>NCE loss 依附于 logistic regression 模型。为此, 我们需要定义词汇中每个单词的weights和bias。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>nce_weights</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>tf</span><span class=o>.</span><span class=n>truncated_normal</span><span class=p>([</span><span class=n>vocabulary_size</span><span class=p>,</span> <span class=n>embedding_size</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                      <span class=n>stddev</span><span class=o>=</span><span class=mf>1.0</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>embedding_size</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>nce_biases</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=n>vocabulary_size</span><span class=p>]))</span>
</span></span></code></pre></div><p>参数已经就位, 接下来定义模型图.</p><p>The skip-gram model 有两个输入. 一个是以word indice表达的一个batch的context words, 另一个是目标单词。为这些输入创建placeholder节点, 以便后续馈送数据。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Placeholders for inputs</span>
</span></span><span class=line><span class=cl><span class=n>train_inputs</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>batch_size</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>train_labels</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span></code></pre></div><p>利用<code>embedding_lookup</code>来高效查找word indice对应的vector.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embed</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>embedding_lookup</span><span class=p>(</span><span class=n>embeddings</span><span class=p>,</span> <span class=n>train_inputs</span><span class=p>)</span>
</span></span></code></pre></div><p>使用NCE作为训练目标函数来预测target word:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Compute the NCE loss, using a sample of the negative labels each time.</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>nce_loss</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=n>nce_weights</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>biases</span><span class=o>=</span><span class=n>nce_biases</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>labels</span><span class=o>=</span><span class=n>train_labels</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>inputs</span><span class=o>=</span><span class=n>embed</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>num_sampled</span><span class=o>=</span><span class=n>num_sampled</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>num_classes</span><span class=o>=</span><span class=n>vocabulary_size</span><span class=p>))</span>
</span></span></code></pre></div><p>然后添加计算梯度和更新参数等所需的节点。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># We use the SGD optimizer.</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>GradientDescentOptimizer</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=training-the-model>Training the model<a hidden class=anchor aria-hidden=true href=#training-the-model>#</a></h3><p>使用<code>feed_dict</code>推送数据到<code>placeholders</code>, 调用<code>tf.Session.run</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>generate_batch</span><span class=p>(</span><span class=o>...</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>feed_dict</span> <span class=o>=</span> <span class=p>{</span><span class=n>train_inputs</span><span class=p>:</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>train_labels</span><span class=p>:</span> <span class=n>labels</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=n>_</span><span class=p>,</span> <span class=n>cur_loss</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>([</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>loss</span><span class=p>],</span> <span class=n>feed_dict</span><span class=o>=</span><span class=n>feed_dict</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=evaluating-the-model>Evaluating the model<a hidden class=anchor aria-hidden=true href=#evaluating-the-model>#</a></h3><p>Embeddings 对于 NLP 中的各种下游预测任务非常有用。可以利用analogical reasoning, 也就是预测句法和语义关系来简单而直观地评估embeddings, 如<code>king is to queen as father is to ?</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># https://github.com/tensorflow/models/blob/8c7a0e752f9605d284b2f08a346fdc1d51935d75/tutorials/embedding/word2vec.py#L292</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_eval_graph</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Build the eval graph.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=c1># Eval graph</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Each analogy task is to predict the 4th word (d) given three</span>
</span></span><span class=line><span class=cl>  <span class=c1># words: a, b, c.  E.g., a=italy, b=rome, c=france, we should</span>
</span></span><span class=line><span class=cl>  <span class=c1># predict d=paris.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># The eval feeds three vectors of word ids for a, b, c, each of</span>
</span></span><span class=line><span class=cl>  <span class=c1># which is of size N, where N is the number of analogies we want to</span>
</span></span><span class=line><span class=cl>  <span class=c1># evaluate in one batch.</span>
</span></span><span class=line><span class=cl>  <span class=n>analogy_a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>  <span class=c1># [N]</span>
</span></span><span class=line><span class=cl>  <span class=n>analogy_b</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>  <span class=c1># [N]</span>
</span></span><span class=line><span class=cl>  <span class=n>analogy_c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>  <span class=c1># [N]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Normalized word embeddings of shape [vocab_size, emb_dim].</span>
</span></span><span class=line><span class=cl>  <span class=n>nemb</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>l2_normalize</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_emb</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Each row of a_emb, b_emb, c_emb is a word&#39;s embedding vector.</span>
</span></span><span class=line><span class=cl>  <span class=c1># They all have the shape [N, emb_dim]</span>
</span></span><span class=line><span class=cl>  <span class=n>a_emb</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>nemb</span><span class=p>,</span> <span class=n>analogy_a</span><span class=p>)</span>  <span class=c1># a&#39;s embs</span>
</span></span><span class=line><span class=cl>  <span class=n>b_emb</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>nemb</span><span class=p>,</span> <span class=n>analogy_b</span><span class=p>)</span>  <span class=c1># b&#39;s embs</span>
</span></span><span class=line><span class=cl>  <span class=n>c_emb</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>nemb</span><span class=p>,</span> <span class=n>analogy_c</span><span class=p>)</span>  <span class=c1># c&#39;s embs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># We expect that d&#39;s embedding vectors on the unit hyper-sphere is</span>
</span></span><span class=line><span class=cl>  <span class=c1># near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].</span>
</span></span><span class=line><span class=cl>  <span class=n>target</span> <span class=o>=</span> <span class=n>c_emb</span> <span class=o>+</span> <span class=p>(</span><span class=n>b_emb</span> <span class=o>-</span> <span class=n>a_emb</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Compute cosine distance between each pair of target and vocab.</span>
</span></span><span class=line><span class=cl>  <span class=c1># dist has shape [N, vocab_size].</span>
</span></span><span class=line><span class=cl>  <span class=n>dist</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>target</span><span class=p>,</span> <span class=n>nemb</span><span class=p>,</span> <span class=n>transpose_b</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># For each question (row in dist), find the top 4 words.</span>
</span></span><span class=line><span class=cl>  <span class=n>_</span><span class=p>,</span> <span class=n>pred_idx</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>top_k</span><span class=p>(</span><span class=n>dist</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Nodes for computing neighbors for a given word according to</span>
</span></span><span class=line><span class=cl>  <span class=c1># their cosine distance.</span>
</span></span><span class=line><span class=cl>  <span class=n>nearby_word</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>  <span class=c1># word id</span>
</span></span><span class=line><span class=cl>  <span class=n>nearby_emb</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>nemb</span><span class=p>,</span> <span class=n>nearby_word</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>nearby_dist</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>nearby_emb</span><span class=p>,</span> <span class=n>nemb</span><span class=p>,</span> <span class=n>transpose_b</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>nearby_val</span><span class=p>,</span> <span class=n>nearby_idx</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>top_k</span><span class=p>(</span><span class=n>nearby_dist</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                       <span class=nb>min</span><span class=p>(</span><span class=mi>1000</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>_options</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>))</span>
</span></span></code></pre></div><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ol><li><a href=https://tensorflow.google.cn/tutorials/representation/word2vec>https://tensorflow.google.cn/tutorials/representation/word2vec</a></li><li><a href=https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf>Learning word embeddings efficiently with noise-contrastive estimation</a></li><li>Learning word embeddings efficiently with noise-contrastive estimation</li><li><a href=http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/>http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></li><li><a href=http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/>http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li><li><a href="https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html#:~:text=Noise%20Contrastive%20Estimation%20is%20a%20way%20of%20learning,unsupervised%20problem%20as%20a%20supervised%20logistic%20regression%20problem.">A Gentle Introduction to Noise Contrastive Estimation</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/python/>Python</a></li><li><a href=https://congchan.github.io/tags/programming-language/>Programming Language</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/inf-course-note-software-testing/><span class=title>« Prev</span><br><span>Inf Course Note - Software Testing</span>
</a><a class=next href=https://congchan.github.io/posts/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><span class=title>Next »</span><br><span>循环神经网络</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 深入理解word2vec on x" href="https://x.com/intent/tweet/?text=%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3word2vec&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25B7%25B1%25E5%2585%25A5%25E7%2590%2586%25E8%25A7%25A3word2vec%2f&amp;hashtags=Python%2cProgrammingLanguage"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深入理解word2vec on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25B7%25B1%25E5%2585%25A5%25E7%2590%2586%25E8%25A7%25A3word2vec%2f&amp;title=%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3word2vec&amp;summary=%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3word2vec&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25B7%25B1%25E5%2585%25A5%25E7%2590%2586%25E8%25A7%25A3word2vec%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深入理解word2vec on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25B7%25B1%25E5%2585%25A5%25E7%2590%2586%25E8%25A7%25A3word2vec%2f&title=%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3word2vec"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深入理解word2vec on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25B7%25B1%25E5%2585%25A5%25E7%2590%2586%25E8%25A7%25A3word2vec%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深入理解word2vec on whatsapp" href="https://api.whatsapp.com/send?text=%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3word2vec%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25B7%25B1%25E5%2585%25A5%25E7%2590%2586%25E8%25A7%25A3word2vec%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深入理解word2vec on telegram" href="https://telegram.me/share/url?text=%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3word2vec&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25B7%25B1%25E5%2585%25A5%25E7%2590%2586%25E8%25A7%25A3word2vec%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深入理解word2vec on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3word2vec&u=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25B7%25B1%25E5%2585%25A5%25E7%2590%2586%25E8%25A7%25A3word2vec%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>