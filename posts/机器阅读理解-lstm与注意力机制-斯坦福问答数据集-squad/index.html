<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) | Cong's Log</title><meta name=keywords content="Attention,NLP,TensorFlow"><meta name=description content="本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.

注意力机制应用于阅读理解
The Standford question and answer dataset (SQuAD) 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。
由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。
但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。
虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用全局注意力机制中提到的

$$
\begin{aligned}
    score_{general}(t' t) &= s^\top_{t'} W_\alpha h_t, \\\
\end{aligned}
$$
就是用一个交互矩阵$W_\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 Bilinear."><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-lstm%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E6%96%AF%E5%9D%A6%E7%A6%8F%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86-squad/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-lstm%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E6%96%AF%E5%9D%A6%E7%A6%8F%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86-squad/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-lstm%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E6%96%AF%E5%9D%A6%E7%A6%8F%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86-squad/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)"><meta property="og:description" content="本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.
注意力机制应用于阅读理解 The Standford question and answer dataset (SQuAD) 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。
由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。
但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。
虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用全局注意力机制中提到的 $$ \begin{aligned} score_{general}(t' t) &= s^\top_{t'} W_\alpha h_t, \\\ \end{aligned} $$ 就是用一个交互矩阵$W_\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 Bilinear."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-07-20T00:00:00+00:00"><meta property="article:modified_time" content="2018-07-20T00:00:00+00:00"><meta property="article:tag" content="Attention"><meta property="article:tag" content="NLP"><meta property="article:tag" content="TensorFlow"><meta name=twitter:card content="summary"><meta name=twitter:title content="机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)"><meta name=twitter:description content="本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.

注意力机制应用于阅读理解
The Standford question and answer dataset (SQuAD) 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。
由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。
但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。
虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用全局注意力机制中提到的

$$
\begin{aligned}
    score_{general}(t' t) &= s^\top_{t'} W_\alpha h_t, \\\
\end{aligned}
$$
就是用一个交互矩阵$W_\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 Bilinear."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)","item":"https://congchan.github.io/posts/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-lstm%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E6%96%AF%E5%9D%A6%E7%A6%8F%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86-squad/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)","name":"机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)","description":"本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.\n注意力机制应用于阅读理解 The Standford question and answer dataset (SQuAD) 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。\n由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。\n但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。\n虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用全局注意力机制中提到的 $$ \\begin{aligned} score_{general}(t' t) \u0026= s^\\top_{t'} W_\\alpha h_t, \\\\\\ \\end{aligned} $$ 就是用一个交互矩阵$W_\\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 Bilinear.\n","keywords":["Attention","NLP","TensorFlow"],"articleBody":"本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.\n注意力机制应用于阅读理解 The Standford question and answer dataset (SQuAD) 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。\n由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。\n但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。\n虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用全局注意力机制中提到的 $$ \\begin{aligned} score_{general}(t' t) \u0026= s^\\top_{t'} W_\\alpha h_t, \\\\\\ \\end{aligned} $$ 就是用一个交互矩阵$W_\\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 Bilinear.\nclass Attention(object): def forwards_bilinear(self, hc, hq, hc_mask, hq_mask, max_context_length_placeholder, max_question_length_placeholder, is_train, keep_prob): '''combine context hidden state(hc) and question hidden state(hq) with global attention bilinear score = hc.T *W *hq ''' d_en = hc.get_shape().as_list()[-1] # (BS, MPL, MQL) interaction_weights = tf.get_variable(\"W_interaction\", shape=[d_en, d_en]) hc_W = tf.reshape(tf.reshape(hc, shape=[-1, d_en]) @ interaction_weights, shape=[-1, max_context_length_placeholder, d_en]) # (BS, MPL, HS * 2) @ (BS, HS * 2, MCL) -\u003e (BS ,MCL, MQL) score = hc_W @ tf.transpose(hq, [0, 2, 1]) # Create mask (BS, MPL) -\u003e (BS, MPL, 1) -\u003e (BS, MPL, MQL) hc_mask_aug = tf.tile(tf.expand_dims(hc_mask, -1), [1, 1, max_question_length_placeholder]) hq_mask_aug = tf.tile(tf.expand_dims(hq_mask, -2), [1, max_context_length_placeholder, 1]) hq_mask_aug = hc_mask_aug \u0026 hq_mask_aug score = softmax_mask_prepro(score, hq_mask_aug) # (BS, MPL, MQL) alignment_weights = tf.nn.softmax(score) # (BS, MPL, MQL) @ (BS, MQL, HS * 2) -\u003e (BS, MPL, HS * 2) context_aware = tf.matmul(alignment_weights, hq) concat_hidden = tf.concat([context_aware, hc], axis=2) concat_hidden = tf.cond(is_train, lambda: tf.nn.dropout(concat_hidden, keep_prob), lambda: concat_hidden) # (HS * 4, HS * 2) Ws = tf.get_variable(\"Ws\", shape=[d_en * 2, d_en]) attention = tf.nn.tanh(tf.reshape(tf.reshape(concat_hidden, [-1, d_en * 2]) @ Ws, [-1, max_context_length_placeholder, d_en])) return (attention) def _similarity_matrix(self, hq, hc, max_question_length, max_context_length, question_mask, context_mask, is_train, keep_prob): def _flatten(tensor, keep): fixed_shape = tensor.get_shape().as_list() start = len(fixed_shape) - keep # Calculate (BS * MCL * MQL) left = reduce(mul, [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start)]) # out_shape is simply HS * 2 out_shape = [left] + [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start, len(fixed_shape))] # (BS * MCL * MQL, HS * 2) flat = tf.reshape(tensor, out_shape) return (flat) def _reconstruct(tensor, ref, keep): ref_shape = ref.get_shape().as_list() tensor_shape = tensor.get_shape().as_list() ref_stop = len(ref_shape) - keep tensor_start = len(tensor_shape) - keep # [BS, MCL, MQL] pre_shape = [ref_shape[i] or tf.shape(ref)[i] for i in range(ref_stop)] # [1] keep_shape = [tensor_shape[i] or tf.shape(tensor)[i] for i in range(tensor_start, len(tensor_shape))] # pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))] # keep_shape = tensor.get_shape().as_list()[-keep:] # [BS, MCL, MQL, 1] target_shape = pre_shape + keep_shape out = tf.reshape(tensor, target_shape) out = tf.squeeze(out, [len(args[0].get_shape().as_list()) - 1]) return (out) # (BS, MCL, MQL, HS * 2) d = hq.get_shape().as_list()[-1] logging.debug(\"d is: {}\".format(d)) hc_aug = tf.tile(tf.reshape(hc, shape=[-1, max_context_length, 1, d]), [1, 1, max_question_length, 1]) # (BS, MCL, MQL, HS * 2) hq_aug = tf.tile(tf.reshape(hq, shape=[-1, 1, max_question_length, d]), [1, max_context_length, 1, 1]) # [(BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2)] args = [hc_aug, hq_aug, hc_aug * hq_aug] # [(BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2)] args_flat = [_flatten(arg, 1) for arg in args] args_flat = [tf.cond(is_train, lambda: tf.nn.dropout(arg, keep_prob), lambda: arg) for arg in args_flat] d_concat = d * 3 W = tf.get_variable(\"W\", shape=[d_concat, 1]) b = tf.get_variable(\"b\", shape=[1]) # Calculating a(h, u) = w_s^(t)[h; u; h * u] # (BS * MCL * MQL, HS * 6) @ (HS * 6, 1) + (1) -\u003e (BS * MCL * MQL, 1) res = tf.concat(args_flat, 1) @ W + b # (BS * MCL * MQL, 1) -\u003e (BS, MCL, MQL) similarity_matrix = _reconstruct(res, args[0], 1) logging.debug(\"similiarity_matrix after reconstruct: {}\".format(similarity_matrix.get_shape())) context_mask_aug = tf.tile(tf.expand_dims(context_mask, 2), [1, 1, max_question_length]) question_mask_aug = tf.tile(tf.expand_dims(question_mask, 1), [1, max_context_length, 1]) mask_aug = context_mask_aug \u0026 question_mask_aug similarity_matrix = softmax_mask_prepro(similarity_matrix, mask_aug) return (similarity_matrix) Bi-Directional Attention Flow lSeo et al. (2016)针对SQuAD提出了一个另一种更复杂的注意力机制, Bi-Directional Attention Flow (BiDAF)。 BiDAF顾名思义那个就是问题与段落的双向的注意力机制, 分别是 Context-to-query (C2Q) attention 和 Query-to-context (Q2C) attention. 两者都是基于传统的段落的背景向量 $H$ 与问题的背景向量 $U$ 间相似矩阵(similarity matrix) $S \\in \\mathbb{R^{T×J}}$衍生出来的. $$ S_{tj} = \\alpha(H_{:t}, U_{:j}) \\in R \\\\\\ \\alpha(h, u) = w^{\\top}_{(S)}[h; u; h \\odot u] $$ Where $S_{tj}$ indicates the similarity between t-th context word and j-th query word, $\\alpha$ is a trainable scalar function that encodes the similarity between its two input vectors, $H_{:t}$ is t-th column vector of H, and $U_{:j}$ is j-th column vector of U, $w_{(S)} \\in R^{6d}$ is a trainable weight vector, $[;]$ is vector concatenation across row.\n相似矩阵S被用于计算两种方向的注意力向量.\nContext-to-query (C2Q) attention signifies which query words are most relevant to each context word\n$$ \\tilde{U_{:t}} = \\sum_j \\alpha_{tj} U_{:j} \\\\\\ \\alpha_t = softmax(S_{t:}) $$ 其中 $\\alpha_t \\in R^J 表示$t$段落词对各个问题词的注意力权重\nQuery-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query.\n对段落的注意力权重为: $$ b = softmax(max_{col}(S)) \\in R^T $$ 其中$max_{col}$是在每行选出最大值. 然后对段落背景向量进行注意力加权: $$ \\tilde{h} = \\sum_t b_t H_{:t} \\in R^{2d} $$ 这个$\\tilde{h}$向量指的是在query眼里最重要的段落次的加权求和. 因为$\\tilde{h}$是在每一个内去最大值, 所以还需要从新把$\\tilde{h}$的值在每一个铺开$T$次得到一个$\\tilde{H} \\in R^{2dxT}$向量以方便后续的计算.\n最后, 段落的embeddings向量和注意力向量结合为$G$, $G$的每一列向量可以理解为每个段落词的 query-aware representation: $$ G_{:t} = \\beta(H_{:t}, \\tilde{U_{:t}}, \\tilde{H_{:t}}) \\in R^{d_G} $$ where $G_{:t}$ is the t-th column vector (corresponding to t-th context word), β is a trainable vector function that fuses its (three) input vectors, and $d_G$ is the output dimension of the β function.\nβ 函数可以是任意的神经网络, 但是文章中指出使用简单的函数如 $\\beta(h, \\tilde{u}, \\tilde{h}) = [h; \\tilde{u}; h \\odot \\tilde{u}; h \\odot \\tilde{h}] \\in R^{8dxT}$ (i.e., dG = 8d) 表现已经很好了。\nclass Attention(object): def forwards_complex(self, hc, hq, hc_mask, hq_mask, max_context_length_placeholder, max_question_length_placeholder, is_train, keep_prob): '''combine context hidden state(hc) and question hidden state(hq) with attention measured similarity = hc : hq : hc.T * hq ''' s = self._similarity_matrix(hq, hc, max_question_length_placeholder, max_context_length_placeholder, hq_mask, hc_mask, is_train, keep_prob) # C2Q # (BS, MCL, MQL) weights_c2q = tf.nn.softmax(s) # (BS, MCL, MQL) @ (BS, MQL, HS * 2) -\u003e (BS, MCL, HS * 2) query_aware = weights_c2q @ hq # Q2C # (BS, MCL, MQL) -\u003e (BS, MCL) # We are effectively looking through all the question words j's to some context word i and finding the # maximum of those context words score_q2c = tf.reduce_max(s, axis=-1) # (BS, MCL) weights_q2c = tf.expand_dims(tf.nn.softmax(score_q2c), -1) # (BS, HS) context_aware = tf.reduce_sum(tf.multiply(weights_q2c, hc), axis=1) # (BS, MCL, HS * 2) context_aware = tf.tile(tf.expand_dims(context_aware, 1), [1, max_context_length_placeholder, 1]) # [(BS, MCL, HS * 2), (BS, MCL, HS * 2), (BS, MCL, HS * 2), (BS, MCL, HS * 2)] biattention = tf.nn.tanh(tf.concat([hc, query_aware, hc * query_aware, hc * context_aware], 2)) return (biattention) def _similarity_matrix(self, hq, hc, max_question_length, max_context_length, question_mask, context_mask, is_train, keep_prob): def _flatten(tensor, keep): fixed_shape = tensor.get_shape().as_list() start = len(fixed_shape) - keep # Calculate (BS * MCL * MQL) left = reduce(mul, [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start)]) # out_shape is simply HS * 2 out_shape = [left] + [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start, len(fixed_shape))] # (BS * MCL * MQL, HS * 2) flat = tf.reshape(tensor, out_shape) return (flat) def _reconstruct(tensor, ref, keep): ref_shape = ref.get_shape().as_list() tensor_shape = tensor.get_shape().as_list() ref_stop = len(ref_shape) - keep tensor_start = len(tensor_shape) - keep # [BS, MCL, MQL] pre_shape = [ref_shape[i] or tf.shape(ref)[i] for i in range(ref_stop)] # [1] keep_shape = [tensor_shape[i] or tf.shape(tensor)[i] for i in range(tensor_start, len(tensor_shape))] # pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))] # keep_shape = tensor.get_shape().as_list()[-keep:] # [BS, MCL, MQL, 1] target_shape = pre_shape + keep_shape out = tf.reshape(tensor, target_shape) out = tf.squeeze(out, [len(args[0].get_shape().as_list()) - 1]) return (out) # (BS, MCL, MQL, HS * 2) d = hq.get_shape().as_list()[-1] logging.debug(\"d is: {}\".format(d)) hc_aug = tf.tile(tf.reshape(hc, shape=[-1, max_context_length, 1, d]), [1, 1, max_question_length, 1]) # (BS, MCL, MQL, HS * 2) hq_aug = tf.tile(tf.reshape(hq, shape=[-1, 1, max_question_length, d]), [1, max_context_length, 1, 1]) # [(BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2)] args = [hc_aug, hq_aug, hc_aug * hq_aug] # [(BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2)] args_flat = [_flatten(arg, 1) for arg in args] args_flat = [tf.cond(is_train, lambda: tf.nn.dropout(arg, keep_prob), lambda: arg) for arg in args_flat] d_concat = d * 3 W = tf.get_variable(\"W\", shape=[d_concat, 1]) b = tf.get_variable(\"b\", shape=[1]) # Calculating a(h, u) = w_s^(t)[h; u; h * u] # (BS * MCL * MQL, HS * 6) @ (HS * 6, 1) + (1) -\u003e (BS * MCL * MQL, 1) res = tf.concat(args_flat, 1) @ W + b # (BS * MCL * MQL, 1) -\u003e (BS, MCL, MQL) similarity_matrix = _reconstruct(res, args[0], 1) logging.debug(\"similiarity_matrix after reconstruct: {}\".format(similarity_matrix.get_shape())) context_mask_aug = tf.tile(tf.expand_dims(context_mask, 2), [1, 1, max_question_length]) question_mask_aug = tf.tile(tf.expand_dims(question_mask, 1), [1, max_context_length, 1]) mask_aug = context_mask_aug \u0026 question_mask_aug similarity_matrix = softmax_mask_prepro(similarity_matrix, mask_aug) return (similarity_matrix) 数据处理 内容段落摘自维基百科文章中的536篇文章，包含107,785对问题和答案，这使得SQuAD显着大于以前任何人类标注的数据集。在该数据集中，80％的数据用于训练，10％用于验证, 剩余10％用于测试。在训练集中，进一步划分出5％用于训练时的验证。\n与其他问答数据集相比，SQUAD具有比较独特的特征，所有答案都是出自相应的上下文中。对于每一个段落, 众包人员生成几个问题，并选择原段落中的一小段作为答案. 答案由两个index组成, 对应答案在段落中的起始位置。因此，SQuAD数据集的答案可能比其他以单个单词和实体为答案为主的数据集长得多。实例:\nQuestion: Why was Tesla returned to Gospic?\nContext paragraph: On 24 March 1879, Tesla was returned to Gospicunder police guard for not having a residence permit…\nAnswer: {12, 16}\nEmbedding 词向量使用预训练好的 Glove embedding.\nGlove is a log-bilinear regression model that combines the advantages of global matrix factorization and local context window methods.\ndef load_glove_embeddings(embed_path): logger.info(\"Loading glove embedding...\") glove = np.load(embed_path)['glove'] logger.info(\"Dimension: {}\".format(glove.shape[1])) logger.info(\"Vocabulary: {}\" .format(glove.shape[0])) return glove embeddings = load_glove_embeddings(embed_path) class Model(metaclass=ABCMeta): ... @abstractmethod def setup_embeddings(self): pass def setup_embeddings(self): \"\"\" Loads distributed word representations based on placeholder tokens :return: embeddings representaion of question and context. \"\"\" with tf.variable_scope(\"embeddings\"): if self.config.RE_TRAIN_EMBED: embeddings = tf.get_variable(\"embeddings\", initializer=self.embeddings) else: embeddings = tf.cast(self.embeddings, dtype=tf.float32) question_embeddings = tf.nn.embedding_lookup(embeddings, self.question_placeholder) question_embeddings = tf.reshape(question_embeddings, shape = [-1, self.max_question_length_placeholder, self.config.embedding_size]) context_embeddings = tf.nn.embedding_lookup(embeddings, self.context_placeholder) context_embeddings = tf.reshape(context_embeddings, shape = [-1, self.max_context_length_placeholder, self.config.embedding_size]) return question_embeddings, context_embeddings 模型 整体的模型由Embedding层，Encodr层，Attention层，Decoder层组成\nEncoder 编码器就是一个双向GRU层:\nclass Encoder(object): \"\"\" In a generalized encode function, you pass in your inputs, masks, and an initial hidden state input into this function. :param inputs: Symbolic representations of your input :param masks: this is to make sure tf.nn.dynamic_rnn doesn't iterate through masked steps :param encoder_state_input: (Optional) pass this as initial hidden state to tf.nn.dynamic_rnn to build conditional representations :return: outputs: The RNN output Tensor an encoded representation of your input. It can be context-level representation, word-level representation, or both. state: The final state. \"\"\" def __init__(self, state_size): self.state_size = state_size def encode(self, inputs, masks, initial_state_fw=None, initial_state_bw=None, reuse=False, keep_prob = 1.0): return BiGRU_layer(inputs, masks, self.state_size, initial_state_fw, initial_state_bw, reuse, keep_prob) def BiGRU_layer(inputs, masks, state_size, initial_state_fw=None, initial_state_bw=None, reuse = False, keep_prob=1.0): ''' Wrapped BiGRU_layer for reuse''' # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size] cell_fw = tf.contrib.rnn.GRUCell(state_size, reuse = reuse) cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob) cell_bw = tf.contrib.rnn.GRUCell(state_size, reuse = reuse) cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob) sequence_length = tf.reduce_sum(tf.cast(masks, 'int32'), axis=1) sequence_length = tf.reshape(sequence_length, [-1,]) # Outputs Tensor shaped: [batch_size, max_time, cell.output_size] (outputs_fw, outputs_bw), (final_state_fw, final_state_bw) = tf.nn.bidirectional_dynamic_rnn( cell_fw = cell_fw,\\ cell_bw = cell_bw,\\ inputs = inputs,\\ sequence_length = sequence_length, initial_state_fw = initial_state_fw,\\ initial_state_bw = initial_state_bw, dtype = tf.float32) outputs = tf.concat([outputs_fw, outputs_bw], 2) return outputs, final_state_fw, final_state_bw Decoder 解码器也包含一个双向GRU层，输出的状态分别由两个softmax分类器计算出预测的答案的 start 和 end index 位置:\nclass Decoder(object): \"\"\" takes in a knowledge representation and output a probability estimation over all paragraph tokens on which token should be the start of the answer span, and which should be the end of the answer span. :param knowledge_rep: it is a representation of the paragraph and question, decided by how you choose to implement the encoder :return: (start, end) \"\"\" def __init__(self, output_size, state_size=None): self.output_size = output_size self.state_size = state_size def decode(self, knowledge_rep, mask, max_input_length, keep_prob = 1.0): '''Decode with BiGRU''' with tf.variable_scope('Modeling'): outputs, _, _ = BiGRU_layer(knowledge_rep, mask, self.state_size, keep_prob=keep_prob) with tf.variable_scope(\"start\"): start = self.get_logit(outputs, max_input_length) start = softmax_mask_prepro(start, mask) with tf.variable_scope(\"end\"): end = self.get_logit(outputs, max_input_length) end = softmax_mask_prepro(end, mask) return (start, end) def get_logit(self, inputs, max_inputs_length): ''' Get the logit (-inf, inf). ''' d = inputs.get_shape().as_list()[-1] assert inputs.get_shape().ndims == 3, (\"Got {}\".format(inputs.get_shape().ndims)) inputs = tf.reshape(inputs, shape = [-1, d]) W = tf.get_variable('W', initializer=tf.contrib.layers.xavier_initializer(), shape=(d, 1), dtype=tf.float32) pred = tf.matmul(inputs, W) pred = tf.reshape(pred, shape = [-1, max_inputs_length]) tf.summary.histogram('logit', pred) return pred 搭建整个系统 在整个QASystem类中初始化这些功能层:\nclass QASystem(Model): def __init__(self, embeddings, config): \"\"\" Initializes System \"\"\" self.embeddings = embeddings self.config = config self.encoder = Encoder(config.encoder_state_size) self.decoder = Decoder(output_size=config.output_size, state_size = config.decoder_state_size) self.attention = Attention() # ==== set up placeholder tokens ======== self.context_placeholder = tf.placeholder(tf.int32, shape=(None, None)) self.context_mask_placeholder = tf.placeholder(tf.bool, shape=(None, None)) self.question_placeholder = tf.placeholder(tf.int32, shape=(None, None)) self.question_mask_placeholder = tf.placeholder(tf.bool, shape=(None, None)) self.answer_start_placeholder = tf.placeholder(tf.int32) self.answer_end_placeholder = tf.placeholder(tf.int32) self.max_context_length_placeholder = tf.placeholder(tf.int32) self.max_question_length_placeholder = tf.placeholder(tf.int32) self.dropout_placeholder = tf.placeholder(tf.float32) # ==== assemble pieces ==== with tf.variable_scope(self.config.which_model, initializer=tf.uniform_unit_scaling_initializer(1.0)): self.question_embeddings, self.context_embeddings = self.setup_embeddings() self.preds = self.setup_system() self.loss = self.setup_loss(self.preds) self.f1_train = tf.Variable(0., tf.float64) self.EM_train = tf.Variable(0., tf.float64) self.f1_val = tf.Variable(0., tf.float64) self.EM_val = tf.Variable(0., tf.float64) tf.summary.scalar('f1_train', self.f1_train) tf.summary.scalar('EM_train', self.EM_train) tf.summary.scalar('f1_val', self.f1_val) tf.summary.scalar('EM_val', self.EM_val) # ==== set up training/updating procedure ==== ''' With gradient clipping''' opt_op = get_optimizer(self.config.optimizer, self.loss, config.max_gradient_norm, config.learning_rate) if config.exdma_weight_decay is not None: self.train_op = self.build_exdma(opt_op) else: self.train_op = opt_op self.merged = tf.summary.merge_all() 把各个功能层搭建成一个完整的模型:\ndef setup_system(self): \"\"\" Connect all parts of your system here: After your modularized implementation of encoder and decoder you should call various functions inside encoder, decoder here to assemble your reading comprehension system! context: [None, max_context_length, d] question: [None, max_question_length, d] :return: \"\"\" d = self.context_embeddings.get_shape().as_list()[-1] '''Step 1: encode context and question, respectively, with independent weights e.g. hq = encode_question(question) # get U (d*J) as representation of q e.g. hc = encode_context(context, q_state) # get H (d*T) as representation of x ''' with tf.variable_scope('question'): hq, question_state_fw, question_state_bw = \\ self.encoder.BiGRU_encode(self.question_embeddings, self.question_mask_placeholder, keep_prob = self.dropout_placeholder) if self.config.QA_ENCODER_SHARE: hc, context_state_fw, context_state_bw =\\ self.encoder.BiGRU_encode(self.context_embeddings, self.context_mask_placeholder, initial_state_fw = question_state_fw, initial_state_bw = question_state_bw, reuse = True, keep_prob = self.dropout_placeholder) if not self.config.QA_ENCODER_SHARE: with tf.variable_scope('context'): hc, context_state_fw, context_state_bw =\\ self.encoder.BiGRU_encode(self.context_embeddings, self.context_mask_placeholder, initial_state_fw = question_state_fw, initial_state_bw = question_state_bw, keep_prob=self.dropout_placeholder) d_Bi = self.config.encoder_state_size*2 assert hc.get_shape().as_list() == [None, None, d_Bi], ( \"Expected {}, got {}\".format([None, self.max_context_length_placeholder, self.config.encoder_state_size], hc.get_shape().as_list())) assert hq.get_shape().as_list() == [None, None, d_Bi], ( \"Expected {}, got {}\".format([None, self.max_question_length_placeholder, self.config.encoder_state_size], hq.get_shape().as_list())) '''Step 2: combine context hidden state(hc) and question hidden state(hq) with attention measured similarity = hc.T * hq Context-to-query (C2Q) attention signifies which query words are most relevant to each P context word. attention_c2q = softmax(similarity) hq_hat = sum(attention_c2q*hq) Query-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query. attention_q2c = softmax(similarity.T) hc_hat = sum(attention_q2c*hc) combine with β activation: β function can be an arbitrary trainable neural network g = β(hc, hq, hc_hat, hq_hat) ''' # concat[h, u_a, h*u_a, h*h_a] attention = self.attention.forwards_bilinear(hc, hq, self.context_mask_placeholder, self.question_mask_placeholder, max_context_length_placeholder = self.max_context_length_placeholder, max_question_length_placeholder = self.max_question_length_placeholder, is_train=(self.dropout_placeholder \u003c 1.0), keep_prob=self.dropout_placeholder) d_com = d_Bi*4 '''Step 3: decoding ''' with tf.variable_scope(\"decoding\"): start, end = self.decoder.BiGRU_decode(attention, self.context_mask_placeholder, self.max_context_length_placeholder, self.dropout_placeholder) return start, end ","wordCount":"2383","inLanguage":"en","datePublished":"2018-07-20T00:00:00Z","dateModified":"2018-07-20T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-lstm%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E6%96%AF%E5%9D%A6%E7%A6%8F%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86-squad/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD)</h1><div class=post-meta><span title='2018-07-20 00:00:00 +0000 UTC'>2018-07-20</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/NLP-attention-02-lstm-reading-comprehension.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e5%ba%94%e7%94%a8%e4%ba%8e%e9%98%85%e8%af%bb%e7%90%86%e8%a7%a3 aria-label=注意力机制应用于阅读理解>注意力机制应用于阅读理解</a><ul><li><a href=#bi-directional-attention-flow aria-label="Bi-Directional Attention Flow">Bi-Directional Attention Flow</a></li></ul></li><li><a href=#%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86 aria-label=数据处理>数据处理</a><ul><li><a href=#embedding aria-label=Embedding>Embedding</a></li></ul></li><li><a href=#%e6%a8%a1%e5%9e%8b aria-label=模型>模型</a><ul><li><a href=#encoder aria-label=Encoder>Encoder</a></li><li><a href=#decoder aria-label=Decoder>Decoder</a></li><li><a href=#%e6%90%ad%e5%bb%ba%e6%95%b4%e4%b8%aa%e7%b3%bb%e7%bb%9f aria-label=搭建整个系统>搭建整个系统</a></li></ul></li></ul></div></details></div><div class=post-content><p>本文介绍注意力机制如何应用于阅读理解类任务, 并介绍了由此任务催生的一些注意力变种.</p><h2 id=注意力机制应用于阅读理解>注意力机制应用于阅读理解<a hidden class=anchor aria-hidden=true href=#注意力机制应用于阅读理解>#</a></h2><p>The Standford question and answer dataset <a href=https://rajpurkar.github.io/SQuAD-explorer/>(SQuAD)</a> 是由 Rajpurkar 等人提出的一个较有挑战性的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。SQuAD 和之前的完形填空类阅读理解数据集如 CNN/DM，CBT 等最大的区别在于：SQuAD 中的答案不在是单个实体或单词，而可能是一段短语，这使得其答案更难预测。SQuAD 包含公开的训练集和开发集，以及一个隐藏的测试集，其采用了与 ImageNet 类似的封闭评测的方式，研究人员需提交算法到一个开放平台，并由 SQuAD 官方人员进行测试并公布结果。</p><p>由于 SQuAD 的答案限定于来自原文，模型只需要判断原文中哪些词是答案即可，因此是一种抽取式的 QA 任务而不是生成式任务。简单的 SQuAD 的模型框架可以参考seq2seq：Embed 层，Encode 层 和 Decode 层。Embed 层负责将原文和问题中的 tokens 映射为向量表示；Encode 层主要使用 RNN 来对原文和问题进行编码，这样编码后每个 token 的向量表示就蕴含了上下文的语义信息；Decode 层则基于 query-aware 的原文表示来预测答案起始位置。</p><p>但这个文本数据集涉及问题，原文，答案三个部分, 特别是需要根据问题在原文中搜寻答案的范围, 这就涉及如果把问题的信息提取出来并作用于原文. 目前各种前沿模型的关注点几乎都是在如何捕捉问题和原文之间的交互关系，也就是在 Encode 层和 Decode 层之间, 使用一个 Interaction 层处理编码了问题语义信息的原文表示，即 query-aware 的原文表示，再输入给 Decode 层。而本来应用机器翻译Attention机制就能很好的处理这种交互。</p><p>虽然注意力机制大同小异，但是不同的注意力权重（打分函数）带来的效果是不一样的。比较常用的是就是使用<a href=%5Cattention#%E5%85%A8%E5%B1%80%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6>全局注意力机制</a>中提到的</p>$$
\begin{aligned}
score_{general}(t' t) &= s^\top_{t'} W_\alpha h_t, \\\
\end{aligned}
$$<p>就是用一个交互矩阵$W_\alpha$来捕捉问题和原文之间的交互关系. 原文作者称之为 <strong>Bilinear</strong>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Attention</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forwards_bilinear</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hc</span><span class=p>,</span> <span class=n>hq</span><span class=p>,</span> <span class=n>hc_mask</span><span class=p>,</span> <span class=n>hq_mask</span><span class=p>,</span> <span class=n>max_context_length_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>max_question_length_placeholder</span><span class=p>,</span> <span class=n>is_train</span><span class=p>,</span> <span class=n>keep_prob</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;combine context hidden state(hc) and question hidden state(hq) with global attention
</span></span></span><span class=line><span class=cl><span class=s1>            bilinear score = hc.T *W *hq
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>d_en</span> <span class=o>=</span> <span class=n>hc</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=c1># (BS, MPL, MQL)</span>
</span></span><span class=line><span class=cl>        <span class=n>interaction_weights</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s2>&#34;W_interaction&#34;</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>d_en</span><span class=p>,</span> <span class=n>d_en</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>hc_W</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>hc</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>d_en</span><span class=p>])</span> <span class=o>@</span> <span class=n>interaction_weights</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length_placeholder</span><span class=p>,</span> <span class=n>d_en</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (BS, MPL, HS * 2) @ (BS, HS * 2, MCL) -&gt; (BS ,MCL, MQL)</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>hc_W</span> <span class=o>@</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>hq</span><span class=p>,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=c1># Create mask (BS, MPL) -&gt; (BS, MPL, 1) -&gt; (BS, MPL, MQL)</span>
</span></span><span class=line><span class=cl>        <span class=n>hc_mask_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>hc_mask</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>),</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>max_question_length_placeholder</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>hq_mask_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>hq_mask</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>),</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length_placeholder</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>hq_mask_aug</span> <span class=o>=</span> <span class=n>hc_mask_aug</span> <span class=o>&amp;</span> <span class=n>hq_mask_aug</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>softmax_mask_prepro</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>hq_mask_aug</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (BS, MPL, MQL)</span>
</span></span><span class=line><span class=cl>        <span class=n>alignment_weights</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (BS, MPL, MQL) @ (BS, MQL, HS * 2) -&gt; (BS, MPL, HS * 2)</span>
</span></span><span class=line><span class=cl>        <span class=n>context_aware</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>alignment_weights</span><span class=p>,</span> <span class=n>hq</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>concat_hidden</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>([</span><span class=n>context_aware</span><span class=p>,</span> <span class=n>hc</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>concat_hidden</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cond</span><span class=p>(</span><span class=n>is_train</span><span class=p>,</span> <span class=k>lambda</span><span class=p>:</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>concat_hidden</span><span class=p>,</span> <span class=n>keep_prob</span><span class=p>),</span> <span class=k>lambda</span><span class=p>:</span> <span class=n>concat_hidden</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (HS * 4, HS * 2)</span>
</span></span><span class=line><span class=cl>        <span class=n>Ws</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s2>&#34;Ws&#34;</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>d_en</span> <span class=o>*</span> <span class=mi>2</span><span class=p>,</span> <span class=n>d_en</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>attention</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>concat_hidden</span><span class=p>,</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>d_en</span> <span class=o>*</span> <span class=mi>2</span><span class=p>])</span> <span class=o>@</span> <span class=n>Ws</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                          <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length_placeholder</span><span class=p>,</span> <span class=n>d_en</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=n>attention</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_similarity_matrix</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hq</span><span class=p>,</span> <span class=n>hc</span><span class=p>,</span> <span class=n>max_question_length</span><span class=p>,</span> <span class=n>max_context_length</span><span class=p>,</span> <span class=n>question_mask</span><span class=p>,</span> <span class=n>context_mask</span><span class=p>,</span> <span class=n>is_train</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                           <span class=n>keep_prob</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>_flatten</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>keep</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>fixed_shape</span> <span class=o>=</span> <span class=n>tensor</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>start</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>fixed_shape</span><span class=p>)</span> <span class=o>-</span> <span class=n>keep</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Calculate (BS * MCL * MQL)</span>
</span></span><span class=line><span class=cl>            <span class=n>left</span> <span class=o>=</span> <span class=n>reduce</span><span class=p>(</span><span class=n>mul</span><span class=p>,</span> <span class=p>[</span><span class=n>fixed_shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=ow>or</span> <span class=n>tf</span><span class=o>.</span><span class=n>shape</span><span class=p>(</span><span class=n>tensor</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>start</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># out_shape is simply HS * 2</span>
</span></span><span class=line><span class=cl>            <span class=n>out_shape</span> <span class=o>=</span> <span class=p>[</span><span class=n>left</span><span class=p>]</span> <span class=o>+</span> <span class=p>[</span><span class=n>fixed_shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=ow>or</span> <span class=n>tf</span><span class=o>.</span><span class=n>shape</span><span class=p>(</span><span class=n>tensor</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>start</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>fixed_shape</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># (BS * MCL * MQL, HS * 2)</span>
</span></span><span class=line><span class=cl>            <span class=n>flat</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>out_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=p>(</span><span class=n>flat</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>_reconstruct</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>ref</span><span class=p>,</span> <span class=n>keep</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>ref_shape</span> <span class=o>=</span> <span class=n>ref</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>tensor_shape</span> <span class=o>=</span> <span class=n>tensor</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>ref_stop</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>ref_shape</span><span class=p>)</span> <span class=o>-</span> <span class=n>keep</span>
</span></span><span class=line><span class=cl>            <span class=n>tensor_start</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tensor_shape</span><span class=p>)</span> <span class=o>-</span> <span class=n>keep</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># [BS, MCL, MQL]</span>
</span></span><span class=line><span class=cl>            <span class=n>pre_shape</span> <span class=o>=</span> <span class=p>[</span><span class=n>ref_shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=ow>or</span> <span class=n>tf</span><span class=o>.</span><span class=n>shape</span><span class=p>(</span><span class=n>ref</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>ref_stop</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># [1]</span>
</span></span><span class=line><span class=cl>            <span class=n>keep_shape</span> <span class=o>=</span> <span class=p>[</span><span class=n>tensor_shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=ow>or</span> <span class=n>tf</span><span class=o>.</span><span class=n>shape</span><span class=p>(</span><span class=n>tensor</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>tensor_start</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>tensor_shape</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>            <span class=c1># pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))]</span>
</span></span><span class=line><span class=cl>            <span class=c1># keep_shape = tensor.get_shape().as_list()[-keep:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># [BS, MCL, MQL, 1]</span>
</span></span><span class=line><span class=cl>            <span class=n>target_shape</span> <span class=o>=</span> <span class=n>pre_shape</span> <span class=o>+</span> <span class=n>keep_shape</span>
</span></span><span class=line><span class=cl>            <span class=n>out</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>target_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>out</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>out</span><span class=p>,</span> <span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>args</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>())</span> <span class=o>-</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (BS, MCL, MQL, HS * 2)</span>
</span></span><span class=line><span class=cl>        <span class=n>d</span> <span class=o>=</span> <span class=n>hq</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>debug</span><span class=p>(</span><span class=s2>&#34;d is: </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>d</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>hc_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>hc</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>d</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                         <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>max_question_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (BS, MCL, MQL, HS * 2)</span>
</span></span><span class=line><span class=cl>        <span class=n>hq_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>hq</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>max_question_length</span><span class=p>,</span> <span class=n>d</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                         <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># [(BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2)]</span>
</span></span><span class=line><span class=cl>        <span class=n>args</span> <span class=o>=</span> <span class=p>[</span><span class=n>hc_aug</span><span class=p>,</span> <span class=n>hq_aug</span><span class=p>,</span> <span class=n>hc_aug</span> <span class=o>*</span> <span class=n>hq_aug</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># [(BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2)]</span>
</span></span><span class=line><span class=cl>        <span class=n>args_flat</span> <span class=o>=</span> <span class=p>[</span><span class=n>_flatten</span><span class=p>(</span><span class=n>arg</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=k>for</span> <span class=n>arg</span> <span class=ow>in</span> <span class=n>args</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>args_flat</span> <span class=o>=</span> <span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>cond</span><span class=p>(</span><span class=n>is_train</span><span class=p>,</span> <span class=k>lambda</span><span class=p>:</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>arg</span><span class=p>,</span> <span class=n>keep_prob</span><span class=p>),</span> <span class=k>lambda</span><span class=p>:</span> <span class=n>arg</span><span class=p>)</span> <span class=k>for</span> <span class=n>arg</span> <span class=ow>in</span> <span class=n>args_flat</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>d_concat</span> <span class=o>=</span> <span class=n>d</span> <span class=o>*</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl>        <span class=n>W</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s2>&#34;W&#34;</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>d_concat</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>b</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s2>&#34;b&#34;</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Calculating a(h, u) = w_s^(t)[h; u; h * u]</span>
</span></span><span class=line><span class=cl>        <span class=c1># (BS * MCL * MQL, HS * 6) @ (HS * 6, 1) + (1) -&gt; (BS * MCL * MQL, 1)</span>
</span></span><span class=line><span class=cl>        <span class=n>res</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>(</span><span class=n>args_flat</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=o>@</span> <span class=n>W</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (BS * MCL * MQL, 1) -&gt; (BS, MCL, MQL)</span>
</span></span><span class=line><span class=cl>        <span class=n>similarity_matrix</span> <span class=o>=</span> <span class=n>_reconstruct</span><span class=p>(</span><span class=n>res</span><span class=p>,</span> <span class=n>args</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>debug</span><span class=p>(</span><span class=s2>&#34;similiarity_matrix after reconstruct: </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>similarity_matrix</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>        <span class=n>context_mask_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>context_mask</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>max_question_length</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>question_mask_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>question_mask</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>mask_aug</span> <span class=o>=</span> <span class=n>context_mask_aug</span> <span class=o>&amp;</span> <span class=n>question_mask_aug</span>
</span></span><span class=line><span class=cl>        <span class=n>similarity_matrix</span> <span class=o>=</span> <span class=n>softmax_mask_prepro</span><span class=p>(</span><span class=n>similarity_matrix</span><span class=p>,</span> <span class=n>mask_aug</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=n>similarity_matrix</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=bi-directional-attention-flow>Bi-Directional Attention Flow<a hidden class=anchor aria-hidden=true href=#bi-directional-attention-flow>#</a></h3><p><a href=https://arxiv.org/abs/1611.01603>lSeo et al. (2016)</a>针对SQuAD提出了一个另一种更复杂的注意力机制, Bi-Directional Attention Flow (BiDAF)。
<img loading=lazy src=https://allenai.github.io/bi-att-flow/BiDAF.png title="image from: https://allenai.github.io/bi-att-flow/">
BiDAF顾名思义那个就是问题与段落的双向的注意力机制, 分别是 Context-to-query (C2Q) attention 和 Query-to-context (Q2C) attention. 两者都是基于传统的段落的背景向量 $H$ 与问题的背景向量 $U$ 间相似矩阵(similarity matrix) $S \in \mathbb{R^{T×J}}$衍生出来的.</p>$$
S_{tj} = \alpha(H_{:t}, U_{:j}) \in R \\\
\alpha(h, u) = w^{\top}_{(S)}[h; u; h \odot u]
$$<blockquote><p>Where $S_{tj}$ indicates the similarity between t-th context word and j-th query word, $\alpha$ is a trainable scalar function that encodes the similarity between its two input vectors, $H_{:t}$ is t-th column vector of H, and $U_{:j}$ is j-th column vector of U, $w_{(S)} \in R^{6d}$ is a trainable weight vector, $[;]$ is vector concatenation across row.</p></blockquote><p>相似矩阵S被用于计算两种方向的注意力向量.</p><blockquote><p>Context-to-query (C2Q) attention signifies which query words are most relevant to each context word</p></blockquote>$$
\tilde{U_{:t}} = \sum_j \alpha_{tj} U_{:j} \\\
\alpha_t = softmax(S_{t:})
$$<p>其中 $\alpha_t \in R^J 表示$t$段落词对各个问题词的注意力权重</p><blockquote><p>Query-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query.</p></blockquote><p>对段落的注意力权重为:</p>$$
b = softmax(max_{col}(S)) \in R^T
$$<p>其中$max_{col}$是在每行选出最大值.
然后对段落背景向量进行注意力加权:</p>$$
\tilde{h} = \sum_t b_t H_{:t} \in R^{2d}
$$<p>这个$\tilde{h}$向量指的是在query眼里最重要的段落次的加权求和. 因为$\tilde{h}$是在每一个内去最大值, 所以还需要从新把$\tilde{h}$的值在每一个铺开$T$次得到一个$\tilde{H} \in R^{2dxT}$向量以方便后续的计算.</p><p>最后, 段落的embeddings向量和注意力向量结合为$G$, $G$的每一列向量可以理解为每个段落词的 query-aware representation:</p>$$
G_{:t} = \beta(H_{:t}, \tilde{U_{:t}}, \tilde{H_{:t}}) \in R^{d_G}
$$<blockquote><p>where $G_{:t}$ is the t-th column vector (corresponding to t-th context word), β is a trainable vector function that fuses its (three) input vectors, and $d_G$ is the output dimension of the β function.</p></blockquote><p>β 函数可以是任意的神经网络, 但是文章中指出使用简单的函数如 $\beta(h, \tilde{u}, \tilde{h}) = [h; \tilde{u}; h \odot \tilde{u}; h \odot \tilde{h}] \in R^{8dxT}$ (i.e., dG = 8d) 表现已经很好了。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Attention</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forwards_complex</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hc</span><span class=p>,</span> <span class=n>hq</span><span class=p>,</span> <span class=n>hc_mask</span><span class=p>,</span> <span class=n>hq_mask</span><span class=p>,</span> <span class=n>max_context_length_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                  <span class=n>max_question_length_placeholder</span><span class=p>,</span> <span class=n>is_train</span><span class=p>,</span> <span class=n>keep_prob</span><span class=p>):</span>
</span></span><span class=line><span class=cl>       <span class=s1>&#39;&#39;&#39;combine context hidden state(hc) and question hidden state(hq) with attention
</span></span></span><span class=line><span class=cl><span class=s1>            measured similarity = hc : hq : hc.T * hq
</span></span></span><span class=line><span class=cl><span class=s1>       &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>       <span class=n>s</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_similarity_matrix</span><span class=p>(</span><span class=n>hq</span><span class=p>,</span> <span class=n>hc</span><span class=p>,</span> <span class=n>max_question_length_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=n>max_context_length_placeholder</span><span class=p>,</span> <span class=n>hq_mask</span><span class=p>,</span> <span class=n>hc_mask</span><span class=p>,</span> <span class=n>is_train</span><span class=p>,</span> <span class=n>keep_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=c1># C2Q</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS, MCL, MQL)</span>
</span></span><span class=line><span class=cl>       <span class=n>weights_c2q</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>s</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS, MCL, MQL) @ (BS, MQL, HS * 2) -&gt; (BS, MCL, HS * 2)</span>
</span></span><span class=line><span class=cl>       <span class=n>query_aware</span> <span class=o>=</span> <span class=n>weights_c2q</span> <span class=o>@</span> <span class=n>hq</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># Q2C</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS, MCL, MQL) -&gt; (BS, MCL)</span>
</span></span><span class=line><span class=cl>       <span class=c1># We are effectively looking through all the question words j&#39;s to some context word i and finding the</span>
</span></span><span class=line><span class=cl>       <span class=c1># maximum of those context words</span>
</span></span><span class=line><span class=cl>       <span class=n>score_q2c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_max</span><span class=p>(</span><span class=n>s</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS, MCL)</span>
</span></span><span class=line><span class=cl>       <span class=n>weights_q2c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>score_q2c</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS, HS)</span>
</span></span><span class=line><span class=cl>       <span class=n>context_aware</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>weights_q2c</span><span class=p>,</span> <span class=n>hc</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS, MCL, HS * 2)</span>
</span></span><span class=line><span class=cl>       <span class=n>context_aware</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>context_aware</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length_placeholder</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># [(BS, MCL, HS * 2), (BS, MCL, HS * 2), (BS, MCL, HS * 2), (BS, MCL, HS * 2)]</span>
</span></span><span class=line><span class=cl>       <span class=n>biattention</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>([</span><span class=n>hc</span><span class=p>,</span> <span class=n>query_aware</span><span class=p>,</span> <span class=n>hc</span> <span class=o>*</span> <span class=n>query_aware</span><span class=p>,</span> <span class=n>hc</span> <span class=o>*</span> <span class=n>context_aware</span><span class=p>],</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=p>(</span><span class=n>biattention</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_similarity_matrix</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hq</span><span class=p>,</span> <span class=n>hc</span><span class=p>,</span> <span class=n>max_question_length</span><span class=p>,</span> <span class=n>max_context_length</span><span class=p>,</span> <span class=n>question_mask</span><span class=p>,</span> <span class=n>context_mask</span><span class=p>,</span> <span class=n>is_train</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>keep_prob</span><span class=p>):</span>
</span></span><span class=line><span class=cl>       <span class=k>def</span> <span class=nf>_flatten</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>keep</span><span class=p>):</span>
</span></span><span class=line><span class=cl>           <span class=n>fixed_shape</span> <span class=o>=</span> <span class=n>tensor</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>           <span class=n>start</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>fixed_shape</span><span class=p>)</span> <span class=o>-</span> <span class=n>keep</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=c1># Calculate (BS * MCL * MQL)</span>
</span></span><span class=line><span class=cl>           <span class=n>left</span> <span class=o>=</span> <span class=n>reduce</span><span class=p>(</span><span class=n>mul</span><span class=p>,</span> <span class=p>[</span><span class=n>fixed_shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=ow>or</span> <span class=n>tf</span><span class=o>.</span><span class=n>shape</span><span class=p>(</span><span class=n>tensor</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>start</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=c1># out_shape is simply HS * 2</span>
</span></span><span class=line><span class=cl>           <span class=n>out_shape</span> <span class=o>=</span> <span class=p>[</span><span class=n>left</span><span class=p>]</span> <span class=o>+</span> <span class=p>[</span><span class=n>fixed_shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=ow>or</span> <span class=n>tf</span><span class=o>.</span><span class=n>shape</span><span class=p>(</span><span class=n>tensor</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>start</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>fixed_shape</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=c1># (BS * MCL * MQL, HS * 2)</span>
</span></span><span class=line><span class=cl>           <span class=n>flat</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>out_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>           <span class=k>return</span> <span class=p>(</span><span class=n>flat</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=k>def</span> <span class=nf>_reconstruct</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>ref</span><span class=p>,</span> <span class=n>keep</span><span class=p>):</span>
</span></span><span class=line><span class=cl>           <span class=n>ref_shape</span> <span class=o>=</span> <span class=n>ref</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>           <span class=n>tensor_shape</span> <span class=o>=</span> <span class=n>tensor</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>           <span class=n>ref_stop</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>ref_shape</span><span class=p>)</span> <span class=o>-</span> <span class=n>keep</span>
</span></span><span class=line><span class=cl>           <span class=n>tensor_start</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tensor_shape</span><span class=p>)</span> <span class=o>-</span> <span class=n>keep</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=c1># [BS, MCL, MQL]</span>
</span></span><span class=line><span class=cl>           <span class=n>pre_shape</span> <span class=o>=</span> <span class=p>[</span><span class=n>ref_shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=ow>or</span> <span class=n>tf</span><span class=o>.</span><span class=n>shape</span><span class=p>(</span><span class=n>ref</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>ref_stop</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=c1># [1]</span>
</span></span><span class=line><span class=cl>           <span class=n>keep_shape</span> <span class=o>=</span> <span class=p>[</span><span class=n>tensor_shape</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=ow>or</span> <span class=n>tf</span><span class=o>.</span><span class=n>shape</span><span class=p>(</span><span class=n>tensor</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>tensor_start</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>tensor_shape</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>           <span class=c1># pre_shape = [tf.shape(ref)[i] for i in range(len(ref.get_shape().as_list()[:-keep]))]</span>
</span></span><span class=line><span class=cl>           <span class=c1># keep_shape = tensor.get_shape().as_list()[-keep:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=c1># [BS, MCL, MQL, 1]</span>
</span></span><span class=line><span class=cl>           <span class=n>target_shape</span> <span class=o>=</span> <span class=n>pre_shape</span> <span class=o>+</span> <span class=n>keep_shape</span>
</span></span><span class=line><span class=cl>           <span class=n>out</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>target_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>           <span class=n>out</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>out</span><span class=p>,</span> <span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>args</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>())</span> <span class=o>-</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>           <span class=k>return</span> <span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS, MCL, MQL, HS * 2)</span>
</span></span><span class=line><span class=cl>       <span class=n>d</span> <span class=o>=</span> <span class=n>hq</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>       <span class=n>logging</span><span class=o>.</span><span class=n>debug</span><span class=p>(</span><span class=s2>&#34;d is: </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>d</span><span class=p>))</span>
</span></span><span class=line><span class=cl>       <span class=n>hc_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>hc</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>d</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                        <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>max_question_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS, MCL, MQL, HS * 2)</span>
</span></span><span class=line><span class=cl>       <span class=n>hq_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>hq</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>max_question_length</span><span class=p>,</span> <span class=n>d</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                        <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># [(BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2), (BS, MCL, MQL, HS * 2)]</span>
</span></span><span class=line><span class=cl>       <span class=n>args</span> <span class=o>=</span> <span class=p>[</span><span class=n>hc_aug</span><span class=p>,</span> <span class=n>hq_aug</span><span class=p>,</span> <span class=n>hc_aug</span> <span class=o>*</span> <span class=n>hq_aug</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># [(BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2), (BS * MCL * MQL, HS * 2)]</span>
</span></span><span class=line><span class=cl>       <span class=n>args_flat</span> <span class=o>=</span> <span class=p>[</span><span class=n>_flatten</span><span class=p>(</span><span class=n>arg</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=k>for</span> <span class=n>arg</span> <span class=ow>in</span> <span class=n>args</span><span class=p>]</span>
</span></span><span class=line><span class=cl>       <span class=n>args_flat</span> <span class=o>=</span> <span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>cond</span><span class=p>(</span><span class=n>is_train</span><span class=p>,</span> <span class=k>lambda</span><span class=p>:</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>arg</span><span class=p>,</span> <span class=n>keep_prob</span><span class=p>),</span> <span class=k>lambda</span><span class=p>:</span> <span class=n>arg</span><span class=p>)</span> <span class=k>for</span> <span class=n>arg</span> <span class=ow>in</span> <span class=n>args_flat</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=n>d_concat</span> <span class=o>=</span> <span class=n>d</span> <span class=o>*</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl>       <span class=n>W</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s2>&#34;W&#34;</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>d_concat</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>       <span class=n>b</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s2>&#34;b&#34;</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># Calculating a(h, u) = w_s^(t)[h; u; h * u]</span>
</span></span><span class=line><span class=cl>       <span class=c1># (BS * MCL * MQL, HS * 6) @ (HS * 6, 1) + (1) -&gt; (BS * MCL * MQL, 1)</span>
</span></span><span class=line><span class=cl>       <span class=n>res</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>(</span><span class=n>args_flat</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=o>@</span> <span class=n>W</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=c1># (BS * MCL * MQL, 1) -&gt; (BS, MCL, MQL)</span>
</span></span><span class=line><span class=cl>       <span class=n>similarity_matrix</span> <span class=o>=</span> <span class=n>_reconstruct</span><span class=p>(</span><span class=n>res</span><span class=p>,</span> <span class=n>args</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>logging</span><span class=o>.</span><span class=n>debug</span><span class=p>(</span><span class=s2>&#34;similiarity_matrix after reconstruct: </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>similarity_matrix</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>       <span class=n>context_mask_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>context_mask</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>max_question_length</span><span class=p>])</span>
</span></span><span class=line><span class=cl>       <span class=n>question_mask_aug</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>question_mask</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_context_length</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=n>mask_aug</span> <span class=o>=</span> <span class=n>context_mask_aug</span> <span class=o>&amp;</span> <span class=n>question_mask_aug</span>
</span></span><span class=line><span class=cl>       <span class=n>similarity_matrix</span> <span class=o>=</span> <span class=n>softmax_mask_prepro</span><span class=p>(</span><span class=n>similarity_matrix</span><span class=p>,</span> <span class=n>mask_aug</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=p>(</span><span class=n>similarity_matrix</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=数据处理>数据处理<a hidden class=anchor aria-hidden=true href=#数据处理>#</a></h2><p>内容段落摘自维基百科文章中的536篇文章，包含107,785对问题和答案，这使得SQuAD显着大于以前任何人类标注的数据集。在该数据集中，80％的数据用于训练，10％用于验证, 剩余10％用于测试。在训练集中，进一步划分出5％用于训练时的验证。</p><p>与其他问答数据集相比，SQUAD具有比较独特的特征，所有答案都是出自相应的上下文中。对于每一个段落, 众包人员生成几个问题，并选择原段落中的一小段作为答案. 答案由两个index组成, 对应答案在段落中的起始位置。因此，SQuAD数据集的答案可能比其他以单个单词和实体为答案为主的数据集长得多。实例:</p><blockquote><p>Question:
Why was Tesla returned to Gospic?</p></blockquote><blockquote><p>Context paragraph:
On 24 March 1879, Tesla was returned to Gospicunder police guard for <strong>not having a residence permit</strong>&mldr;</p></blockquote><blockquote><p>Answer:
{12, 16}</p></blockquote><h3 id=embedding>Embedding<a hidden class=anchor aria-hidden=true href=#embedding>#</a></h3><p>词向量使用预训练好的 Glove embedding.</p><blockquote><p>Glove is a log-bilinear regression model that combines the advantages of global matrix factorization and local context window methods.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_glove_embeddings</span><span class=p>(</span><span class=n>embed_path</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Loading glove embedding...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>glove</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>embed_path</span><span class=p>)[</span><span class=s1>&#39;glove&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Dimension: </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>glove</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Vocabulary: </span><span class=si>{}</span><span class=s2>&#34;</span> <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>glove</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>glove</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=n>load_glove_embeddings</span><span class=p>(</span><span class=n>embed_path</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Model</span><span class=p>(</span><span class=n>metaclass</span><span class=o>=</span><span class=n>ABCMeta</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    <span class=nd>@abstractmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>setup_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>pass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>setup_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Loads distributed word representations based on placeholder tokens
</span></span></span><span class=line><span class=cl><span class=s2>        :return: embeddings representaion of question and context.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s2>&#34;embeddings&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>RE_TRAIN_EMBED</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>embeddings</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s2>&#34;embeddings&#34;</span><span class=p>,</span> <span class=n>initializer</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>embeddings</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>question_embeddings</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>embedding_lookup</span><span class=p>(</span><span class=n>embeddings</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>question_placeholder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>question_embeddings</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>question_embeddings</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>shape</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_question_length_placeholder</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>embedding_size</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>context_embeddings</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>embedding_lookup</span><span class=p>(</span><span class=n>embeddings</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>context_placeholder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>context_embeddings</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>context_embeddings</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>shape</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_context_length_placeholder</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>embedding_size</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>question_embeddings</span><span class=p>,</span> <span class=n>context_embeddings</span>
</span></span></code></pre></div><h2 id=模型>模型<a hidden class=anchor aria-hidden=true href=#模型>#</a></h2><p>整体的模型由Embedding层，Encodr层，Attention层，Decoder层组成</p><h3 id=encoder>Encoder<a hidden class=anchor aria-hidden=true href=#encoder>#</a></h3><p>编码器就是一个双向GRU层:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Encoder</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    In a generalized encode function, you pass in your inputs,
</span></span></span><span class=line><span class=cl><span class=s2>    masks, and an initial hidden state input into this function.
</span></span></span><span class=line><span class=cl><span class=s2>    :param inputs: Symbolic representations of your input
</span></span></span><span class=line><span class=cl><span class=s2>    :param masks: this is to make sure tf.nn.dynamic_rnn doesn&#39;t iterate
</span></span></span><span class=line><span class=cl><span class=s2>                  through masked steps
</span></span></span><span class=line><span class=cl><span class=s2>    :param encoder_state_input: (Optional) pass this as initial hidden state
</span></span></span><span class=line><span class=cl><span class=s2>                                to tf.nn.dynamic_rnn to build conditional representations
</span></span></span><span class=line><span class=cl><span class=s2>    :return:
</span></span></span><span class=line><span class=cl><span class=s2>            outputs: The RNN output Tensor
</span></span></span><span class=line><span class=cl><span class=s2>                      an encoded representation of your input.
</span></span></span><span class=line><span class=cl><span class=s2>                      It can be context-level representation,
</span></span></span><span class=line><span class=cl><span class=s2>                      word-level representation, or both.
</span></span></span><span class=line><span class=cl><span class=s2>            state: The final state.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>state_size</span> <span class=o>=</span> <span class=n>state_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>masks</span><span class=p>,</span> <span class=n>initial_state_fw</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>initial_state_bw</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>reuse</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>keep_prob</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>BiGRU_layer</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>masks</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>state_size</span><span class=p>,</span> <span class=n>initial_state_fw</span><span class=p>,</span> <span class=n>initial_state_bw</span><span class=p>,</span> <span class=n>reuse</span><span class=p>,</span> <span class=n>keep_prob</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>BiGRU_layer</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>masks</span><span class=p>,</span> <span class=n>state_size</span><span class=p>,</span> <span class=n>initial_state_fw</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>initial_state_bw</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>reuse</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span> <span class=n>keep_prob</span><span class=o>=</span><span class=mf>1.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39; Wrapped BiGRU_layer for reuse&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=c1># &#39;outputs&#39; is a tensor of shape [batch_size, max_time, cell_state_size]</span>
</span></span><span class=line><span class=cl>        <span class=n>cell_fw</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>rnn</span><span class=o>.</span><span class=n>GRUCell</span><span class=p>(</span><span class=n>state_size</span><span class=p>,</span> <span class=n>reuse</span> <span class=o>=</span> <span class=n>reuse</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>cell_fw</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>rnn</span><span class=o>.</span><span class=n>DropoutWrapper</span><span class=p>(</span><span class=n>cell_fw</span><span class=p>,</span> <span class=n>input_keep_prob</span> <span class=o>=</span> <span class=n>keep_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>cell_bw</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>rnn</span><span class=o>.</span><span class=n>GRUCell</span><span class=p>(</span><span class=n>state_size</span><span class=p>,</span> <span class=n>reuse</span> <span class=o>=</span> <span class=n>reuse</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>cell_bw</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>rnn</span><span class=o>.</span><span class=n>DropoutWrapper</span><span class=p>(</span><span class=n>cell_bw</span><span class=p>,</span> <span class=n>input_keep_prob</span> <span class=o>=</span> <span class=n>keep_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>sequence_length</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_sum</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>masks</span><span class=p>,</span> <span class=s1>&#39;int32&#39;</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>sequence_length</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>sequence_length</span><span class=p>,</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Outputs Tensor shaped: [batch_size, max_time, cell.output_size]</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=n>outputs_fw</span><span class=p>,</span> <span class=n>outputs_bw</span><span class=p>),</span> <span class=p>(</span><span class=n>final_state_fw</span><span class=p>,</span> <span class=n>final_state_bw</span><span class=p>)</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>bidirectional_dynamic_rnn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                                            <span class=n>cell_fw</span> <span class=o>=</span> <span class=n>cell_fw</span><span class=p>,</span>\
</span></span><span class=line><span class=cl>                                            <span class=n>cell_bw</span> <span class=o>=</span> <span class=n>cell_bw</span><span class=p>,</span>\
</span></span><span class=line><span class=cl>                                            <span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>,</span>\
</span></span><span class=line><span class=cl>                                            <span class=n>sequence_length</span> <span class=o>=</span> <span class=n>sequence_length</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                            <span class=n>initial_state_fw</span> <span class=o>=</span> <span class=n>initial_state_fw</span><span class=p>,</span>\
</span></span><span class=line><span class=cl>                                            <span class=n>initial_state_bw</span> <span class=o>=</span> <span class=n>initial_state_bw</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                            <span class=n>dtype</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>([</span><span class=n>outputs_fw</span><span class=p>,</span> <span class=n>outputs_bw</span><span class=p>],</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>outputs</span><span class=p>,</span> <span class=n>final_state_fw</span><span class=p>,</span> <span class=n>final_state_bw</span>
</span></span></code></pre></div><h3 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h3><p>解码器也包含一个双向GRU层，输出的状态分别由两个softmax分类器计算出预测的答案的 start 和 end index 位置:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Decoder</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    takes in a knowledge representation
</span></span></span><span class=line><span class=cl><span class=s2>    and output a probability estimation over
</span></span></span><span class=line><span class=cl><span class=s2>    all paragraph tokens on which token should be
</span></span></span><span class=line><span class=cl><span class=s2>    the start of the answer span, and which should be
</span></span></span><span class=line><span class=cl><span class=s2>    the end of the answer span.
</span></span></span><span class=line><span class=cl><span class=s2>    :param knowledge_rep: it is a representation of the paragraph and question,
</span></span></span><span class=line><span class=cl><span class=s2>                          decided by how you choose to implement the encoder
</span></span></span><span class=line><span class=cl><span class=s2>    :return: (start, end)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>output_size</span><span class=p>,</span> <span class=n>state_size</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_size</span> <span class=o>=</span> <span class=n>output_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>state_size</span> <span class=o>=</span> <span class=n>state_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>knowledge_rep</span><span class=p>,</span> <span class=n>mask</span><span class=p>,</span> <span class=n>max_input_length</span><span class=p>,</span> <span class=n>keep_prob</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;Decode with BiGRU&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;Modeling&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>outputs</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>BiGRU_layer</span><span class=p>(</span><span class=n>knowledge_rep</span><span class=p>,</span> <span class=n>mask</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>state_size</span><span class=p>,</span> <span class=n>keep_prob</span><span class=o>=</span><span class=n>keep_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s2>&#34;start&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>start</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_logit</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>max_input_length</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>start</span> <span class=o>=</span> <span class=n>softmax_mask_prepro</span><span class=p>(</span><span class=n>start</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s2>&#34;end&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>end</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_logit</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>max_input_length</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>end</span> <span class=o>=</span> <span class=n>softmax_mask_prepro</span><span class=p>(</span><span class=n>end</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=n>start</span><span class=p>,</span> <span class=n>end</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_logit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>max_inputs_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39; Get the logit (-inf, inf). &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>d</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>inputs</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>ndims</span> <span class=o>==</span> <span class=mi>3</span><span class=p>,</span> <span class=p>(</span><span class=s2>&#34;Got </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>ndims</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>shape</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>d</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>W</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s1>&#39;W&#39;</span><span class=p>,</span> <span class=n>initializer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>xavier_initializer</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                             <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>d</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pred</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pred</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>pred</span><span class=p>,</span> <span class=n>shape</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_inputs_length</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>histogram</span><span class=p>(</span><span class=s1>&#39;logit&#39;</span><span class=p>,</span> <span class=n>pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>pred</span>
</span></span></code></pre></div><h3 id=搭建整个系统>搭建整个系统<a hidden class=anchor aria-hidden=true href=#搭建整个系统>#</a></h3><p>在整个QASystem类中初始化这些功能层:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>QASystem</span><span class=p>(</span><span class=n>Model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embeddings</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34; Initializes System &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span> <span class=o>=</span> <span class=n>embeddings</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>Encoder</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>encoder_state_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>Decoder</span><span class=p>(</span><span class=n>output_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>output_size</span><span class=p>,</span> <span class=n>state_size</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>decoder_state_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>Attention</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ==== set up placeholder tokens ========</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>context_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>context_mask_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>bool</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>question_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>question_mask_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>bool</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>answer_start_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>answer_end_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>max_context_length_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>max_question_length_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout_placeholder</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ==== assemble pieces ====</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>which_model</span><span class=p>,</span> <span class=n>initializer</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>uniform_unit_scaling_initializer</span><span class=p>(</span><span class=mf>1.0</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>question_embeddings</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>context_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>setup_embeddings</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>preds</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>setup_system</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>setup_loss</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>preds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>f1_train</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=mf>0.</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>EM_train</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=mf>0.</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>f1_val</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=mf>0.</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>EM_val</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=mf>0.</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>scalar</span><span class=p>(</span><span class=s1>&#39;f1_train&#39;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>f1_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>scalar</span><span class=p>(</span><span class=s1>&#39;EM_train&#39;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>EM_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>scalar</span><span class=p>(</span><span class=s1>&#39;f1_val&#39;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>f1_val</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>scalar</span><span class=p>(</span><span class=s1>&#39;EM_val&#39;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>EM_val</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ==== set up training/updating procedure ====</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39; With gradient clipping&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>opt_op</span> <span class=o>=</span> <span class=n>get_optimizer</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>optimizer</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>max_gradient_norm</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>learning_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>exdma_weight_decay</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>train_op</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>build_exdma</span><span class=p>(</span><span class=n>opt_op</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>train_op</span> <span class=o>=</span> <span class=n>opt_op</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>merged</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>merge_all</span><span class=p>()</span>
</span></span></code></pre></div><p>把各个功能层搭建成一个完整的模型:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>setup_system</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Connect all parts of your system here:
</span></span></span><span class=line><span class=cl><span class=s2>    After your modularized implementation of encoder and decoder
</span></span></span><span class=line><span class=cl><span class=s2>    you should call various functions inside encoder, decoder here
</span></span></span><span class=line><span class=cl><span class=s2>    to assemble your reading comprehension system!
</span></span></span><span class=line><span class=cl><span class=s2>    context: [None, max_context_length, d]
</span></span></span><span class=line><span class=cl><span class=s2>    question: [None, max_question_length, d]
</span></span></span><span class=line><span class=cl><span class=s2>    :return:
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>d</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>context_embeddings</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Step 1: encode context and question, respectively, with independent weights
</span></span></span><span class=line><span class=cl><span class=s1>    e.g. hq = encode_question(question)  # get U (d*J) as representation of q
</span></span></span><span class=line><span class=cl><span class=s1>    e.g. hc = encode_context(context, q_state)   # get H (d*T) as representation of x
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;question&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>hq</span><span class=p>,</span> <span class=n>question_state_fw</span><span class=p>,</span> <span class=n>question_state_bw</span> <span class=o>=</span> \
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=o>.</span><span class=n>BiGRU_encode</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>question_embeddings</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>question_mask_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>keep_prob</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout_placeholder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>QA_ENCODER_SHARE</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>hc</span><span class=p>,</span> <span class=n>context_state_fw</span><span class=p>,</span> <span class=n>context_state_bw</span> <span class=o>=</span>\
</span></span><span class=line><span class=cl>                 <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=o>.</span><span class=n>BiGRU_encode</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>context_embeddings</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>context_mask_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>initial_state_fw</span> <span class=o>=</span> <span class=n>question_state_fw</span><span class=p>,</span> <span class=n>initial_state_bw</span> <span class=o>=</span> <span class=n>question_state_bw</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>reuse</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span> <span class=n>keep_prob</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout_placeholder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>QA_ENCODER_SHARE</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;context&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>hc</span><span class=p>,</span> <span class=n>context_state_fw</span><span class=p>,</span> <span class=n>context_state_bw</span> <span class=o>=</span>\
</span></span><span class=line><span class=cl>                 <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=o>.</span><span class=n>BiGRU_encode</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>context_embeddings</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>context_mask_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>initial_state_fw</span> <span class=o>=</span> <span class=n>question_state_fw</span><span class=p>,</span> <span class=n>initial_state_bw</span> <span class=o>=</span> <span class=n>question_state_bw</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                     <span class=n>keep_prob</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout_placeholder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>d_Bi</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>encoder_state_size</span><span class=o>*</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>hc</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()</span> <span class=o>==</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=n>d_Bi</span><span class=p>],</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Expected </span><span class=si>{}</span><span class=s2>, got </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>([</span><span class=kc>None</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_context_length_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>encoder_state_size</span><span class=p>],</span> <span class=n>hc</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>hq</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()</span> <span class=o>==</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=n>d_Bi</span><span class=p>],</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Expected </span><span class=si>{}</span><span class=s2>, got </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>([</span><span class=kc>None</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_question_length_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>encoder_state_size</span><span class=p>],</span> <span class=n>hq</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()</span><span class=o>.</span><span class=n>as_list</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Step 2: combine context hidden state(hc) and question hidden state(hq) with attention
</span></span></span><span class=line><span class=cl><span class=s1>         measured similarity = hc.T * hq
</span></span></span><span class=line><span class=cl><span class=s1>         Context-to-query (C2Q) attention signifies which query words are most relevant to each P context word.
</span></span></span><span class=line><span class=cl><span class=s1>            attention_c2q = softmax(similarity)
</span></span></span><span class=line><span class=cl><span class=s1>            hq_hat = sum(attention_c2q*hq)
</span></span></span><span class=line><span class=cl><span class=s1>         Query-to-context (Q2C) attention signifies which context words have the closest similarity
</span></span></span><span class=line><span class=cl><span class=s1>            to one of the query words and are hence critical for answering the query.
</span></span></span><span class=line><span class=cl><span class=s1>            attention_q2c = softmax(similarity.T)
</span></span></span><span class=line><span class=cl><span class=s1>            hc_hat = sum(attention_q2c*hc)
</span></span></span><span class=line><span class=cl><span class=s1>         combine with β activation: β function can be an arbitrary trainable neural network
</span></span></span><span class=line><span class=cl><span class=s1>         g = β(hc, hq, hc_hat, hq_hat)
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=c1># concat[h, u_a, h*u_a, h*h_a]</span>
</span></span><span class=line><span class=cl>    <span class=n>attention</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=o>.</span><span class=n>forwards_bilinear</span><span class=p>(</span><span class=n>hc</span><span class=p>,</span> <span class=n>hq</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>context_mask_placeholder</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>question_mask_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>max_context_length_placeholder</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_context_length_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>max_question_length_placeholder</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_question_length_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>is_train</span><span class=o>=</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout_placeholder</span> <span class=o>&lt;</span> <span class=mf>1.0</span><span class=p>),</span> <span class=n>keep_prob</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout_placeholder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>d_com</span> <span class=o>=</span> <span class=n>d_Bi</span><span class=o>*</span><span class=mi>4</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Step 3: decoding   &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s2>&#34;decoding&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>start</span><span class=p>,</span> <span class=n>end</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>BiGRU_decode</span><span class=p>(</span><span class=n>attention</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>context_mask_placeholder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=bp>self</span><span class=o>.</span><span class=n>max_context_length_placeholder</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout_placeholder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>start</span><span class=p>,</span> <span class=n>end</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/attention/>Attention</a></li><li><a href=https://congchan.github.io/tags/nlp/>NLP</a></li><li><a href=https://congchan.github.io/tags/tensorflow/>TensorFlow</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/randomized-queue-with-reservoir-sampling/><span class=title>« Prev</span><br><span>Randomized Queue with Reservoir Sampling</span>
</a><a class=next href=https://congchan.github.io/posts/value-based-reinforcement-learning/><span class=title>Next »</span><br><span>Value-based Reinforcement Learning</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) on x" href="https://x.com/intent/tweet/?text=%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%90%86%e8%a7%a3%20-%20LSTM%e4%b8%8e%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%20-%20%e6%96%af%e5%9d%a6%e7%a6%8f%e9%97%ae%e7%ad%94%e6%95%b0%e6%8d%ae%e9%9b%86%20%28SQuAD%29&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%259C%25BA%25E5%2599%25A8%25E9%2598%2585%25E8%25AF%25BB%25E7%2590%2586%25E8%25A7%25A3-lstm%25E4%25B8%258E%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6-%25E6%2596%25AF%25E5%259D%25A6%25E7%25A6%258F%25E9%2597%25AE%25E7%25AD%2594%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586-squad%2f&amp;hashtags=Attention%2cNLP%2cTensorFlow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%259C%25BA%25E5%2599%25A8%25E9%2598%2585%25E8%25AF%25BB%25E7%2590%2586%25E8%25A7%25A3-lstm%25E4%25B8%258E%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6-%25E6%2596%25AF%25E5%259D%25A6%25E7%25A6%258F%25E9%2597%25AE%25E7%25AD%2594%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586-squad%2f&amp;title=%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%90%86%e8%a7%a3%20-%20LSTM%e4%b8%8e%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%20-%20%e6%96%af%e5%9d%a6%e7%a6%8f%e9%97%ae%e7%ad%94%e6%95%b0%e6%8d%ae%e9%9b%86%20%28SQuAD%29&amp;summary=%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%90%86%e8%a7%a3%20-%20LSTM%e4%b8%8e%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%20-%20%e6%96%af%e5%9d%a6%e7%a6%8f%e9%97%ae%e7%ad%94%e6%95%b0%e6%8d%ae%e9%9b%86%20%28SQuAD%29&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%259C%25BA%25E5%2599%25A8%25E9%2598%2585%25E8%25AF%25BB%25E7%2590%2586%25E8%25A7%25A3-lstm%25E4%25B8%258E%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6-%25E6%2596%25AF%25E5%259D%25A6%25E7%25A6%258F%25E9%2597%25AE%25E7%25AD%2594%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586-squad%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%259C%25BA%25E5%2599%25A8%25E9%2598%2585%25E8%25AF%25BB%25E7%2590%2586%25E8%25A7%25A3-lstm%25E4%25B8%258E%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6-%25E6%2596%25AF%25E5%259D%25A6%25E7%25A6%258F%25E9%2597%25AE%25E7%25AD%2594%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586-squad%2f&title=%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%90%86%e8%a7%a3%20-%20LSTM%e4%b8%8e%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%20-%20%e6%96%af%e5%9d%a6%e7%a6%8f%e9%97%ae%e7%ad%94%e6%95%b0%e6%8d%ae%e9%9b%86%20%28SQuAD%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%259C%25BA%25E5%2599%25A8%25E9%2598%2585%25E8%25AF%25BB%25E7%2590%2586%25E8%25A7%25A3-lstm%25E4%25B8%258E%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6-%25E6%2596%25AF%25E5%259D%25A6%25E7%25A6%258F%25E9%2597%25AE%25E7%25AD%2594%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586-squad%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) on whatsapp" href="https://api.whatsapp.com/send?text=%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%90%86%e8%a7%a3%20-%20LSTM%e4%b8%8e%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%20-%20%e6%96%af%e5%9d%a6%e7%a6%8f%e9%97%ae%e7%ad%94%e6%95%b0%e6%8d%ae%e9%9b%86%20%28SQuAD%29%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%259C%25BA%25E5%2599%25A8%25E9%2598%2585%25E8%25AF%25BB%25E7%2590%2586%25E8%25A7%25A3-lstm%25E4%25B8%258E%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6-%25E6%2596%25AF%25E5%259D%25A6%25E7%25A6%258F%25E9%2597%25AE%25E7%25AD%2594%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586-squad%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) on telegram" href="https://telegram.me/share/url?text=%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%90%86%e8%a7%a3%20-%20LSTM%e4%b8%8e%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%20-%20%e6%96%af%e5%9d%a6%e7%a6%8f%e9%97%ae%e7%ad%94%e6%95%b0%e6%8d%ae%e9%9b%86%20%28SQuAD%29&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%259C%25BA%25E5%2599%25A8%25E9%2598%2585%25E8%25AF%25BB%25E7%2590%2586%25E8%25A7%25A3-lstm%25E4%25B8%258E%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6-%25E6%2596%25AF%25E5%259D%25A6%25E7%25A6%258F%25E9%2597%25AE%25E7%25AD%2594%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586-squad%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 机器阅读理解 - LSTM与注意力机制 - 斯坦福问答数据集 (SQuAD) on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%90%86%e8%a7%a3%20-%20LSTM%e4%b8%8e%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%20-%20%e6%96%af%e5%9d%a6%e7%a6%8f%e9%97%ae%e7%ad%94%e6%95%b0%e6%8d%ae%e9%9b%86%20%28SQuAD%29&u=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%259C%25BA%25E5%2599%25A8%25E9%2598%2585%25E8%25AF%25BB%25E7%2590%2586%25E8%25A7%25A3-lstm%25E4%25B8%258E%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6-%25E6%2596%25AF%25E5%259D%25A6%25E7%25A6%258F%25E9%2597%25AE%25E7%25AD%2594%25E6%2595%25B0%25E6%258D%25AE%25E9%259B%2586-squad%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>