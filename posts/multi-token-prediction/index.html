<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Multi-token Prediction | Cong's Log</title><meta name=keywords content="2025,Large Language Model,Multi-token Prediction"><meta name=description content="Multi-token prediction vs Next-token prediction
Next-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \( x_{t+1} \) given the context \( x_{1:t} \) (all tokens up to position \( t \)).
The cross-entropy loss for next-token prediction is defined as:
"><meta name=author content="Cong"><link rel=canonical href=https://congchan.github.io/posts/multi-token-prediction/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/multi-token-prediction/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/multi-token-prediction/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Multi-token Prediction"><meta property="og:description" content="Multi-token prediction vs Next-token prediction Next-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \( x_{t+1} \) given the context \( x_{1:t} \) (all tokens up to position \( t \)).
The cross-entropy loss for next-token prediction is defined as:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-29T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-29T00:00:00+00:00"><meta property="article:tag" content="2025"><meta property="article:tag" content="Large Language Model"><meta property="article:tag" content="Multi-Token Prediction"><meta name=twitter:card content="summary"><meta name=twitter:title content="Multi-token Prediction"><meta name=twitter:description content="Multi-token prediction vs Next-token prediction
Next-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \( x_{t+1} \) given the context \( x_{1:t} \) (all tokens up to position \( t \)).
The cross-entropy loss for next-token prediction is defined as:
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Multi-token Prediction","item":"https://congchan.github.io/posts/multi-token-prediction/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Multi-token Prediction","name":"Multi-token Prediction","description":"Multi-token prediction vs Next-token prediction Next-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \\( x_{t+1} \\) given the context \\( x_{1:t} \\) (all tokens up to position \\( t \\)).\nThe cross-entropy loss for next-token prediction is defined as:\n","keywords":["2025","Large Language Model","Multi-token Prediction"],"articleBody":"Multi-token prediction vs Next-token prediction Next-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \\( x_{t+1} \\) given the context \\( x_{1:t} \\) (all tokens up to position \\( t \\)).\nThe cross-entropy loss for next-token prediction is defined as:\n\\[ L_1 = -\\sum_t \\log P_\\theta(x_{t+1} | x_{1:t}) \\]\nwhere \\( P_\\theta \\) is the model parameterized by \\( \\theta \\), and \\( x_{1:t} \\) denotes the sequence of tokens from position 1 to \\( t \\).\nThe model is trained in an autoregressive manner, where each prediction relies solely on the ground-truth history of tokens, not on its own prior predictions (teacher forcing).\nThe core idea of Multi-token prediction (MTP) (Gloeckle, Fabian, et al. Better \u0026 Faster Large Language Models via Multi-Token Prediction. arXiv:2404.19737, arXiv, 30 Apr. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2404.19737.) is to enable the model to predict multiple future tokens simultaneously at each position in the training corpus, which improves sample efficiency and inference speed.\nComparison Between Next-Token Prediction and Multi-Token Prediction\nAspect Next-Token Prediction Multi-Token Prediction (MTP) in the Papers Prediction Scope Single future token \\( n \\) future tokens simultaneously (e.g., \\( n=4 \\)) Training Efficiency Requires more data due to local focus Improves sample efficiency by capturing long-term dependencies Inference Speed Sequential decoding (slow for long sequences) Speculative decoding via extra heads (3× faster in Gloeckle et al., 1.8× TPS in DeepSeek-V3) Architectural Cost Simple single head Additional heads but with minimal overhead (sequential computation in Gloeckle et al., shared modules in DeepSeek-V3) Multi-token prediction Gloeckle et al. argue that next-token prediction is inefficient because it focuses on local patterns and overlooks long-term dependencies. This requires significantly more training data compared to human learning (e.g., children learn language with far fewer examples). During inference, the model must generate tokens autoregressively (using its own prior predictions), while training uses ground-truth tokens. This creates a gap between training and inference, leading to error accumulation.\nDuring training, for any time step t:\nThe model uses a shared transformer trunk to generate a latent representation \\( z_{t:1} \\) from the context \\( x_{t:1} \\) (all tokens up to position \\( t \\)). This latent representation is then fed into \\( n \\) independent output heads, each dedicated to predicting a specific future token \\( x_{t+i} \\) for \\( i = 1, \\dots, n \\), in parallel. More specifically, the model’s heads for predicting tokens at positions \\( t+k \\) (where \\( k \u003e 1 \\)) do not rely on previous predicted tokens (e.g., \\( t+1 \\)) during training. This design avoids additional training overhead by sequentially computing forward/backward passes for each head, reducing peak GPU memory usage from \\(O(nV + d)\\) to \\(O(V + d)\\). For example, to predict tokens at \\( t+1, t+2, t+3, t+4 \\), each head \\( i \\) directly uses \\( z_{t:1} \\) to generate \\( P_\\theta(x_{t+i} | x_{t:1}) \\), without using the output of head \\( i-1 \\) (which predicts \\( x_{t+i-1} \\)). For the sequence Hi, I would like _t1, _t2, _t3, _t4, the prediction of \\( _t2 \\) ( \\( t+2 \\)) is based on Hi, I would like ( \\( x_{t:1} \\) where \\( t \\) is the position before \\( _t1 \\)), not on the predicted \\( _t1 \\). The model is trained to predict \\( _t1, _t2, _t3, _t4 \\) in parallel, each relying solely on the initial context, not on a chain of prior predictions. Unlike autoregressive decoding (where each step uses the previous token’s prediction), MTP during training is non-causal for future tokens—each head looks directly at the original context to predict its target token, avoiding dependency on intermediate predictions. This design enables parallel training and faster inference via speculative decoding. This is explicitly stated in the paper’s formula:\n\\[ P_\\theta(x_{t+i} | x_{t:1}) = \\text{softmax}(f_u(f_{h_i}(f_s(x_{t:1}))) \\]\nwhere \\( f_s \\) is the shared trunk, \\( f_{h_i} \\) is the \\( i \\)-th head, and \\( f_u \\) is the unembedding matrix. Each head \\( f_{h_i} \\) operates independently on \\( f_s(x_{t:1}) \\), not on previous heads’ outputs. The loss function aggregates predictions for \\( n \\) future tokens simultaneously, but each prediction is conditioned only on the original context \\( x_{t:1} \\), not on intermediate predictions: \\[ L_n = -\\sum_t \\log P_\\theta(x_{t+n:t+1} | x_{t:1}) \\] This encourages the model to learn longer-term dependencies and mitigates the distribution mismatch between teacher-forced training and autoregressive inference. During inference, only the next-token output head is employed. Optionally, the other extra heads enable self-speculative decoding, where the model pre-emptively predicts multiple tokens to skip redundant computations. This achieves up to 3× faster inference on code tasks with large batch sizes.\nOn code benchmarks like HumanEval and MBPP, 13B-parameter models with 4-token prediction solved 12% and 17% more problems, respectively, compared to next-token baselines. The approach also improved algorithmic reasoning and induction capabilities in synthetic tasks.\nComparison with Multi-Token Prediction in DeepSeek-V3 DeepSeek-V3(DeepSeek-AI, et al. DeepSeek-V3 Technical Report. arXiv:2412.19437, arXiv, 27 Dec. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2412.19437. ) notes that next-token prediction encourages the model to prioritize short-term dependencies, which may hinder its ability to learn global structures or complex reasoning tasks (e.g., coding, math). DeepSeek-V3 also employs MTP but with distinct architectural and operational differences:\nArchitectural Implementation Gloeckle et al.: Parallel independent heads for simultaneous prediction of \\(n\\) tokens, with each head operating on the shared trunk’s output. Example: 4-token prediction uses 4 separate heads. DeepSeek-V3: Sequential MTP modules that maintain a causal chain for each prediction depth. Each module \\(k\\) combines the current token representation with the embedding of the \\((i+k)\\)-th token, passing through a Transformer block \\(TRM_k\\) before prediction. This design preserves the dependency chain for deeper context reasoning. Training Objective and Loss Function Gloeckle et al.: The loss is the average cross-entropy over \\(n\\) heads, with a fixed weight \\(\\lambda\\) (e.g., \\(\\lambda = 0.3\\) for early training). DeepSeek-V3: MTP loss is a weighted average of cross-entropy losses from \\(D\\) sequential modules, with the weight \\(\\lambda\\) decaying from 0.3 to 0.1 during training. The sequential design allows deeper integration with the model’s causal attention mechanism. Memory and Computation Efficiency Gloeckle et al.: Optimizes memory via sequential head computation, reducing GPU memory overhead. Training speed remains comparable to baselines. DeepSeek-V3: Leverages shared embedding layers and output heads between MTP modules and the main model, further reducing parameter redundancy. The MoE architecture (with 37B activated parameters) combined with MTP enables efficient scaling. Inference Acceleration Strategies Gloeckle et al.: Self-speculative decoding using extra heads to pre-predict tokens, achieving 3× speedup on code tasks. DeepSeek-V3: MTP modules are repurposed for speculative decoding, with an 85–90% acceptance rate for the second predicted token, leading to 1.8× higher tokens per second (TPS). The MoE routing further optimizes communication during decoding. Domain Focus and Performance Gloeckle et al.: Primarily targets code and math tasks, demonstrating significant gains on HumanEval (12% improvement) and MBPP (17% improvement) for 13B models. DeepSeek-V3: MTP complements its MoE architecture to excel across broader domains: code (LiveCodeBench Pass@1: 40.5%), math (MATH-500 EM: 90.2%), and multilingual tasks (MMMLU-non-English: 79.4%). The model outperforms open-source baselines and rivals closed-source models like GPT-4o. Integration with Other Techniques Gloeckle et al.: MTP is a standalone training objective without auxiliary losses for load balancing. DeepSeek-V3: MTP is combined with an auxiliary-loss-free load balancing strategy for MoE, ensuring expert utilization while minimizing performance degradation. This integration enables more stable training for extremely large models. Key Differences Summary\nAspect Gloeckle et al. (2024) DeepSeek-V3 Architecture Parallel independent heads Sequential causal modules Prediction Flow Simultaneous parallel prediction Sequential prediction with causal chain Integration with MoE Not applicable (dense model) Tightly integrated with MoE routing Inference Speedup 3× via self-speculative decoding 1.8× TPS with high token acceptance Training Focus Code/math efficiency Broad-domain performance + MoE scaling Auxiliary Losses None (pure MTP) Combined with load-balancing strategy Both approaches leverage MTP to enhance training efficiency and inference speed, but DeepSeek-V3 extends the paradigm by integrating it with MoE architecture and auxiliary techniques, making it suitable for extremely large models with broader task coverage. Gloeckle et al.’s method is simpler and more focused on code/math tasks, demonstrating that MTP alone can drive significant improvements in sample efficiency and reasoning.\n","wordCount":"1383","inLanguage":"en","datePublished":"2025-06-29T00:00:00Z","dateModified":"2025-06-29T00:00:00Z","author":{"@type":"Person","name":"Cong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/multi-token-prediction/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Multi-token Prediction</h1><div class=post-meta><span title='2025-06-29 00:00:00 +0000 UTC'>2025-06-29</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Cong&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/llm-multi-token-prediction.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#multi-token-prediction-vs-next-token-prediction aria-label="Multi-token prediction vs Next-token prediction">Multi-token prediction vs Next-token prediction</a><ul><li><a href=#multi-token-prediction aria-label="Multi-token prediction">Multi-token prediction</a></li><li><a href=#comparison-with-multi-token-prediction-in-deepseek-v3 aria-label="Comparison with Multi-Token Prediction in DeepSeek-V3">Comparison with Multi-Token Prediction in DeepSeek-V3</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=multi-token-prediction-vs-next-token-prediction>Multi-token prediction vs Next-token prediction<a hidden class=anchor aria-hidden=true href=#multi-token-prediction-vs-next-token-prediction>#</a></h2><p>Next-token prediction is the standard training objective for most large language models (LLMs), where the model learns to predict the subsequent token in a sequence given all preceding tokens. The model is trained to maximize the probability of the next token \( x_{t+1} \) given the context \( x_{1:t} \) (all tokens up to position \( t \)).</p><p>The cross-entropy loss for next-token prediction is defined as:<br></p>\[
L_1 = -\sum_t \log P_\theta(x_{t+1} | x_{1:t})
\]<p><br>where \( P_\theta \) is the model parameterized by \( \theta \), and \( x_{1:t} \) denotes the sequence of tokens from position 1 to \( t \).</p><p>The model is trained in an autoregressive manner, where each prediction relies solely on the ground-truth history of tokens, not on its own prior predictions (teacher forcing).</p><p>The core idea of Multi-token prediction (MTP) (Gloeckle, Fabian, et al. Better & Faster Large Language Models via Multi-Token Prediction. arXiv:2404.19737, arXiv, 30 Apr. 2024. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2404.19737>https://doi.org/10.48550/arXiv.2404.19737</a>.) is to enable the model to predict multiple future tokens simultaneously at each position in the training corpus, which improves sample efficiency and inference speed.</p><p><strong>Comparison Between Next-Token Prediction and Multi-Token Prediction</strong></p><table><thead><tr><th>Aspect</th><th>Next-Token Prediction</th><th>Multi-Token Prediction (MTP) in the Papers</th></tr></thead><tbody><tr><td><strong>Prediction Scope</strong></td><td>Single future token</td><td>\( n \) future tokens simultaneously (e.g., \( n=4 \))</td></tr><tr><td><strong>Training Efficiency</strong></td><td>Requires more data due to local focus</td><td>Improves sample efficiency by capturing long-term dependencies</td></tr><tr><td><strong>Inference Speed</strong></td><td>Sequential decoding (slow for long sequences)</td><td>Speculative decoding via extra heads (3× faster in Gloeckle et al., 1.8× TPS in DeepSeek-V3)</td></tr><tr><td><strong>Architectural Cost</strong></td><td>Simple single head</td><td>Additional heads but with minimal overhead (sequential computation in Gloeckle et al., shared modules in DeepSeek-V3)</td></tr></tbody></table><h3 id=multi-token-prediction>Multi-token prediction<a hidden class=anchor aria-hidden=true href=#multi-token-prediction>#</a></h3><p>Gloeckle et al. argue that next-token prediction is inefficient because it focuses on local patterns and overlooks long-term dependencies. This requires significantly more training data compared to human learning (e.g., children learn language with far fewer examples). During inference, the model must generate tokens autoregressively (using its own prior predictions), while training uses ground-truth tokens. This creates a gap between training and inference, leading to error accumulation.</p><p><img loading=lazy src=/images/Meta-MTP-1.png></p><p>During training, for any time step t:</p><ol><li>The model uses a <strong>shared transformer trunk</strong> to generate a latent representation \( z_{t:1} \) from the context \( x_{t:1} \) (all tokens up to position \( t \)). This latent representation is then fed into \( n \) independent output heads, each dedicated to predicting a specific future token \( x_{t+i} \) for \( i = 1, \dots, n \), in parallel. More specifically, the model&rsquo;s heads for predicting tokens at positions \( t+k \) (where \( k > 1 \)) <strong>do not rely on previous predicted tokens</strong> (e.g., \( t+1 \)) during training. This design avoids additional training overhead by sequentially computing forward/backward passes for each head, reducing peak GPU memory usage from \(O(nV + d)\) to \(O(V + d)\). For example, to predict tokens at \( t+1, t+2, t+3, t+4 \), each head \( i \) directly uses \( z_{t:1} \) to generate \( P_\theta(x_{t+i} | x_{t:1}) \), without using the output of head \( i-1 \) (which predicts \( x_{t+i-1} \)). For the sequence <code>Hi, I would like _t1, _t2, _t3, _t4</code>, the prediction of \( _t2 \) ( \( t+2 \)) is based on <code>Hi, I would like</code> ( \( x_{t:1} \) where \( t \) is the position before \( _t1 \)), not on the predicted \( _t1 \). The model is trained to predict \( _t1, _t2, _t3, _t4 \) in parallel, each relying solely on the initial context, not on a chain of prior predictions. Unlike autoregressive decoding (where each step uses the previous token’s prediction), MTP during training is <strong>non-causal for future tokens</strong>—each head looks directly at the original context to predict its target token, avoiding dependency on intermediate predictions. This design enables parallel training and faster inference via speculative decoding. This is explicitly stated in the paper’s formula:<br>\[
P_\theta(x_{t+i} | x_{t:1}) = \text{softmax}(f_u(f_{h_i}(f_s(x_{t:1})))
\]<br>where \( f_s \) is the shared trunk, \( f_{h_i} \) is the \( i \)-th head, and \( f_u \) is the unembedding matrix. Each head \( f_{h_i} \) operates independently on \( f_s(x_{t:1}) \), not on previous heads’ outputs.</li><li>The loss function aggregates predictions for \( n \) future tokens simultaneously, but each prediction is conditioned only on the original context \( x_{t:1} \), not on intermediate predictions:
\[
L_n = -\sum_t \log P_\theta(x_{t+n:t+1} | x_{t:1})
\]
This encourages the model to learn longer-term dependencies and mitigates the distribution mismatch between teacher-forced training and autoregressive inference.</li></ol><p><img loading=lazy src=/images/Meta-MTP-2.png></p><p>During inference, only the next-token output head is employed. Optionally, the other extra heads enable self-speculative decoding, where the model pre-emptively predicts multiple tokens to skip redundant computations. This achieves up to 3× faster inference on code tasks with large batch sizes.</p><p>On code benchmarks like HumanEval and MBPP, 13B-parameter models with 4-token prediction solved 12% and 17% more problems, respectively, compared to next-token baselines. The approach also improved algorithmic reasoning and induction capabilities in synthetic tasks.</p><h3 id=comparison-with-multi-token-prediction-in-deepseek-v3>Comparison with Multi-Token Prediction in DeepSeek-V3<a hidden class=anchor aria-hidden=true href=#comparison-with-multi-token-prediction-in-deepseek-v3>#</a></h3><p>DeepSeek-V3(DeepSeek-AI, et al. DeepSeek-V3 Technical Report. arXiv:2412.19437, arXiv, 27 Dec. 2024. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2412.19437>https://doi.org/10.48550/arXiv.2412.19437</a>.
) notes that next-token prediction encourages the model to prioritize short-term dependencies, which may hinder its ability to learn global structures or complex reasoning tasks (e.g., coding, math). DeepSeek-V3 also employs MTP but with distinct architectural and operational differences:</p><ol><li><strong>Architectural Implementation</strong><ul><li>Gloeckle et al.: Parallel independent heads for simultaneous prediction of \(n\) tokens, with each head operating on the shared trunk&rsquo;s output. Example: 4-token prediction uses 4 separate heads.</li><li>DeepSeek-V3: Sequential MTP modules that maintain a causal chain for each prediction depth. Each module \(k\) combines the current token representation with the embedding of the \((i+k)\)-th token, passing through a Transformer block \(TRM_k\) before prediction. This design preserves the dependency chain for deeper context reasoning.</li></ul></li><li><strong>Training Objective and Loss Function</strong><ul><li>Gloeckle et al.: The loss is the average cross-entropy over \(n\) heads, with a fixed weight \(\lambda\) (e.g., \(\lambda = 0.3\) for early training).</li><li>DeepSeek-V3: MTP loss is a weighted average of cross-entropy losses from \(D\) sequential modules, with the weight \(\lambda\) decaying from 0.3 to 0.1 during training. The sequential design allows deeper integration with the model&rsquo;s causal attention mechanism.</li></ul></li><li><strong>Memory and Computation Efficiency</strong><ul><li>Gloeckle et al.: Optimizes memory via sequential head computation, reducing GPU memory overhead. Training speed remains comparable to baselines.</li><li>DeepSeek-V3: Leverages shared embedding layers and output heads between MTP modules and the main model, further reducing parameter redundancy. The MoE architecture (with 37B activated parameters) combined with MTP enables efficient scaling.</li></ul></li><li><strong>Inference Acceleration Strategies</strong><ul><li>Gloeckle et al.: Self-speculative decoding using extra heads to pre-predict tokens, achieving 3× speedup on code tasks.</li><li>DeepSeek-V3: MTP modules are repurposed for speculative decoding, with an 85–90% acceptance rate for the second predicted token, leading to 1.8× higher tokens per second (TPS). The MoE routing further optimizes communication during decoding.</li></ul></li><li><strong>Domain Focus and Performance</strong><ul><li>Gloeckle et al.: Primarily targets code and math tasks, demonstrating significant gains on HumanEval (12% improvement) and MBPP (17% improvement) for 13B models.</li><li>DeepSeek-V3: MTP complements its MoE architecture to excel across broader domains: code (LiveCodeBench Pass@1: 40.5%), math (MATH-500 EM: 90.2%), and multilingual tasks (MMMLU-non-English: 79.4%). The model outperforms open-source baselines and rivals closed-source models like GPT-4o.</li></ul></li><li><strong>Integration with Other Techniques</strong><ul><li>Gloeckle et al.: MTP is a standalone training objective without auxiliary losses for load balancing.</li><li>DeepSeek-V3: MTP is combined with an auxiliary-loss-free load balancing strategy for MoE, ensuring expert utilization while minimizing performance degradation. This integration enables more stable training for extremely large models.</li></ul></li></ol><p><img loading=lazy src=/images/DeepSeek-v3-MTP.png></p><p>Key Differences Summary</p><table><thead><tr><th>Aspect</th><th>Gloeckle et al. (2024)</th><th>DeepSeek-V3</th></tr></thead><tbody><tr><td><strong>Architecture</strong></td><td>Parallel independent heads</td><td>Sequential causal modules</td></tr><tr><td><strong>Prediction Flow</strong></td><td>Simultaneous parallel prediction</td><td>Sequential prediction with causal chain</td></tr><tr><td><strong>Integration with MoE</strong></td><td>Not applicable (dense model)</td><td>Tightly integrated with MoE routing</td></tr><tr><td><strong>Inference Speedup</strong></td><td>3× via self-speculative decoding</td><td>1.8× TPS with high token acceptance</td></tr><tr><td><strong>Training Focus</strong></td><td>Code/math efficiency</td><td>Broad-domain performance + MoE scaling</td></tr><tr><td><strong>Auxiliary Losses</strong></td><td>None (pure MTP)</td><td>Combined with load-balancing strategy</td></tr></tbody></table><p>Both approaches leverage MTP to enhance training efficiency and inference speed, but DeepSeek-V3 extends the paradigm by integrating it with MoE architecture and auxiliary techniques, making it suitable for extremely large models with broader task coverage. Gloeckle et al.&rsquo;s method is simpler and more focused on code/math tasks, demonstrating that MTP alone can drive significant improvements in sample efficiency and reasoning.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/2025/>2025</a></li><li><a href=https://congchan.github.io/tags/large-language-model/>Large Language Model</a></li><li><a href=https://congchan.github.io/tags/multi-token-prediction/>Multi-Token Prediction</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/><span class=title>« Prev</span><br><span>Awesome Large Language Model (LLM) Post-training - [2025 Update]</span>
</a><a class=next href=https://congchan.github.io/posts/the-evolution-of-reward-modeling-from-human-feedback-to-generative-inference-time-scaling/><span class=title>Next »</span><br><span>The Evolution of Reward Modeling - From Human Feedback to Generative Inference-Time Scaling</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Multi-token Prediction on x" href="https://x.com/intent/tweet/?text=Multi-token%20Prediction&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fmulti-token-prediction%2f&amp;hashtags=2025%2cLargeLanguageModel%2cMulti-tokenPrediction"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multi-token Prediction on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fmulti-token-prediction%2f&amp;title=Multi-token%20Prediction&amp;summary=Multi-token%20Prediction&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fmulti-token-prediction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multi-token Prediction on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fmulti-token-prediction%2f&title=Multi-token%20Prediction"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multi-token Prediction on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fmulti-token-prediction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multi-token Prediction on whatsapp" href="https://api.whatsapp.com/send?text=Multi-token%20Prediction%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fmulti-token-prediction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multi-token Prediction on telegram" href="https://telegram.me/share/url?text=Multi-token%20Prediction&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fmulti-token-prediction%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multi-token Prediction on ycombinator" href="https://news.ycombinator.com/submitlink?t=Multi-token%20Prediction&u=https%3a%2f%2fcongchan.github.io%2fposts%2fmulti-token-prediction%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>