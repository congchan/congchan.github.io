<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Better Practice to Define Reward Model with HuggingFace's transformers | Cong's Log</title><meta name=keywords content="LLM,Reward Modeling,RLHF"><meta name=description content="Cong Chen
University of Edinburgh
There are various implementation of reward modeling in RLHF(reinforcement learning with human feedback), each has different pros and cons. Inspired by some open-sourced works about reward modeling, I would like to share one of the best practice for reward modeling. For those who are not familiar with reward modeling and RLHF, I recommend take a look at the Huggingface rlhf blog1 or OpenAI rlhf paper2."><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/a-better-practice-to-define-reward-model-with-huggingfaces-transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/a-better-practice-to-define-reward-model-with-huggingfaces-transformers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/a-better-practice-to-define-reward-model-with-huggingfaces-transformers/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="A Better Practice to Define Reward Model with HuggingFace's transformers"><meta property="og:description" content="Cong Chen
University of Edinburgh
There are various implementation of reward modeling in RLHF(reinforcement learning with human feedback), each has different pros and cons. Inspired by some open-sourced works about reward modeling, I would like to share one of the best practice for reward modeling. For those who are not familiar with reward modeling and RLHF, I recommend take a look at the Huggingface rlhf blog1 or OpenAI rlhf paper2."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-25T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-25T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Reward Modeling"><meta property="article:tag" content="RLHF"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Better Practice to Define Reward Model with HuggingFace's transformers"><meta name=twitter:description content="Cong Chen
University of Edinburgh
There are various implementation of reward modeling in RLHF(reinforcement learning with human feedback), each has different pros and cons. Inspired by some open-sourced works about reward modeling, I would like to share one of the best practice for reward modeling. For those who are not familiar with reward modeling and RLHF, I recommend take a look at the Huggingface rlhf blog1 or OpenAI rlhf paper2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"A Better Practice to Define Reward Model with HuggingFace's transformers","item":"https://congchan.github.io/posts/a-better-practice-to-define-reward-model-with-huggingfaces-transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Better Practice to Define Reward Model with HuggingFace's transformers","name":"A Better Practice to Define Reward Model with HuggingFace\u0027s transformers","description":"Cong Chen\nUniversity of Edinburgh\nThere are various implementation of reward modeling in RLHF(reinforcement learning with human feedback), each has different pros and cons. Inspired by some open-sourced works about reward modeling, I would like to share one of the best practice for reward modeling. For those who are not familiar with reward modeling and RLHF, I recommend take a look at the Huggingface rlhf blog1 or OpenAI rlhf paper2.\n","keywords":["LLM","Reward Modeling","RLHF"],"articleBody":"Cong Chen\nUniversity of Edinburgh\nThere are various implementation of reward modeling in RLHF(reinforcement learning with human feedback), each has different pros and cons. Inspired by some open-sourced works about reward modeling, I would like to share one of the best practice for reward modeling. For those who are not familiar with reward modeling and RLHF, I recommend take a look at the Huggingface rlhf blog1 or OpenAI rlhf paper2.\nSuggested Practice Reward modeling involves training a model to predict a scalar value as a reward for a given input text. This is achieved through a high-level architecture consisting of a deep transformers block and a value head with an output dimension of one. To streamline the training and inference stages, the AutoModelForSequenceClassification class can be utilized. One effective way to ensure consistency and compatibility is to use Huggingface transformers’ self-defined model with auto_map. This involves defining different reward model backbones depending on the model type (such as Llama3 or Bloom4), then using the from_pretrained method to load the model for either training or inference purposes.\nLet us use a Llama pre-trained causal language model from HuggingFace decapoda-research/llama-7b-hf5 as an example and provide step-by-step instructions.\nFirst, we would like to train this pre-trained causal language model on some reward datasets(such as the Anthropic/hh-rlhf Datasets6). Let’s create a script called modeling_llama_rw.py in the pretrained model directory to let transformers know how to load the model.\nfrom transformers import LlamaPreTrainedModel, LlamaConfig, LlamaModel import torch.nn as nn import torch class LlamaRewardModel(LlamaPreTrainedModel): \"\"\" The LLaMa Model transformer with a sequence classification head on top (linear layer). uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the last token. If a `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in each row of the batch). \"\"\" config_class = LlamaConfig def __init__(self, config): super().__init__(config) self.num_labels = 1 self.model = LlamaModel(config) self.value_head = nn.Linear(config.hidden_size, self.num_labels) def get_input_embeddings(self): return self.model.embed_tokens def set_input_embeddings(self, value): self.model.embed_tokens = value def gradient_checkpointing_enable(self): self.model.gradient_checkpointing_enable() def forward(): ... Next, we need to add some extra information to the config.json file by adding a new class called LlamaRewardModel in the \"architectures\" key and a new class mapping of \"auto_map\": [\"AutoModelForSequenceClassification\": \"modeling_llama_rm.LlamaRewardModel\"].\n{ \"_name_or_path\": \"LLaMA-7B-Reward\", \"architectures\": [ \"LlamaRewardModel\" ], \"auto_map\": { \"AutoModelForSequenceClassification\": \"modeling_llama_rm.LlamaRewardModel\" }, ... # we could leave the rest untouched } With these changes, we can now load the model using the following line: model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, trust_remote_code=True). The trust_remote_code=True is required to make sure transformers API could call our customed class. This line can be used for both training and inference stages. After training the model, we can share it, along with the configuration and modeling files, for downstream use.\nI have also provided an example of Bloom reward modeling in the my GitHub repository7.\nSome Details in Reward Modeling There is no definitive method for calculating scalar rewards based on the output logits of input text. However, OpenAI has provided some reference implementations for us to learn from. They use the output logits of the final token of the input sentence, including paddings. This is similar to the Huggingface AutoModelForSequenceClassification. You can directly use this class.\nMy implementation adds more fine-grained details to the process of computing scalar rewards. Specifically, the logit of the last token before the sentence padding is retrieved to compute the reward. In cases where the input sentence is too long to contain padding, the last token of the truncated sentence is used instead. All of these details can be defined in the forward function of the modeling class. Additionally, aligning the model’s pad_token_id with how the tokenizer preprocesses the input data is necessary for this implementation, which can be easily achieved by setting model.config.pad_token_id = tokenizer.pad_token_id.\nfrom transformers import LlamaPreTrainedModel, LlamaConfig, LlamaModel import torch.nn as nn import torch class LlamaRewardModel(LlamaPreTrainedModel): \"\"\" The LLaMa Model transformer with a sequence classification head on top (linear layer). uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the last token. If a `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in each row of the batch). \"\"\" config_class = LlamaConfig def __init__(self, config): super().__init__(config) self.num_labels = 1 self.model = LlamaModel(config) self.value_head = nn.Linear(config.hidden_size, self.num_labels) def forward( self, input_ids=None, past_key_values=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, mc_token_ids=None, labels=None, lm_labels=None, mc_labels=None, return_dict=False, output_attentions=False, output_hidden_states=False, ): transformer_outputs = self.model( input_ids, attention_mask=attention_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, ) hidden_states = transformer_outputs[0] rewards = self.value_head(hidden_states).squeeze(-1) pad_token_id = self.config.pad_token_id if pad_token_id is None: pad_token_id = self.model.config.pad_token_id ends = input_ids.shape[1] - (input_ids == pad_token_id).type(torch.int64).sum(dim=1).view(-1, 1) ends = torch.clamp(ends - 1, min=0) rewards = torch.gather(rewards, 1, ends) return rewards def get_input_embeddings(self): return self.model.embed_tokens def set_input_embeddings(self, value): self.model.embed_tokens = value def gradient_checkpointing_enable(self): self.model.gradient_checkpointing_enable() Introduction to Other Noteworthy Implementations DeepSpeedChat’s solution89 involves building an nn.module model and utilizing Huggingface‘s Pre-trained Transformers blocks as self.base_model. However, in my opinion, this approach is not ideal as it lacks consistency between the training and inference stages. Nonetheless, their breakdown of the Huggingface API and torch API showcases a well-designed implementation.\ndef create_hf_model(model_class, model_name_or_path, tokenizer, ds_config=None, rlhf_training=False, disable_dropout=False): model_config = AutoConfig.from_pretrained(model_name_or_path) if disable_dropout: model_config.dropout = 0.0 # Note: dschf is defined in function scope to avoid global effects # https://huggingface.co/docs/transformers/main_classes/deepspeed#nontrainer-deepspeed-integration if ds_config is not None and ds_config[\"zero_optimization\"][\"stage\"] == 3: dschf = HfDeepSpeedConfig(ds_config) else: dschf = None if rlhf_training: # the weight loading is handled by create critic model model = model_class.from_config(model_config) else: model = model_class.from_pretrained( model_name_or_path, from_tf=bool(\".ckpt\" in model_name_or_path), config=model_config) model.config.end_token_id = tokenizer.eos_token_id model.config.pad_token_id = model.config.eos_token_id model.resize_token_embeddings(int( 8 * math.ceil(len(tokenizer) / 8.0))) # make the vocab size multiple of 8 return model def create_critic_model(model_name_or_path, tokenizer, ds_config, num_padding_at_beginning=0, rlhf_training=False, disable_dropout=False): # OPT model family always put a padding token at the beginning of the sequence, # we did not see this in other models but not sure if it is a general rule critic_model = create_hf_model(AutoModel, model_name_or_path, tokenizer, ds_config, rlhf_training, disable_dropout) critic_model = RewardModel( critic_model, tokenizer, num_padding_at_beginning=num_padding_at_beginning) if rlhf_training: if not os.path.isdir(model_name_or_path): model_name_or_path = snapshot_download(model_name_or_path) # critic model needs to load the weight here model_ckpt_path = os.path.join(model_name_or_path, 'pytorch_model.bin') assert os.path.exists( model_ckpt_path ), f\"Cannot find model checkpoint at {model_ckpt_path}\" critic_model.load_state_dict( torch.load(model_ckpt_path, map_location='cpu')) return critic_model This is how DeepSpeedChat define their reward_model:\n# Copyright (c) Microsoft Corporation. # SPDX-License-Identifier: Apache-2.0 # DeepSpeed Team import torch from torch import nn ## Note that the following code is modified from ## https://github.com/CarperAI/trlx/blob/main/examples/summarize_rlhf/reward_model/reward_model.py class RewardModel(nn.Module): def __init__(self, base_model, tokenizer, num_padding_at_beginning=0): super().__init__() self.config = base_model.config self.num_padding_at_beginning = num_padding_at_beginning if hasattr(self.config, \"word_embed_proj_dim\"): # `OPT` models use word_embed_proj_dim as final output # https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L497 self.v_head = nn.Linear(self.config.word_embed_proj_dim, 1, bias=False) else: # `gpt-neo(x)` models use `hidden_size` attribute names instead of `n_embd`` self.config.n_embd = self.config.hidden_size if hasattr( self.config, \"hidden_size\") else self.config.n_embd self.v_head = nn.Linear(self.config.n_embd, 1, bias=False) self.rwtranrsformer = base_model self.PAD_ID = tokenizer.pad_token_id def gradient_checkpointing_enable(self): self.rwtranrsformer.gradient_checkpointing_enable() def gradient_checkpointing_disable(self): self.rwtranrsformer.gradient_checkpointing_disable() https://huggingface.co/blog/rlhf ↩︎\nOuyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155. ↩︎\nhttps://ai.facebook.com/blog/large-language-model-llama-meta-ai/ ↩︎\nBigScience, BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. International, May 2021-May 2022 ↩︎\nhttps://huggingface.co/decapoda-research/llama-7b-hf ↩︎\nhttps://huggingface.co/datasets/Anthropic/hh-rlhf ↩︎\nhttps://github.com/congchan/rlhf_exps/tree/main/reward_modeling/test/bloomz-7b1 ↩︎\nmicrosoft/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/model/model_utils.py#L18 ↩︎\nmicrosoft/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/model/reward_model.py#L11 ↩︎\n","wordCount":"1285","inLanguage":"en","datePublished":"2023-09-25T00:00:00Z","dateModified":"2023-09-25T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/a-better-practice-to-define-reward-model-with-huggingfaces-transformers/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">A Better Practice to Define Reward Model with HuggingFace's transformers</h1><div class=post-meta><span title='2023-09-25 00:00:00 +0000 UTC'>2023-09-25</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/llm-a-better-practice-of-reward-modeling-for-large-language-models.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#suggested-practice aria-label="Suggested Practice">Suggested Practice</a></li><li><a href=#some-details-in-reward-modeling aria-label="Some Details in Reward Modeling">Some Details in Reward Modeling</a></li><li><a href=#introduction-to-other-noteworthy-implementations aria-label="Introduction to Other Noteworthy Implementations">Introduction to Other Noteworthy Implementations</a></li></ul></div></details></div><div class=post-content><p><a href=https://congchan.github.io/>Cong Chen</a><br>University of Edinburgh</p><p>There are various implementation of reward modeling in RLHF(reinforcement learning with human feedback), each has different pros and cons. Inspired by some open-sourced works about reward modeling, I would like to share one of the best practice for reward modeling. For those who are not familiar with reward modeling and RLHF, I recommend take a look at the Huggingface rlhf blog<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> or OpenAI rlhf paper<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><h1 id=suggested-practice>Suggested Practice<a hidden class=anchor aria-hidden=true href=#suggested-practice>#</a></h1><p>Reward modeling involves training a model to predict a scalar value as a reward for a given input text. This is achieved through a high-level architecture consisting of a deep transformers block and a value head with an output dimension of one. To streamline the training and inference stages, the <code>AutoModelForSequenceClassification</code> class can be utilized. One effective way to ensure consistency and compatibility is to use Huggingface transformers&rsquo; self-defined model with <code>auto_map</code>. This involves defining different reward model backbones depending on the model type (such as Llama<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> or Bloom<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>), then using the <code>from_pretrained</code> method to load the model for either training or inference purposes.</p><p>Let us use a Llama pre-trained causal language model from HuggingFace decapoda-research/llama-7b-hf<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> as an example and provide step-by-step instructions.</p><p>First, we would like to train this pre-trained causal language model on some reward datasets(such as the Anthropic/hh-rlhf Datasets<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>). Let’s create a script called <code>modeling_llama_rw.py</code> in the pretrained model directory to let transformers know how to load the model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>LlamaPreTrainedModel</span><span class=p>,</span> <span class=n>LlamaConfig</span><span class=p>,</span> <span class=n>LlamaModel</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LlamaRewardModel</span><span class=p>(</span><span class=n>LlamaPreTrainedModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    The LLaMa Model transformer with a sequence classification head on top (linear layer).
</span></span></span><span class=line><span class=cl><span class=s2>    uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do.
</span></span></span><span class=line><span class=cl><span class=s2>    Since it does classification on the last token, it requires to know the position of the last token. If a
</span></span></span><span class=line><span class=cl><span class=s2>    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
</span></span></span><span class=line><span class=cl><span class=s2>    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
</span></span></span><span class=line><span class=cl><span class=s2>    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
</span></span></span><span class=line><span class=cl><span class=s2>    each row of the batch).
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>config_class</span> <span class=o>=</span> <span class=n>LlamaConfig</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_labels</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>LlamaModel</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>value_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>embed_tokens</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>set_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>value</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=n>value</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>gradient_checkpointing_enable</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>gradient_checkpointing_enable</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=o>...</span>
</span></span></code></pre></div><p>Next, we need to add some extra information to the config.json file by adding a new class called <code>LlamaRewardModel</code> in the <code>"architectures"</code> key and a new class mapping of <code>"auto_map": ["AutoModelForSequenceClassification": "modeling_llama_rm.LlamaRewardModel"]</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;_name_or_path&#34;</span><span class=p>:</span> <span class=s2>&#34;LLaMA-7B-Reward&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;architectures&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;LlamaRewardModel&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;auto_map&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;AutoModelForSequenceClassification&#34;</span><span class=p>:</span> <span class=s2>&#34;modeling_llama_rm.LlamaRewardModel&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=o>...</span>  <span class=c1># we could leave the rest untouched</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>With these changes, we can now load the model using the following line: <code>model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, trust_remote_code=True)</code>. The <code>trust_remote_code=True</code> is required to make sure transformers API could call our customed class. This line can be used for both training and inference stages. After training the model, we can share it, along with the configuration and modeling files, for downstream use.</p><p>I have also provided an example of Bloom reward modeling in the my GitHub repository<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>.</p><h1 id=some-details-in-reward-modeling>Some Details in Reward Modeling<a hidden class=anchor aria-hidden=true href=#some-details-in-reward-modeling>#</a></h1><p>There is no definitive method for calculating scalar rewards based on the output logits of input text. However, OpenAI has provided some reference implementations for us to learn from. They use the output logits of the final token of the input sentence, including paddings. This is similar to the Huggingface <code>AutoModelForSequenceClassification</code>. You can directly use this class.</p><p>My implementation adds more fine-grained details to the process of computing scalar rewards. Specifically, the logit of the last token before the sentence padding is retrieved to compute the reward. In cases where the input sentence is too long to contain padding, the last token of the truncated sentence is used instead. All of these details can be defined in the forward function of the modeling class. Additionally, aligning the model&rsquo;s pad_token_id with how the tokenizer preprocesses the input data is necessary for this implementation, which can be easily achieved by setting <code>model.config.pad_token_id = tokenizer.pad_token_id</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>LlamaPreTrainedModel</span><span class=p>,</span> <span class=n>LlamaConfig</span><span class=p>,</span> <span class=n>LlamaModel</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LlamaRewardModel</span><span class=p>(</span><span class=n>LlamaPreTrainedModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    The LLaMa Model transformer with a sequence classification head on top (linear layer).
</span></span></span><span class=line><span class=cl><span class=s2>    uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do.
</span></span></span><span class=line><span class=cl><span class=s2>    Since it does classification on the last token, it requires to know the position of the last token. If a
</span></span></span><span class=line><span class=cl><span class=s2>    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
</span></span></span><span class=line><span class=cl><span class=s2>    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
</span></span></span><span class=line><span class=cl><span class=s2>    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
</span></span></span><span class=line><span class=cl><span class=s2>    each row of the batch).
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>config_class</span> <span class=o>=</span> <span class=n>LlamaConfig</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_labels</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>LlamaModel</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>value_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>input_ids</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>past_key_values</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>token_type_ids</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>position_ids</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>head_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>inputs_embeds</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>mc_token_ids</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>labels</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>lm_labels</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>mc_labels</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>return_dict</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>output_attentions</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>output_hidden_states</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>transformer_outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>past_key_values</span><span class=o>=</span><span class=n>past_key_values</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>inputs_embeds</span><span class=o>=</span><span class=n>inputs_embeds</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>transformer_outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>value_head</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pad_token_id</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>pad_token_id</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>pad_token_id</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span>
</span></span><span class=line><span class=cl>        <span class=n>ends</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=p>(</span><span class=n>input_ids</span> <span class=o>==</span> <span class=n>pad_token_id</span><span class=p>)</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>int64</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ends</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>ends</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>min</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>rewards</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>ends</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>rewards</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>embed_tokens</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>set_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>value</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=n>value</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>gradient_checkpointing_enable</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>gradient_checkpointing_enable</span><span class=p>()</span>
</span></span></code></pre></div><h1 id=introduction-to-other-noteworthy-implementations>Introduction to Other Noteworthy Implementations<a hidden class=anchor aria-hidden=true href=#introduction-to-other-noteworthy-implementations>#</a></h1><p>DeepSpeedChat&rsquo;s solution<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup><sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> involves building an <code>nn.module</code> model and utilizing Huggingface‘s Pre-trained Transformers blocks as <code>self.base_model</code>. However, in my opinion, this approach is not ideal as it lacks consistency between the training and inference stages. Nonetheless, their breakdown of the Huggingface API and torch API showcases a well-designed implementation.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_hf_model</span><span class=p>(</span><span class=n>model_class</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>model_name_or_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>ds_config</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>rlhf_training</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>disable_dropout</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model_config</span> <span class=o>=</span> <span class=n>AutoConfig</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name_or_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>disable_dropout</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>model_config</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=c1># Note: dschf is defined in function scope to avoid global effects</span>
</span></span><span class=line><span class=cl>    <span class=c1># https://huggingface.co/docs/transformers/main_classes/deepspeed#nontrainer-deepspeed-integration</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>ds_config</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>ds_config</span><span class=p>[</span><span class=s2>&#34;zero_optimization&#34;</span><span class=p>][</span><span class=s2>&#34;stage&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=mi>3</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dschf</span> <span class=o>=</span> <span class=n>HfDeepSpeedConfig</span><span class=p>(</span><span class=n>ds_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dschf</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>rlhf_training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># the weight loading is handled by create critic model</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>model_class</span><span class=o>.</span><span class=n>from_config</span><span class=p>(</span><span class=n>model_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>model_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>model_name_or_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>from_tf</span><span class=o>=</span><span class=nb>bool</span><span class=p>(</span><span class=s2>&#34;.ckpt&#34;</span> <span class=ow>in</span> <span class=n>model_name_or_path</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>=</span><span class=n>model_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>end_token_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token_id</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>eos_token_id</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>resize_token_embeddings</span><span class=p>(</span><span class=nb>int</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=mi>8</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>        <span class=n>math</span><span class=o>.</span><span class=n>ceil</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokenizer</span><span class=p>)</span> <span class=o>/</span> <span class=mf>8.0</span><span class=p>)))</span>  <span class=c1># make the vocab size multiple of 8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_critic_model</span><span class=p>(</span><span class=n>model_name_or_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>ds_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>num_padding_at_beginning</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>rlhf_training</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>disable_dropout</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># OPT model family always put a padding token at the beginning of the sequence,</span>
</span></span><span class=line><span class=cl>    <span class=c1># we did not see this in other models but not sure if it is a general rule</span>
</span></span><span class=line><span class=cl>    <span class=n>critic_model</span> <span class=o>=</span> <span class=n>create_hf_model</span><span class=p>(</span><span class=n>AutoModel</span><span class=p>,</span> <span class=n>model_name_or_path</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                   <span class=n>ds_config</span><span class=p>,</span> <span class=n>rlhf_training</span><span class=p>,</span> <span class=n>disable_dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>critic_model</span> <span class=o>=</span> <span class=n>RewardModel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>critic_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>num_padding_at_beginning</span><span class=o>=</span><span class=n>num_padding_at_beginning</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>rlhf_training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>isdir</span><span class=p>(</span><span class=n>model_name_or_path</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>model_name_or_path</span> <span class=o>=</span> <span class=n>snapshot_download</span><span class=p>(</span><span class=n>model_name_or_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># critic model needs to load the weight here</span>
</span></span><span class=line><span class=cl>        <span class=n>model_ckpt_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>model_name_or_path</span><span class=p>,</span> <span class=s1>&#39;pytorch_model.bin&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>exists</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>model_ckpt_path</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span> <span class=sa>f</span><span class=s2>&#34;Cannot find model checkpoint at </span><span class=si>{</span><span class=n>model_ckpt_path</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>critic_model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>model_ckpt_path</span><span class=p>,</span> <span class=n>map_location</span><span class=o>=</span><span class=s1>&#39;cpu&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>critic_model</span>
</span></span></code></pre></div><p>This is how DeepSpeedChat define their reward_model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Copyright (c) Microsoft Corporation.</span>
</span></span><span class=line><span class=cl><span class=c1># SPDX-License-Identifier: Apache-2.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># DeepSpeed Team</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Note that the following code is modified from</span>
</span></span><span class=line><span class=cl><span class=c1>## https://github.com/CarperAI/trlx/blob/main/examples/summarize_rlhf/reward_model/reward_model.py</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>RewardModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>base_model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>num_padding_at_beginning</span><span class=o>=</span><span class=mi>0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>base_model</span><span class=o>.</span><span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_padding_at_beginning</span> <span class=o>=</span> <span class=n>num_padding_at_beginning</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>,</span> <span class=s2>&#34;word_embed_proj_dim&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># `OPT` models use word_embed_proj_dim as final output</span>
</span></span><span class=line><span class=cl>            <span class=c1># https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L497</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>v_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>word_embed_proj_dim</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># `gpt-neo(x)` models use `hidden_size` attribute names instead of `n_embd``</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>n_embd</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span> <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>,</span> <span class=s2>&#34;hidden_size&#34;</span><span class=p>)</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>n_embd</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>v_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>n_embd</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rwtranrsformer</span> <span class=o>=</span> <span class=n>base_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>PAD_ID</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>gradient_checkpointing_enable</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rwtranrsformer</span><span class=o>.</span><span class=n>gradient_checkpointing_enable</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>gradient_checkpointing_disable</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rwtranrsformer</span><span class=o>.</span><span class=n>gradient_checkpointing_disable</span><span class=p>()</span>
</span></span></code></pre></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://huggingface.co/blog/rlhf>https://huggingface.co/blog/rlhf</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, <a href=http://arxiv.org/abs/2203.02155>http://arxiv.org/abs/2203.02155</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://ai.facebook.com/blog/large-language-model-llama-meta-ai/>https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>BigScience, BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. International, May 2021-May 2022&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a href=https://huggingface.co/decapoda-research/llama-7b-hf>https://huggingface.co/decapoda-research/llama-7b-hf</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p><a href=https://huggingface.co/datasets/Anthropic/hh-rlhf>https://huggingface.co/datasets/Anthropic/hh-rlhf</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p><a href=https://github.com/congchan/rlhf_exps/tree/main/reward_modeling/test/bloomz-7b1>https://github.com/congchan/rlhf_exps/tree/main/reward_modeling/test/bloomz-7b1</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p><a href=https://github.com/microsoft/DeepSpeedExamples/blob/95adffb17720b66d2888793851e4652ae28202ba/applications/DeepSpeed-Chat/training/utils/model/model_utils.py#L18>microsoft/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/model/model_utils.py#L18</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p><a href=https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/utils/model/reward_model.py#L11>microsoft/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/model/reward_model.py#L11</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/llm/>LLM</a></li><li><a href=https://congchan.github.io/tags/reward-modeling/>Reward Modeling</a></li><li><a href=https://congchan.github.io/tags/rlhf/>RLHF</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/paper-reading-weak-to-strong-generalization-eliciting-strong-capabilities-with-weak-supervision/><span class=title>« Prev</span><br><span>Paper Reading - Weak-to-Strong Generalization - Eliciting Strong Capabilities With Weak Supervision</span>
</a><a class=next href=https://congchan.github.io/posts/boosting-large-language-models-alignment-a-data-driven-bootstrap-flywheel/><span class=title>Next »</span><br><span>Boosting Large Language Models Alignment - A Data-Driven Bootstrap Flywheel</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share A Better Practice to Define Reward Model with HuggingFace's transformers on x" href="https://x.com/intent/tweet/?text=A%20Better%20Practice%20to%20Define%20Reward%20Model%20with%20HuggingFace%27s%20transformers&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fa-better-practice-to-define-reward-model-with-huggingfaces-transformers%2f&amp;hashtags=LLM%2cRewardModeling%2cRLHF"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Better Practice to Define Reward Model with HuggingFace's transformers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fa-better-practice-to-define-reward-model-with-huggingfaces-transformers%2f&amp;title=A%20Better%20Practice%20to%20Define%20Reward%20Model%20with%20HuggingFace%27s%20transformers&amp;summary=A%20Better%20Practice%20to%20Define%20Reward%20Model%20with%20HuggingFace%27s%20transformers&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fa-better-practice-to-define-reward-model-with-huggingfaces-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Better Practice to Define Reward Model with HuggingFace's transformers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fa-better-practice-to-define-reward-model-with-huggingfaces-transformers%2f&title=A%20Better%20Practice%20to%20Define%20Reward%20Model%20with%20HuggingFace%27s%20transformers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Better Practice to Define Reward Model with HuggingFace's transformers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fa-better-practice-to-define-reward-model-with-huggingfaces-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Better Practice to Define Reward Model with HuggingFace's transformers on whatsapp" href="https://api.whatsapp.com/send?text=A%20Better%20Practice%20to%20Define%20Reward%20Model%20with%20HuggingFace%27s%20transformers%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fa-better-practice-to-define-reward-model-with-huggingfaces-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Better Practice to Define Reward Model with HuggingFace's transformers on telegram" href="https://telegram.me/share/url?text=A%20Better%20Practice%20to%20Define%20Reward%20Model%20with%20HuggingFace%27s%20transformers&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fa-better-practice-to-define-reward-model-with-huggingfaces-transformers%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Better Practice to Define Reward Model with HuggingFace's transformers on ycombinator" href="https://news.ycombinator.com/submitlink?t=A%20Better%20Practice%20to%20Define%20Reward%20Model%20with%20HuggingFace%27s%20transformers&u=https%3a%2f%2fcongchan.github.io%2fposts%2fa-better-practice-to-define-reward-model-with-huggingfaces-transformers%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>