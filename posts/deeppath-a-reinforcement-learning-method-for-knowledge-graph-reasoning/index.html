<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning | Cong's Log</title><meta name=keywords content="NLP,RL,2017,EMNLP,Knowledge Graph Reasoning"><meta name=description content="2017, EMNLP
data: FB15K-237, FB15K
task: Knowledge Graph Reasoning

Use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.
方法
RL 系统包含两部分，

第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。
系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。


关系推理的强化学习
行为 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。
状态 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量："><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning"><meta property="og:description" content="2017, EMNLP
data: FB15K-237, FB15K
task: Knowledge Graph Reasoning
Use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.
方法 RL 系统包含两部分，
第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。 系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。 关系推理的强化学习 行为 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。
状态 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量："><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-03-11T00:00:00+00:00"><meta property="article:modified_time" content="2020-03-11T00:00:00+00:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="RL"><meta property="article:tag" content="2017"><meta property="article:tag" content="EMNLP"><meta property="article:tag" content="Knowledge Graph Reasoning"><meta name=twitter:card content="summary"><meta name=twitter:title content="DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning"><meta name=twitter:description content="2017, EMNLP
data: FB15K-237, FB15K
task: Knowledge Graph Reasoning

Use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.
方法
RL 系统包含两部分，

第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。
系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。


关系推理的强化学习
行为 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。
状态 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning","item":"https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning","name":"DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning","description":"2017, EMNLP\ndata: FB15K-237, FB15K\ntask: Knowledge Graph Reasoning\nUse a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.\n方法 RL 系统包含两部分，\n第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。 系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。 关系推理的强化学习 行为 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。\n状态 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量：\n","keywords":["NLP","RL","2017","EMNLP","Knowledge Graph Reasoning"],"articleBody":"2017, EMNLP\ndata: FB15K-237, FB15K\ntask: Knowledge Graph Reasoning\nUse a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.\n方法 RL 系统包含两部分，\n第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。 系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。 关系推理的强化学习 行为 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。\n状态 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量：\n其中 e.t 表示当前实体结点的嵌入，e.target 表示目标实体的嵌入。在最初状态，e.t 即 e.source。我们没有在状态中加入推理关系，因为在寻路过程中推理关系的嵌入保持不变，不利于训练。然而，我们发现通过使用一组特定关系的正样本训练 RL 代理，该 智能体 可以成功地发现关系语义。\n奖励 对于我们的环境设置，智能体 可以执行的操作数量可能非常大。换句话说，错误的顺序决策比正确的顺序决策多得多。这些错误的决策序列的数量会随着路径的长度呈指数增长。\nGlobal accuracy： Path efficiency Path diversity: 策略网络 我们使用全连接神经网络来参数化策略函数，它讲状态向量映射到所有可能行为的概率分布上。神经网络包含两个隐藏层，每一层后接 ReLU。输出层通过 softmax 函数归一化。\n3.2 训练 对于一个典型的KG, RL 智能体 常常面临上千种可能的操作。换句话说，策略网络的输出层具有较大的维数。由于关系图的复杂性和较大的动作空间，如果直接采用 RL 算法中典型的试错推理来训练RL模型，将会导致 RL 模型收敛性很差。经过长时间的训练，智能体都可能无法找到任何有价值的路径。\n为了解决这个问题，我们从一个监督策略开始我们的训练，这个策略的灵感来自 AlphaGo 使用的模仿学习流水线。在围棋游戏中，玩家每走一步都要面对近 250 种可能的合法走法。直接训练智能体从原始动作空间中挑选动作可能是一项困难的任务。AlphaGo 首先使用专家训练一个有监督的策略网络。在该例子中，使用随机的广度优先搜索(BFS)训练监督策略。\n监督策略学习 对于每个关系，我们首先使用所有正样本（实体对）的子集来学习有监督的策略。对于每个正样本(esource, etarget)，一个两端 BFS 被用于找到实体之间的正确路径。对于路径 p，使用蒙塔卡洛策略梯度（REINFORCE 方法）来最大化期望的累积奖励。\n原生 BFS 是有偏的搜索算法，它倾向于使用短路径。当插入这些有偏向的路径时，agent 很难找到可能有用的较长路径。我们希望路径仅由定义的奖励函数控制。为了防止偏向搜索，我们采用了一种简单的技巧为 BFS 添加一些随机机制。我们不是直接搜索 esource 和 etarget 之间的路径，而是随机选择一个中间节点einter，然后在（esource，einter）和（einter，etarget）之间进行两个 BFS。连接的路径用于训练智能体。监督学习可以节省智能体从失败行为中学习的大量精力。借助所学的经验，我们然后训练智能体寻找理想的路径。\nRetraining with Rewards 为了找到受奖励函数控制的推理路径，我们使用奖励函数来限制监督策略网络。对于每个关系，一个实体对的推理被视为一个事件(episode)。从源结点开始，智能体根据随机策略选择关系，它是所有关系上的概率分布，以扩展推理路径。关系链接可能引向一个新实体，或者失败。这些失败的步骤可能导致智能体获得负奖励。智能体在失败步骤后保持状态。由于智能体遵循随机策略，所以智能体不会因为重复错误的步骤而陷入困境。为了提高训练效率，我们将训练集长度设定一个上限。上限达到时，如智能体仍未找到目标实体则事件结束。每个事件结束后，策略网络通过以下梯度进行更新：\n3.3 Bi-directional Path-constrained Search In a typical KG, one entity node can be linked to a large number of neighbors with the same relation link. If we verify the formula from the inverse direction. The number of intermediate nodes can be tremendously decreased.\n4 Experiments we explore two standard KG reason- ing tasks: link prediction (predicting target en- tities) and fact prediction (predicting whether an unknown fact holds or not).\n4.1 Dataset and Settings The triples in FB15K-237 (Toutanova et al., 2015) are sampled from FB15K (Bordes et al., 2013) with redun- dant relations removed.\n4.3 Results ","wordCount":"237","inLanguage":"en","datePublished":"2020-03-11T00:00:00Z","dateModified":"2020-03-11T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/deeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning</h1><div class=post-meta><span title='2020-03-11 00:00:00 +0000 UTC'>2020-03-11</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/paper-DeepPath-A-Reinforcement-Learning-Method-for-Knowledge-Graph-Reasoning.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%96%b9%e6%b3%95 aria-label=方法>方法</a><ul><li><a href=#%e5%85%b3%e7%b3%bb%e6%8e%a8%e7%90%86%e7%9a%84%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0 aria-label=关系推理的强化学习>关系推理的强化学习</a></li><li><a href=#32-%e8%ae%ad%e7%bb%83 aria-label="3.2 训练">3.2 训练</a><ul><li><a href=#%e7%9b%91%e7%9d%a3%e7%ad%96%e7%95%a5%e5%ad%a6%e4%b9%a0 aria-label=监督策略学习>监督策略学习</a></li><li><a href=#retraining-with-rewards aria-label="Retraining with Rewards">Retraining with Rewards</a></li></ul></li><li><a href=#33-bi-directional-path-constrained-search aria-label="3.3 Bi-directional Path-constrained Search">3.3 Bi-directional Path-constrained Search</a></li></ul></li><li><a href=#4-experiments aria-label="4 Experiments">4 Experiments</a><ul><li><a href=#41-dataset-and-settings aria-label="4.1 Dataset and Settings">4.1 Dataset and Settings</a></li><li><a href=#43-results aria-label="4.3 Results">4.3 Results</a></li></ul></li></ul></div></details></div><div class=post-content><p>2017, EMNLP</p><p>data: FB15K-237, FB15K</p><p>task: Knowledge Graph Reasoning</p><p>Use a policy-based agent with continuous states based on knowledge graph embeddings, which <strong>reasons in a KG vector space</strong> by sampling the most promising relation to extend its path.</p><h1 id=方法>方法<a hidden class=anchor aria-hidden=true href=#方法>#</a></h1><p>RL 系统包含两部分，</p><ul><li>第一部分是外部环境，指定了 智能体 和知识图谱之间的动态交互。环境被建模为马尔可夫决策过程。</li><li>系统的第二部分，RL 智能体，表示为策略网络，将状态向量映射到随机策略中。神经网络参数通过随机梯度下降更新。相比于 DQN，基于策略的 RL 方法更适合该知识图谱场景。一个原因是知识图谱的路径查找过程，行为空间因为关系图的复杂性可能非常大。这可能导致 DQN 的收敛性变差。另外，策略网络能学习梯度策略，防止 智能体 陷入某种中间状态，而避免基于值的方法如 DQN 在学习策略梯度中遇到的问题。</li></ul><p><img alt=/images/papers/paper7.png loading=lazy src=/images/papers/paper7.png></p><h2 id=关系推理的强化学习>关系推理的强化学习<a hidden class=anchor aria-hidden=true href=#关系推理的强化学习>#</a></h2><p><strong>行为</strong> 给定一些实体对和一个关系，我们想让 智能体 找到最有信息量的路径来连接这些实体对。从源实体开始，智能体 使用策略网络找到最有希望的关系并每步扩展它的路径直到到达目标实体。为了保持策略网络的输出维度一致，动作空间被定义为知识图谱中的所有关系。</p><p><strong>状态</strong> 知识图谱中的实体和关系是自然的离散原子符号。现有的实际应用的知识图谱例如 Freebase 和 NELL 通常有大量三元组，不可能直接将所有原子符号建模为状态。为了捕捉这些符号的语义信息，我们使用基于平移的嵌入方法，例如 TransE 和 TransH 来表示实体和关系。这些嵌入将所有符号映射到低维向量空间。在该框架中，每个状态捕捉 智能体 在知识图谱中的位置。在执行一个行为后，智能体 会从一个实体移动到另一个实体。两个状态通过刚执行的行为（关系）由 智能体 连接。第 t 步的状态向量：</p><p><img alt=/images/papers/paper7-1.png loading=lazy src=/images/papers/paper7-1.png></p><p>其中 e.t 表示当前实体结点的嵌入，e.target 表示目标实体的嵌入。在最初状态，e.t 即 e.source。我们没有在状态中加入推理关系，因为在寻路过程中推理关系的嵌入保持不变，不利于训练。然而，我们发现通过使用一组特定关系的正样本训练 RL 代理，该 智能体 可以成功地发现关系语义。</p><p><strong>奖励</strong> 对于我们的环境设置，智能体 可以执行的操作数量可能非常大。换句话说，错误的顺序决策比正确的顺序决策多得多。这些错误的决策序列的数量会随着路径的长度呈指数增长。</p><ul><li>Global accuracy：</li></ul><p><img alt=/images/papers/paper7-2.png loading=lazy src=/images/papers/paper7-2.png></p><ul><li>Path efficiency</li></ul><p><img alt=/images/papers/paper7-3.png loading=lazy src=/images/papers/paper7-3.png></p><ul><li>Path diversity:</li></ul><p><img alt=/images/papers/paper7-4.png loading=lazy src=/images/papers/paper7-4.png></p><p><strong>策略网络</strong> 我们使用全连接神经网络来参数化策略函数，它讲状态向量映射到所有可能行为的概率分布上。神经网络包含两个隐藏层，每一层后接 ReLU。输出层通过 softmax 函数归一化。</p><h2 id=32-训练>3.2 训练<a hidden class=anchor aria-hidden=true href=#32-训练>#</a></h2><p>对于一个典型的KG, RL 智能体 常常面临上千种可能的操作。换句话说，策略网络的输出层具有较大的维数。由于关系图的复杂性和较大的动作空间，如果直接采用 RL 算法中典型的试错推理来训练RL模型，将会导致 RL 模型收敛性很差。经过长时间的训练，智能体都可能无法找到任何有价值的路径。</p><p>为了解决这个问题，我们从一个监督策略开始我们的训练，这个策略的灵感来自 AlphaGo 使用的模仿学习流水线。在围棋游戏中，玩家每走一步都要面对近 250 种可能的合法走法。直接训练智能体从原始动作空间中挑选动作可能是一项困难的任务。AlphaGo 首先使用专家训练一个有监督的策略网络。在该例子中，使用随机的广度优先搜索(BFS)训练监督策略。</p><h3 id=监督策略学习>监督策略学习<a hidden class=anchor aria-hidden=true href=#监督策略学习>#</a></h3><p>对于每个关系，我们首先使用所有正样本（实体对）的子集来学习有监督的策略。对于每个正样本<code>(esource, etarget)</code>，一个两端 BFS 被用于找到实体之间的正确路径。对于路径 p，使用蒙塔卡洛策略梯度（REINFORCE 方法）来最大化期望的累积奖励。</p><p><img alt=/images/papers/paper7-5.png loading=lazy src=/images/papers/paper7-5.png></p><p>原生 BFS 是有偏的搜索算法，它倾向于使用短路径。当插入这些有偏向的路径时，agent 很难找到可能有用的较长路径。我们希望路径仅由定义的奖励函数控制。为了防止偏向搜索，我们采用了一种简单的技巧为 BFS 添加一些随机机制。我们不是直接搜索 esource 和 etarget 之间的路径，而是随机选择一个中间节点einter，然后在（esource，einter）和（einter，etarget）之间进行两个 BFS。连接的路径用于训练智能体。监督学习可以节省智能体从失败行为中学习的大量精力。借助所学的经验，我们然后训练智能体寻找理想的路径。</p><h3 id=retraining-with-rewards>Retraining with Rewards<a hidden class=anchor aria-hidden=true href=#retraining-with-rewards>#</a></h3><p>为了找到受奖励函数控制的推理路径，我们使用奖励函数来限制监督策略网络。对于每个关系，一个实体对的推理被视为一个事件(episode)。从源结点开始，智能体根据随机策略选择关系，它是所有关系上的概率分布，以扩展推理路径。关系链接可能引向一个新实体，或者失败。这些失败的步骤可能导致智能体获得负奖励。智能体在失败步骤后保持状态。由于智能体遵循随机策略，所以智能体不会因为重复错误的步骤而陷入困境。为了提高训练效率，我们将训练集长度设定一个上限。上限达到时，如智能体仍未找到目标实体则事件结束。每个事件结束后，策略网络通过以下梯度进行更新：</p><p><img alt=/images/papers/paper7-6.png loading=lazy src=/images/papers/paper7-6.png></p><h2 id=33-bi-directional-path-constrained-search>3.3 Bi-directional Path-constrained Search<a hidden class=anchor aria-hidden=true href=#33-bi-directional-path-constrained-search>#</a></h2><p>In a typical KG, one entity node can be linked to a large number of neighbors with the same relation link. If we verify the formula from the inverse direction. The number of intermediate nodes can be tremendously decreased.</p><h1 id=4-experiments>4 Experiments<a hidden class=anchor aria-hidden=true href=#4-experiments>#</a></h1><p>we explore two standard KG reason- ing tasks: <strong>link prediction (predicting target en- tities)</strong> and <strong>fact prediction (predicting whether an unknown fact holds or not)</strong>.</p><h2 id=41-dataset-and-settings>4.1 Dataset and Settings<a hidden class=anchor aria-hidden=true href=#41-dataset-and-settings>#</a></h2><p><img alt=/images/papers/paper7-7.png loading=lazy src=/images/papers/paper7-7.png></p><p>The triples in FB15K-237 (Toutanova et al., 2015) are sampled from FB15K (Bordes et al., 2013) with redun- dant relations removed.</p><h2 id=43-results>4.3 Results<a hidden class=anchor aria-hidden=true href=#43-results>#</a></h2><p><img alt=/images/papers/paper7-8.png loading=lazy src=/images/papers/paper7-8.png></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/nlp/>NLP</a></li><li><a href=https://congchan.github.io/tags/rl/>RL</a></li><li><a href=https://congchan.github.io/tags/2017/>2017</a></li><li><a href=https://congchan.github.io/tags/emnlp/>EMNLP</a></li><li><a href=https://congchan.github.io/tags/knowledge-graph-reasoning/>Knowledge Graph Reasoning</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/dqn-double-dqn-dueling-doubleqn-rainbow-dqn/><span class=title>« Prev</span><br><span>DQN, Double DQN, Dueling DoubleQN, Rainbow DQN</span>
</a><a class=next href=https://congchan.github.io/posts/knowledge-graph-embedding%E7%9A%84translate%E6%97%8Ftransetranshtransrtransd/><span class=title>Next »</span><br><span>Knowledge-Graph-Embedding的Translate族（TransE，TransH，TransR，TransD）</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning on x" href="https://x.com/intent/tweet/?text=DeepPath%20-%20A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fdeeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning%2f&amp;hashtags=NLP%2cRL%2c2017%2cEMNLP%2cKnowledgeGraphReasoning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fdeeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning%2f&amp;title=DeepPath%20-%20A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning&amp;summary=DeepPath%20-%20A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fdeeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fdeeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning%2f&title=DeepPath%20-%20A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fdeeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning on whatsapp" href="https://api.whatsapp.com/send?text=DeepPath%20-%20A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fdeeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning on telegram" href="https://telegram.me/share/url?text=DeepPath%20-%20A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fdeeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepPath - A Reinforcement Learning Method for Knowledge Graph Reasoning on ycombinator" href="https://news.ycombinator.com/submitlink?t=DeepPath%20-%20A%20Reinforcement%20Learning%20Method%20for%20Knowledge%20Graph%20Reasoning&u=https%3a%2f%2fcongchan.github.io%2fposts%2fdeeppath-a-reinforcement-learning-method-for-knowledge-graph-reasoning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>