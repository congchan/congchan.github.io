<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Cross-media Structured Common Space for Multimedia Event Extraction | Cong's Log</title><meta name=keywords content="Multimodal,2020,ACL,Event Extraction"><meta name=description content="2020, ACL
Task: MultiMedia Event Extraction

Introduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles
Propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities"><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/cross-media-structured-common-space-for-multimedia-event-extraction/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/cross-media-structured-common-space-for-multimedia-event-extraction/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/cross-media-structured-common-space-for-multimedia-event-extraction/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Cross-media Structured Common Space for Multimedia Event Extraction"><meta property="og:description" content="2020, ACL Task: MultiMedia Event Extraction
Introduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles
Propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-03-24T00:00:00+00:00"><meta property="article:modified_time" content="2021-03-24T00:00:00+00:00"><meta property="article:tag" content="Multimodal"><meta property="article:tag" content="2020"><meta property="article:tag" content="ACL"><meta property="article:tag" content="Event Extraction"><meta name=twitter:card content="summary"><meta name=twitter:title content="Cross-media Structured Common Space for Multimedia Event Extraction"><meta name=twitter:description content="2020, ACL
Task: MultiMedia Event Extraction

Introduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles
Propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Cross-media Structured Common Space for Multimedia Event Extraction","item":"https://congchan.github.io/posts/cross-media-structured-common-space-for-multimedia-event-extraction/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Cross-media Structured Common Space for Multimedia Event Extraction","name":"Cross-media Structured Common Space for Multimedia Event Extraction","description":"2020, ACL Task: MultiMedia Event Extraction\nIntroduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles\nPropose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities\n","keywords":["Multimodal","2020","ACL","Event Extraction"],"articleBody":"2020, ACL Task: MultiMedia Event Extraction\nIntroduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles\nPropose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities\n数据 Each input document consists of:\na set of images M = {m1,m2, . . . } and a set of sentences S = {s1, s2, . . . } a set of entities T = {t1, t2, . . . } extracted from the document text The tasks of M2E2 Event Extraction: Given a multimedia document, extract a set of event mentions, where each event mention e has a type ye and is grounded on a text trigger word w or an image m or both $e = (y_e, {w,m})$. Argument Extraction: The second task is to extract a set of arguments of event mention e. Each argument a has an argument role type ya, and is grounded on a text entity t or an image object o (represented as a bounding box), $a = (y_a, {t, o})$ Two types of bounding boxes:\nunion bounding box: for each role, we annotate the smallest bounding box covering all constituents instance bounding box: for each role, we annotate a set of bounding boxes, where each box is the smallest region that covers an individual participant 方法 represent each image or sentence as a graph, where each node represents an event or entity and each edge represents an argument role.\nthe training phase contains three tasks:\nText event extraction Text Structured Representation: run the CAMR parser (Wang et al., 2015b,a, 2016) to generate an AMR graph. based on the named entity recognition and part- of-speech (POS) tagging results from Stanford CoreNLP (Manning et al., 2014)\nEmbedding: pre-trained GloVe word embedding (Pennington et al., 2014), POS embedding, entity type embed- ding and position embedding\nEncode embedding: Bi-LSTM\nEncode AMR: AMR graph as input to GCN:\nFor each entity t, we obtain its representation $t^C$ by averaging the embeddings of its tokens\nEvent and Argument Classifier\nuse BIO tag schema to decide trigger word boundary, classify each word w into event types $y_e$ classify entity t into argument role $y_a$ 训练时, 用Ground-true entity\n测试时, 用named entity extractor\nImage Event Extraction (Section 3.3) Image Structured Representation: represent each image with a situation graph, the central node is labeled as a verb v (e.g., destroying), and the neighbor nodes are arguments labeled as {(n, r)}, where n is a noun (e.g., ship) derived from WordNet synsets (Miller, 1995) to indicate the entity type, and r indicates the role (e.g., item) played by the entity in the event, based on FrameNet (Fillmore et al., 2003).\ntwo methods to construct situation graphs:\nObject-based Graph: object detection, and obtain the object bounding boxes detected by a Faster R-CNN (Ren et al., 2015) model trained on Open Images (Kuznetsova et al., 2018) with 600 object types (classes). employ a VGG-16 CNN (Si- monyan and Zisserman, 2014) to extract visual features of an image m and and another VGG-16 to encode the bounding boxes {oi}. apply a Multi-Layer Perceptron (MLP) to predict a verb embedding from m and another MLP to predict a noun embedding for each oi\ncompare the predicted verb embedding to all verbs v in the imSitu taxonomy in order to classify the verb, and similarly compare each predicted noun embedding to all imSitu nouns n which re- sults in probability distributions. where v and n are word embeddings initialized with GloVE.\nuse another MLP with one hidden layer followed by Softmax (σ) to classify role ri for each object oi\ndefine the situation loss\nAttention-based Graph\nMany salient objects such as bomb, stone and stretcher are not covered in these ontologies. Hence, propose an open-vocabulary alternative to the object-based graph construction model.\nand cross- media alignment (Section 3.4)\n效果 Compared to unimodal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction.\n","wordCount":"719","inLanguage":"en","datePublished":"2021-03-24T00:00:00Z","dateModified":"2021-03-24T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/cross-media-structured-common-space-for-multimedia-event-extraction/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Cross-media Structured Common Space for Multimedia Event Extraction</h1><div class=post-meta><span title='2021-03-24 00:00:00 +0000 UTC'>2021-03-24</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/paper-Cross-media-Structured-Common-Space-for-Multimedia-Event-Extraction.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%95%b0%e6%8d%ae aria-label=数据>数据</a><ul><li><a href=#the-tasks-of-m2e2 aria-label="The tasks of M2E2">The tasks of M2E2</a></li></ul></li><li><a href=#%e6%96%b9%e6%b3%95 aria-label=方法>方法</a><ul><li><a href=#text-event-extraction aria-label="Text event extraction">Text event extraction</a></li><li><a href=#image-event-extraction-section-33 aria-label="Image Event Extraction (Section 3.3)">Image Event Extraction (Section 3.3)</a></li></ul></li><li><a href=#%e6%95%88%e6%9e%9c aria-label=效果>效果</a></li></ul></div></details></div><div class=post-content><p>2020, ACL
Task: MultiMedia Event Extraction</p><p>Introduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. Construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles</p><p>Propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. which takes advantage of annotated unimodal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities</p><h1 id=数据>数据<a hidden class=anchor aria-hidden=true href=#数据>#</a></h1><p>Each input document consists of:</p><ul><li>a set of images <code>M = {m1,m2, . . . }</code></li><li>and a set of sentences <code>S = {s1, s2, . . . }</code></li><li>a set of entities <code>T = {t1, t2, . . . }</code> extracted from the document text</li></ul><p><img alt=/images/papers/paper1-0.png loading=lazy src=/images/papers/paper1-0.png></p><h2 id=the-tasks-of-m2e2>The tasks of M2E2<a hidden class=anchor aria-hidden=true href=#the-tasks-of-m2e2>#</a></h2><ul><li>Event Extraction: Given a multimedia document, extract a set of event mentions, where each event mention e has a type ye and is grounded on <strong>a text trigger word w</strong> or <strong>an image m</strong> or both $e = (y_e, {w,m})$.</li><li>Argument Extraction: The second task is to extract a set of arguments of event mention e. Each argument a has an argument role type ya, and is grounded on <strong>a text entity t</strong> or <strong>an image object o</strong> (represented as a bounding box), $a = (y_a, {t, o})$</li></ul><p><img alt=/images/papers/paper1-1.png loading=lazy src=/images/papers/paper1-1.png></p><p><img alt=/images/papers/paper1-2.png loading=lazy src=/images/papers/paper1-2.png></p><p>Two types of bounding boxes:</p><ul><li>union bounding box: for each role, we annotate the smallest bounding box covering all constituents</li><li>instance bounding box: for each role, we annotate a set of bounding boxes, where each box is the smallest region that covers an individual participant</li></ul><p><img alt=/images/papers/paper1-3.png loading=lazy src=/images/papers/paper1-3.png></p><h1 id=方法>方法<a hidden class=anchor aria-hidden=true href=#方法>#</a></h1><p>represent each image or sentence as a graph, where each node represents an event or entity and each edge represents an argument role.</p><p>the training phase contains three tasks:</p><h2 id=text-event-extraction>Text event extraction<a hidden class=anchor aria-hidden=true href=#text-event-extraction>#</a></h2><p>Text Structured Representation: run the CAMR parser (Wang et al., 2015b,a, 2016) to generate an <strong>AMR</strong> graph. based on the <strong>named entity recognition</strong> and <strong>part- of-speech (POS)</strong> tagging results from Stanford CoreNLP (Manning et al., 2014)</p><ul><li><p>Embedding: pre-trained GloVe word embedding (Pennington et al., 2014), POS embedding, entity type embed- ding and position embedding</p></li><li><p>Encode embedding: Bi-LSTM</p></li><li><p>Encode AMR: AMR graph as input to GCN:</p><p><img alt=/images/papers/paper1-4.png loading=lazy src=/images/papers/paper1-4.png></p></li><li><p>For each entity t, we obtain its representation $t^C$ by averaging the embeddings of its tokens</p></li></ul><p>Event and Argument Classifier</p><ul><li>use BIO tag schema to decide trigger word boundary, classify each word w into event types $y_e$</li><li>classify entity t into argument role $y_a$</li></ul><p>训练时, 用Ground-true entity</p><p>测试时, 用named entity extractor</p><h2 id=image-event-extraction-section-33>Image Event Extraction (Section 3.3)<a hidden class=anchor aria-hidden=true href=#image-event-extraction-section-33>#</a></h2><p>Image Structured Representation: represent each image with a <strong>situation graph</strong>, t<strong>he central node is labeled as a verb v</strong> (e.g., destroying), and <strong>the neighbor nodes are arguments labeled as {(n, r)}</strong>, where <strong>n is a noun</strong> (e.g., ship) derived from <strong>WordNet synsets</strong> (Miller, 1995) to indicate the entity type, and <strong>r indicates the role</strong> (e.g., item) played by the entity in the event, based on <strong>FrameNet</strong> (Fillmore et al., 2003).</p><p>two methods to construct situation graphs:</p><ol><li><p>Object-based Graph: <strong>object detection</strong>, and obtain the <strong>object bounding boxes</strong> detected by a <strong>Faster R-CNN</strong> (Ren et al., 2015) model trained on Open Images (Kuznetsova et al., 2018) with 600 object types (classes). employ a VGG-16 CNN (Si- monyan and Zisserman, 2014) to extract visual features of an image m and and another VGG-16 to encode <strong>the bounding boxes {oi}.</strong> apply a Multi-Layer Perceptron (MLP) to <strong>predict a verb embedding from m</strong> and another MLP to <strong>predict a noun embedding for each oi</strong></p><p><img alt=/images/papers/paper1-5.png loading=lazy src=/images/papers/paper1-5.png></p><p>compare the predicted verb embedding to all verbs v in the imSitu taxonomy in order to classify the verb, and similarly compare each predicted noun embedding to all imSitu nouns n which re- sults in probability distributions. where v and n are word embeddings initialized with GloVE.</p><p>use another MLP with one hidden layer followed by Softmax (σ) to classify role ri for each object oi</p><p>define the situation loss</p><p><img alt=/images/papers/paper1-6.png loading=lazy src=/images/papers/paper1-6.png></p></li><li><p>Attention-based Graph</p><p>Many salient objects such as bomb, stone and stretcher are not covered in these ontologies. Hence, propose an <strong>open-vocabulary</strong> alternative to the object-based graph construction model.</p></li></ol><p>and cross- media alignment (Section 3.4)</p><h1 id=效果>效果<a hidden class=anchor aria-hidden=true href=#效果>#</a></h1><p>Compared to unimodal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/multimodal/>Multimodal</a></li><li><a href=https://congchan.github.io/tags/2020/>2020</a></li><li><a href=https://congchan.github.io/tags/acl/>ACL</a></li><li><a href=https://congchan.github.io/tags/event-extraction/>Event Extraction</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/improving-event-detection-via-open-domain-trigger-knowledge/><span class=title>« Prev</span><br><span>Improving Event Detection via Open-domain Trigger Knowledge</span>
</a><a class=next href=https://congchan.github.io/posts/dqn-double-dqn-dueling-doubleqn-rainbow-dqn/><span class=title>Next »</span><br><span>DQN, Double DQN, Dueling DoubleQN, Rainbow DQN</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Cross-media Structured Common Space for Multimedia Event Extraction on x" href="https://x.com/intent/tweet/?text=Cross-media%20Structured%20Common%20Space%20for%20Multimedia%20Event%20Extraction&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fcross-media-structured-common-space-for-multimedia-event-extraction%2f&amp;hashtags=Multimodal%2c2020%2cACL%2cEventExtraction"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Cross-media Structured Common Space for Multimedia Event Extraction on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fcross-media-structured-common-space-for-multimedia-event-extraction%2f&amp;title=Cross-media%20Structured%20Common%20Space%20for%20Multimedia%20Event%20Extraction&amp;summary=Cross-media%20Structured%20Common%20Space%20for%20Multimedia%20Event%20Extraction&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fcross-media-structured-common-space-for-multimedia-event-extraction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Cross-media Structured Common Space for Multimedia Event Extraction on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fcross-media-structured-common-space-for-multimedia-event-extraction%2f&title=Cross-media%20Structured%20Common%20Space%20for%20Multimedia%20Event%20Extraction"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Cross-media Structured Common Space for Multimedia Event Extraction on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fcross-media-structured-common-space-for-multimedia-event-extraction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Cross-media Structured Common Space for Multimedia Event Extraction on whatsapp" href="https://api.whatsapp.com/send?text=Cross-media%20Structured%20Common%20Space%20for%20Multimedia%20Event%20Extraction%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fcross-media-structured-common-space-for-multimedia-event-extraction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Cross-media Structured Common Space for Multimedia Event Extraction on telegram" href="https://telegram.me/share/url?text=Cross-media%20Structured%20Common%20Space%20for%20Multimedia%20Event%20Extraction&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fcross-media-structured-common-space-for-multimedia-event-extraction%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Cross-media Structured Common Space for Multimedia Event Extraction on ycombinator" href="https://news.ycombinator.com/submitlink?t=Cross-media%20Structured%20Common%20Space%20for%20Multimedia%20Event%20Extraction&u=https%3a%2f%2fcongchan.github.io%2fposts%2fcross-media-structured-common-space-for-multimedia-event-extraction%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>