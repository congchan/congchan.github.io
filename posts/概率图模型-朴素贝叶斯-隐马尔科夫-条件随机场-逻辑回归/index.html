<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归 | Cong's Log</title><meta name=keywords content="NLP"><meta name=description content="序列标注（Sequence Labeling）
序列标注任务是指根据观察得到的序列（如一个句子）, 推断出序列每个元素（单词）对应的标注。
具体的任务包括分词(Segmentation), 词性标注（Part-of-Speach tagging, POS）, 实体识别(Named Entity Recognition, NER), 等等. 所谓POS, 就是对于一个句子, 如Bob drank coffee at Starbucks, 标注可能为Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN).
除此之外, 还有其他涉及到需要根据观察序列推断隐含状态的问题, 这种问题的特点是每一个位置的标签都不是独立的, 而是和上下文相关依存的, 可以用序列标注的思路来处理.
单个分类器仅能预测单个类变量，但是序列标注基于概率图模型, 图模型(Graphical Models)的真正功能在于它们能够对许多有相互依赖的变量进行建模。最简单的依赖关系可以描述为一种线性链(Linear Chain), 也就是后续介绍到的隐马尔可夫模型(Hidden Markov Model, HMM)用到的假设.

概率图模型
Graphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。
G = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立.
 从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为P(当前节点 | 父节点)."><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归"><meta property="og:description" content="序列标注（Sequence Labeling） 序列标注任务是指根据观察得到的序列（如一个句子）, 推断出序列每个元素（单词）对应的标注。
具体的任务包括分词(Segmentation), 词性标注（Part-of-Speach tagging, POS）, 实体识别(Named Entity Recognition, NER), 等等. 所谓POS, 就是对于一个句子, 如Bob drank coffee at Starbucks, 标注可能为Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN).
除此之外, 还有其他涉及到需要根据观察序列推断隐含状态的问题, 这种问题的特点是每一个位置的标签都不是独立的, 而是和上下文相关依存的, 可以用序列标注的思路来处理.
单个分类器仅能预测单个类变量，但是序列标注基于概率图模型, 图模型(Graphical Models)的真正功能在于它们能够对许多有相互依赖的变量进行建模。最简单的依赖关系可以描述为一种线性链(Linear Chain), 也就是后续介绍到的隐马尔可夫模型(Hidden Markov Model, HMM)用到的假设.
概率图模型 Graphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。
G = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立. 从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为P(当前节点 | 父节点)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-09-16T00:00:00+00:00"><meta property="article:modified_time" content="2018-09-16T00:00:00+00:00"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归"><meta name=twitter:description content="序列标注（Sequence Labeling）
序列标注任务是指根据观察得到的序列（如一个句子）, 推断出序列每个元素（单词）对应的标注。
具体的任务包括分词(Segmentation), 词性标注（Part-of-Speach tagging, POS）, 实体识别(Named Entity Recognition, NER), 等等. 所谓POS, 就是对于一个句子, 如Bob drank coffee at Starbucks, 标注可能为Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN).
除此之外, 还有其他涉及到需要根据观察序列推断隐含状态的问题, 这种问题的特点是每一个位置的标签都不是独立的, 而是和上下文相关依存的, 可以用序列标注的思路来处理.
单个分类器仅能预测单个类变量，但是序列标注基于概率图模型, 图模型(Graphical Models)的真正功能在于它们能够对许多有相互依赖的变量进行建模。最简单的依赖关系可以描述为一种线性链(Linear Chain), 也就是后续介绍到的隐马尔可夫模型(Hidden Markov Model, HMM)用到的假设.

概率图模型
Graphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。
G = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立.
 从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为P(当前节点 | 父节点)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归","item":"https://congchan.github.io/posts/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归","name":"概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归","description":"序列标注（Sequence Labeling） 序列标注任务是指根据观察得到的序列（如一个句子）, 推断出序列每个元素（单词）对应的标注。\n具体的任务包括分词(Segmentation), 词性标注（Part-of-Speach tagging, POS）, 实体识别(Named Entity Recognition, NER), 等等. 所谓POS, 就是对于一个句子, 如Bob drank coffee at Starbucks, 标注可能为Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN).\n除此之外, 还有其他涉及到需要根据观察序列推断隐含状态的问题, 这种问题的特点是每一个位置的标签都不是独立的, 而是和上下文相关依存的, 可以用序列标注的思路来处理.\n单个分类器仅能预测单个类变量，但是序列标注基于概率图模型, 图模型(Graphical Models)的真正功能在于它们能够对许多有相互依赖的变量进行建模。最简单的依赖关系可以描述为一种线性链(Linear Chain), 也就是后续介绍到的隐马尔可夫模型(Hidden Markov Model, HMM)用到的假设.\n概率图模型 Graphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。\nG = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立. 从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为P(当前节点 | 父节点).\n","keywords":["NLP"],"articleBody":"序列标注（Sequence Labeling） 序列标注任务是指根据观察得到的序列（如一个句子）, 推断出序列每个元素（单词）对应的标注。\n具体的任务包括分词(Segmentation), 词性标注（Part-of-Speach tagging, POS）, 实体识别(Named Entity Recognition, NER), 等等. 所谓POS, 就是对于一个句子, 如Bob drank coffee at Starbucks, 标注可能为Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN).\n除此之外, 还有其他涉及到需要根据观察序列推断隐含状态的问题, 这种问题的特点是每一个位置的标签都不是独立的, 而是和上下文相关依存的, 可以用序列标注的思路来处理.\n单个分类器仅能预测单个类变量，但是序列标注基于概率图模型, 图模型(Graphical Models)的真正功能在于它们能够对许多有相互依赖的变量进行建模。最简单的依赖关系可以描述为一种线性链(Linear Chain), 也就是后续介绍到的隐马尔可夫模型(Hidden Markov Model, HMM)用到的假设.\n概率图模型 Graphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。\nG = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立. 从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为P(当前节点 | 父节点).\n而马尔可夫网络则是无向图. 无向图形模型是指一整个家族的概率分布，每个概率分布都根据给定的因子集合进行因式分解。一般用random field来指代无向图中定义的特定分布. 数学上表达无向图, 指给定子集$\\\\{Y_a \\\\}_{a=1}^A$, 对于所有$\\mathbf{y}_a$和任何因子选项$\\mathcal{F}=\\\\{\\Psi_a\\\\}$, $\\Psi_a(\\mathbf{y}_a) \\geq 0$, 无向图定义的各个分布可以写成:\n$$p(\\mathbf{y})=\\frac{1}{Z} \\prod_{a=1}^A \\Psi_{a}\\left(\\mathbf{y}_{a}\\right)$$Z是正则项用于保证分布$p$和为$1$ $$Z=\\sum_{\\mathbf{y}} \\prod_{a=1}^{A} \\Psi_{a}\\left(\\mathbf{y}_{a}\\right)$$Markov Net 包含了一组具有马尔可夫性质的随机变量. **马尔可夫随机场(Markov Random Fields, MRF)**是由参数$λ=(S, π, A)$表示, 其中S是状态的集合，π是初始状态的概率, A是状态间的转移概率。一阶马尔可夫链就是假设t时刻的状态只依赖于前一时刻的状态，与其他时刻的状态和观测无关。这个性质可以用于简化概率链的计算。使用类似性质作为假设的模型还有Bi-gram语言模型等.\n朴素贝叶斯分类器与隐马尔可夫模型 朴素贝叶斯分类器(NBs)假设条件独立性(朴素贝叶斯假设, Hand and Yu, 2001)：$p(x_i | y, x_j) = p(x_i | y)$, 在给定目标值 y 时，x的属性值之间相互条件独立。这样, 计算可以简化为 $$p(y | \\overrightarrow{x}) \\propto p(y, \\overrightarrow{x}) = p(y) \\prod_{i=1} p(x_i | y).$$朴素贝叶斯模型只考虑了单个输出变量y。如果要为一个观察序列$\\overrightarrow{x} =(x_1, ..., x_n)$预测对应的分类序列$\\overrightarrow{y} =（y_1, ..., y_n)$ ，一个简单的序列模型可以表示为多个NBs的乘积。此时不考虑序列单个位置之间的相互依赖。\n$$p(\\overrightarrow{y}, \\overrightarrow{x}) = \\prod^n_{i=1} p(y_i) p(x_i | y_i).$$ 此时每个观察值$x_i$仅取决于对应序列位置的类变量$y_i$。由于这种独立性假设，从一个步骤到另一个步骤的转换概率不包括在该模型中。然而这种假设在实践中几乎不会符合，这导致这种模型的性能很有限。\n因此，比较合理的假设是观测序列在连续相邻位置间的状态存在依赖。要模拟这种依赖关系, 就要引入状态转移概率$p(y_i | y_{i-1})$, 由此引出著名的隐马尔可夫模型 Hidden Markov model, HMM, Rabiner (1989).\nHMM参数$λ = (Y, X, π, A, B)$ ，其中Y是隐状态（输出变量）的集合，X是观察值（输入）集合，π是初始状态的概率，A是状态转移概率矩阵$p(y_i | y_{i-1})$，B是输出观察值概率矩阵$p(x_i | y_{i})$。在POS任务中, X就是观察到的句子, Y就是待推导的标注序列, 因为词性待求的, 所以人们称之为隐含状态.\n概括一下HMM设定的假设:\nMarkov assumption：假设每个状态仅依赖于其前一个状态, $p(y_t|y_{t−1})$ Stationarity assumption：状态的转换概率与转换发生的实际时间（实际输入）无关 Output independence assumption: 假设每一个观察值x仅依赖于当前状态值y, $p(x_t|y_t)$, 而与前面的观察值无关。 那么状态序列y和观察序列x的联合概率可以分解为 $$p(\\mathbf{y}, \\mathbf{x})=\\prod_{t=1}^{\\mathrm{T}} p\\left(y_{t} | y_{t-1}\\right) p\\left(x_{t} | y_{t}\\right)$$总的来说, 隐马尔可夫模型（HMM）是具有随机状态转移和观测值的有限状态自动机（Rabiner，1989）。自动机对概率生成过程进行建模: 先从某个初始状态开始，发射(emit)该状态生成的观察值，再转移到下一个状态，再发射另一个观察值，以此类推，直到达到指定的最终状态，从而产生一系列观察值。\nHMM作为生成式的概率模型, 对观察特征的条件独立约束非常严格. 而且为了定义观察值序列和序列标记的联合概率，生成模型需要枚举所有可能的观察序列. 对于表示多个相互作用的特征或观测值的长距离相关性, 这种枚举是不切实际的，因为此类模型的inference很棘手。但很多任务往往受益于这种相互作用、相互交叉重叠的特征，比如除了传统的单词自身外，还有大小写，单词结尾，词性，格式，在页面上的位置以及WordNet中的节点成员身份等等。\n除此之外, 大部分文本任务是根据给定的观察序列（如纯文本）来预测对应的状态序列，也就是判别问题。换句话说，HMM不恰当地用了生成联合概率的模型去判别问题。\n隐马尔可夫模型与最大熵马尔可夫模型 最大熵马尔可夫模型(Maximum Entropy Markov Models, MEMM)跟HMM的生成式概率图不同，MEMM对当前状态的判断依赖于前一时间步的状态和当前观察值的状态。\n首先所谓\"熵\"就是信息论中的概念:\nEntropy: the uncertainty of a distribution.\n量化Entropy: surprise. Entropy = expected surprise\nEvent $x$, Probability $p_x$, “Surprise” $log(1/p_x)$, Entropy: $$ \\begin{aligned} \u0026H(p)=E_{p}\\left[\\log \\frac{1}{p_{x}}\\right]\\\\ \u0026\\mathrm{H}(p)=-\\sum_{x} p_{x} \\log p_{x} \\end{aligned} $$熵最大的分布就是均匀分布，也就是每一个选项都一样，等于什么信息都没给。如果给了额外的信息，如约束，特征之后，熵就可以降低。\n“最大熵”是指遵循最大熵原则：\nmodel all that is known and assume nothing about that which is unknown.\n也就说, 如果给定了一组事实，我们最好选择一个符合这些事实的模型，剩余情况则尽可能地一视同仁不做任何区别对待。最佳的模型是符合训练数据衍生的约束条件的模型，同时尽可能少地做假设，也就是少做承诺，也就避免过拟合。\nMEMM 把HMM中的转移概率和发射概率替换为一个概率：当前状态$s$依赖于前一个状态$s^{\\prime}$和当前观察值$o$, $\\mathrm{P}\\left(s | s^{\\prime}, o\\right)$\nMEMM的训练思路是这样: 对每个状态$s^{\\prime}$, 将训练数据分为\u003c观察-目标状态\u003e对 $$, 也就是把 $\\mathrm{P}\\left(s | s^{\\prime}, o\\right)$ 分成 $|S|$ 个分别训练的exponential model $\\mathrm{P}_{s^{\\prime}}(s | o)=\\mathrm{P}\\left(s | s^{\\prime}, o\\right)$, 再通过最大化熵来训练exponential models, 换种说法叫logistic regression classifier.\n用的约束条件是学习分布中每个特征$a$的期望值与训练数据的观测序列上每个特征的期望值相同. 满足这些约束的最大熵分布（Della Pietra，Della Pietra和Lafferty，1997）是唯一的，与最大似然分布一致，对每一个位置的状态$s^{\\prime}$, 具有指数形式： $$ P_{s^{\\prime}}(s | o)=\\frac{1}{Z\\left(o, s^{\\prime}\\right)} \\exp \\left(\\sum_{a} \\lambda_{a} f_{a}(o, s)\\right) $$其中$\\lambda_{a}$是待估计的参数, $Z\\left(o, s^{\\prime}\\right)$是归一化因子 $$ Z\\left(o, s^{\\prime}\\right)=\\sum_{s \\in S} P\\left(s | s^{\\prime}, o\\right) $$ $S$是标签集.\n如果把问题简化为线性的相邻依赖, 那么每一个状态$s_{i}$仅依赖于前一个状态$s_{i-1}$. 用$Y$表达标签序列, 用$X$表达观察序列, 那么 $$P\\left(y_{1}, y_{2}, \\ldots, y_{n} | \\mathbb{x}\\right)=P\\left(y_{1} | \\mathbb{x}\\right) P\\left(y_{2} | \\mathbb{x}, y_{1}\\right) P\\left(y_{3} | \\mathbb{x}, y_{2}\\right) \\ldots P\\left(y_{n} | \\mathbb{x}, y_{n-1}\\right)$$ 其中 $$P\\left(y_{1} | \\mathbb{x}\\right)=\\frac{e^{f\\left(y_{1} ; \\mathbb{x}\\right)}}{\\sum_{y_{1} \\in S} e^{f\\left(y_{k} ; \\mathbb{x}\\right)}}, \\quad P\\left(y_{k} | \\mathbb{x}, y_{k-1}\\right)=\\frac{e^{g\\left(y_{k-1}, y_{k}\\right)+f\\left(y_{k} ; \\mathbb{x}\\right)}}{\\sum_{y_{k} \\in S} e^{g\\left(y_{k-1}, y_{k}\\right)+f\\left(y_{k} ; \\mathbb{x}\\right)}}$$ 则 $$P(\\mathbb{y} | \\mathbb{x})=\\frac{e^{f\\left(y_{1} ; \\mathbb{x}\\right)+g\\left(y_{1}, y_{2}\\right)+f\\left(y_{2} ; \\mathbb{x}\\right)+\\cdots+g\\left(y_{n-1}, y_{n}\\right)+f\\left(y_{n} ; \\mathbb{x}\\right)}}{\\left(\\sum_{y_{1} \\in S} e^{f\\left(y_{1} ; \\mathbb{x}\\right)}\\right)\\left(\\sum_{y_{2} \\in S} e^{g\\left(y_{1}, y_{2}\\right)+f\\left(y_{2} ; \\mathbb{x}\\right)}\\right) \\cdots\\left(\\sum_{y_{n} \\in S} e^{g\\left(y_{n-1}, y_{n}\\right)+f\\left(y_{n} ; \\mathbb{x}\\right)}\\right)}$$ MEMM将整体的概率分布分解为每一个时间步的分布之积，所以算loss只需要把每一步的交叉熵求和。只需要每一步单独执行softmax，所以MEMM是完全可以并行的，速度跟直接逐步Softmax基本一样。\n虽然MEMM能克服HMM的很多弱点, 但是MEMM自身也有一个 label bias 问题, 就是标签偏差, 离开给定状态的转移仅相互对比，而不是与全局所有其他转移对比。转移分数是分别对每个状态的归一化, 这意味到达一个状态的所有质量必须在可能的后续状态之间分配。观察值可以影响哪个目标状态获得质量，但无法影响多少质量可以被传递。这会导致模型偏向输出选择较少的状态, 比如极端情况下, 在训练集中某个状态$s_a$只发现了有一种可能的转移$s_b$, 那么状态$s_a$别无选择，只能将所有质量传递给它的唯一的 transition output $s_b$。\n随机场 随机场, 可以看成是一组随机变量的集合（这组随机变量对应同一个样本空间）。当给每一个位置按照某种分布随机赋予一个值之后，其全体就叫做随机场。这些随机变量之间可能有依赖关系，一般来说，也只有当这些变量之间有依赖关系的时候，我们将其单独拿出来看成一个随机场才有实际意义。\n如果给定的MRF中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个MRF的分布，也就是条件分布，那么这个MRF就称为 Conditional random fields (CRF)。它的条件分布形式完全类似于MRF的分布形式，只不过多了一个观察集合X。所以, CRF本质上是给定了条件(观察值observations)集合的MRF.\n1.特征函数的选择: 特征函数的选取直接关系模型的性能。 2.参数估计: 从已经标注好的训练数据集学习条件随机场模型的参数，即各特征函数的权重向量λ。 3.模型推断: 在给定条件随机场模型参数λ下，预测出最可能的状态序列。\nMEMM和CRF 在CRF的序列标注问题中，我们要计算的是条件概率 $$ P\\left(y_{1}, \\ldots, y_{n} \\mid \\mathbb{x}\\right), \\quad \\mathbb{x}=\\left(x_{1}, \\ldots, x_{n}\\right) $$CRF和MEMM的关键区别在于，MEMM使用每个状态的指数模型来确定给定当前状态的下一状态的条件概率，而CRF则使用一个指数模型来表示整个标签序列的联合概率, 这个概率条件依赖于给定的完整观察序列。二者区别仅在于分母（也就是归一化因子）的计算方式不同，CRF的是全局归一化的，而MEMM的是局部归一化的. 也就是说CRF是一个以观测序列$X$为全局条件的随机场. 存在函数$f(y_1,\\dots,y_n;\\mathbb{x})$，使得 $$ P(y_1,\\dots,y_n|\\mathbb{x})=\\frac{1}{Z(\\mathbb{x})}\\exp\\Big(f(y_1,\\dots,y_n;\\mathbb{x})\\Big) $$可以得到对应得概率是 $$P(\\mathbb{y} | \\mathbb{x})=\\frac{e^{f\\left(y_{1}, y_{2}, \\ldots, y_{n} ; \\mathbb{x}\\right)}}{\\sum_{y_{1}, y_{2}, \\ldots, y_{n} \\in S^n} e^{f\\left(y_{1}, y_{2}, \\ldots, y_{n} ; \\mathbb{x}\\right)}}$$ CRF的计算困难之处在于上式的分母项包含了所有可能路径$S^n$的求和，搜索空间非常庞大.\n因此做出一些简化，假设输出之间的关联仅发生在相邻位置，并且关联是指数加性的:\n$$\\begin{aligned} f\\left(y_{1}, y_{2}, \\ldots, y_{n} ; \\mathbb{x}\\right) \u0026=f\\left(y_{1} ; \\mathbb{x}\\right)+g\\left(y_{1}, y_{2};\\mathbb{x}\\right)+\\cdots+g\\left(y_{n-1}, y_{n};\\mathbb{x}\\right)+f\\left(y_{n} ; \\mathbb{x}\\right) \\\\\\\\ \u0026=f\\left(y_{1} ; \\mathbb{x}\\right)+\\sum_{k=2}^{n}\\left(g\\left(y_{k-1}, y_{k};\\mathbb{x}\\right)+f\\left(y_{k} ; \\mathbb{x}\\right)\\right) \\end{aligned}\\tag{1}$$只需要对每一个标签和每一个相邻标签对分别打分，然后将所有打分结果求和得到总分。\nLinear Chain CRF 尽管CRF已经做了一些简化假设，但一般来说，(1)式所表示的概率模型还是过于复杂，难以求解。于是考虑到当前深度学习模型中，RNN或者层叠CNN等模型已经能够比较充分捕捉各个$y$与输入$x$的联系，因此，我们不妨考虑函数$g$跟$x$无关，那么 $$\\begin{aligned} f\\left(y_{1}, y_{2}, \\ldots, y_{n} ; \\mathbb{x}\\right) \u0026=h\\left(y_{1} ; \\mathbb{x}\\right)+g\\left(y_{1}, y_{2}\\right)+\\cdots+g\\left(y_{n-1}, y_{n}\\right)+h\\left(y_{n} ; \\mathbb{x}\\right) \\\\\\\\ \u0026=h\\left(y_{1} ; \\mathbb{x}\\right)+\\sum_{k=2}^{n}\\left(g\\left(y_{k-1}, y_{k}\\right)+h\\left(y_{k} ; \\mathbb{x}\\right)\\right) \\end{aligned}$$ 其中$g\\left(y_{k-1}, y_{k}\\right)$是一个有限的、待训练的参数矩阵，而单标签的打分函数$h(y_i;\\mathbb{x})$我们可以通过RNN或者CNN来建模。因此，该模型是可以建立的，其中概率分布变为 $$ P(y_1,\\dots,y_n|\\mathbb{x})=\\frac{1}{Z(\\mathbb{x})}\\exp\\left(h(y_1;\\mathbb{x})+\\sum_{k=1}^{n-1}\\Big[g(y_k,y_{k+1})+h(y_{k+1};\\mathbb{x})\\Big]\\right)\\tag{2} $$直接引用(Sutton, C. 2010)的定义: 在CRF中，首先需要定义特征函数.\n然后为每个特征函数$f_{j}$分配权重$\\lambda_j$, 权重是从数据中学习而来. 对$j$个特征方程求和, 对序列每个位置$i$求和:\n$$ score(y | x) = \\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(s, i, y_i, y_{i-1})$$CRF的每个特征函数都是一个输入的函数, 对应的输出是一个实数值（只是0或1）。例如, 选择特征函数$f_1(x, i, y_i, y_{i-1}) = 1$, 当且仅当$y_i = ADVERB$, 且第i个单词以“-ly”结尾; 否则为0. 如果与此特征相关的权重$\\lambda_j$很大且为正，那么这个特征等同于说模型倾向于把以-ly结尾的单词标记为ADVERB。\n通过指数化和归一化把这些得分转换为概率值: $$p(y | x) = \\frac{exp\\left\\(score(y|x)\\right\\)}{\\sum_{y^\\prime} exp\\left\\(score(y^\\prime|x)\\right\\)} = \\frac{exp\\left\\(\\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(x, i, y_i, y_{i-1})\\right\\)}{\\sum_{y^{\\prime}} exp\\left\\(\\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(x, i, y^\\prime_i, l^\\prime_{i-1})\\right\\)} $$Linear-Chain CRF特征函数的定义非常灵活, 不同的形式用于不同的功能. 比如对于HMM而言, 不管输入怎么变, 状态转换$transition(i, j)$的分值是一样的$\\log p (y_t = j | y_{t−1} = i)$; 那么此时在CRF中, 我们通过增加这样一个特征$1_{\\\\{y_{t}=j\\\\}} 1_{\\\\{y_{t-1}=1\\\\}} 1_{\\\\{x_{t}=o\\\\}}$, 使$transition(i, j)$分值依赖于当前的观察序列:\n这种特征常常用于文本处理中, 比如:\n一个句子提供观察值$x_{i-1, i}$ 单词的标签$y_{i-1, i}$ 需要指出的是在线性链CRF的定义中每个feature的依赖值并不仅限于当前和上一时间步的观察值. 事实上, 因为CRF并不表达变量之间的依赖关系, 我们可以让因子$\\Psi_{t}$依赖于整个观察向量$x$并保持线性图结构, 这时候的特征函数就是$f_{k}\\left(y_{t}, y_{t-1}, \\mathbf{x}\\right)$, 可以自由检视所有输入变量$x$, 这个特性可以拓展到所有CRFs而不仅限于线性链CRF.\nCRF既具有判别式模型的优点，又考虑到长距离上下文标记间的转移概率，以序列化形式进行全局参数优化和解码的特点，解决了其他判别式模型(如MEMM)难以避免的标记偏见问题。\n隐马尔可夫模型和Linear-Chain CRF的联系 HMM的生成式概率模型是$p(y,x)$, 它的条件概率$p(y|x)$本质上就是选取了特定特征函数的CRF. HMM和CRF的对应关系类似于Naive-Bayes和Logistics regression, 都是生成式和判别式的对比. HMM则采用生成式方法进行标签生成, CRF将各种特征组合在一起判断标签. HMM可以推演出特定形式的CRF. 把上式的HMM改写成如下形式:\n$$ \\begin{aligned} p(y, x)=\u0026 \\frac{1}{Z} \\prod_{t=1}^T \\exp \\left( \\sum_{i, j \\in S} \\theta_{i j} 1_{\\\\{y_t=i\\\\}} 1_{\\\\{y_\\{t-1}=j\\\\}} \\right. \\\\\\\\ \u0026\\left.+ \\sum_{i \\in S} \\sum_{o \\in O} \\mu_{o i} 1_{\\\\{y_{t}=i\\\\}} 1_{\\\\{x_{t}=o\\\\}} \\right) \\end{aligned} $$其中$\\theta=\\\\{\\theta_{i j}, \\mu_{o i}\\\\}$是分布的实参数, $Z$是常数正则项. $$ \\begin{aligned} \\theta_{i j} \u0026=\\log p\\left(y^{\\prime}=i | y=j\\right) \\\\\\\\ \\mu_{o i} \u0026=\\log p(x=o | y=i) \\\\\\\\ Z \u0026=1 \\end{aligned} $$HMM是生成式的, 借鉴Naive Bayes 到 logistics regression的方式, 通过引入特征函数这个概念, $f_{k}\\left(y_{t}, y_{t-1}, x_{t}\\right)$, 对于每一个$(i, j)$跳转, 加入特征函数$f_{i j}\\left(y, y^{\\prime}, x\\right)=1_{\\\\{y=i\\\\}} 1_{\\\\{y^{\\prime}=j\\\\}}$, 对于每一个状态-观察值对$(i,o)$, 加入特征函数$f_{i o}\\left(y, y^{\\prime}, x\\right)=1_{\\\\{y=i\\\\}} \\mathbf{1}_{\\\\{x=o\\\\}}$. 以上特征函数统一表示为$f_k$, 那么可以进一步把HMM写成:\n$$p(\\mathbf{y}, \\mathbf{x})=\\frac{1}{Z} \\prod_{t=1}^{T} \\exp \\left\\(\\sum_{k=1}^{K} \\theta_{k} f_{k}\\left(y_{t}, y_{t-1}, x_{t}\\right)\\right\\)$$可以得出条件概率$p(y|x)$\n$$p(\\mathbf{y} | \\mathbf{x})=\\frac{p(\\mathbf{y}, \\mathbf{x})}{\\sum_{\\mathbf{y}^{\\prime}} p\\left(\\mathbf{y}^{\\prime}, \\mathbf{x}\\right)}=\\frac{\\prod_{t=1}^{T} \\exp \\left\\(\\sum_{k=1}^{K} \\theta_{k} f_{k}\\left(y_{t}, y_{t-1}, x_{t}\\right)\\right\\)}{\\sum_{\\mathbf{y}^{\\prime}} \\prod_{t=1}^{T} \\exp \\left\\(\\sum_{k=1}^{K} \\theta_{k} f_{k}\\left(y_{t}^{\\prime}, y_{t-1}^{\\prime}, x_{t}\\right)\\right\\)}$$所以当联合概率$p(y,x)$以HMM的形式因式分解, 则关联的条件分布$p(y|x)$就是一种特定形式的linear-chain CRF，即一种仅使用当前单词自身作为特征的CRF. 通过恰当地设置特征函数, 可以从CRF中构建出一个HMM. 在CRF的对数线性形式中, 设置权重为对应HMM(取对数后)的二元转换和发射概率: $\\log p(s,o) = \\log p(s_0) + \\sum_i \\log p(s_i | s_{i-1}) + \\sum_i \\log p(o_i | s_i)$\n对于每个状态pair$\\left(y_{i-1}, y_i\\right)$, 对应HMM的每个状态转换概率$p(s_i = y_i | s_{i-1} = y_{i-1})$, CRF定义一组特征函数为$f_{y_{i-1},y_i}(o, i, s_i, s_{i-1}) = 1$ 如果 $s_i = y_i$ 且 $s_{i-1} = y_{i-1}$, 为这些特征赋予权重$g_{y_{i-1},y_i} = \\log p(s_i = y_i | s_{i-1} = y_{i-1})$ 对于每个状态-观察值pair, 对应HMM的每个发射概率$p(o_i = x | s_{i} = y_i)$, CRF定义一组特征函数为$f_{x,y}(o, i, s_i, s_{i-1}) = 1$ 如果 $o_i = x$ 且 $s_i = y_i$, 赋予权重$w_{x,y} = \\log p(o_i = x | s_i = y)$. 如此, CRF计算的似然$p(y|x)$就精确地正比于对应的HMM, 也就是说, 任意的HMM都可以由CRF表达出来.\nCRF比HMM更强大, 更泛用\nCRF可以定义更广泛的特征函数：HMM受限于相邻位置的状态转换（二元转换）和发射概率函数，迫使每个单词仅依赖于当前标签，并且每个标签仅依赖于前一个标签。而CRF可以使用更多样的全局特征。例如，如果句子的结尾包含问号，则可以给给CRF模型增加一个特征函数，记录此时将句子的第一个单词标记为VERB的概率。这使得CRF可以使用长距离依赖的特征。 CRF可以有任意的权重值：HMM的概率值必须满足特定的约束， $0 \u003c= p(o_i | s_i) \u003c= 1, \\sum_o p(o_i = x | y_1) = 1)$, 而CRF的权重值是不受限制的。 CRF与Logistic Regression CRF的概率计算与Logistic Regression (LR)的形式类似， $$CRF: p(l | s) = \\frac{exp \\left\\(\\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(s, i, l_i, l_{i-1})\\right\\)}{\\sum_{l’} exp\\left\\(\\sum_{j = 1}^m \\sum_{i = 1}^n \\lambda_j f_j(s, i, l^\\prime_i, l^\\prime_{i-1})\\right\\)}$$$$LR: P(y|x) = \\frac{\\exp \\bigg( \\sum\\limits_{i=1}^{N} w_{i} \\cdot f_{i}(x,y) \\bigg)} {\\sum\\limits_{y' \\in Y} \\exp \\bigg( \\sum\\limits_{i=1}^{N} w_{i} \\cdot f_{i}(x,y') \\bigg)}$$ 在LR中, $f_i(y, x)$是一个特征，$w_i$是与该特征相关的权重。提取的特征是二元特征，取值0或1，通常称为指示函数。这些特征中的每一个都由与输入$x$和分类$y$相关联的函数计算。\n实际上，CRF基本上就是逻辑回归的序列化：与逻辑回归是用于分类的对数线性模型不同，CRF是标签序列的对数线性模型。\nCRF模型训练 如何通过数据训练CRF模型, 估计特征函数的权重? 利用极大似然估计（Maximum Likelihood Estimation，MLE)和梯度优化(gradient descent).\n$\\log p(l | s)$相对于参数$λ_i$的梯度为:\n$$\\frac{\\partial}{\\partial w_j} \\log p(l | s) = \\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \\sum_{l’} p(l’ | s) \\sum_{j = 1}^m f_i(s, j, l^\\prime_j, l^\\prime_{j-1})$$导数的第一项是真实标签下的特征$f_i$的贡献，第二项是当前模型下特征$f_i$的期望贡献。\n对于一堆训练样例（句子和相关的词性标签）。随机初始化CRF模型的权重。要将这些随机初始化的权重转移到正确的权重，对于每个训练示例:\n遍历每个特征函数$f_i$，计算训练示例相对于$λ_i$的对数概率的梯度 以learning rate $\\alpha$的速率沿梯度方向不断修正$λ_i$: $$\\lambda_i = \\lambda_i + \\alpha [\\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \\sum_{l’} p(l’ | s) \\sum_{j = 1}^m f_i(s, j, l^\\prime_j, l^\\prime_{j-1})]$$ 重复这些训练步骤，直到满足停止条件（例如，更新低于某个阈值）。 CRF的缺点是模型训练时收敛速度比较慢.\n训练后的CRF模型, 可以用于预测一个未标记序列的最大可能标记. 我们需要每个标记的概率$p(l | s)$, 对于大小为k的标签集和长度为m的句子, 需要比较的$p(l | s)$组合有$k^m$种. 但是计算时, 可以利用动态规划的方法, 原理类似于Viterbi算法.\nTensorflow实现的CRF就是线性链CRF $$\\begin{aligned}\u0026P_Q(a_1,a_2,\\dots,a_n)\\\\ =\u0026\\frac{1}{Z} \\exp \\Big[f(a_1;Q)+g(a_1, a_2;Q) + f(a_2;Q) +\\dots + g(a_{n-1}, a_n;Q) + f(a_n;Q)\\Big] \\end{aligned}$$ 所谓线性链，就是直接认为函数$g$实际上跟$Q$没关系，即对于任何的输入文本，$g(a_{k-1},a_k)$是个常数矩阵。剩下的则跟逐标签softmax的情形差不多了，认为$f(a_k;Q)\\equiv f(a_k;q_k)$. 相对于逐标签softmax，CRF只是换了一个loss，多了一个转移矩阵，并且解码的时候需要用到viterbi算法。按照极大似然的思想，loss应该取为： $$\\begin{aligned} \u0026-\\log P_Q(a_1,a_2,\\dots,a_n)\\\\ =\u0026 - \\sum_{k=1}^n f(a_k;q_k) - \\sum_{k=2}^n g(a_{k-1},a_k) + \\log Z \\end{aligned}$$如果前面模型用BERT或者BiLSTM来得到特征$q_k$，那么就得到了序列标注任务中的Encoder-CRF了。\nCRF中文命名实体识别 比如中文命名实体识别任务, 假如需要判断人名、地名、组织名三类命名实体.\n对于人名, 通过一些模板来筛选特征。模板是对上下文的特定位置和特定信息的考虑, 适用于人名的特征模板:\n人名的指界词：主要包括称谓词、动词和副词等，句首位置和标点符号也可。根据指界词与人名共现的概率的大小，将人名的左右指界词各分为两级，生成4个人名指界词列表： 人名识别特征的原子模板，每个模板都只考虑了一种因素： 当特征函数取特定值时，特征模板被实例化, 就可以得到具体的特征。比如当前词的前一个词 $w_{i-1}$ 在人名1级左指界词列表中出现, $f_i(x, y) = 1, if: PBW1(w_{i-1}) = true, y = PERSON$\n类似的，做地名、组织名的特征提取和选择，并将其实例化，得到所有的特征函数。\n评测指标: 召回 recall = $ \\frac{正确识别的命名实体首部（尾部）的个数}{标准结果中命名实体首部（尾部）的的总数} \\times 100\\%$\n精确率 precision = $ \\frac{正确识别的命名实体首部（尾部）的个数}{识别出的命名实体首部（尾部）的总数} \\times 100\\%$\nF1 = $ \\frac{2 \\times precision \\times recall}{precision + recall}$\n谈谈生成式模型和判别式模型 从朴素贝叶斯, 到HMM; 从Logistic Regression到CRF, 这些概率图模型有如下转换关系: 而在朴素贝叶斯与Logistic Regression, 以及HMM和CRF之间, 又有生成式和判别式的区别.\n生成式模型描述标签向量y如何有概率地生成特征向量x, 即尝试构建x和y的联合分布$p(y, x)$, 典型的模型有HMM，贝叶斯模型，MRF。生成式模型 而判别模型直接描述如何根据特征向量x判断其标签y, 即尝试构建$p(y | x)$的条件概率分布, 典型模型如如LR, SVM，CRF，MEMM等. 不构建$p(x)$是因为分类时用不到. 原则上，任何类型的模型都可以使用贝叶斯规则转换为另一种类型，但实际上这些方法是不同的. 生成模型和判别模型都描述了$p(y, x)$的概率分布，但努力的方向不同。生成模型，例如朴素贝叶斯分类器和HMM，是一类可以因式分解为$p(y, x) = p(y)p(x|y)$的联合分布, 也就是说，它们描述了如何为给定标签的特征采样或“生成”值。生成式模型从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，不关心判别边界。生成式模型的优点是: • 实际上带的信息要比判别模型丰富， 研究单类问题比判别模型灵活性强 • 能更充分的利用先验知识 • 模型可以通过增量学习得到 缺点也很明显: • 学习过程比较复杂; • 在目标分类问题中准确度不高\n而判别式模型, 比如 LR, 是一系列条件分布$p(y | x)$. 也就是说，分类规则是直接建模的。原则上，判别模型也可通过为输入提供边际分布$p(x)$来获得联合分布$p(y, x)$，但很少需要这样。条件分布$p(y | x)$不包括$p(x)$的信息，在分类任务中其实无论如何也用不到。其次，对$p(x)$建模的困难之处在于它通常包含很多建模难度较高的有高度依赖性的特征。判别式模型寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。优点是: • 分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。 • 能清晰的分辨出多类或某一类与其他类之间的差异特征 • 在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好 •适用于较多类别的识别 缺点是：• 不能反映训练数据本身的特性。• 能力有限，可以分类, 但无法把整个场景描述出来。\n参考资料 An Introduction to Conditional Random Fields, Sutton, C., \u0026 McCallum, A. (2011) http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/ Classical probabilistic models and conditional random fields https://kexue.fm/archives/5542 McCallum, A. (1909). Maximum Entropy Markov Models for Information Extraction and Segmentation. Berichte Der Deutschen Chemischen Gesellschaft, 42(1), 310–317. https://doi.org/10.1002/cber.19090420146 ","wordCount":"1172","inLanguage":"en","datePublished":"2018-09-16T00:00:00Z","dateModified":"2018-09-16T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归</h1><div class=post-meta><span title='2018-09-16 00:00:00 +0000 UTC'>2018-09-16</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/NLP-HMM-CRF.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%ba%8f%e5%88%97%e6%a0%87%e6%b3%a8sequence-labeling aria-label="序列标注（Sequence Labeling）">序列标注（Sequence Labeling）</a></li><li><a href=#%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b aria-label=概率图模型>概率图模型</a><ul><li><a href=#%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8%e4%b8%8e%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b aria-label=朴素贝叶斯分类器与隐马尔可夫模型>朴素贝叶斯分类器与隐马尔可夫模型</a></li><li><a href=#%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b%e4%b8%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b aria-label=隐马尔可夫模型与最大熵马尔可夫模型>隐马尔可夫模型与最大熵马尔可夫模型</a></li></ul></li><li><a href=#%e9%9a%8f%e6%9c%ba%e5%9c%ba aria-label=随机场>随机场</a><ul><li><a href=#memm%e5%92%8ccrf aria-label=MEMM和CRF>MEMM和CRF</a></li><li><a href=#linear-chain-crf aria-label="Linear Chain CRF">Linear Chain CRF</a></li><li><a href=#%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b%e5%92%8clinear-chain-crf%e7%9a%84%e8%81%94%e7%b3%bb aria-label="隐马尔可夫模型和Linear-Chain CRF的联系">隐马尔可夫模型和Linear-Chain CRF的联系</a></li><li><a href=#crf%e4%b8%8elogistic-regression aria-label="CRF与Logistic Regression">CRF与Logistic Regression</a></li><li><a href=#crf%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83 aria-label=CRF模型训练>CRF模型训练</a></li><li><a href=#crf%e4%b8%ad%e6%96%87%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab aria-label=CRF中文命名实体识别>CRF中文命名实体识别</a></li><li><a href=#%e8%b0%88%e8%b0%88%e7%94%9f%e6%88%90%e5%bc%8f%e6%a8%a1%e5%9e%8b%e5%92%8c%e5%88%a4%e5%88%ab%e5%bc%8f%e6%a8%a1%e5%9e%8b aria-label=谈谈生成式模型和判别式模型>谈谈生成式模型和判别式模型</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label=参考资料>参考资料</a></li></ul></div></details></div><div class=post-content><h2 id=序列标注sequence-labeling>序列标注（Sequence Labeling）<a hidden class=anchor aria-hidden=true href=#序列标注sequence-labeling>#</a></h2><p>序列标注任务是指根据观察得到的序列（如一个句子）, 推断出序列每个元素（单词）对应的标注。</p><p>具体的任务包括分词(Segmentation), 词性标注（Part-of-Speach tagging, POS）, 实体识别(Named Entity Recognition, NER), 等等. 所谓POS, 就是对于一个句子, 如<code>Bob drank coffee at Starbucks</code>, 标注可能为<code>Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN)</code>.</p><p>除此之外, 还有其他涉及到需要根据观察序列推断隐含状态的问题, 这种问题的特点是每一个位置的标签都不是独立的, 而是和上下文相关依存的, 可以用序列标注的思路来处理.</p><p>单个分类器仅能预测单个类变量，但是序列标注基于概率图模型, 图模型(Graphical Models)的真正功能在于它们能够对许多有相互依赖的变量进行建模。最简单的依赖关系可以描述为一种线性链(Linear Chain), 也就是后续介绍到的隐马尔可夫模型(Hidden Markov Model, HMM)用到的假设.</p><h2 id=概率图模型>概率图模型<a hidden class=anchor aria-hidden=true href=#概率图模型>#</a></h2><p>Graphical Models, 用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。图中的节点表示随机变量，缺少边表示条件独立假设。</p><p>G = (V, E). 其中 V: vertex, 顶点/节点, 表示随机变量. E: edge, 边/弧. 如果两个节点不存在边, 则二者条件独立.
<img loading=lazy src=/images/probabilistic_graphical_models.png title="image from: Probabilistic Graphical Models Principles and Techniques"> 从图上可以看到, 贝叶斯网络(Bayesian Networks, BNs)是有向图, 每个节点的条件概率分布表示为<code>P(当前节点 | 父节点)</code>.</p><p>而马尔可夫网络则是<strong>无向图</strong>. 无向图形模型是指一整个家族的概率分布，每个概率分布都根据给定的因子集合进行因式分解。一般用<strong>random field</strong>来指代无向图中定义的特定分布. 数学上表达无向图, 指给定子集$\\{Y_a \\}_{a=1}^A$, 对于所有$\mathbf{y}_a$和任何因子选项$\mathcal{F}=\\{\Psi_a\\}$, $\Psi_a(\mathbf{y}_a) \geq 0$, 无向图定义的各个分布可以写成:</p>$$p(\mathbf{y})=\frac{1}{Z} \prod_{a=1}^A \Psi_{a}\left(\mathbf{y}_{a}\right)$$<p>Z是正则项用于保证分布$p$和为$1$</p>$$Z=\sum_{\mathbf{y}} \prod_{a=1}^{A} \Psi_{a}\left(\mathbf{y}_{a}\right)$$<p>Markov Net 包含了一组具有马尔可夫性质的随机变量. **马尔可夫随机场(Markov Random Fields, MRF)**是由参数$λ=(S, π, A)$表示, 其中S是状态的集合，π是初始状态的概率, A是状态间的转移概率。一阶马尔可夫链就是假设t时刻的状态只依赖于前一时刻的状态，与其他时刻的状态和观测无关。这个性质可以用于简化概率链的计算。使用类似性质作为假设的模型还有Bi-gram语言模型等.</p><h3 id=朴素贝叶斯分类器与隐马尔可夫模型>朴素贝叶斯分类器与隐马尔可夫模型<a hidden class=anchor aria-hidden=true href=#朴素贝叶斯分类器与隐马尔可夫模型>#</a></h3><p>朴素贝叶斯分类器(NBs)假设条件独立性(朴素贝叶斯假设, Hand and Yu, 2001)：$p(x_i | y, x_j) = p(x_i | y)$, 在给定目标值 y 时，x的属性值之间相互条件独立。这样, 计算可以简化为</p>$$p(y | \overrightarrow{x}) \propto p(y, \overrightarrow{x}) = p(y) \prod_{i=1} p(x_i | y).$$<p>朴素贝叶斯模型只考虑了单个输出变量y。如果要为一个观察序列$\overrightarrow{x} =(x_1, ..., x_n)$预测对应的分类序列$\overrightarrow{y} =（y_1, ..., y_n)$ ，一个简单的序列模型可以表示为多个NBs的乘积。此时不考虑序列单个位置之间的相互依赖。</p>$$p(\overrightarrow{y}, \overrightarrow{x}) = \prod^n_{i=1} p(y_i) p(x_i | y_i).$$<p>此时每个观察值$x_i$仅取决于对应序列位置的类变量$y_i$。由于这种独立性假设，从一个步骤到另一个步骤的转换概率不包括在该模型中。然而这种假设在实践中几乎不会符合，这导致这种模型的性能很有限。</p><p>因此，比较合理的假设是观测序列在连续相邻位置间的状态存在依赖。要模拟这种依赖关系, 就要引入状态转移概率$p(y_i | y_{i-1})$, 由此引出著名的隐马尔可夫模型 Hidden Markov model, HMM, Rabiner (1989).</p><p>HMM参数$λ = (Y, X, π, A, B)$ ，其中Y是隐状态（输出变量）的集合，X是观察值（输入）集合，π是初始状态的概率，A是状态转移概率矩阵$p(y_i | y_{i-1})$，B是输出观察值概率矩阵$p(x_i | y_{i})$。在POS任务中, X就是观察到的句子, Y就是待推导的标注序列, 因为词性待求的, 所以人们称之为<strong>隐含状态</strong>.</p><p>概括一下HMM设定的假设:</p><ol><li>Markov assumption：假设每个状态仅依赖于其前一个状态, $p(y_t|y_{t−1})$</li><li>Stationarity assumption：状态的转换概率与转换发生的实际时间（实际输入）无关</li><li>Output independence assumption: 假设每一个观察值x仅依赖于当前状态值y, $p(x_t|y_t)$, 而与前面的观察值无关。</li></ol><p>那么状态序列y和观察序列x的联合概率可以分解为</p>$$p(\mathbf{y}, \mathbf{x})=\prod_{t=1}^{\mathrm{T}} p\left(y_{t} | y_{t-1}\right) p\left(x_{t} | y_{t}\right)$$<p>总的来说, 隐马尔可夫模型（HMM）是具有随机状态转移和观测值的有限状态自动机（Rabiner，1989）。自动机对概率生成过程进行建模: 先从某个初始状态开始，发射(emit)该状态生成的观察值，再转移到下一个状态，再发射另一个观察值，以此类推，直到达到指定的最终状态，从而产生一系列观察值。</p><p>HMM作为生成式的概率模型, 对观察特征的条件独立约束非常严格. 而且为了定义观察值序列和序列标记的联合概率，生成模型需要枚举所有可能的观察序列. 对于表示多个相互作用的特征或观测值的长距离相关性, 这种枚举是不切实际的，因为此类模型的inference很棘手。但很多任务往往受益于这种相互作用、相互交叉重叠的特征，比如除了传统的单词自身外，还有大小写，单词结尾，词性，格式，在页面上的位置以及WordNet中的节点成员身份等等。</p><p>除此之外, 大部分文本任务是根据给定的观察序列（如纯文本）来预测对应的状态序列，也就是判别问题。换句话说，HMM不恰当地用了生成联合概率的模型去判别问题。</p><h3 id=隐马尔可夫模型与最大熵马尔可夫模型>隐马尔可夫模型与最大熵马尔可夫模型<a hidden class=anchor aria-hidden=true href=#隐马尔可夫模型与最大熵马尔可夫模型>#</a></h3><p>最大熵马尔可夫模型(Maximum Entropy Markov Models, MEMM)跟HMM的生成式概率图不同，MEMM对当前状态的判断依赖于前一时间步的状态和当前观察值的状态。<img loading=lazy src=/images/HMM-MEMM.png title="image from McCallum, A. (1909)"></p><p>首先所谓"熵"就是信息论中的概念:</p><blockquote><p>Entropy: the uncertainty of a distribution.</p></blockquote><p>量化Entropy: surprise.
Entropy = expected surprise</p><p>Event $x$,
Probability $p_x$,
&ldquo;Surprise&rdquo; $log(1/p_x)$,
Entropy:</p>$$
\begin{aligned}
&H(p)=E_{p}\left[\log \frac{1}{p_{x}}\right]\\
&\mathrm{H}(p)=-\sum_{x} p_{x} \log p_{x}
\end{aligned}
$$<p>熵最大的分布就是均匀分布，也就是每一个选项都一样，等于什么信息都没给。如果给了额外的信息，如约束，特征之后，熵就可以降低。</p><p>“最大熵”是指遵循最大熵原则：</p><blockquote><p>model all that is known and assume nothing about that which is unknown.</p></blockquote><p>也就说, 如果给定了一组事实，我们最好选择一个符合这些事实的模型，剩余情况则尽可能地一视同仁不做任何区别对待。最佳的模型是符合训练数据衍生的约束条件的模型，同时尽可能少地做假设，也就是少做承诺，也就避免过拟合。</p><p>MEMM 把HMM中的转移概率和发射概率替换为一个概率：当前状态$s$依赖于前一个状态$s^{\prime}$和当前观察值$o$, $\mathrm{P}\left(s | s^{\prime}, o\right)$</p><p>MEMM的训练思路是这样: 对每个状态$s^{\prime}$, 将训练数据分为<code>&lt;观察-目标状态>对</code> $<o, s>$, 也就是把 $\mathrm{P}\left(s | s^{\prime}, o\right)$ 分成 $|S|$ 个分别训练的exponential model $\mathrm{P}_{s^{\prime}}(s | o)=\mathrm{P}\left(s | s^{\prime}, o\right)$, 再通过最大化熵来训练exponential models, 换种说法叫<code>logistic regression classifier</code>.</p><p>用的约束条件是学习分布中每个特征$a$的期望值与训练数据的观测序列上每个特征的期望值相同. 满足这些约束的最大熵分布（Della Pietra，Della Pietra和Lafferty，1997）是唯一的，与最大似然分布一致，对每一个位置的状态$s^{\prime}$, 具有指数形式：</p>$$
P_{s^{\prime}}(s | o)=\frac{1}{Z\left(o, s^{\prime}\right)} \exp \left(\sum_{a} \lambda_{a} f_{a}(o, s)\right)
$$<p>其中$\lambda_{a}$是待估计的参数, $Z\left(o, s^{\prime}\right)$是归一化因子</p>$$
Z\left(o, s^{\prime}\right)=\sum_{s \in S} P\left(s | s^{\prime}, o\right)
$$<p>$S$是标签集.</p><p>如果把问题简化为线性的相邻依赖, 那么每一个状态$s_{i}$仅依赖于前一个状态$s_{i-1}$. 用$Y$表达标签序列, 用$X$表达观察序列, 那么</p>$$P\left(y_{1}, y_{2}, \ldots, y_{n} | \mathbb{x}\right)=P\left(y_{1} | \mathbb{x}\right) P\left(y_{2} | \mathbb{x}, y_{1}\right) P\left(y_{3} | \mathbb{x}, y_{2}\right) \ldots P\left(y_{n} | \mathbb{x}, y_{n-1}\right)$$<p>其中</p>$$P\left(y_{1} | \mathbb{x}\right)=\frac{e^{f\left(y_{1} ; \mathbb{x}\right)}}{\sum_{y_{1} \in S} e^{f\left(y_{k} ; \mathbb{x}\right)}}, \quad P\left(y_{k} | \mathbb{x}, y_{k-1}\right)=\frac{e^{g\left(y_{k-1}, y_{k}\right)+f\left(y_{k} ; \mathbb{x}\right)}}{\sum_{y_{k} \in S} e^{g\left(y_{k-1}, y_{k}\right)+f\left(y_{k} ; \mathbb{x}\right)}}$$<p>则</p>$$P(\mathbb{y} | \mathbb{x})=\frac{e^{f\left(y_{1} ; \mathbb{x}\right)+g\left(y_{1}, y_{2}\right)+f\left(y_{2} ; \mathbb{x}\right)+\cdots+g\left(y_{n-1}, y_{n}\right)+f\left(y_{n} ; \mathbb{x}\right)}}{\left(\sum_{y_{1} \in S} e^{f\left(y_{1} ; \mathbb{x}\right)}\right)\left(\sum_{y_{2} \in S} e^{g\left(y_{1}, y_{2}\right)+f\left(y_{2} ; \mathbb{x}\right)}\right) \cdots\left(\sum_{y_{n} \in S} e^{g\left(y_{n-1}, y_{n}\right)+f\left(y_{n} ; \mathbb{x}\right)}\right)}$$<p>MEMM将整体的概率分布分解为每一个时间步的分布之积，所以算loss只需要把每一步的交叉熵求和。只需要每一步单独执行softmax，所以MEMM是完全可以并行的，速度跟直接逐步Softmax基本一样。</p><p>虽然MEMM能克服HMM的很多弱点, 但是MEMM自身也有一个 <strong>label bias</strong> 问题, 就是标签偏差, 离开给定状态的转移仅相互对比，而不是与全局所有其他转移对比。转移分数是分别对每个状态的归一化, 这意味到达一个状态的所有质量必须在可能的后续状态之间分配。观察值可以影响哪个目标状态获得质量，但无法影响多少质量可以被传递。这会导致模型偏向输出选择较少的状态, 比如极端情况下, 在训练集中某个状态$s_a$只发现了有一种可能的转移$s_b$, 那么状态$s_a$别无选择，只能将所有质量传递给它的唯一的 transition output $s_b$。</p><h2 id=随机场>随机场<a hidden class=anchor aria-hidden=true href=#随机场>#</a></h2><p>随机场, 可以看成是一组随机变量的集合（这组随机变量对应同一个样本空间）。当给每一个位置按照某种分布随机赋予一个值之后，其全体就叫做随机场。这些随机变量之间可能有依赖关系，一般来说，也只有当这些变量之间有依赖关系的时候，我们将其单独拿出来看成一个随机场才有实际意义。</p><p>如果给定的MRF中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个MRF的分布，也就是<strong>条件分布</strong>，那么这个MRF就称为 Conditional random fields (CRF)。它的条件分布形式完全类似于MRF的分布形式，只不过多了一个观察集合X。所以, CRF本质上是给定了条件(观察值observations)集合的MRF.</p><p>1.特征函数的选择: 特征函数的选取直接关系模型的性能。
2.参数估计: 从已经标注好的训练数据集学习条件随机场模型的参数，即各特征函数的权重向量λ。
3.模型推断: 在给定条件随机场模型参数λ下，预测出最可能的状态序列。</p><h3 id=memm和crf>MEMM和CRF<a hidden class=anchor aria-hidden=true href=#memm和crf>#</a></h3><p>在CRF的序列标注问题中，我们要计算的是条件概率</p>$$
P\left(y_{1}, \ldots, y_{n} \mid \mathbb{x}\right), \quad \mathbb{x}=\left(x_{1}, \ldots, x_{n}\right)
$$<p>CRF和MEMM的关键区别在于，MEMM使用每个状态的指数模型来确定给定当前状态的下一状态的条件概率，而CRF则使用<strong>一个指数模型</strong>来表示整个标签序列的联合概率, 这个概率条件依赖于给定的完整观察序列。二者区别仅在于分母（也就是归一化因子）的计算方式不同，CRF的是全局归一化的，而MEMM的是局部归一化的. 也就是说CRF是一个以观测序列$X$为全局条件的随机场. 存在函数$f(y_1,\dots,y_n;\mathbb{x})$，使得</p>$$
P(y_1,\dots,y_n|\mathbb{x})=\frac{1}{Z(\mathbb{x})}\exp\Big(f(y_1,\dots,y_n;\mathbb{x})\Big)
$$<p>可以得到对应得概率是</p>$$P(\mathbb{y} | \mathbb{x})=\frac{e^{f\left(y_{1}, y_{2}, \ldots, y_{n} ; \mathbb{x}\right)}}{\sum_{y_{1}, y_{2}, \ldots, y_{n} \in S^n} e^{f\left(y_{1}, y_{2}, \ldots, y_{n} ; \mathbb{x}\right)}}$$<p>CRF的计算困难之处在于上式的分母项包含了所有可能路径$S^n$的求和，搜索空间非常庞大.</p><p>因此做出一些简化，假设输出之间的关联仅发生在相邻位置，并且关联是指数加性的:</p>$$\begin{aligned}
f\left(y_{1}, y_{2}, \ldots, y_{n} ; \mathbb{x}\right) &=f\left(y_{1} ; \mathbb{x}\right)+g\left(y_{1}, y_{2};\mathbb{x}\right)+\cdots+g\left(y_{n-1}, y_{n};\mathbb{x}\right)+f\left(y_{n} ; \mathbb{x}\right) \\\\
&=f\left(y_{1} ; \mathbb{x}\right)+\sum_{k=2}^{n}\left(g\left(y_{k-1}, y_{k};\mathbb{x}\right)+f\left(y_{k} ; \mathbb{x}\right)\right)
\end{aligned}\tag{1}$$<p>只需要对每一个标签和每一个相邻标签对分别打分，然后将所有打分结果求和得到总分。</p><h3 id=linear-chain-crf>Linear Chain CRF<a hidden class=anchor aria-hidden=true href=#linear-chain-crf>#</a></h3><p>尽管CRF已经做了一些简化假设，但一般来说，(1)式所表示的概率模型还是过于复杂，难以求解。于是考虑到当前深度学习模型中，RNN或者层叠CNN等模型已经能够比较充分捕捉各个$y$与输入$x$的联系，因此，我们不妨考虑函数$g$跟$x$无关，那么</p>$$\begin{aligned}
f\left(y_{1}, y_{2}, \ldots, y_{n} ; \mathbb{x}\right) &=h\left(y_{1} ; \mathbb{x}\right)+g\left(y_{1}, y_{2}\right)+\cdots+g\left(y_{n-1}, y_{n}\right)+h\left(y_{n} ; \mathbb{x}\right) \\\\
&=h\left(y_{1} ; \mathbb{x}\right)+\sum_{k=2}^{n}\left(g\left(y_{k-1}, y_{k}\right)+h\left(y_{k} ; \mathbb{x}\right)\right)
\end{aligned}$$<p>其中$g\left(y_{k-1}, y_{k}\right)$是一个有限的、待训练的参数矩阵，而单标签的打分函数$h(y_i;\mathbb{x})$我们可以通过RNN或者CNN来建模。因此，该模型是可以建立的，其中概率分布变为</p>$$
P(y_1,\dots,y_n|\mathbb{x})=\frac{1}{Z(\mathbb{x})}\exp\left(h(y_1;\mathbb{x})+\sum_{k=1}^{n-1}\Big[g(y_k,y_{k+1})+h(y_{k+1};\mathbb{x})\Big]\right)\tag{2}
$$<p>直接引用(Sutton, C. 2010)的定义:
<img loading=lazy src=/images/linear-chain-crf.png title="from: An Introduction to Conditional Random Fields, by Charles Sutton and Andrew McCallum"></p><p>在CRF中，首先需要定义特征函数.</p><p>然后为每个特征函数$f_{j}$分配权重$\lambda_j$, 权重是从数据中学习而来. 对$j$个特征方程求和, 对序列每个位置$i$求和:</p>$$ score(y | x) = \sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, y_i, y_{i-1})$$<p>CRF的每个特征函数都是一个输入的函数, 对应的输出是一个实数值（只是0或1）。例如, 选择特征函数$f_1(x, i, y_i, y_{i-1}) = 1$, 当且仅当$y_i = ADVERB$, 且第i个单词以“<code>-ly</code>”结尾; 否则为0. 如果与此特征相关的权重$\lambda_j$很大且为正，那么这个特征等同于说模型倾向于把以<code>-ly</code>结尾的单词标记为ADVERB。</p><p>通过指数化和归一化把这些得分转换为概率值:</p>$$p(y | x) = \frac{exp\left\(score(y|x)\right\)}{\sum_{y^\prime} exp\left\(score(y^\prime|x)\right\)} = \frac{exp\left\(\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(x, i, y_i, y_{i-1})\right\)}{\sum_{y^{\prime}} exp\left\(\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(x, i, y^\prime_i, l^\prime_{i-1})\right\)} $$<p>Linear-Chain CRF特征函数的定义非常灵活, 不同的形式用于不同的功能. 比如对于HMM而言, 不管输入怎么变, 状态转换$transition(i, j)$的分值是一样的$\log p (y_t = j | y_{t−1} = i)$; 那么此时在CRF中, 我们通过增加这样一个特征$1_{\\{y_{t}=j\\}} 1_{\\{y_{t-1}=1\\}} 1_{\\{x_{t}=o\\}}$, 使$transition(i, j)$分值依赖于当前的观察序列:<img loading=lazy src=/images/linear-chain-crf-depend-on-current-observation.png title="from: An Introduction to Conditional Random Fields, by Charles Sutton and Andrew McCallum"></p><p>这种特征常常用于文本处理中, 比如:</p><ol><li>一个句子提供观察值$x_{i-1, i}$</li><li>单词的标签$y_{i-1, i}$</li></ol><p>需要指出的是在线性链CRF的定义中每个feature的依赖值并不仅限于当前和上一时间步的观察值. 事实上, 因为CRF并不表达变量之间的依赖关系, 我们可以让因子$\Psi_{t}$依赖于整个观察向量$x$并保持线性图结构, 这时候的特征函数就是$f_{k}\left(y_{t}, y_{t-1}, \mathbf{x}\right)$, 可以自由检视所有输入变量$x$, <img loading=lazy src=/images/linear-chain-crf-depend-on-all-observations.png title="from: An Introduction to Conditional Random Fields, by Charles Sutton and Andrew McCallum"> 这个特性可以拓展到所有CRFs而不仅限于线性链CRF.</p><p>CRF既具有判别式模型的优点，又考虑到长距离上下文标记间的转移概率，以序列化形式进行全局参数优化和解码的特点，解决了其他判别式模型(如MEMM)难以避免的标记偏见问题。</p><h3 id=隐马尔可夫模型和linear-chain-crf的联系>隐马尔可夫模型和Linear-Chain CRF的联系<a hidden class=anchor aria-hidden=true href=#隐马尔可夫模型和linear-chain-crf的联系>#</a></h3><p>HMM的生成式概率模型是$p(y,x)$, 它的条件概率$p(y|x)$本质上就是选取了特定特征函数的CRF. HMM和CRF的对应关系类似于Naive-Bayes和Logistics regression, 都是生成式和判别式的对比. HMM则采用生成式方法进行标签生成, CRF将各种特征组合在一起判断标签. HMM可以推演出特定形式的CRF. 把上式的HMM改写成如下形式:</p>$$
\begin{aligned}
p(y, x)=& \frac{1}{Z} \prod_{t=1}^T \exp \left( \sum_{i, j \in S} \theta_{i j} 1_{\\{y_t=i\\}} 1_{\\{y_\{t-1}=j\\}} \right. \\\\
&\left.+ \sum_{i \in S} \sum_{o \in O} \mu_{o i} 1_{\\{y_{t}=i\\}} 1_{\\{x_{t}=o\\}} \right)
\end{aligned}
$$<p>其中$\theta=\\{\theta_{i j}, \mu_{o i}\\}$是分布的实参数, $Z$是常数正则项.</p>$$
\begin{aligned}
\theta_{i j} &=\log p\left(y^{\prime}=i | y=j\right) \\\\
\mu_{o i} &=\log p(x=o | y=i) \\\\
Z &=1
\end{aligned}
$$<p>HMM是生成式的, 借鉴Naive Bayes 到 logistics regression的方式, 通过引入特征函数这个概念, $f_{k}\left(y_{t}, y_{t-1}, x_{t}\right)$, 对于每一个$(i, j)$跳转, 加入特征函数$f_{i j}\left(y, y^{\prime}, x\right)=1_{\\{y=i\\}} 1_{\\{y^{\prime}=j\\}}$, 对于每一个<code>状态-观察值</code>对$(i,o)$, 加入特征函数$f_{i o}\left(y, y^{\prime}, x\right)=1_{\\{y=i\\}} \mathbf{1}_{\\{x=o\\}}$. 以上特征函数统一表示为$f_k$, 那么可以进一步把HMM写成:</p>$$p(\mathbf{y}, \mathbf{x})=\frac{1}{Z} \prod_{t=1}^{T} \exp \left\(\sum_{k=1}^{K} \theta_{k} f_{k}\left(y_{t}, y_{t-1}, x_{t}\right)\right\)$$<p>可以得出条件概率$p(y|x)$</p>$$p(\mathbf{y} | \mathbf{x})=\frac{p(\mathbf{y}, \mathbf{x})}{\sum_{\mathbf{y}^{\prime}} p\left(\mathbf{y}^{\prime}, \mathbf{x}\right)}=\frac{\prod_{t=1}^{T} \exp \left\(\sum_{k=1}^{K} \theta_{k} f_{k}\left(y_{t}, y_{t-1}, x_{t}\right)\right\)}{\sum_{\mathbf{y}^{\prime}} \prod_{t=1}^{T} \exp \left\(\sum_{k=1}^{K} \theta_{k} f_{k}\left(y_{t}^{\prime}, y_{t-1}^{\prime}, x_{t}\right)\right\)}$$<p>所以当联合概率$p(y,x)$以HMM的形式因式分解, 则关联的条件分布$p(y|x)$就是一种特定形式的linear-chain CRF，即一种仅使用当前单词自身作为特征的CRF. <img loading=lazy src=/images/HMM-like-linear-chain-crf.png title="Graphical model of the HMM-like linear-chain CRF. by Sutton, C. 2010"></p><p>通过恰当地设置特征函数, 可以从CRF中构建出一个HMM. 在CRF的对数线性形式中, 设置权重为对应HMM(取对数后)的二元转换和发射概率: $\log p(s,o) = \log p(s_0) + \sum_i \log p(s_i | s_{i-1}) + \sum_i \log p(o_i | s_i)$</p><ul><li>对于每个状态pair$\left(y_{i-1}, y_i\right)$, 对应HMM的每个状态转换概率$p(s_i = y_i | s_{i-1} = y_{i-1})$, CRF定义一组特征函数为$f_{y_{i-1},y_i}(o, i, s_i, s_{i-1}) = 1$ 如果 $s_i = y_i$ 且 $s_{i-1} = y_{i-1}$, 为这些特征赋予权重$g_{y_{i-1},y_i} = \log p(s_i = y_i | s_{i-1} = y_{i-1})$</li><li>对于每个状态-观察值pair, 对应HMM的每个发射概率$p(o_i = x | s_{i} = y_i)$, CRF定义一组特征函数为$f_{x,y}(o, i, s_i, s_{i-1}) = 1$ 如果 $o_i = x$ 且 $s_i = y_i$, 赋予权重$w_{x,y} = \log p(o_i = x | s_i = y)$.</li></ul><p>如此, CRF计算的似然$p(y|x)$就精确地正比于对应的HMM, 也就是说, 任意的HMM都可以由CRF表达出来.</p><p>CRF比HMM更强大, 更泛用</p><ol><li>CRF可以定义更广泛的特征函数：HMM受限于相邻位置的状态转换（二元转换）和发射概率函数，迫使每个单词仅依赖于当前标签，并且每个标签仅依赖于前一个标签。而CRF可以使用更多样的全局特征。例如，如果句子的结尾包含问号，则可以给给CRF模型增加一个特征函数，记录此时将句子的第一个单词标记为VERB的概率。这使得CRF可以使用长距离依赖的特征。</li><li>CRF可以有任意的权重值：HMM的概率值必须满足特定的约束， $0 <= p(o_i | s_i) <= 1, \sum_o p(o_i = x | y_1) = 1)$, 而CRF的权重值是不受限制的。</li></ol><h3 id=crf与logistic-regression>CRF与Logistic Regression<a hidden class=anchor aria-hidden=true href=#crf与logistic-regression>#</a></h3><p>CRF的概率计算与Logistic Regression (LR)的形式类似，</p>$$CRF: p(l | s) = \frac{exp \left\(\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})\right\)}{\sum_{l’} exp\left\(\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l^\prime_i, l^\prime_{i-1})\right\)}$$$$LR: P(y|x) = \frac{\exp \bigg( \sum\limits_{i=1}^{N} w_{i} \cdot f_{i}(x,y) \bigg)} {\sum\limits_{y' \in Y} \exp \bigg( \sum\limits_{i=1}^{N} w_{i} \cdot f_{i}(x,y') \bigg)}$$<p>在LR中, $f_i(y, x)$是一个特征，$w_i$是与该特征相关的权重。提取的特征是二元特征，取值0或1，通常称为指示函数。这些特征中的每一个都由与输入$x$和分类$y$相关联的函数计算。</p><p>实际上，CRF基本上就是逻辑回归的序列化：与逻辑回归是用于分类的对数线性模型不同，CRF是标签序列的对数线性模型。</p><h3 id=crf模型训练>CRF模型训练<a hidden class=anchor aria-hidden=true href=#crf模型训练>#</a></h3><p>如何通过数据训练CRF模型, 估计特征函数的权重? 利用极大似然估计（Maximum Likelihood Estimation，MLE)和梯度优化(gradient descent).</p><p>$\log p(l | s)$相对于参数$λ_i$的梯度为:</p>$$\frac{\partial}{\partial w_j} \log p(l | s) = \sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l’} p(l’ | s) \sum_{j = 1}^m f_i(s, j, l^\prime_j, l^\prime_{j-1})$$<p>导数的第一项是真实标签下的特征$f_i$的贡献，第二项是当前模型下特征$f_i$的期望贡献。</p><p>对于一堆训练样例（句子和相关的词性标签）。随机初始化CRF模型的权重。要将这些随机初始化的权重转移到正确的权重，对于每个训练示例:</p><ul><li>遍历每个特征函数$f_i$，计算训练示例相对于$λ_i$的对数概率的梯度</li><li>以learning rate $\alpha$的速率沿梯度方向不断修正$λ_i$:</li></ul>$$\lambda_i = \lambda_i + \alpha [\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l’} p(l’ | s) \sum_{j = 1}^m f_i(s, j, l^\prime_j, l^\prime_{j-1})]$$<ul><li>重复这些训练步骤，直到满足停止条件（例如，更新低于某个阈值）。</li></ul><p>CRF的缺点是模型训练时收敛速度比较慢.</p><p>训练后的CRF模型, 可以用于预测一个未标记序列的最大可能标记. 我们需要每个标记的概率$p(l | s)$, 对于大小为k的标签集和长度为m的句子, 需要比较的$p(l | s)$组合有$k^m$种. 但是计算时, 可以利用动态规划的方法, 原理类似于Viterbi算法.</p><p>Tensorflow实现的CRF就是线性链CRF</p>$$\begin{aligned}&P_Q(a_1,a_2,\dots,a_n)\\
=&\frac{1}{Z} \exp \Big[f(a_1;Q)+g(a_1, a_2;Q) + f(a_2;Q) +\dots + g(a_{n-1}, a_n;Q) + f(a_n;Q)\Big]
\end{aligned}$$<p>所谓线性链，就是直接认为函数$g$实际上跟$Q$没关系，即对于任何的输入文本，$g(a_{k-1},a_k)$是个常数矩阵。剩下的则跟逐标签softmax的情形差不多了，认为$f(a_k;Q)\equiv f(a_k;q_k)$. 相对于逐标签softmax，CRF只是换了一个loss，多了一个转移矩阵，并且解码的时候需要用到viterbi算法。按照极大似然的思想，loss应该取为：</p>$$\begin{aligned} &-\log P_Q(a_1,a_2,\dots,a_n)\\
=& - \sum_{k=1}^n f(a_k;q_k) - \sum_{k=2}^n g(a_{k-1},a_k) + \log Z
\end{aligned}$$<p>如果前面模型用BERT或者BiLSTM来得到特征$q_k$，那么就得到了序列标注任务中的Encoder-CRF了。</p><h3 id=crf中文命名实体识别>CRF中文命名实体识别<a hidden class=anchor aria-hidden=true href=#crf中文命名实体识别>#</a></h3><p>比如中文命名实体识别任务, 假如需要判断人名、地名、组织名三类命名实体.</p><p>对于人名, 通过一些模板来筛选特征。模板是对上下文的特定位置和特定信息的考虑, 适用于人名的特征模板:</p><ul><li>人名的指界词：主要包括称谓词、动词和副词等，句首位置和标点符号也可。根据指界词与人名共现的概率的大小，将人名的左右指界词各分为两级，生成4个人名指界词列表：<img loading=lazy src=/images/%E4%BA%BA%E5%90%8D%E6%8C%87%E7%95%8C%E8%AF%8D.png></li><li>人名识别特征的原子模板，每个模板都只考虑了一种因素：<img loading=lazy src=/images/%E4%BA%BA%E5%90%8D%E8%AF%86%E5%88%AB%E7%89%B9%E5%BE%81%E5%8E%9F%E5%AD%90%E6%A8%A1%E6%9D%BF.png></li></ul><p>当特征函数取特定值时，特征模板被实例化, 就可以得到具体的特征。比如当前词的前一个词 $w_{i-1}$ 在人名1级左指界词列表中出现, $f_i(x, y) = 1, if: PBW1(w_{i-1}) = true, y = PERSON$</p><p>类似的，做地名、组织名的特征提取和选择，并将其实例化，得到所有的特征函数。</p><p>评测指标:
召回 recall = $ \frac{正确识别的命名实体首部（尾部）的个数}{标准结果中命名实体首部（尾部）的的总数} \times 100\%$</p><p>精确率 precision = $ \frac{正确识别的命名实体首部（尾部）的个数}{识别出的命名实体首部（尾部）的总数} \times 100\%$</p><p>F1 = $ \frac{2 \times precision \times recall}{precision + recall}$</p><h3 id=谈谈生成式模型和判别式模型>谈谈生成式模型和判别式模型<a hidden class=anchor aria-hidden=true href=#谈谈生成式模型和判别式模型>#</a></h3><p>从朴素贝叶斯, 到HMM; 从Logistic Regression到CRF, 这些概率图模型有如下转换关系:
<img loading=lazy src=/images/relationship_nbs_hmm_lr_crf.png title="Diagram of the relationship between naive Bayes, logistic regression, HMMs, linear-chain CRFs, generative models, and general CRFs. image from: An Introduction to Conditional Random Fields, by Charles Sutton and Andrew McCallum"></p><p>而在朴素贝叶斯与Logistic Regression, 以及HMM和CRF之间, 又有生成式和判别式的区别.</p><ul><li>生成式模型描述标签向量y如何有概率地<strong>生成</strong>特征向量x, 即尝试构建x和y的联合分布$p(y, x)$, 典型的模型有HMM，贝叶斯模型，MRF。生成式模型</li><li>而判别模型直接描述如何根据特征向量x判断其标签y, 即尝试构建$p(y | x)$的条件概率分布, 典型模型如如LR, SVM，CRF，MEMM等. 不构建$p(x)$是因为分类时用不到.</li></ul><p>原则上，任何类型的模型都可以使用贝叶斯规则转换为另一种类型，但实际上这些方法是不同的. 生成模型和判别模型都描述了$p(y, x)$的概率分布，但努力的方向不同。生成模型，例如朴素贝叶斯分类器和HMM，是一类可以因式分解为$p(y, x) = p(y)p(x|y)$的联合分布, 也就是说，它们描述了如何为给定标签的特征采样或“生成”值。生成式模型从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，不关心判别边界。生成式模型的优点是:
• 实际上带的信息要比判别模型丰富， 研究单类问题比判别模型灵活性强
• 能更充分的利用先验知识
• 模型可以通过增量学习得到
缺点也很明显: • 学习过程比较复杂; • 在目标分类问题中准确度不高</p><p>而判别式模型, 比如 LR, 是一系列条件分布$p(y | x)$. 也就是说，分类规则是直接建模的。原则上，判别模型也可通过为输入提供边际分布$p(x)$来获得联合分布$p(y, x)$，但很少需要这样。条件分布$p(y | x)$不包括$p(x)$的信息，在分类任务中其实无论如何也用不到。其次，对$p(x)$建模的困难之处在于它通常包含很多建模难度较高的有高度依赖性的特征。判别式模型寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。优点是:
• 分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。
• 能清晰的分辨出多类或某一类与其他类之间的差异特征
• 在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好
•适用于较多类别的识别
缺点是：• 不能反映训练数据本身的特性。• 能力有限，可以分类, 但无法把整个场景描述出来。</p><h2 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h2><ol><li><a href=http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf>An Introduction to Conditional Random Fields</a>, Sutton, C., & McCallum, A. (2011)</li><li><a href=http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/>http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/</a></li><li>Classical probabilistic models and conditional random fields</li><li><a href=https://kexue.fm/archives/5542>https://kexue.fm/archives/5542</a></li><li>McCallum, A. (1909). Maximum Entropy Markov Models for Information Extraction and Segmentation. Berichte Der Deutschen Chemischen Gesellschaft, 42(1), 310–317. <a href=https://doi.org/10.1002/cber.19090420146>https://doi.org/10.1002/cber.19090420146</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/transformer-self-attention-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BC%96%E7%A0%81/><span class=title>« Prev</span><br><span>Transformer & Self-Attention (多头)自注意力编码</span>
</a><a class=next href=https://congchan.github.io/posts/find-all-collinear-points-a-pattern-recognition-problem/><span class=title>Next »</span><br><span>Find All Collinear Points - A Pattern Recognition Problem</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归 on x" href="https://x.com/intent/tweet/?text=%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%20-%20%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%20-%20%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%20-%20%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba%20-%20%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25A6%2582%25E7%258E%2587%25E5%259B%25BE%25E6%25A8%25A1%25E5%259E%258B-%25E6%259C%25B4%25E7%25B4%25A0%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF-%25E9%259A%2590%25E9%25A9%25AC%25E5%25B0%2594%25E7%25A7%2591%25E5%25A4%25AB-%25E6%259D%25A1%25E4%25BB%25B6%25E9%259A%258F%25E6%259C%25BA%25E5%259C%25BA-%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%2592%2f&amp;hashtags=NLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25A6%2582%25E7%258E%2587%25E5%259B%25BE%25E6%25A8%25A1%25E5%259E%258B-%25E6%259C%25B4%25E7%25B4%25A0%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF-%25E9%259A%2590%25E9%25A9%25AC%25E5%25B0%2594%25E7%25A7%2591%25E5%25A4%25AB-%25E6%259D%25A1%25E4%25BB%25B6%25E9%259A%258F%25E6%259C%25BA%25E5%259C%25BA-%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%2592%2f&amp;title=%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%20-%20%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%20-%20%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%20-%20%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba%20-%20%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&amp;summary=%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%20-%20%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%20-%20%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%20-%20%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba%20-%20%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25A6%2582%25E7%258E%2587%25E5%259B%25BE%25E6%25A8%25A1%25E5%259E%258B-%25E6%259C%25B4%25E7%25B4%25A0%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF-%25E9%259A%2590%25E9%25A9%25AC%25E5%25B0%2594%25E7%25A7%2591%25E5%25A4%25AB-%25E6%259D%25A1%25E4%25BB%25B6%25E9%259A%258F%25E6%259C%25BA%25E5%259C%25BA-%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%2592%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25A6%2582%25E7%258E%2587%25E5%259B%25BE%25E6%25A8%25A1%25E5%259E%258B-%25E6%259C%25B4%25E7%25B4%25A0%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF-%25E9%259A%2590%25E9%25A9%25AC%25E5%25B0%2594%25E7%25A7%2591%25E5%25A4%25AB-%25E6%259D%25A1%25E4%25BB%25B6%25E9%259A%258F%25E6%259C%25BA%25E5%259C%25BA-%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%2592%2f&title=%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%20-%20%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%20-%20%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%20-%20%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba%20-%20%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25A6%2582%25E7%258E%2587%25E5%259B%25BE%25E6%25A8%25A1%25E5%259E%258B-%25E6%259C%25B4%25E7%25B4%25A0%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF-%25E9%259A%2590%25E9%25A9%25AC%25E5%25B0%2594%25E7%25A7%2591%25E5%25A4%25AB-%25E6%259D%25A1%25E4%25BB%25B6%25E9%259A%258F%25E6%259C%25BA%25E5%259C%25BA-%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%2592%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归 on whatsapp" href="https://api.whatsapp.com/send?text=%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%20-%20%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%20-%20%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%20-%20%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba%20-%20%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25A6%2582%25E7%258E%2587%25E5%259B%25BE%25E6%25A8%25A1%25E5%259E%258B-%25E6%259C%25B4%25E7%25B4%25A0%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF-%25E9%259A%2590%25E9%25A9%25AC%25E5%25B0%2594%25E7%25A7%2591%25E5%25A4%25AB-%25E6%259D%25A1%25E4%25BB%25B6%25E9%259A%258F%25E6%259C%25BA%25E5%259C%25BA-%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%2592%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归 on telegram" href="https://telegram.me/share/url?text=%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%20-%20%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%20-%20%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%20-%20%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba%20-%20%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25A6%2582%25E7%258E%2587%25E5%259B%25BE%25E6%25A8%25A1%25E5%259E%258B-%25E6%259C%25B4%25E7%25B4%25A0%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF-%25E9%259A%2590%25E9%25A9%25AC%25E5%25B0%2594%25E7%25A7%2591%25E5%25A4%25AB-%25E6%259D%25A1%25E4%25BB%25B6%25E9%259A%258F%25E6%259C%25BA%25E5%259C%25BA-%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%2592%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 概率图模型 - 朴素贝叶斯 - 隐马尔科夫 - 条件随机场 - 逻辑回归 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%20-%20%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%20-%20%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%20-%20%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba%20-%20%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&u=https%3a%2f%2fcongchan.github.io%2fposts%2f%25E6%25A6%2582%25E7%258E%2587%25E5%259B%25BE%25E6%25A8%25A1%25E5%259E%258B-%25E6%259C%25B4%25E7%25B4%25A0%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF-%25E9%259A%2590%25E9%25A9%25AC%25E5%25B0%2594%25E7%25A7%2591%25E5%25A4%25AB-%25E6%259D%25A1%25E4%25BB%25B6%25E9%259A%258F%25E6%259C%25BA%25E5%259C%25BA-%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%2592%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>