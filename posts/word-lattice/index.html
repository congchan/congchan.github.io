<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Word Lattice | Cong's Log</title><meta name=keywords content="NLP"><meta name=description content="What is Word Lattices?

A word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)


语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的k-best条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。
在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。
所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题："><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/word-lattice/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/word-lattice/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/word-lattice/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Word Lattice"><meta property="og:description" content="What is Word Lattices?
A word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)
语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的k-best条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。
在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。
所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题："><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-01-24T00:00:00+00:00"><meta property="article:modified_time" content="2019-01-24T00:00:00+00:00"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="Word Lattice"><meta name=twitter:description content="What is Word Lattices?

A word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)


语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的k-best条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。
在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。
所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Word Lattice","item":"https://congchan.github.io/posts/word-lattice/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Word Lattice","name":"Word Lattice","description":"What is Word Lattices?\nA word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)\n语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的k-best条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。\n在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。\n所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题：\n","keywords":["NLP"],"articleBody":"What is Word Lattices?\nA word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)\n语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的k-best条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。\n在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。\n所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题：\n未登录词，易引起切分错误 粒度不可控 节点权重如何设定, 比如每夜总会加班涉及每夜和夜总会 因此我们需要把词典匹配方法和神经网络NER模型结合使用. 需要结合CRF模型的实体(term-标签)和基于领域字典匹配的Term(可以加上pos标签)，求解文本的分词+NER划分的最优解。最优解的评判标准就是概率模型, 如果把一句话当作各个term序列, 那么文本序列标签最优解就是序列的最大联合概率 $$ \\begin{aligned} \\arg \\max \\prod_i P(w_i) \u0026 = \\arg \\max \\log \\prod_i P(w_i)\\\\\\ \u0026 = \\arg \\max \\sum_i \\log P(w_i) \\end{aligned} $$ 这里$w_i$指文本的每个term.\n因为一句话中, NER模型和词典匹配的结果可能粒度不同或者互相交叉, 所以我们需要在所有组合中找出一个联合概率最大的组合.\n利用分词工具的词典匹配功能 词典匹配的方法很多, 比如使用Trie树匹配. 这里为了简便直接使用Jieba分词来模拟词典匹配, 因为其底层实现也是一种词典匹配. 为了检索词典中的词，jieba一开始采取的思路是构建前缀Trie树以缩短查询时间。Jieba用了两个dict，trie dict用于保存trie树，lfreq dict用于存储词 -\u003e 词频. 后来Pull request 187提出把前缀信息也放到lfreq, 解决纯Python中Trie空间效率低下的问题. 引用部分说明如下:\n对于get_DAG()函数来说，用Trie数据结构，特别是在Python环境，内存使用量过大。经实验，可构造一个前缀集合解决问题。\n该集合储存词语及其前缀，如set(['数', '数据', '数据结', '数据结构'])。在句子中按字正向查找词语，在前缀列表中就继续查找，直到不在前缀列表中或超出句子范围。大约比原词库增加40%词条。\n建模 一个句子所有的分词和实体组合构成了有向无环图（Directed Acyclic Graph, DAG）$G=(V,E)$，一个词对应与DAG中的的一条边$e \\in E$，边的起点为词的初始字符，边的结点为词的结束字符。DAG可以用dict表示，key为边的起点，value为边的终点集合, 比如sentence = '微软银行收购tiktok'的DAG就是\n{0: [(0, '微'), (1, '微软'), (3, '微软银行')], 1: [(1, '软'), (2, '软银')], 2: [(2, '银'), (3, '银行')], 3: [(3, '行')], 4: [(4, '收'), (5, '收购')], 5: [(5, '购')], 6: [(6, 't'), (8, 'tik'), (11, 'tiktok')], ...} 4 -\u003e 5表示词'收购'。 这里面有的是词, 有的是实体如'微软'等. 我们可以给它们赋予不同的权重分值, 以强化我们的输出偏好. 一般以工程统计的频率作为权重.\n将词频的log值作为图$G$边的权值，将联合概率求解从连乘变为求和, 最大概率问题转换为最大分值路径问题；在上面的DAG中，节点0表示源节点，节点m-1表示尾节点；则$V=\\{0, \\cdots , m-1 \\}$，且DAG顶点的序号的顺序与图流向是一致的： $$v \u003e u, \\quad \\forall \\ (u,v) \\in E$$参考jieba.get_DAG()函数，我们修改一下DAG的格式，使其包含一些我们想要的信息，比如权重等等.\ndef get_DAG(self, sentence): self.check_initialized() DAG = {} N = len(sentence) for k in range(N): tmplist = [] i = k frag = sentence[k] while i \u003c N and frag in self.FREQ: if self.FREQ[frag]: tmplist.append((i, self.get_weight(frag), 'SEG')) i += 1 frag = sentence[k:i + 1] if not tmplist: tmplist.append((k, self.get_weight(frag), 'SEG')) DAG[k] = tmplist return DAG get_weight可以在线的提取每个分词对应的权重\ndef get_weight(self, segment): return log(self.FREQ.get(segment) or 1) - self.logtotal 这里主要是涉及到热更新词和词频的考虑, 所以每一次都从头计算一遍, 但是在分布式中这些都可以通过工程设计规避掉.\n然后把NER的结果也按照类似Ditc{offest: [(end_id, weight, tag), ...]}格式加入到DAG中，就可以统一求解了。\n求解概率图 图的最大路径问题其实是最短路径的对称问题. 在图论中针对DAG的最短路径求解的经典算法是按照图节点的Topological顺序, 更新从每一个节点出发的所有边. 而计算Topological顺序需要DFS遍历所有节点和边.\n不过这里使用比较容易实现的动态规划方法, 直接计算最大分值路径. 如果用$d_i$表示源节点到节点$i$的最大路径的值，则有 $$d_i = \\max_{(j,i) \\in E} \\ \\{ d_j+w(j,i) \\}$$ 其中，$w(j,i)$表示词$c_j^i$的词频log值，若$j = i$就表示字符独立成词的词频log值。\n考虑到DAG是以Start -\u003e [end0, end1, ...]的形式表达, 在定义状态时, 用$r_i$标记节点$i$到尾节点的最大路径的值, 这样可以从句子尾部往前计算, 保证考虑的每一个边不会往前越界. $$r_i = \\max_{(i,j) \\in E} \\ \\{ r_j+w(i,j) \\}$$根据上面设定的DAG的格式Dict{offest: [(end_id, weight, tag), ...]}, 则可以这样计算:\ndef calc(self, sentence, DAG, route={}): N = len(sentence) route[N] = (0, 0) for idx in xrange(N - 1, -1, -1): route[idx] = max((x[1] + route[x[0] + 1][0], x[0], x[2]) for x in DAG[idx]) return route 返回最大路径\ndef get_seg(self, sentence, route): N = len(sentence) x = 0 while x \u003c N: y = route[x][1] + 1 word = sentence[x:y] yield (word, route[x][2]) x = y return segs 参考资料 https://github.com/fxsjy/jieba/ https://www.cnblogs.com/en-heng/p/6234006.html ","wordCount":"374","inLanguage":"en","datePublished":"2019-01-24T00:00:00Z","dateModified":"2019-01-24T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/word-lattice/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Word Lattice</h1><div class=post-meta><span title='2019-01-24 00:00:00 +0000 UTC'>2019-01-24</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/NLP-word-lattice.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%88%a9%e7%94%a8%e5%88%86%e8%af%8d%e5%b7%a5%e5%85%b7%e7%9a%84%e8%af%8d%e5%85%b8%e5%8c%b9%e9%85%8d%e5%8a%9f%e8%83%bd aria-label=利用分词工具的词典匹配功能>利用分词工具的词典匹配功能</a></li><li><a href=#%e5%bb%ba%e6%a8%a1 aria-label=建模>建模</a></li><li><a href=#%e6%b1%82%e8%a7%a3%e6%a6%82%e7%8e%87%e5%9b%be aria-label=求解概率图>求解概率图</a></li><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label=参考资料>参考资料</a></li></ul></div></details></div><div class=post-content><p>What is <a href="http://www.statmt.org/moses/?n=Moses.WordLattices#:~:text=A%20word%20lattice%20is%20a%20directed%20acyclic%20graph,and%20edges%20labeled%20with%20a%20word%20and%20weight.">Word Lattices</a>?</p><blockquote><p>A word lattice is a directed acyclic graph with a single start point and edges labeled with a word and weight. Unlike confusion networks which additionally impose the requirement that every path must pass through every node, word lattices can represent any finite set of strings (although this generality makes word lattices slightly less space-efficient than confusion networks)</p></blockquote><p><img loading=lazy src=/images/lattice.png></p><p>语音识别结果的最优路径不一定与实际字序列匹配，所以人们一般希望能够得到得分最靠前的<code>k-best</code>条候选路径。为了紧凑地保存候选路径，防止占用过多内存空间，可以采用词格（Word Lattice）来保存识别的候选序列。</p><p>在序列标注任务中，一般的编码器+CRF的分词模型，因为实体标签的定义不同，词汇不同，语料不同等等原因，普遍无法适应垂直领域的问题。如果要适配，需要走一遍数据准备和模型训练验证的流程。</p><p>所以实践中一般都需要词典来匹配。词典匹配方法直接针对文本进行匹配从而获得成分识别候选集合，再基于词频（基于各种工程经验统计获得）筛选输出最终结果。这种策略比较简陋，对词库准确度和覆盖度要求极高，所以存在以下几个问题：</p><ol><li>未登录词，易引起切分错误</li><li>粒度不可控</li><li>节点权重如何设定, 比如<code>每夜总会加班</code>涉及<code>每夜</code>和<code>夜总会</code></li></ol><p>因此我们需要把词典匹配方法和神经网络NER模型结合使用. 需要结合CRF模型的实体(term-标签)和基于领域字典匹配的Term(可以加上pos标签)，求解文本的分词+NER划分的最优解。最优解的评判标准就是概率模型, 如果把一句话当作各个term序列, 那么文本序列标签最优解就是序列的最大联合概率</p>$$
\begin{aligned}
\arg \max \prod_i P(w_i) & = \arg \max \log \prod_i P(w_i)\\\
& = \arg \max \sum_i \log P(w_i)
\end{aligned}
$$<p>这里$w_i$指文本的每个term.</p><p>因为一句话中, NER模型和词典匹配的结果可能粒度不同或者互相交叉, 所以我们需要在所有组合中找出一个联合概率最大的组合.</p><h3 id=利用分词工具的词典匹配功能>利用分词工具的词典匹配功能<a hidden class=anchor aria-hidden=true href=#利用分词工具的词典匹配功能>#</a></h3><p>词典匹配的方法很多, 比如使用Trie树匹配. 这里为了简便直接使用Jieba分词来模拟词典匹配, 因为其底层实现也是一种词典匹配. 为了检索词典中的词，jieba一开始采取的思路是构建前缀Trie树以缩短查询时间。Jieba用了两个<code>dict</code>，<code>trie dict</code>用于保存trie树，<code>lfreq dict</code>用于存储<code>词 -> 词频</code>. 后来<a href=https://github.com/fxsjy/jieba/pull/187>Pull request 187</a>提出把前缀信息也放到<code>lfreq</code>, 解决纯Python中Trie空间效率低下的问题. 引用部分说明如下:</p><blockquote><p>对于<code>get_DAG()</code>函数来说，用Trie数据结构，特别是在Python环境，内存使用量过大。经实验，可构造一个前缀集合解决问题。</p><p>该集合储存词语及其前缀，如<code>set(['数', '数据', '数据结', '数据结构'])</code>。在句子中按字正向查找词语，在前缀列表中就继续查找，直到不在前缀列表中或超出句子范围。大约比原词库增加40%词条。</p></blockquote><h3 id=建模>建模<a hidden class=anchor aria-hidden=true href=#建模>#</a></h3><p>一个句子所有的分词和实体组合构成了有向无环图（Directed Acyclic Graph, DAG）$G=(V,E)$，一个词对应与DAG中的的一条边$e \in E$，边的起点为词的初始字符，边的结点为词的结束字符。DAG可以用<code>dict</code>表示，<code>key</code>为边的起点，<code>value</code>为边的终点集合, 比如<code>sentence = '微软银行收购tiktok'</code>的DAG就是</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>{</span><span class=mi>0</span><span class=p>:</span> <span class=p>[(</span><span class=mi>0</span><span class=p>,</span> <span class=s1>&#39;微&#39;</span><span class=p>),</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;微软&#39;</span><span class=p>),</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=s1>&#39;微软银行&#39;</span><span class=p>)],</span>
</span></span><span class=line><span class=cl> <span class=mi>1</span><span class=p>:</span> <span class=p>[(</span><span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;软&#39;</span><span class=p>),</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=s1>&#39;软银&#39;</span><span class=p>)],</span>
</span></span><span class=line><span class=cl> <span class=mi>2</span><span class=p>:</span> <span class=p>[(</span><span class=mi>2</span><span class=p>,</span> <span class=s1>&#39;银&#39;</span><span class=p>),</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=s1>&#39;银行&#39;</span><span class=p>)],</span>
</span></span><span class=line><span class=cl> <span class=mi>3</span><span class=p>:</span> <span class=p>[(</span><span class=mi>3</span><span class=p>,</span> <span class=s1>&#39;行&#39;</span><span class=p>)],</span>
</span></span><span class=line><span class=cl> <span class=mi>4</span><span class=p>:</span> <span class=p>[(</span><span class=mi>4</span><span class=p>,</span> <span class=s1>&#39;收&#39;</span><span class=p>),</span> <span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=s1>&#39;收购&#39;</span><span class=p>)],</span>
</span></span><span class=line><span class=cl> <span class=mi>5</span><span class=p>:</span> <span class=p>[(</span><span class=mi>5</span><span class=p>,</span> <span class=s1>&#39;购&#39;</span><span class=p>)],</span>
</span></span><span class=line><span class=cl> <span class=mi>6</span><span class=p>:</span> <span class=p>[(</span><span class=mi>6</span><span class=p>,</span> <span class=s1>&#39;t&#39;</span><span class=p>),</span> <span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=s1>&#39;tik&#39;</span><span class=p>),</span> <span class=p>(</span><span class=mi>11</span><span class=p>,</span> <span class=s1>&#39;tiktok&#39;</span><span class=p>)],</span>
</span></span><span class=line><span class=cl><span class=o>...</span><span class=p>}</span>
</span></span></code></pre></div><p><code>4 -> 5</code>表示词<code>'收购'</code>。
<img loading=lazy src=/images/segDAG.png></p><p>这里面有的是词, 有的是实体如<code>'微软'</code>等. 我们可以给它们赋予不同的权重分值, 以强化我们的输出偏好. 一般以工程统计的频率作为权重.</p><p>将词频的<code>log</code>值作为图$G$边的权值，将联合概率求解从连乘变为求和, 最大概率问题转换为最大分值路径问题；在上面的DAG中，节点<code>0</code>表示源节点，节点<code>m-1</code>表示尾节点；则$V=\{0, \cdots , m-1 \}$，且DAG顶点的序号的顺序与图流向是一致的：</p>$$v > u, \quad \forall \ (u,v) \in E$$<p>参考<code>jieba.get_DAG()</code>函数，我们修改一下DAG的格式，使其包含一些我们想要的信息，比如权重等等.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_DAG</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>sentence</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>check_initialized</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>DAG</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>tmplist</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>i</span> <span class=o>=</span> <span class=n>k</span>
</span></span><span class=line><span class=cl>        <span class=n>frag</span> <span class=o>=</span> <span class=n>sentence</span><span class=p>[</span><span class=n>k</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>while</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span> <span class=ow>and</span> <span class=n>frag</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>FREQ</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>FREQ</span><span class=p>[</span><span class=n>frag</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=n>tmplist</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>i</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_weight</span><span class=p>(</span><span class=n>frag</span><span class=p>),</span> <span class=s1>&#39;SEG&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>i</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=n>frag</span> <span class=o>=</span> <span class=n>sentence</span><span class=p>[</span><span class=n>k</span><span class=p>:</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=n>tmplist</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>tmplist</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>k</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_weight</span><span class=p>(</span><span class=n>frag</span><span class=p>),</span> <span class=s1>&#39;SEG&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>DAG</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>tmplist</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>DAG</span>
</span></span></code></pre></div><p><code>get_weight</code>可以在线的提取每个分词对应的权重</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_weight</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>segment</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>log</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>FREQ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>segment</span><span class=p>)</span> <span class=ow>or</span> <span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>logtotal</span>
</span></span></code></pre></div><p>这里主要是涉及到热更新词和词频的考虑, 所以每一次都从头计算一遍, 但是在分布式中这些都可以通过工程设计规避掉.</p><p>然后把NER的结果也按照类似<code>Ditc{offest: [(end_id, weight, tag), ...]}</code>格式加入到DAG中，就可以统一求解了。</p><h3 id=求解概率图>求解概率图<a hidden class=anchor aria-hidden=true href=#求解概率图>#</a></h3><p>图的最大路径问题其实是最短路径的对称问题. 在图论中针对DAG的最短路径求解的经典算法是按照图节点的Topological顺序, 更新从每一个节点出发的所有边. 而计算Topological顺序需要DFS遍历所有节点和边.</p><p>不过这里使用比较容易实现的动态规划方法, 直接计算最大分值路径. 如果用$d_i$表示源节点到节点$i$的最大路径的值，则有</p>$$d_i = \max_{(j,i) \in E} \ \{ d_j+w(j,i) \}$$<p>其中，$w(j,i)$表示词$c_j^i$的词频log值，若$j = i$就表示字符独立成词的词频log值。</p><p>考虑到DAG是以<code>Start -> [end0, end1, ...]</code>的形式表达, 在定义状态时, 用$r_i$标记节点$i$到尾节点的最大路径的值, 这样可以从句子尾部往前计算, 保证考虑的每一个边不会往前越界.</p>$$r_i = \max_{(i,j) \in E} \ \{ r_j+w(i,j) \}$$<p>根据上面设定的DAG的格式<code>Dict{offest: [(end_id, weight, tag), ...]}</code>, 则可以这样计算:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>calc</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>sentence</span><span class=p>,</span> <span class=n>DAG</span><span class=p>,</span> <span class=n>route</span><span class=o>=</span><span class=p>{}):</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>route</span><span class=p>[</span><span class=n>N</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>xrange</span><span class=p>(</span><span class=n>N</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>route</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=nb>max</span><span class=p>((</span><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>route</span><span class=p>[</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>2</span><span class=p>])</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>DAG</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>route</span>
</span></span></code></pre></div><p>返回最大路径</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_seg</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>sentence</span><span class=p>,</span> <span class=n>route</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=n>x</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=n>route</span><span class=p>[</span><span class=n>x</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=n>word</span> <span class=o>=</span> <span class=n>sentence</span><span class=p>[</span><span class=n>x</span><span class=p>:</span><span class=n>y</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>yield</span> <span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>route</span><span class=p>[</span><span class=n>x</span><span class=p>][</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>segs</span>
</span></span></code></pre></div><h3 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h3><ul><li><a href=https://github.com/fxsjy/jieba/>https://github.com/fxsjy/jieba/</a></li><li><a href=https://www.cnblogs.com/en-heng/p/6234006.html>https://www.cnblogs.com/en-heng/p/6234006.html</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/bert%E7%9A%84adam-weight-decay/><span class=title>« Prev</span><br><span>BERT的Adam Weight Decay</span>
</a><a class=next href=https://congchan.github.io/posts/%E5%88%A9%E7%94%A8bert%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/><span class=title>Next »</span><br><span>利用bert进行迁移学习</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Word Lattice on x" href="https://x.com/intent/tweet/?text=Word%20Lattice&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fword-lattice%2f&amp;hashtags=NLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Word Lattice on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fword-lattice%2f&amp;title=Word%20Lattice&amp;summary=Word%20Lattice&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fword-lattice%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Word Lattice on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fword-lattice%2f&title=Word%20Lattice"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Word Lattice on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fword-lattice%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Word Lattice on whatsapp" href="https://api.whatsapp.com/send?text=Word%20Lattice%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fword-lattice%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Word Lattice on telegram" href="https://telegram.me/share/url?text=Word%20Lattice&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fword-lattice%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Word Lattice on ycombinator" href="https://news.ycombinator.com/submitlink?t=Word%20Lattice&u=https%3a%2f%2fcongchan.github.io%2fposts%2fword-lattice%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>