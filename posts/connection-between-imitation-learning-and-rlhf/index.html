<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Connection Between Imitation Learning and RLHF | Cong's Log</title><meta name=keywords content="LLM,RLHF"><meta name=description content="There have been many questions about whether DPO is a form of imitation learning or (offline) reinforcement learning. The more I observe the distributions of DPO&rsquo;s chosen and rejection losses, the stronger the feeling becomes that DPO is more like a form of imitation learning.
The paper, Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.050791 also expresses that DPO is a form of imitation learning."><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/connection-between-imitation-learning-and-rlhf/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/connection-between-imitation-learning-and-rlhf/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/connection-between-imitation-learning-and-rlhf/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Connection Between Imitation Learning and RLHF"><meta property="og:description" content="There have been many questions about whether DPO is a form of imitation learning or (offline) reinforcement learning. The more I observe the distributions of DPO’s chosen and rejection losses, the stronger the feeling becomes that DPO is more like a form of imitation learning.
The paper, Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.050791 also expresses that DPO is a form of imitation learning."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-10T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-10T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RLHF"><meta name=twitter:card content="summary"><meta name=twitter:title content="Connection Between Imitation Learning and RLHF"><meta name=twitter:description content="There have been many questions about whether DPO is a form of imitation learning or (offline) reinforcement learning. The more I observe the distributions of DPO&rsquo;s chosen and rejection losses, the stronger the feeling becomes that DPO is more like a form of imitation learning.
The paper, Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.050791 also expresses that DPO is a form of imitation learning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Connection Between Imitation Learning and RLHF","item":"https://congchan.github.io/posts/connection-between-imitation-learning-and-rlhf/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Connection Between Imitation Learning and RLHF","name":"Connection Between Imitation Learning and RLHF","description":"There have been many questions about whether DPO is a form of imitation learning or (offline) reinforcement learning. The more I observe the distributions of DPO\u0026rsquo;s chosen and rejection losses, the stronger the feeling becomes that DPO is more like a form of imitation learning.\nThe paper, Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.050791 also expresses that DPO is a form of imitation learning.\n","keywords":["LLM","RLHF"],"articleBody":"There have been many questions about whether DPO is a form of imitation learning or (offline) reinforcement learning. The more I observe the distributions of DPO’s chosen and rejection losses, the stronger the feeling becomes that DPO is more like a form of imitation learning.\nThe paper, Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.050791 also expresses that DPO is a form of imitation learning.\nDirect Imitation Learning (DIL) This study explores the alignment problem between large language models and preference data from the perspective of imitation learning. Researchers establish a close theoretical connection between Reinforcement Learning from Human Feedback (RLHF) and Imitation Learning (IL), revealing that RLHF implicitly performs imitation learning on the preference data distribution. Based on this connection, Direct Imitation Learning (DIL) is proposed, a principled framework that directly optimizes the imitation learning objective. DIL provides a unified imitation learning perspective for the alignment problem, encompassing existing alignment algorithms as special cases while naturally introducing new variants. By linking IL and RLHF, DIL offers new insights into alignment with RLHF. Extensive experiments show that DIL outperforms existing methods on various challenging benchmarks.\nBackground Aligning Large Language Models (LLMs) with human preferences is crucial to ensuring that the responses generated by LLMs meet human expectations.\nThe Proposal of RLHF and Its Issues In recent years, Reinforcement Learning from Human Feedback (RLHF) has emerged as a widely adopted framework for fine-tuning language models based on human preference data.\nDPO Addressing RLHF’s Issues and Its Own Limitations RLHF relies on a two-step reinforcement learning process, which leads to problems such as low computational efficiency and instability during training. To alleviate these limitations, researchers have proposed alternative one-stage methods, such as Direct Preference Optimization (DPO) and its variants. These methods replace RLHF with supervised learning, eliminating the need for explicit reward modeling. Instead, they directly define an implicit reward based on the likelihood of preference data, significantly improving efficiency while maintaining competitive performance.\nProblems with DPO and Triggered Reflections Although DPO theoretically aims to find the same optimal policy as RLHF, it and its variants essentially still follow a reward maximization objective, determined by parametric models (e.g., the Bradley-Terry (BT) model). This makes them prone to overfitting, leading to suboptimal alignment with preference data. This raises a fundamental and open research question: Can we understand and design effective preference optimization algorithms from a new perspective?\nSignificance of This Study This paper re-examines RLHF from the perspective of imitation learning. Specifically, researchers show that RLHF is a special case of a general imitation learning problem, expressed solely through pairwise preferences. They theoretically demonstrate that alignment with RLHF is highly similar to imitation learning and implicitly optimizes the same objective. Leveraging this insight, they design DIL, a general framework for effective alignment based on density ratio reward estimation.\nKey Contributions It is proven that RLHF for alignment is essentially an imitation learning problem, providing a novel analysis that offers clear guidance for the design of alignment algorithms. DIL, a simple and general imitation learning alignment framework, is proposed. DIL unifies imitation learning on preference data and bridges the gap between density ratio estimation and preference alignment. Empirically, the effectiveness of DIL is verified on widely used benchmarks, demonstrating its superiority over previous alignment methods. Theoretical Derivations Preliminary Knowledge Problem Setup\nLet \\( x = [x_1, x_2, \\ldots] \\) be the input prompt, \\( y_w = [y_1, y_2, \\ldots] \\) be the positive sample (preferred response), and \\( y_l \\) be the negative sample (non-preferred response). These two samples are typically drawn from the same reference policy \\( \\pi_{\\text{ref}}(y|x) \\). Meanwhile, \\( y_w \\succ y_l | x \\) indicates that for the same input \\( x \\), \\( y_w \\) is more in line with human preferences than \\( y_l \\). Thus, the preference distribution is generally expressed as:\n\\[ p(y_w \\succ y_l | x) = g(r(x, y_w) - r(x, y_l)) \\tag{1} \\]\nHere, \\( g \\) refers to the sigmoid function \\( \\sigma(x) = \\frac{1}{1+e^{-x}} \\), a conclusion derived from the Bradley-Terry model (which can be verified by dividing the numerator and denominator by \\( r(x, y_l) \\) to convert it into a sigmoid form). Given a preference dataset \\( \\mathcal{D} \\) containing feedback, where each data entry is formatted as \\( (x, y_w, y_l) \\), our alignment goal is to learn an LLM policy \\( \\pi(y|x) \\) based on the preference data.\nReinforcement Learning from Human Feedback (RLHF)\nGiven an estimated reward function \\( r(x, y) \\), RLHF fine-tunes the policy \\( \\pi_\\theta \\) according to human preferences through the following optimization objective:\n\\[ \\mathop{\\max}\\limits_{\\pi_\\theta}\\mathbb{E}_{y \\sim \\pi_\\theta(y|x)}[r(x, y)] - \\beta\\mathbb{D}_{KL}[\\pi_\\theta(y|x)||\\pi_{\\text{ref}}(y|x)] \\tag{2} \\]\nThe core idea of this formula is to maximize the reward signal of human preferences while preventing the model from deviating too much from the original pre-trained distribution (to avoid collapse). Here, \\( \\mathbb{D}_{KL} = \\mathbb{E}_{y \\sim \\pi_{\\theta}} \\left[ \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right] \\) is the Kullback-Leibler divergence, describing the difference between the model’s strategy distribution and the original reference model’s. \\( \\beta \u003e 0 \\) is an appropriate KL penalty coefficient. Optimization methods typically use RL approaches such as Proximal Policy Optimization (PPO).\nReward Modeling\nStandard reward modeling uses the BT preference model in Equation (1) to fit a reward function \\( r_\\phi(x, y) \\). Specifically, the reward function can be estimated by maximizing the log-likelihood of preference feedback \\( (x, y_w, y_l) \\):\n\\[ \\mathcal{L}_{\\text{RM}}(\\phi;\\mathcal{D})=\\mathbb{E}_{(x, y_w,y_l)\\sim\\mathcal{D}}[-\\log\\sigma(r_{\\phi}(x,y_w)-r_{\\phi}(x,y_l))] \\tag{3} \\] Supervised Fine-Tuning (SFT)\nGiven a demonstration dataset \\( \\mathcal{D} \\), the goal of SFT is to minimize the negative log-likelihood of the model on the demonstration dataset:\n\\[ \\mathcal{L}_{\\text{SFT}}(\\theta;\\mathcal{D})=-\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\log\\pi_{\\theta}(y|x)] \\tag{4} \\]\nSFT is equivalent to Behavior Cloning (BC), a classic offline imitation learning method. Its goal is to minimize the forward KL divergence between the learned policy \\( \\pi_{\\theta} \\) and the data policy \\( \\pi_{\\text{data}} \\):\n\\[ \\mathop{\\min}\\limits_{\\theta}KL(\\pi_{\\text{data}}(y|x)||\\pi_\\theta(y|x))=-\\mathbb{E}_{\\pi_{\\text{data}}(y|x)}[\\log\\pi_{\\theta}(y|x)] \\tag{5} \\]\nIt is evident that SFT and BC have the same optimal solution.\nDirect Preference Optimization (DPO)\nTo simplify RLHF’s optimization process, DPO uses the log-likelihood of the learned policy to implicitly represent the reward function:\n\\[ r_\\theta(x,y)=\\beta[\\log\\pi_\\theta(y|x)-\\log\\pi_{\\text{ref}}(y|x)] + \\beta\\log Z_\\theta(x) \\tag{6} \\]\nHere, \\( Z_\\theta(x)=\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y)/\\beta) \\) is the partition function.\nConcept Supplement: Partition Function\nThe partition function ensures that the probability distribution is normalized, i.e., the sum of probabilities of all possible states equals 1. Specifically, in a probabilistic model, given an input \\( x \\), the probability of output \\( y \\) can be expressed as:\n\\[ p(y|x) = \\frac{1}{Z(x)} \\exp(-E(y, x)) \\]\nwhere \\( E(y, x) \\) is the energy function, measuring the “mismatch” or “cost” of a specific output \\( y \\) for a given input \\( x \\). The partition function \\( Z(x) \\) is defined as the sum of energy exponents over all possible outputs \\( y \\):\n\\[ Z(x) = \\sum_y \\exp(-E(y, x)) \\]\nFor continuous variables, the sum is replaced by an integral:\n\\[ Z(x) = \\int \\exp(-E(y, x)) dy \\]\nDoes this principle resemble the softmax function? Indeed, softmax is an application of the partition function.\nReturning to the reward function, rearranging terms gives:\n\\[ r_\\theta(x,y)+\\beta\\log\\pi_{\\text{ref}}(y|x)-\\beta\\log Z_\\theta(x)=\\beta\\log\\pi_\\theta(y|x) \\]\nApplying the natural exponential function to both sides of the equation:\n\\[ \\frac{\\exp(r_\\theta(x,y))e^\\beta\\pi_{\\text{ref}}(y|x)}{e^\\beta Z_\\theta(x)}=e^\\beta\\pi_\\theta(y|x) \\]\nSince \\( Z_\\theta(x)=\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y)/\\beta)=e^{-\\beta}\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y)) \\), substituting into the equation and eliminating common factors on both sides yields:\n\\[ \\pi_\\theta(y|x)=\\frac{\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y))}{\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\theta(x,y))} \\]\nIntuitively, the higher the reward corresponding to parameter \\( \\theta \\), the closer the model policy \\( \\pi_\\theta(y|x) \\) is to 1 (i.e., the higher the probability of adopting that policy), while the model policy \\( \\pi_\\theta(y|x) \\) is normalized to a distribution where the sum of probabilities equals 1—this is the essence of the reward function design.\nBy incorporating this reward into the BT model in Equation (1) and simplifying, DPO’s objective promotes the comparison and differentiation of preferred and non-preferred data:\n\\[ \\mathcal{L}_{\\text{DPO}}(\\theta;\\mathcal{D})=\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\left[-\\log\\sigma\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right] \\tag{7} \\] Energy-based Models (EBMs)\nEBMs define distributions through energy functions. For \\( y\\in \\mathbb{R}^D \\), the probability density can be expressed as:\n\\[ p_\\theta(y)=\\exp(-E_\\theta(y)/Z_\\theta(y)) \\tag{8} \\]\nwhere \\( E_\\theta(y):\\mathbb{R}^D\\rightarrow\\mathbb{R} \\) is the energy function, mapping \\( y \\) to a scalar, and \\( Z_\\theta(y) = \\sum_y \\exp(-E_\\theta(y)) \\) is the unknown normalization constant, as mentioned in the supplementary knowledge about the partition function.\nCore Derivations The derivations are mainly refereced from this blog2\n1 RLHF as a Form of Imitation Learning Researchers demonstrate that RLHF is a special case of imitation learning based on reverse KL divergence over the distribution of selected responses.\nThe specific proof is as follows:\nFirst, define the following policy based on energy-based models (EBMs):\n\\[ \\pi_\\phi(y|x)=\\pi_{\\text{ref}}(y|x)\\exp(r_\\phi(x,y))/Z_\\phi(x) \\tag{9} \\]\nwhere \\( \\phi \\) denotes model parameters, and as described in the preliminary knowledge, \\( Z_\\phi(x)=\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\phi(x,y)) \\).\nTo learn parameter \\( \\phi \\), Behavior Cloning (BC)—a classic and widely used imitation learning method (as mentioned in the preliminary knowledge, SFT is equivalent to BC)—can be applied. This method formulates the task as minimizing the KL divergence between the policy \\( \\pi_\\phi \\) and the expert policy \\( \\pi_{\\text{chosen}} \\) that generates preferred responses \\( y_w \\). In other words, IL learns parameter \\( \\phi \\) such that the model distribution imitates the distribution of preferred responses in the preference dataset:\n\\[ \\mathop{\\min}\\limits_{\\phi}KL(\\pi_{\\text{chosen}}(y|x)||\\pi_\\phi(y|x)) \\tag{10} \\]By selecting responses from the preference data to minimize the above forward KL divergence:\n\\[ \\mathop{\\min}\\limits_{\\phi}\\mathbb{E}_{(x,y_w)\\sim\\mathcal{D}}[-\\log\\pi_{\\text{ref}}(y_w|x)\\exp(r_\\phi(x,y_w))/Z_\\phi(x)] \\Rightarrow \\\\\\mathop{\\min}\\limits_{\\phi}\\mathbb{E}_{(x,y_w)\\sim\\mathcal{D}}\\left[-r_\\phi(x,y_w)+\\log\\sum_y\\pi_{\\text{ref}}(y|x)\\exp(r_\\phi(x,y))\\right] \\tag{11} \\]\n(Equation (11) is obtained by removing constants and substituting the partition function.)\nThere are multiple choices for sampling from the reference distribution \\( \\pi_{\\text{ref}}(y|x) \\). One setting that simplifies the above expression and practically yields RLHF is: \\( \\pi_{\\text{ref}}(y|x) = \\frac{1}{2} \\mathbb{I}(Y = y_l) + \\frac{1}{2} \\mathbb{I}(Y = y_w) \\) (note: this is a key approximation that allows RLHF to be reduced to IL). In this case, the sample-based approximation of the second term is:\n\\[ \\mathop{\\min}\\limits_{\\phi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\left[-r_\\phi(x,y_w)+\\log(\\exp(r_\\phi(x,y_w)) + \\exp(r_\\phi(x,y_l)))\\right] \\]This equation is derived by substituting \\( \\sum_y \\pi_{\\text{ref}}(y|x)\\exp(r_\\varphi(x, y)) = \\frac{1}{2} \\exp(r_\\varphi(x, y_w)) + \\frac{1}{2} \\exp(r_\\varphi(x, y_l)) \\) into Equation (11) and removing the constant term \\( -\\log2 \\). Further merging the \\( -r_\\phi(x,y_w) \\) term into the right-hand side \\( \\log(\\exp(r_\\phi(x,y_w)) + \\exp(r_\\phi(x,y_l))) \\) and simplifying gives the equivalent form:\n\\[ \\mathop{\\min}\\limits_{\\phi}\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\left[-\\log\\sigma(r_\\phi(x,y_w) - r_\\phi(x,y_l))\\right] \\tag{12} \\]It can be noted that the imitation learning loss based on the energy strategy is identical to the reward loss of RLHF based on the BT assumption (Equation (3)). By optimizing this loss function, we can directly obtain the optimal energy strategy in Equation (9). Unfortunately, even if we use the estimated reward function \\( r_\\phi \\), estimating the partition function \\( Z_\\phi(x) \\) remains costly, making the derived representation impractical and leading to significantly increased inference costs.\nKey Discussion: Why This Method Is Costly\nExcessively Large Output Space\nThe output of a language model is a sequence of tokens; for example, a response may contain dozens or even hundreds of tokens. Each token has thousands to tens of thousands of candidate words (depending on the vocabulary size), resulting in an exponentially growing number of total output combinations:\nAssuming 50,000 word choices per step and generating 20 tokens, there are \\( 50000^{20} \\) possible responses!\nThis means it is impossible to enumerate all \\( y \\) to compute \\( Z_\\phi(x) \\).\nRequiring Extensive Sampling to Approximate Summation\nSince enumeration is impossible, we can only estimate the summation through sampling:\n\\[ Z_\\phi(x) \\approx \\frac{1}{N} \\sum_{i=1}^N \\exp(r_\\phi(x, y_i)), \\quad y_i \\sim \\pi_{\\text{ref}}(y|x) \\]\nHowever, to ensure estimation accuracy, it is necessary to:\nSample enough \\( y_i \\) Run the reward model \\( r_\\phi(x, y_i) \\) for each \\( y_i \\)\nThis leads to: High computational resource consumption (running the reward model many times per inference) Increased inference time Reward Models May Be “Heavy”\nThe reward model \\( r_\\phi(x, y) \\) is typically a large neural network (e.g., based on the GPT architecture), which is already time-consuming to run once. Running it multiple times on multiple samples incurs significant costs.\nTo address the issue of high computational costs, researchers propose a method. Before introducing this method, we first introduce forward and reverse KL divergences (those already familiar can skip this part):\nConcept Supplement: Forward and Reverse KL Divergences\nBasic Concept of KL Divergence\nKL divergence (Kullback-Leibler Divergence), also known as relative entropy, measures the difference between two probability distributions \\( P \\) and \\( Q \\). For discrete distributions, KL divergence is defined as:\n\\[ D_{\\text{KL}}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\]\nFor continuous distributions, the sum is replaced by an integral:\n\\[ D_{\\text{KL}}(P \\| Q) = \\int P(x) \\log \\frac{P(x)}{Q(x)}dx \\] Forward KL Divergence (\\( D_{\\text{KL}}(P \\| Q) \\))\nGoal: Make \\( Q \\) as close as possible to \\( P \\). Characteristics: Heavily penalizes events with high probability in \\( P \\) but low or zero probability in \\( Q \\). In other words, it focuses on accurately capturing all modes in \\( P \\). Application Scenarios: When there is a true distribution \\( P \\) (e.g., the true distribution of data) and a model \\( Q \\) needs to be trained to approximate this distribution, forward KL divergence is typically used. This is because forward KL encourages the model to cover all modes, even if it means generating some low-probability but existing samples. Properties: If \\( P \\) has a point with probability greater than 0 where \\( Q \\) has zero probability, the KL divergence tends to infinity. Thus, using forward KL divergence, the model tends to cover more possibilities, even unlikely events. Reverse KL Divergence (\\( D_{\\text{KL}}(Q \\| P) \\))\nGoal: Make \\( P \\) as close as possible to \\( Q \\). Characteristics: More focused on avoiding generating events with very low probability in \\( P \\) but high probability in \\( Q \\). That is, it tends to find the most likely modes and ignore less likely cases. Application Scenarios: In some cases, such as Generative Adversarial Networks (GANs) or when a more deterministic model output is desired, reverse KL divergence may be a better choice. Because it encourages the model to focus on the most likely outcomes rather than trying to cover all possibilities. Properties: Unlike forward KL, reverse KL allows \\( Q \\) to be zero in some regions as long as \\( P \\) is also zero or very small there. This means reverse KL can produce sparser and more concentrated distributions, sometimes leading to “mode collapse”—generating only a few specific types of samples while ignoring others. Using reverse knowledge distillation (which employs reverse KL divergence, with characteristics consistent with those of reverse KL divergence in the model distillation process), the optimal strategy in Equation (9) is “distilled” into a strategy with an analytical form using reverse KL divergence, enabling the final strategy \\( \\pi_\\theta \\) to require only one sampling during inference (note: here \\( \\theta \\) is the main model parameter, and \\( \\phi \\) is the reward model parameter):\n\\[ \\mathop{\\min}\\limits_{\\theta}KL\\left(\\pi_\\theta(y|x)||\\pi_{\\text{ref}}(y|x)\\exp(r_\\phi(x,y)/\\beta)/Z_\\phi(x)\\right) \\tag{13} \\]\nwhere \\( \\beta \\) is the temperature hyperparameter in the distillation process. After isolating the term \\( -\\mathbb{E}_{\\pi_\\theta(y|x)}[r_\\phi(x,y)] \\), removing multiplicative and additive constants, and combining the remaining terms into \\( \\beta KL(\\pi_\\theta(y|x)||\\pi_{\\text{ref}}(y|x)) \\), it is transformed into the following objective function:\n\\[ \\mathcal{L}(\\theta)=-\\mathbb{E}_{\\pi_\\theta(y|x)}[r_\\phi(x,y)] + \\beta KL(\\pi_\\theta(y|x)||\\pi_{\\text{ref}}(y|x)) \\tag{14} \\]It can be observed that this distillation objective corresponds exactly to the objective of RLHF in Equation (2). Thus, researchers provide two key conclusions:\n(i) Reward learning in RLHF is equivalent to an imitation learning problem for preferred responses, achieved by minimizing the forward KL divergence between \\( \\pi_{\\text{chosen}} \\) and \\( \\pi_\\phi \\) based on energy-based models (EBMs), as shown in Equation (12);\n(ii) The RL step in RLHF can be interpreted as a reverse knowledge distillation process, where the EBM-based imitation strategy \\( \\pi_\\phi \\) is distilled into the final analytical strategy \\( \\pi_\\theta \\) by minimizing the reverse KL divergence in Equation (13), with the temperature parameter \\( \\beta \\) determining the degree of KL regularization.\nThis problem is transformed into the following proposition:\nAssume the preferred response distribution \\( p(y|x) \\), the energy-based model \\( \\pi_\\phi(y|x) \\), and the model \\( \\pi_\\theta(y|x) \\). When \\( \\beta = 1 \\), KL-regularized RLHF can be regarded as the following problem:\n\\[ \\min_{\\pi_\\theta} \\mathrm{KL}(\\pi_\\theta \\| \\pi^*_\\phi) \\quad \\text{s.t.} \\quad \\pi^*_\\phi = \\arg\\min_{\\pi_\\phi} \\mathrm{KL}(\\pi_{\\text{chosen}} \\| \\pi_\\phi) \\tag{15} \\]\nwhere \\( \\pi_{\\text{chosen}}(y|x) = \\pi_\\phi(y|x) = \\pi_\\theta(y|x) \\) is the equilibrium state.\nThus, imitation learning on preferred responses is equivalent to solving a standard KL-regularized RLHF problem.\nFurthermore, we observe that when \\( \\pi^*_\\phi = \\pi_{\\text{chosen}} \\) (i.e., the optimal solution achieved by the lower-level objective), the upper-level objective essentially optimizes a reverse KL divergence \\( \\mathrm{KL}(\\pi_\\theta \\| \\pi_{\\text{chosen}}) \\).\nAt this point, I am truly amazed; this proof process is akin to Maxwell’s equations unifying electromagnetism in physics.\nTo this end, we have proven that RLHF is a special type of IL, with equivalent conditions:\nUsing EBM and forward KL divergence to fit the Reward Model (RM) Using EBM and reverse knowledge distillation based on reverse KL divergence to complete RL training The proof is complete. Researchers then pose an interesting question:\nWhy does SFT — which directly optimizes the forward KL divergence \\( \\mathrm{KL}(\\pi_{\\text{chosen}} \\| \\pi_\\theta) \\) in Equation (5) — perform worse than RLHF in alignment tasks?\nTheoretically, minimizing the objective functions of SFT and RLHF should lead to the same optimal solution \\( \\pi_\\theta \\). However, in practice, this requires complete data coverage and unlimited computational resources, conditions rarely met.\nThus, in practical settings, minimizing different KL divergences results in learned policies with distinct characteristics. Specifically, the forward KL divergence \\( \\mathrm{KL}(\\pi_{\\text{chosen}} \\| \\pi_\\theta) \\) promotes mass-covering behavior, while the reverse KL divergence \\( \\mathrm{KL}(\\pi_\\theta \\| \\pi_{\\text{chosen}}) \\) encourages mode-seeking behavior (see supplementary knowledge on forward and reverse KL divergences above).\nMass-covering behavior tends to assign similar probabilities to all responses in the dataset, overestimating the long-tail portion of the target distribution; mode-seeking behavior concentrates probability mass in specific high-reward regions. Therefore, the goal of alignment is to generate a certain type of high-quality response, which can be more effectively achieved by minimizing the reverse KL divergence.\nIn summary, the reason RLHF outperforms SFT is that the full performance of forward KL divergence requires complete data coverage and unlimited computational resources, which is nearly impossible to achieve. In contrast, reverse KL divergence, although unable to learn to cover the distribution, can still learn high-quality responses, resulting in better performance of reverse KL divergence over forward KL divergence.\n2 Direct Imitation Learning (DIL) In the previous section, we re-examined RLHF from the perspective of imitation learning. The analysis clearly indicates that RLHF is essentially optimized to closely align with the distribution of preferred responses. The sample-based approximation of EBM in RLHF leads to a reward loss similar to the BT model, as shown in Equation (12). However, the BT assumption does not always hold. Based on these insights, researchers propose a new alignment method, DIL (Direct Imitation Learning), that does not rely on the BT assumption. Thus, the objective of imitation learning is directly formulated as minimizing the reverse KL divergence between \\( \\pi_\\theta \\) and the unknown preferred response distribution \\( \\pi_{\\text{chosen}} \\):\n\\[ \\min_\\theta L_{\\text{DIL}}(\\theta) = \\mathrm{KL} \\left( \\pi_\\theta(y|x) \\| \\pi_{\\text{chosen}}(y|x) \\right) = \\mathbb{E}_{\\pi_\\theta(y|x)} \\left[ \\log \\left( \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{chosen}}(y|x)} \\right) \\right] \\tag{16} \\]\nHere, we minimize the reverse KL divergence, unlike SFT which minimizes the forward KL divergence as in Equation (5). However, using reverse KL divergence for mode concentration is typically challenging. Directly optimizing Equation (16) cannot effectively utilize preference data, especially since the data policy \\( \\pi_{\\text{chosen}} \\) is unknown. In reinforcement learning literature, these challenges are addressed through adversarial training. However, such methods require complex and unstable adversarial training to learn the reward function, which is impractical for large models. In this paper, a simple alternative is proposed to directly utilize offline human preference data without learning the reward function through adversarial training. The DIL objective is reformulated as follows:\n\\[ \\max_\\theta \\mathbb{E}_{\\pi_\\theta(y|x)} \\left[ \\log \\frac{\\pi_{\\text{chosen}}(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right]=\\\\ \\mathbb{E}_{\\pi_\\theta(y|x)} \\left[ \\log r(x, y) \\right] - \\mathrm{KL} \\left( \\pi_\\theta(y|x) \\parallel \\pi_{\\text{ref}}(y|x) \\right) \\tag{17} \\]\nwhere \\( r(x, y) \\triangleq \\frac{\\pi_{\\text{chosen}}(y|x)}{\\pi_{\\text{ref}}(y|x)} \\) can be regarded as an auxiliary reward function. Equations (16) and (17) are equivalent by adding and subtracting the same term \\( \\log \\pi_{\\text{ref}}(y|x) \\) in the expectation.\nInterestingly, researchers find that even when only preference data is available, the form of this objective function is similar to that of RLHF in Equation (2). The main difference is that the reward here is the estimated log density ratio, which is often difficult to obtain directly in practice. Optimizing this objective involving the density ratio \\( r(x, y) \\) is non-intuitive and challenging. The next section will show how to efficiently optimize this objective function by effectively utilizing offline human preference data.\n3 Density Ratio Reward Estimation Before delving into the problem in Equation (17), we first describe how to compute the auxiliary reward function based on the density ratio. In a tabular setting, we can directly compute \\( \\pi_{\\text{ref}}(y|x) \\) and \\( \\pi_{\\text{chosen}}(y|x) \\). However, in high-dimensional language domains, estimating densities separately and computing their ratios is ineffective due to error accumulation.\nBefore introducing the solution, we first understand Bregman divergence:\nConcept Supplement: Bregman Divergence\nBregman Divergence is a measure of the difference between two points, defined by the properties of convex functions. Specifically, given a strictly convex and twice differentiable function \\( F \\), Bregman divergence is defined as the difference between the function and its linear approximation at a certain point.\nDefinition\nSuppose \\( F \\) is a strictly convex function defined on a convex set. The Bregman divergence \\( D_F \\) generated by \\( F \\) can be defined as:\n\\[ D_F(\\mathbf{p} \\| \\mathbf{q}) = F(\\mathbf{p}) - F(\\mathbf{q}) - \\langle \\nabla F(\\mathbf{q}), (\\mathbf{p} - \\mathbf{q}) \\rangle \\]\nwhere:\n\\( \\mathbf{p} \\) and \\( \\mathbf{q} \\) are two points in the space; \\( \\nabla F(\\mathbf{q}) \\) denotes the gradient of \\( F \\) at point \\( \\mathbf{q} \\); \\( \\langle \\cdot, \\cdot \\rangle \\) denotes the inner product operation. Simply put, \\( D_F(\\mathbf{p} \\| \\mathbf{q}) \\) measures the gap between the value of function \\( F \\) at point \\( \\mathbf{p} \\) and the first-order Taylor expansion of \\( F \\) at point \\( \\mathbf{q} \\).\nProperties\nNon-negativity: For all \\( \\mathbf{p} \\) and \\( \\mathbf{q} \\), \\( D_F(\\mathbf{p} \\| \\mathbf{q}) \\geq 0 \\), with equality if and only if \\( \\mathbf{p} = \\mathbf{q} \\). Asymmetry: In general, \\( D_F(\\mathbf{p} \\| \\mathbf{q}) \\neq D_F(\\mathbf{q} \\| \\mathbf{p}) \\), meaning Bregman divergence is not a symmetric measure. Sandwich Inequality: Bregman divergence does not satisfy the triangle inequality, i.e., for any three points \\( \\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\), it is not necessarily true that \\( D_F(\\mathbf{x} \\| \\mathbf{y}) + D_F(\\mathbf{y} \\| \\mathbf{z}) \\geq D_F(\\mathbf{x} \\| \\mathbf{z}) \\). Common Examples\nDifferent convex functions \\( F \\) yield different types of Bregman divergences. Here are some common examples:\nSquared Euclidean distance: If \\( F(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 \\), the corresponding Bregman divergence is the square of the Euclidean distance. Kullback-Leibler divergence: If \\( F(\\mathbf{x}) = \\sum_i x_i \\log x_i \\), the corresponding Bregman divergence is the KL divergence. Itakura-Saito distance: If \\( F(\\mathbf{x}) = -\\sum_i \\log x_i \\), the corresponding Bregman divergence is the Itakura-Saito distance. The solution proposed in this paper is to directly estimate the density ratio \\( \\pi_{\\text{chosen}}(y|x)/\\pi_{\\text{ref}}(y|x) \\) based on Bregman divergence. Assume the target density ratio is \\( r^*(x, y) = \\pi_{\\text{chosen}}(y|x)/\\pi_{\\text{ref}}(y|x) \\), and a parameterized discriminator \\( r_\\phi \\) is used to estimate this ratio:\n\\[ \\min_\\phi D_h(r^* \\| r_\\phi) =\\\\\\sum_y \\pi_{\\text{ref}}(y|x) B_h\\left(r^*(x, y) \\| r_\\phi(x, y)\\right) =\\\\\\sum_y \\pi_{\\text{ref}}(y|x) \\left[ h\\left(r^*(x, y)\\right) - h\\left(r_\\phi(x, y)\\right) - \\partial h\\left(r_\\phi(x, y)\\right) \\left(r^*(x, y) - r_\\phi(x, y)\\right) \\right] \\tag{18} \\]where \\( B_h \\) is the sample-level Bregman divergence.\nFor a twice continuously differentiable convex function \\( h \\) with a bounded derivative \\( \\partial h \\), this divergence measures the difference between two density ratios. By subtracting the constant term \\( \\sum_y \\pi_{\\text{ref}}(y|x) h(r^*(x, y)) \\) and substituting \\( r^*(x, y) = \\pi_{\\text{chosen}}(y|x)/\\pi_{\\text{ref}}(y|x) \\), we obtain (ignoring the constant term):\n\\[ \\sum_y \\pi_{\\text{ref}}(y|x) \\left[ \\partial h\\left(r_\\phi(x, y)\\right) r_\\phi(x, y) - h\\left(r_\\phi(x, y)\\right) \\right] - \\sum_y \\pi_{\\text{chosen}}(y|x) \\left[ \\partial h\\left(r_\\phi(x, y)\\right) \\right] \\tag{19} \\]Non-exhaustive examples of Bregman divergences include Least-Squared Importance Fitting (LSIF), Binary Cross Entropy (BCE), and unbounded Kullback-Leibler (UKL) divergence.\nFor example, LSIF defines \\( h_{\\text{LSIF}} = (r - 1)^2 / 2 \\), resulting in the following form of Bregman divergence for density ratios:\n\\[ \\min_\\phi D_{h_{\\text{LSIF}}}(r^* \\| r_\\phi) = \\sum_y \\frac{1}{2} \\pi_{\\text{ref}}(y|x) r_\\phi^2(x, y) - \\pi_{\\text{chosen}}(y|x) r_\\phi(x, y) \\tag{20} \\]In this case, a sample-based approximation of Equation (20) yields the following loss function:\n\\[ \\mathcal{L}(\\phi; \\mathcal{D}) = \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\frac{1}{2} r_\\phi^2(x, y_l) - r_\\phi(x, y_w) \\right] \\tag{21} \\]Here, the rejected (non-preferred) response set \\( y_l \\sim \\pi_{\\text{ref}}(y|x) \\) is used to approximate the expectation over \\( \\pi_{\\text{ref}}(y|x) \\). Researchers argue that using rejected responses \\( y_l \\) from the preference dataset \\( \\mathcal{D} \\) to approximate the expectation is reasonable; it is even possible to use both preferred and rejected responses. However, since the goal is to reduce the likelihood of rejected responses, rejected responses are chosen to approximate the expectation, and good performance is observed in subsequent experiments.\nIntuitively, the first term pushes the model to reduce the density ratio of rejected responses, while the second term increases the density ratio of preferred responses.\nFurthermore, this direct estimation method based on Bregman divergence indicates that there exists a viable family of divergences for density ratio estimation, as shown in Table 1; other \\( h \\) functions, such as BCE and UKL (introduced later), are further discussed in Appendix A. Researchers also empirically analyze the impact of different \\( h \\) function objectives in Section 6.3.\n4 Method Optimization Thus far, it has been observed that combining the RL-like objective in Equation (17) with the density ratio estimation method in Equation (21) can effectively utilize preference datasets for imitation learning. However, this two-stage process is complex and unstable: first, a reward model needs to be fitted to estimate the density ratio, and then the language model policy is fine-tuned using the RL-like objective in Equation (17).\nTo address these issues, a simpler method is introduced. First, note that the optimal policy in Equation (17) has a closed-form solution:\n\\[ \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\log r^*(x, y)\\right) \\tag{22} \\]where \\( Z(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\log r^*(x, y)\\right) = \\sum_y \\pi_{\\text{chosen}}(y|x) = 1 \\), meaning the optimal policy \\( \\pi^*(y|x) \\) is forced into a self-normalized form!\nThis property, determined by the definition of the reward function in Equation (17), offers a significant advantage: it allows our imitation learning to theoretically generalize to a broader class of loss functions than the pairwise BT preference model used in DPO.\nTaking the logarithm of both sides of Equation (22) and performing some algebraic operations yields the following expression:\n\\[ \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} = \\log r^*(x, y) \\tag{23} \\]\nwhere \\( r^*(x, y) \\) is the density ratio estimated from the preference dataset using Equation (21).\nSince the optimal density ratio is now represented by the optimal policy rather than a discriminator model, we can explicitly derive a maximum likelihood objective for the parameterized policy on the preference dataset. Similar to the approach used in density ratio estimation and leveraging variable substitution techniques, the DIL objective can be formalized as:\n\\[ \\mathcal{L}_{\\text{DIL}}(\\theta; \\mathcal{D}) = \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ - \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} + \\frac{1}{2} \\left( \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right)^2 \\right] \\tag{24} \\]where we use the alternative parameterization in Equation (23) to directly fit the density ratio implicitly defined in Equation (21).\nInterestingly, the loss function has no hyperparameters (those familiar with DPO and its variants will appreciate the value of “no hyperparameters,” as it eliminates the cost of hyperparameter tuning, greatly enhancing the feasibility of algorithm deployment in industrial scenarios), yet experiments show it still achieves satisfactory performance. Since the above process is equivalent to fitting a reparameterized density ratio estimation model, it theoretically performs imitation learning by minimizing the reverse KL divergence relative to the unknown preferred response distribution. Table 1 shows a family of objective functions satisfying the definition of Bregman divergence.\n5 Discussion: DPO as a Special Case of DIL Before proceeding, we introduce a method to prepare for the subsequent proof.\nConcept Supplement: Contrastive Predictive Coding\nContrastive Predictive Coding (CPC) — familiar to those in the speech domain — is a self-supervised learning method proposed by Oord et al. It is primarily used to learn effective representations from unlabeled datasets by leveraging local dependencies in sequence data.\nCore Idea\nCPC aims to enable the model to learn to predict future data points from current ones through contrastive learning, maximizing the mutual information between the current representation and future representations. Specifically, given a sequence \\( \\mathbf{x} = (x_1, x_2, \\ldots, x_T) \\), CPC attempts to learn an encoder \\( f_\\phi \\) that maps each time point \\( x_t \\) to a latent representation \\( z_t = f_\\phi(x_t) \\). A scoring function is then used to compute the similarity between \\( z_t \\) and the representation of a future time point \\( z_{t+k} \\), encouraging high scores for positive pairs (current and true future representations) and low scores for negative pairs (current and other time point representations).\nInfoNCE Loss Function\nCPC uses a specific contrastive loss called InfoNCE loss, defined as:\n\\[ \\mathcal{L}_{\\text{InfoNCE}} = -\\mathbb{E}_{(x_t, x_{t+k})} \\left[ \\log \\frac{\\exp(g(z_t, z_{t+k}))}{\\sum_{x_j} \\exp(g(z_t, z_j))} \\right] \\]\nwhere \\( g(z_t, z_j) \\) is a scoring function measuring the similarity between \\( z_t \\) and \\( z_j \\), and the expectation \\( \\mathbb{E} \\) averages over all possible time point pairs. This loss encourages the model to assign high scores to positive pairs and low scores to negative pairs.\nIn this section, researchers demonstrate that DPO can also be viewed as a special case of the DIL framework by using CPC for density ratio estimation. Given a prompt distribution \\( p(x) \\) and the conditional distribution of preferred responses \\( \\pi_{\\text{chosen}}(y|x) \\), we sample \\( x \\sim p(x) \\), \\( y_w \\sim \\pi_{\\text{chosen}}(y|x) \\), and \\( y_l \\sim \\pi_{\\text{ref}}(y|x) \\). CPC optimizes the following objective:\n\\[ \\mathcal{L}_{\\text{CPC}}(\\phi; \\mathcal{D}) = -\\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log \\frac{\\exp(f_\\phi(x^\\top y_w)/\\beta)}{\\exp(f_\\phi(x^\\top y_w)/\\beta) + \\exp(f_\\phi(x^\\top y_l)/\\beta)} \\right] \\tag{25} \\]where \\( f_\\phi: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R} \\) is a parameterized evaluation function.\nThe optimal evaluation function for this CPC with one negative sample satisfies the following condition:\n\\[ f^*(x, y)/\\beta = \\log \\frac{\\pi_{\\text{chosen}}(y|x)}{\\pi_{\\text{ref}}(y|x)} c(x) = \\log r^*(x, y) - \\log c(x) \\tag{26} \\]where \\( c(x) \\) is a function dependent only on \\( x \\) and not on \\( y \\). Thus, CPC also estimates the density ratio reward in the IL objective, as shown in Equation (17).\nSimilar to the previous section, using the closed-form optimal policy in Equation (22) and leveraging variable substitution, we obtain:\n\\[ \\mathcal{L}_{\\text{DIL}}(\\theta; \\mathcal{D}) = -\\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right] \\tag{27} \\]This is identical to DPO’s objective. Thus, DIL can reinterpret DPO. Specifically, researchers demonstrate that DPO also conforms to the imitation learning objective in Equation (16) and essentially uses the CPC method for density ratio reward estimation.\nIn summary, DPO is equivalent to DIL if it uses the CPC method for density ratio reward estimation.\nKey Discussion: Why the BT Assumption Reduces the Likelihood of Preferred Responses\nOversimplifying Preference Structures:\nThe BT model assumes that preferences between each pair of options can be compared independently and follow a specific probabilistic form. However, in practical applications, especially complex language generation tasks, this assumption may oversimplify the true preference structure. For example, preferences may be based on a combination of multiple complex factors, not just a simple comparison between two options. This may prevent the model from accurately capturing the factors that determine high-quality responses, thereby reducing the likelihood of preferred responses.\nData Bias and Noise:\nWhen using the BT model for preference estimation, if the training data contains bias or noise, the learned preference relationships may be inaccurate. For example, if certain types of responses are over-sampled or under-sampled due to biases in the data collection process, the BT model trained on such data may incorrectly estimate true preferences, leading to lower scores for preferred responses.\nLimitations of the Optimization Objective:\nUsing the BT model as the optimization objective may guide the model to optimize toward maximizing pairwise preference probabilities rather than directly optimizing the ability to generate high-quality responses. This may cause the model to sacrifice overall quality to improve win rates in specific comparisons, especially for responses with high intrinsic quality that are not easily highlighted in pairwise comparisons, whose likelihood may thus decrease.\nXiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.05079, arXiv, 7 Mar. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2503.05079. ↩︎\nhttps://zhuanlan.zhihu.com/p/1910382777079165403 ↩︎\n","wordCount":"5445","inLanguage":"en","datePublished":"2025-07-10T00:00:00Z","dateModified":"2025-07-10T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/connection-between-imitation-learning-and-rlhf/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Connection Between Imitation Learning and RLHF</h1><div class=post-meta><span title='2025-07-10 00:00:00 +0000 UTC'>2025-07-10</span>&nbsp;·&nbsp;26 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/llm-Connection%20Between%20Imitation%20Learning%20and%20RLHF.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#direct-imitation-learning-dil aria-label="Direct Imitation Learning (DIL)">Direct Imitation Learning (DIL)</a><ul><li><a href=#background aria-label=Background>Background</a></li><li><a href=#the-proposal-of-rlhf-and-its-issues aria-label="The Proposal of RLHF and Its Issues">The Proposal of RLHF and Its Issues</a></li><li><a href=#dpo-addressing-rlhfs-issues-and-its-own-limitations aria-label="DPO Addressing RLHF&rsquo;s Issues and Its Own Limitations">DPO Addressing RLHF&rsquo;s Issues and Its Own Limitations</a></li><li><a href=#problems-with-dpo-and-triggered-reflections aria-label="Problems with DPO and Triggered Reflections">Problems with DPO and Triggered Reflections</a></li><li><a href=#significance-of-this-study aria-label="Significance of This Study">Significance of This Study</a></li><li><a href=#key-contributions aria-label="Key Contributions">Key Contributions</a></li><li><a href=#theoretical-derivations aria-label="Theoretical Derivations">Theoretical Derivations</a><ul><li><a href=#preliminary-knowledge aria-label="Preliminary Knowledge">Preliminary Knowledge</a></li><li><a href=#core-derivations aria-label="Core Derivations">Core Derivations</a><ul><li><a href=#1-rlhf-as-a-form-of-imitation-learning aria-label="1 RLHF as a Form of Imitation Learning">1 RLHF as a Form of Imitation Learning</a></li><li><a href=#2-direct-imitation-learning-dil aria-label="2 Direct Imitation Learning (DIL)">2 Direct Imitation Learning (DIL)</a></li><li><a href=#3-density-ratio-reward-estimation aria-label="3 Density Ratio Reward Estimation">3 Density Ratio Reward Estimation</a></li><li><a href=#4-method-optimization aria-label="4 Method Optimization">4 Method Optimization</a></li><li><a href=#5-discussion-dpo-as-a-special-case-of-dil aria-label="5 Discussion: DPO as a Special Case of DIL">5 Discussion: DPO as a Special Case of DIL</a></li></ul></li></ul></li></ul></li></ul></div></details></div><div class=post-content><p>There have been many questions about whether DPO is a form of imitation learning or (offline) reinforcement learning. The more I observe the distributions of DPO&rsquo;s chosen and rejection losses, the stronger the feeling becomes that DPO is more like a form of imitation learning.</p><p><cite>The paper, Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.05079<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite> also expresses that DPO is a form of imitation learning.</p><h2 id=direct-imitation-learning-dil>Direct Imitation Learning (DIL)<a hidden class=anchor aria-hidden=true href=#direct-imitation-learning-dil>#</a></h2><p>This study explores the alignment problem between large language models and preference data from the perspective of imitation learning. Researchers establish a close theoretical connection between Reinforcement Learning from Human Feedback (RLHF) and Imitation Learning (IL), revealing that RLHF implicitly performs imitation learning on the preference data distribution. Based on this connection, Direct Imitation Learning (DIL) is proposed, a principled framework that directly optimizes the imitation learning objective. DIL provides a unified imitation learning perspective for the alignment problem, encompassing existing alignment algorithms as special cases while naturally introducing new variants. By linking IL and RLHF, DIL offers new insights into alignment with RLHF. Extensive experiments show that DIL outperforms existing methods on various challenging benchmarks.</p><h3 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h3><p>Aligning Large Language Models (LLMs) with human preferences is crucial to ensuring that the responses generated by LLMs meet human expectations.</p><h3 id=the-proposal-of-rlhf-and-its-issues>The Proposal of RLHF and Its Issues<a hidden class=anchor aria-hidden=true href=#the-proposal-of-rlhf-and-its-issues>#</a></h3><p>In recent years, Reinforcement Learning from Human Feedback (RLHF) has emerged as a widely adopted framework for fine-tuning language models based on human preference data.</p><h3 id=dpo-addressing-rlhfs-issues-and-its-own-limitations>DPO Addressing RLHF&rsquo;s Issues and Its Own Limitations<a hidden class=anchor aria-hidden=true href=#dpo-addressing-rlhfs-issues-and-its-own-limitations>#</a></h3><p>RLHF relies on a two-step reinforcement learning process, which leads to problems such as low computational efficiency and instability during training. To alleviate these limitations, researchers have proposed alternative one-stage methods, such as Direct Preference Optimization (DPO) and its variants. These methods replace RLHF with supervised learning, eliminating the need for explicit reward modeling. Instead, they directly define an implicit reward based on the likelihood of preference data, significantly improving efficiency while maintaining competitive performance.</p><h3 id=problems-with-dpo-and-triggered-reflections>Problems with DPO and Triggered Reflections<a hidden class=anchor aria-hidden=true href=#problems-with-dpo-and-triggered-reflections>#</a></h3><p>Although DPO theoretically aims to find the same optimal policy as RLHF, it and its variants essentially still follow a reward maximization objective, determined by parametric models (e.g., the Bradley-Terry (BT) model). This makes them prone to overfitting, leading to suboptimal alignment with preference data. This raises a fundamental and open research question: Can we understand and design effective preference optimization algorithms from a new perspective?</p><h3 id=significance-of-this-study>Significance of This Study<a hidden class=anchor aria-hidden=true href=#significance-of-this-study>#</a></h3><p>This paper re-examines RLHF from the perspective of imitation learning. Specifically, researchers show that RLHF is a special case of a general imitation learning problem, expressed solely through pairwise preferences. They theoretically demonstrate that alignment with RLHF is highly similar to imitation learning and implicitly optimizes the same objective. Leveraging this insight, they design DIL, a general framework for effective alignment based on density ratio reward estimation.</p><h3 id=key-contributions>Key Contributions<a hidden class=anchor aria-hidden=true href=#key-contributions>#</a></h3><ol><li>It is proven that RLHF for alignment is essentially an imitation learning problem, providing a novel analysis that offers clear guidance for the design of alignment algorithms.</li><li>DIL, a simple and general imitation learning alignment framework, is proposed. DIL unifies imitation learning on preference data and bridges the gap between density ratio estimation and preference alignment.</li><li>Empirically, the effectiveness of DIL is verified on widely used benchmarks, demonstrating its superiority over previous alignment methods.</li></ol><h3 id=theoretical-derivations>Theoretical Derivations<a hidden class=anchor aria-hidden=true href=#theoretical-derivations>#</a></h3><h4 id=preliminary-knowledge>Preliminary Knowledge<a hidden class=anchor aria-hidden=true href=#preliminary-knowledge>#</a></h4><ol><li><p><strong>Problem Setup</strong><br>Let \( x = [x_1, x_2, \ldots] \) be the input prompt, \( y_w = [y_1, y_2, \ldots] \) be the positive sample (preferred response), and \( y_l \) be the negative sample (non-preferred response). These two samples are typically drawn from the same reference policy \( \pi_{\text{ref}}(y|x) \). Meanwhile, \( y_w \succ y_l | x \) indicates that for the same input \( x \), \( y_w \) is more in line with human preferences than \( y_l \). Thus, the preference distribution is generally expressed as:<br></p>\[
p(y_w \succ y_l | x) = g(r(x, y_w) - r(x, y_l)) \tag{1}
\]<p><br>Here, \( g \) refers to the sigmoid function \( \sigma(x) = \frac{1}{1+e^{-x}} \), a conclusion derived from the Bradley-Terry model (which can be verified by dividing the numerator and denominator by \( r(x, y_l) \) to convert it into a sigmoid form). Given a preference dataset \( \mathcal{D} \) containing feedback, where each data entry is formatted as \( (x, y_w, y_l) \), our alignment goal is to learn an LLM policy \( \pi(y|x) \) based on the preference data.</p></li><li><p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong><br>Given an estimated reward function \( r(x, y) \), RLHF fine-tunes the policy \( \pi_\theta \) according to human preferences through the following optimization objective:<br></p>\[
\mathop{\max}\limits_{\pi_\theta}\mathbb{E}_{y \sim \pi_\theta(y|x)}[r(x, y)] - \beta\mathbb{D}_{KL}[\pi_\theta(y|x)||\pi_{\text{ref}}(y|x)] \tag{2}
\]<p><br>The core idea of this formula is to maximize the reward signal of human preferences while preventing the model from deviating too much from the original pre-trained distribution (to avoid collapse). Here, \( \mathbb{D}_{KL} = \mathbb{E}_{y \sim \pi_{\theta}} \left[ \log \frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)} \right] \) is the Kullback-Leibler divergence, describing the difference between the model&rsquo;s strategy distribution and the original reference model&rsquo;s. \( \beta > 0 \) is an appropriate KL penalty coefficient. Optimization methods typically use RL approaches such as Proximal Policy Optimization (PPO).</p></li><li><p><strong>Reward Modeling</strong><br>Standard reward modeling uses the BT preference model in Equation (1) to fit a reward function \( r_\phi(x, y) \). Specifically, the reward function can be estimated by maximizing the log-likelihood of preference feedback \( (x, y_w, y_l) \):<br></p>\[
\mathcal{L}_{\text{RM}}(\phi;\mathcal{D})=\mathbb{E}_{(x, y_w,y_l)\sim\mathcal{D}}[-\log\sigma(r_{\phi}(x,y_w)-r_{\phi}(x,y_l))] \tag{3}
\]</li><li><p><strong>Supervised Fine-Tuning (SFT)</strong><br>Given a demonstration dataset \( \mathcal{D} \), the goal of SFT is to minimize the negative log-likelihood of the model on the demonstration dataset:<br></p>\[
\mathcal{L}_{\text{SFT}}(\theta;\mathcal{D})=-\mathbb{E}_{(x,y)\sim\mathcal{D}}[\log\pi_{\theta}(y|x)] \tag{4}
\]<p><br>SFT is equivalent to Behavior Cloning (BC), a classic offline imitation learning method. Its goal is to minimize the forward KL divergence between the learned policy \( \pi_{\theta} \) and the data policy \( \pi_{\text{data}} \):<br></p>\[
\mathop{\min}\limits_{\theta}KL(\pi_{\text{data}}(y|x)||\pi_\theta(y|x))=-\mathbb{E}_{\pi_{\text{data}}(y|x)}[\log\pi_{\theta}(y|x)] \tag{5}
\]<p><br>It is evident that SFT and BC have the same optimal solution.</p></li><li><p><strong>Direct Preference Optimization (DPO)</strong><br>To simplify RLHF&rsquo;s optimization process, DPO uses the log-likelihood of the learned policy to implicitly represent the reward function:<br></p>\[
r_\theta(x,y)=\beta[\log\pi_\theta(y|x)-\log\pi_{\text{ref}}(y|x)] + \beta\log Z_\theta(x) \tag{6}
\]<p><br>Here, \( Z_\theta(x)=\sum_y\pi_{\text{ref}}(y|x)\exp(r_\theta(x,y)/\beta) \) is the partition function.</p><p><strong>Concept Supplement: Partition Function</strong><br>The partition function ensures that the probability distribution is normalized, i.e., the sum of probabilities of all possible states equals 1. Specifically, in a probabilistic model, given an input \( x \), the probability of output \( y \) can be expressed as:<br></p>\[
p(y|x) = \frac{1}{Z(x)} \exp(-E(y, x))
\]<p><br>where \( E(y, x) \) is the energy function, measuring the &ldquo;mismatch&rdquo; or &ldquo;cost&rdquo; of a specific output \( y \) for a given input \( x \). The partition function \( Z(x) \) is defined as the sum of energy exponents over all possible outputs \( y \):<br></p>\[
Z(x) = \sum_y \exp(-E(y, x))
\]<p><br>For continuous variables, the sum is replaced by an integral:<br></p>\[
Z(x) = \int \exp(-E(y, x)) dy
\]<p><br>Does this principle resemble the softmax function? Indeed, softmax is an application of the partition function.</p><p>Returning to the reward function, rearranging terms gives:<br></p>\[
r_\theta(x,y)+\beta\log\pi_{\text{ref}}(y|x)-\beta\log Z_\theta(x)=\beta\log\pi_\theta(y|x)
\]<p><br>Applying the natural exponential function to both sides of the equation:<br></p>\[
\frac{\exp(r_\theta(x,y))e^\beta\pi_{\text{ref}}(y|x)}{e^\beta Z_\theta(x)}=e^\beta\pi_\theta(y|x)
\]<p><br>Since \( Z_\theta(x)=\sum_y\pi_{\text{ref}}(y|x)\exp(r_\theta(x,y)/\beta)=e^{-\beta}\sum_y\pi_{\text{ref}}(y|x)\exp(r_\theta(x,y)) \), substituting into the equation and eliminating common factors on both sides yields:<br></p>\[
\pi_\theta(y|x)=\frac{\pi_{\text{ref}}(y|x)\exp(r_\theta(x,y))}{\sum_y\pi_{\text{ref}}(y|x)\exp(r_\theta(x,y))}
\]<p><br>Intuitively, the higher the reward corresponding to parameter \( \theta \), the closer the model policy \( \pi_\theta(y|x) \) is to 1 (i.e., the higher the probability of adopting that policy), while the model policy \( \pi_\theta(y|x) \) is normalized to a distribution where the sum of probabilities equals 1—this is the essence of the reward function design.</p><p>By incorporating this reward into the BT model in Equation (1) and simplifying, DPO&rsquo;s objective promotes the comparison and differentiation of preferred and non-preferred data:<br></p>\[
\mathcal{L}_{\text{DPO}}(\theta;\mathcal{D})=\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[-\log\sigma\left(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right] \tag{7}
\]</li><li><p><strong>Energy-based Models (EBMs)</strong><br>EBMs define distributions through energy functions. For \( y\in \mathbb{R}^D \), the probability density can be expressed as:<br></p>\[
p_\theta(y)=\exp(-E_\theta(y)/Z_\theta(y)) \tag{8}
\]<p><br>where \( E_\theta(y):\mathbb{R}^D\rightarrow\mathbb{R} \) is the energy function, mapping \( y \) to a scalar, and \( Z_\theta(y) = \sum_y \exp(-E_\theta(y)) \) is the unknown normalization constant, as mentioned in the supplementary knowledge about the partition function.</p></li></ol><h4 id=core-derivations>Core Derivations<a hidden class=anchor aria-hidden=true href=#core-derivations>#</a></h4><p>The derivations are mainly refereced from this <cite>blog<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></cite></p><h5 id=1-rlhf-as-a-form-of-imitation-learning>1 RLHF as a Form of Imitation Learning<a hidden class=anchor aria-hidden=true href=#1-rlhf-as-a-form-of-imitation-learning>#</a></h5><p>Researchers demonstrate that RLHF is a special case of imitation learning based on reverse KL divergence over the distribution of selected responses.</p><p>The specific proof is as follows:</p><p>First, define the following policy based on energy-based models (EBMs):<br></p>\[
\pi_\phi(y|x)=\pi_{\text{ref}}(y|x)\exp(r_\phi(x,y))/Z_\phi(x) \tag{9}
\]<p><br>where \( \phi \) denotes model parameters, and as described in the preliminary knowledge, \( Z_\phi(x)=\sum_y\pi_{\text{ref}}(y|x)\exp(r_\phi(x,y)) \).</p><p>To learn parameter \( \phi \), Behavior Cloning (BC)—a classic and widely used imitation learning method (as mentioned in the preliminary knowledge, SFT is equivalent to BC)—can be applied. This method formulates the task as minimizing the KL divergence between the policy \( \pi_\phi \) and the expert policy \( \pi_{\text{chosen}} \) that generates preferred responses \( y_w \). In other words, IL learns parameter \( \phi \) such that the model distribution imitates the distribution of preferred responses in the preference dataset:<br></p>\[
\mathop{\min}\limits_{\phi}KL(\pi_{\text{chosen}}(y|x)||\pi_\phi(y|x)) \tag{10}
\]<p>By selecting responses from the preference data to minimize the above forward KL divergence:<br></p>\[
\mathop{\min}\limits_{\phi}\mathbb{E}_{(x,y_w)\sim\mathcal{D}}[-\log\pi_{\text{ref}}(y_w|x)\exp(r_\phi(x,y_w))/Z_\phi(x)] \Rightarrow \\\mathop{\min}\limits_{\phi}\mathbb{E}_{(x,y_w)\sim\mathcal{D}}\left[-r_\phi(x,y_w)+\log\sum_y\pi_{\text{ref}}(y|x)\exp(r_\phi(x,y))\right] \tag{11}
\]<p><br>(Equation (11) is obtained by removing constants and substituting the partition function.)</p><p>There are multiple choices for sampling from the reference distribution \( \pi_{\text{ref}}(y|x) \). One setting that simplifies the above expression and practically yields RLHF is: \( \pi_{\text{ref}}(y|x) = \frac{1}{2} \mathbb{I}(Y = y_l) + \frac{1}{2} \mathbb{I}(Y = y_w) \) (note: this is a key approximation that allows RLHF to be reduced to IL). In this case, the sample-based approximation of the second term is:<br></p>\[
\mathop{\min}\limits_{\phi}\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[-r_\phi(x,y_w)+\log(\exp(r_\phi(x,y_w)) + \exp(r_\phi(x,y_l)))\right]
\]<p>This equation is derived by substituting \( \sum_y \pi_{\text{ref}}(y|x)\exp(r_\varphi(x, y)) = \frac{1}{2} \exp(r_\varphi(x, y_w)) + \frac{1}{2} \exp(r_\varphi(x, y_l)) \) into Equation (11) and removing the constant term \( -\log2 \). Further merging the \( -r_\phi(x,y_w) \) term into the right-hand side \( \log(\exp(r_\phi(x,y_w)) + \exp(r_\phi(x,y_l))) \) and simplifying gives the equivalent form:<br></p>\[
\mathop{\min}\limits_{\phi}\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[-\log\sigma(r_\phi(x,y_w) - r_\phi(x,y_l))\right] \tag{12}
\]<p>It can be noted that the imitation learning loss based on the energy strategy is identical to the reward loss of RLHF based on the BT assumption (Equation (3)). By optimizing this loss function, we can directly obtain the optimal energy strategy in Equation (9). Unfortunately, even if we use the estimated reward function \( r_\phi \), estimating the partition function \( Z_\phi(x) \) remains costly, making the derived representation impractical and leading to significantly increased inference costs.</p><p><strong>Key Discussion: Why This Method Is Costly</strong></p><ol><li><p><strong>Excessively Large Output Space</strong><br>The output of a language model is a sequence of tokens; for example, a response may contain dozens or even hundreds of tokens. Each token has thousands to tens of thousands of candidate words (depending on the vocabulary size), resulting in an exponentially growing number of total output combinations:<br>Assuming 50,000 word choices per step and generating 20 tokens, there are \( 50000^{20} \) possible responses!<br>This means it is impossible to enumerate all \( y \) to compute \( Z_\phi(x) \).</p></li><li><p><strong>Requiring Extensive Sampling to Approximate Summation</strong><br>Since enumeration is impossible, we can only estimate the summation through sampling:<br></p>\[
Z_\phi(x) \approx \frac{1}{N} \sum_{i=1}^N \exp(r_\phi(x, y_i)), \quad y_i \sim \pi_{\text{ref}}(y|x)
\]<p><br>However, to ensure estimation accuracy, it is necessary to:</p><ul><li>Sample enough \( y_i \)</li><li>Run the reward model \( r_\phi(x, y_i) \) for each \( y_i \)<br>This leads to:</li><li>High computational resource consumption (running the reward model many times per inference)</li><li>Increased inference time</li></ul></li><li><p><strong>Reward Models May Be &ldquo;Heavy&rdquo;</strong><br>The reward model \( r_\phi(x, y) \) is typically a large neural network (e.g., based on the GPT architecture), which is already time-consuming to run once. Running it multiple times on multiple samples incurs significant costs.</p></li></ol><p>To address the issue of high computational costs, researchers propose a method. Before introducing this method, we first introduce forward and reverse KL divergences (those already familiar can skip this part):</p><p><strong>Concept Supplement: Forward and Reverse KL Divergences</strong></p><ol><li><p><strong>Basic Concept of KL Divergence</strong><br>KL divergence (Kullback-Leibler Divergence), also known as relative entropy, measures the difference between two probability distributions \( P \) and \( Q \). For discrete distributions, KL divergence is defined as:<br></p>\[
D_{\text{KL}}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
\]<p><br>For continuous distributions, the sum is replaced by an integral:<br></p>\[
D_{\text{KL}}(P \| Q) = \int P(x) \log \frac{P(x)}{Q(x)}dx
\]</li><li><p><strong>Forward KL Divergence (\( D_{\text{KL}}(P \| Q) \))</strong></p><ul><li><strong>Goal</strong>: Make \( Q \) as close as possible to \( P \).</li><li><strong>Characteristics</strong>: Heavily penalizes events with high probability in \( P \) but low or zero probability in \( Q \). In other words, it focuses on accurately capturing all modes in \( P \).</li><li><strong>Application Scenarios</strong>: When there is a true distribution \( P \) (e.g., the true distribution of data) and a model \( Q \) needs to be trained to approximate this distribution, forward KL divergence is typically used. This is because forward KL encourages the model to cover all modes, even if it means generating some low-probability but existing samples.</li><li><strong>Properties</strong>: If \( P \) has a point with probability greater than 0 where \( Q \) has zero probability, the KL divergence tends to infinity. Thus, using forward KL divergence, the model tends to cover more possibilities, even unlikely events.</li></ul></li><li><p><strong>Reverse KL Divergence (\( D_{\text{KL}}(Q \| P) \))</strong></p><ul><li><strong>Goal</strong>: Make \( P \) as close as possible to \( Q \).</li><li><strong>Characteristics</strong>: More focused on avoiding generating events with very low probability in \( P \) but high probability in \( Q \). That is, it tends to find the most likely modes and ignore less likely cases.</li><li><strong>Application Scenarios</strong>: In some cases, such as Generative Adversarial Networks (GANs) or when a more deterministic model output is desired, reverse KL divergence may be a better choice. Because it encourages the model to focus on the most likely outcomes rather than trying to cover all possibilities.</li><li><strong>Properties</strong>: Unlike forward KL, reverse KL allows \( Q \) to be zero in some regions as long as \( P \) is also zero or very small there. This means reverse KL can produce sparser and more concentrated distributions, sometimes leading to &ldquo;mode collapse&rdquo;—generating only a few specific types of samples while ignoring others.</li></ul></li></ol><p>Using reverse knowledge distillation (which employs reverse KL divergence, with characteristics consistent with those of reverse KL divergence in the model distillation process), the optimal strategy in Equation (9) is &ldquo;distilled&rdquo; into a strategy with an analytical form using reverse KL divergence, enabling the final strategy \( \pi_\theta \) to require only one sampling during inference (note: here \( \theta \) is the main model parameter, and \( \phi \) is the reward model parameter):<br></p>\[
\mathop{\min}\limits_{\theta}KL\left(\pi_\theta(y|x)||\pi_{\text{ref}}(y|x)\exp(r_\phi(x,y)/\beta)/Z_\phi(x)\right) \tag{13}
\]<p><br>where \( \beta \) is the temperature hyperparameter in the distillation process. After isolating the term \( -\mathbb{E}_{\pi_\theta(y|x)}[r_\phi(x,y)] \), removing multiplicative and additive constants, and combining the remaining terms into \( \beta KL(\pi_\theta(y|x)||\pi_{\text{ref}}(y|x)) \), it is transformed into the following objective function:<br></p>\[
\mathcal{L}(\theta)=-\mathbb{E}_{\pi_\theta(y|x)}[r_\phi(x,y)] + \beta KL(\pi_\theta(y|x)||\pi_{\text{ref}}(y|x)) \tag{14}
\]<p>It can be observed that this distillation objective corresponds exactly to the objective of RLHF in Equation (2). Thus, researchers provide two key conclusions:</p><p>(i) Reward learning in RLHF is equivalent to an imitation learning problem for preferred responses, achieved by minimizing the forward KL divergence between \( \pi_{\text{chosen}} \) and \( \pi_\phi \) based on energy-based models (EBMs), as shown in Equation (12);</p><p>(ii) The RL step in RLHF can be interpreted as a reverse knowledge distillation process, where the EBM-based imitation strategy \( \pi_\phi \) is distilled into the final analytical strategy \( \pi_\theta \) by minimizing the reverse KL divergence in Equation (13), with the temperature parameter \( \beta \) determining the degree of KL regularization.</p><p>This problem is transformed into the following proposition:</p><p>Assume the preferred response distribution \( p(y|x) \), the energy-based model \( \pi_\phi(y|x) \), and the model \( \pi_\theta(y|x) \). When \( \beta = 1 \), KL-regularized RLHF can be regarded as the following problem:<br></p>\[
\min_{\pi_\theta} \mathrm{KL}(\pi_\theta \| \pi^*_\phi) \quad \text{s.t.} \quad \pi^*_\phi = \arg\min_{\pi_\phi} \mathrm{KL}(\pi_{\text{chosen}} \| \pi_\phi) \tag{15}
\]<p><br>where \( \pi_{\text{chosen}}(y|x) = \pi_\phi(y|x) = \pi_\theta(y|x) \) is the equilibrium state.</p><p>Thus, imitation learning on preferred responses is equivalent to solving a standard KL-regularized RLHF problem.</p><p>Furthermore, we observe that when \( \pi^*_\phi = \pi_{\text{chosen}} \) (i.e., the optimal solution achieved by the lower-level objective), the upper-level objective essentially optimizes a reverse KL divergence \( \mathrm{KL}(\pi_\theta \| \pi_{\text{chosen}}) \).</p><p>At this point, I am truly amazed; this proof process is akin to Maxwell&rsquo;s equations unifying electromagnetism in physics.</p><p>To this end, we have proven that RLHF is a special type of IL, with equivalent conditions:</p><ul><li>Using EBM and forward KL divergence to fit the Reward Model (RM)</li><li>Using EBM and reverse knowledge distillation based on reverse KL divergence to complete RL training</li></ul><p>The proof is complete. Researchers then pose an interesting question:</p><p>Why does SFT — which directly optimizes the forward KL divergence \( \mathrm{KL}(\pi_{\text{chosen}} \| \pi_\theta) \) in Equation (5) — perform worse than RLHF in alignment tasks?</p><p>Theoretically, minimizing the objective functions of SFT and RLHF should lead to the same optimal solution \( \pi_\theta \). However, in practice, this requires complete data coverage and unlimited computational resources, conditions rarely met.</p><p>Thus, in practical settings, minimizing different KL divergences results in learned policies with distinct characteristics. Specifically, the forward KL divergence \( \mathrm{KL}(\pi_{\text{chosen}} \| \pi_\theta) \) promotes mass-covering behavior, while the reverse KL divergence \( \mathrm{KL}(\pi_\theta \| \pi_{\text{chosen}}) \) encourages mode-seeking behavior (see supplementary knowledge on forward and reverse KL divergences above).</p><p>Mass-covering behavior tends to assign similar probabilities to all responses in the dataset, overestimating the long-tail portion of the target distribution; mode-seeking behavior concentrates probability mass in specific high-reward regions. Therefore, the goal of alignment is to generate a certain type of high-quality response, which can be more effectively achieved by minimizing the reverse KL divergence.</p><p>In summary, the reason RLHF outperforms SFT is that the full performance of forward KL divergence requires complete data coverage and unlimited computational resources, which is nearly impossible to achieve. In contrast, reverse KL divergence, although unable to learn to cover the distribution, can still learn high-quality responses, resulting in better performance of reverse KL divergence over forward KL divergence.</p><h5 id=2-direct-imitation-learning-dil>2 Direct Imitation Learning (DIL)<a hidden class=anchor aria-hidden=true href=#2-direct-imitation-learning-dil>#</a></h5><p>In the previous section, we re-examined RLHF from the perspective of imitation learning. The analysis clearly indicates that RLHF is essentially optimized to closely align with the distribution of preferred responses. The sample-based approximation of EBM in RLHF leads to a reward loss similar to the BT model, as shown in Equation (12). However, the BT assumption does not always hold. Based on these insights, researchers propose a new alignment method, DIL (Direct Imitation Learning), that does not rely on the BT assumption. Thus, the objective of imitation learning is directly formulated as minimizing the reverse KL divergence between \( \pi_\theta \) and the unknown preferred response distribution \( \pi_{\text{chosen}} \):<br></p>\[
\min_\theta L_{\text{DIL}}(\theta) = \mathrm{KL} \left( \pi_\theta(y|x) \| \pi_{\text{chosen}}(y|x) \right) = \mathbb{E}_{\pi_\theta(y|x)} \left[ \log \left( \frac{\pi_\theta(y|x)}{\pi_{\text{chosen}}(y|x)} \right) \right] \tag{16}
\]<p><br>Here, we minimize the reverse KL divergence, unlike SFT which minimizes the forward KL divergence as in Equation (5). However, using reverse KL divergence for mode concentration is typically challenging. Directly optimizing Equation (16) cannot effectively utilize preference data, especially since the data policy \( \pi_{\text{chosen}} \) is unknown. In reinforcement learning literature, these challenges are addressed through adversarial training. However, such methods require complex and unstable adversarial training to learn the reward function, which is impractical for large models. In this paper, a simple alternative is proposed to directly utilize offline human preference data without learning the reward function through adversarial training. The DIL objective is reformulated as follows:<br></p>\[
\max_\theta \mathbb{E}_{\pi_\theta(y|x)} \left[ \log \frac{\pi_{\text{chosen}}(y|x)}{\pi_{\text{ref}}(y|x)} - \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} \right]=\\
\mathbb{E}_{\pi_\theta(y|x)} \left[ \log r(x, y) \right] - \mathrm{KL} \left( \pi_\theta(y|x) \parallel \pi_{\text{ref}}(y|x) \right) \tag{17}
\]<p><br>where \( r(x, y) \triangleq \frac{\pi_{\text{chosen}}(y|x)}{\pi_{\text{ref}}(y|x)} \) can be regarded as an auxiliary reward function. Equations (16) and (17) are equivalent by adding and subtracting the same term \( \log \pi_{\text{ref}}(y|x) \) in the expectation.</p><p>Interestingly, researchers find that even when only preference data is available, the form of this objective function is similar to that of RLHF in Equation (2). The main difference is that the reward here is the estimated log density ratio, which is often difficult to obtain directly in practice. Optimizing this objective involving the density ratio \( r(x, y) \) is non-intuitive and challenging. The next section will show how to efficiently optimize this objective function by effectively utilizing offline human preference data.</p><h5 id=3-density-ratio-reward-estimation>3 Density Ratio Reward Estimation<a hidden class=anchor aria-hidden=true href=#3-density-ratio-reward-estimation>#</a></h5><p>Before delving into the problem in Equation (17), we first describe how to compute the auxiliary reward function based on the density ratio. In a tabular setting, we can directly compute \( \pi_{\text{ref}}(y|x) \) and \( \pi_{\text{chosen}}(y|x) \). However, in high-dimensional language domains, estimating densities separately and computing their ratios is ineffective due to error accumulation.</p><p>Before introducing the solution, we first understand Bregman divergence:</p><p><strong>Concept Supplement: Bregman Divergence</strong></p><p>Bregman Divergence is a measure of the difference between two points, defined by the properties of convex functions. Specifically, given a strictly convex and twice differentiable function \( F \), Bregman divergence is defined as the difference between the function and its linear approximation at a certain point.</p><ol><li><p><strong>Definition</strong><br>Suppose \( F \) is a strictly convex function defined on a convex set. The Bregman divergence \( D_F \) generated by \( F \) can be defined as:<br></p>\[
D_F(\mathbf{p} \| \mathbf{q}) = F(\mathbf{p}) - F(\mathbf{q}) - \langle \nabla F(\mathbf{q}), (\mathbf{p} - \mathbf{q}) \rangle
\]<p><br>where:</p><ul><li>\( \mathbf{p} \) and \( \mathbf{q} \) are two points in the space;</li><li>\( \nabla F(\mathbf{q}) \) denotes the gradient of \( F \) at point \( \mathbf{q} \);</li><li>\( \langle \cdot, \cdot \rangle \) denotes the inner product operation.</li></ul><p>Simply put, \( D_F(\mathbf{p} \| \mathbf{q}) \) measures the gap between the value of function \( F \) at point \( \mathbf{p} \) and the first-order Taylor expansion of \( F \) at point \( \mathbf{q} \).</p></li><li><p><strong>Properties</strong></p><ul><li>Non-negativity: For all \( \mathbf{p} \) and \( \mathbf{q} \), \( D_F(\mathbf{p} \| \mathbf{q}) \geq 0 \), with equality if and only if \( \mathbf{p} = \mathbf{q} \).</li><li>Asymmetry: In general, \( D_F(\mathbf{p} \| \mathbf{q}) \neq D_F(\mathbf{q} \| \mathbf{p}) \), meaning Bregman divergence is not a symmetric measure.</li><li>Sandwich Inequality: Bregman divergence does not satisfy the triangle inequality, i.e., for any three points \( \mathbf{x}, \mathbf{y}, \mathbf{z} \), it is not necessarily true that \( D_F(\mathbf{x} \| \mathbf{y}) + D_F(\mathbf{y} \| \mathbf{z}) \geq D_F(\mathbf{x} \| \mathbf{z}) \).</li></ul></li><li><p><strong>Common Examples</strong><br>Different convex functions \( F \) yield different types of Bregman divergences. Here are some common examples:</p><ul><li>Squared Euclidean distance: If \( F(\mathbf{x}) = \|\mathbf{x}\|^2 \), the corresponding Bregman divergence is the square of the Euclidean distance.</li><li>Kullback-Leibler divergence: If \( F(\mathbf{x}) = \sum_i x_i \log x_i \), the corresponding Bregman divergence is the KL divergence.</li><li>Itakura-Saito distance: If \( F(\mathbf{x}) = -\sum_i \log x_i \), the corresponding Bregman divergence is the Itakura-Saito distance.</li></ul></li></ol><p>The solution proposed in this paper is to directly estimate the density ratio \( \pi_{\text{chosen}}(y|x)/\pi_{\text{ref}}(y|x) \) based on Bregman divergence. Assume the target density ratio is \( r^*(x, y) = \pi_{\text{chosen}}(y|x)/\pi_{\text{ref}}(y|x) \), and a parameterized discriminator \( r_\phi \) is used to estimate this ratio:<br></p>\[
\min_\phi D_h(r^* \| r_\phi) =\\\sum_y \pi_{\text{ref}}(y|x) B_h\left(r^*(x, y) \| r_\phi(x, y)\right) =\\\sum_y \pi_{\text{ref}}(y|x) \left[ h\left(r^*(x, y)\right) - h\left(r_\phi(x, y)\right) - \partial h\left(r_\phi(x, y)\right) \left(r^*(x, y) - r_\phi(x, y)\right) \right] \tag{18}
\]<p>where \( B_h \) is the sample-level Bregman divergence.</p><p>For a twice continuously differentiable convex function \( h \) with a bounded derivative \( \partial h \), this divergence measures the difference between two density ratios. By subtracting the constant term \( \sum_y \pi_{\text{ref}}(y|x) h(r^*(x, y)) \) and substituting \( r^*(x, y) = \pi_{\text{chosen}}(y|x)/\pi_{\text{ref}}(y|x) \), we obtain (ignoring the constant term):<br></p>\[
\sum_y \pi_{\text{ref}}(y|x) \left[ \partial h\left(r_\phi(x, y)\right) r_\phi(x, y) - h\left(r_\phi(x, y)\right) \right] - \sum_y \pi_{\text{chosen}}(y|x) \left[ \partial h\left(r_\phi(x, y)\right) \right] \tag{19}
\]<p>Non-exhaustive examples of Bregman divergences include Least-Squared Importance Fitting (LSIF), Binary Cross Entropy (BCE), and unbounded Kullback-Leibler (UKL) divergence.</p><p>For example, LSIF defines \( h_{\text{LSIF}} = (r - 1)^2 / 2 \), resulting in the following form of Bregman divergence for density ratios:<br></p>\[
\min_\phi D_{h_{\text{LSIF}}}(r^* \| r_\phi) = \sum_y \frac{1}{2} \pi_{\text{ref}}(y|x) r_\phi^2(x, y) - \pi_{\text{chosen}}(y|x) r_\phi(x, y) \tag{20}
\]<p>In this case, a sample-based approximation of Equation (20) yields the following loss function:<br></p>\[
\mathcal{L}(\phi; \mathcal{D}) = \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \frac{1}{2} r_\phi^2(x, y_l) - r_\phi(x, y_w) \right] \tag{21}
\]<p>Here, the rejected (non-preferred) response set \( y_l \sim \pi_{\text{ref}}(y|x) \) is used to approximate the expectation over \( \pi_{\text{ref}}(y|x) \). Researchers argue that using rejected responses \( y_l \) from the preference dataset \( \mathcal{D} \) to approximate the expectation is reasonable; it is even possible to use both preferred and rejected responses. However, since the goal is to reduce the likelihood of rejected responses, rejected responses are chosen to approximate the expectation, and good performance is observed in subsequent experiments.</p><p>Intuitively, the first term pushes the model to reduce the density ratio of rejected responses, while the second term increases the density ratio of preferred responses.</p><p>Furthermore, this direct estimation method based on Bregman divergence indicates that there exists a viable family of divergences for density ratio estimation, as shown in Table 1; other \( h \) functions, such as BCE and UKL (introduced later), are further discussed in Appendix A. Researchers also empirically analyze the impact of different \( h \) function objectives in Section 6.3.</p><h5 id=4-method-optimization>4 Method Optimization<a hidden class=anchor aria-hidden=true href=#4-method-optimization>#</a></h5><p>Thus far, it has been observed that combining the RL-like objective in Equation (17) with the density ratio estimation method in Equation (21) can effectively utilize preference datasets for imitation learning. However, this two-stage process is complex and unstable: first, a reward model needs to be fitted to estimate the density ratio, and then the language model policy is fine-tuned using the RL-like objective in Equation (17).</p><p>To address these issues, a simpler method is introduced. First, note that the optimal policy in Equation (17) has a closed-form solution:<br></p>\[
\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\log r^*(x, y)\right) \tag{22}
\]<p>where \( Z(x) = \sum_y \pi_{\text{ref}}(y|x) \exp\left(\log r^*(x, y)\right) = \sum_y \pi_{\text{chosen}}(y|x) = 1 \), meaning the optimal policy \( \pi^*(y|x) \) is forced into a self-normalized form!</p><p>This property, determined by the definition of the reward function in Equation (17), offers a significant advantage: it allows our imitation learning to theoretically generalize to a broader class of loss functions than the pairwise BT preference model used in DPO.</p><p>Taking the logarithm of both sides of Equation (22) and performing some algebraic operations yields the following expression:<br></p>\[
\log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} = \log r^*(x, y) \tag{23}
\]<p><br>where \( r^*(x, y) \) is the density ratio estimated from the preference dataset using Equation (21).</p><p>Since the optimal density ratio is now represented by the optimal policy rather than a discriminator model, we can explicitly derive a maximum likelihood objective for the parameterized policy on the preference dataset. Similar to the approach used in density ratio estimation and leveraging variable substitution techniques, the DIL objective can be formalized as:<br></p>\[
\mathcal{L}_{\text{DIL}}(\theta; \mathcal{D}) = \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ - \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} + \frac{1}{2} \left( \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right)^2 \right] \tag{24}
\]<p>where we use the alternative parameterization in Equation (23) to directly fit the density ratio implicitly defined in Equation (21).</p><p>Interestingly, the loss function has no hyperparameters (those familiar with DPO and its variants will appreciate the value of &ldquo;no hyperparameters,&rdquo; as it eliminates the cost of hyperparameter tuning, greatly enhancing the feasibility of algorithm deployment in industrial scenarios), yet experiments show it still achieves satisfactory performance. Since the above process is equivalent to fitting a reparameterized density ratio estimation model, it theoretically performs imitation learning by minimizing the reverse KL divergence relative to the unknown preferred response distribution. Table 1 shows a family of objective functions satisfying the definition of Bregman divergence.</p><h5 id=5-discussion-dpo-as-a-special-case-of-dil>5 Discussion: DPO as a Special Case of DIL<a hidden class=anchor aria-hidden=true href=#5-discussion-dpo-as-a-special-case-of-dil>#</a></h5><p>Before proceeding, we introduce a method to prepare for the subsequent proof.</p><p><strong>Concept Supplement: Contrastive Predictive Coding</strong></p><p>Contrastive Predictive Coding (CPC) — familiar to those in the speech domain — is a self-supervised learning method proposed by Oord et al. It is primarily used to learn effective representations from unlabeled datasets by leveraging local dependencies in sequence data.</p><ol><li><p><strong>Core Idea</strong><br>CPC aims to enable the model to learn to predict future data points from current ones through contrastive learning, maximizing the mutual information between the current representation and future representations. Specifically, given a sequence \( \mathbf{x} = (x_1, x_2, \ldots, x_T) \), CPC attempts to learn an encoder \( f_\phi \) that maps each time point \( x_t \) to a latent representation \( z_t = f_\phi(x_t) \). A scoring function is then used to compute the similarity between \( z_t \) and the representation of a future time point \( z_{t+k} \), encouraging high scores for positive pairs (current and true future representations) and low scores for negative pairs (current and other time point representations).</p></li><li><p><strong>InfoNCE Loss Function</strong><br>CPC uses a specific contrastive loss called InfoNCE loss, defined as:<br></p>\[
\mathcal{L}_{\text{InfoNCE}} = -\mathbb{E}_{(x_t, x_{t+k})} \left[ \log \frac{\exp(g(z_t, z_{t+k}))}{\sum_{x_j} \exp(g(z_t, z_j))} \right]
\]<p><br>where \( g(z_t, z_j) \) is a scoring function measuring the similarity between \( z_t \) and \( z_j \), and the expectation \( \mathbb{E} \) averages over all possible time point pairs. This loss encourages the model to assign high scores to positive pairs and low scores to negative pairs.</p></li></ol><p>In this section, researchers demonstrate that DPO can also be viewed as a special case of the DIL framework by using CPC for density ratio estimation. Given a prompt distribution \( p(x) \) and the conditional distribution of preferred responses \( \pi_{\text{chosen}}(y|x) \), we sample \( x \sim p(x) \), \( y_w \sim \pi_{\text{chosen}}(y|x) \), and \( y_l \sim \pi_{\text{ref}}(y|x) \). CPC optimizes the following objective:<br></p>\[
\mathcal{L}_{\text{CPC}}(\phi; \mathcal{D}) = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \frac{\exp(f_\phi(x^\top y_w)/\beta)}{\exp(f_\phi(x^\top y_w)/\beta) + \exp(f_\phi(x^\top y_l)/\beta)} \right] \tag{25}
\]<p>where \( f_\phi: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R} \) is a parameterized evaluation function.</p><p>The optimal evaluation function for this CPC with one negative sample satisfies the following condition:<br></p>\[
f^*(x, y)/\beta = \log \frac{\pi_{\text{chosen}}(y|x)}{\pi_{\text{ref}}(y|x)} c(x) = \log r^*(x, y) - \log c(x) \tag{26}
\]<p>where \( c(x) \) is a function dependent only on \( x \) and not on \( y \). Thus, CPC also estimates the density ratio reward in the IL objective, as shown in Equation (17).</p><p>Similar to the previous section, using the closed-form optimal policy in Equation (22) and leveraging variable substitution, we obtain:<br></p>\[
\mathcal{L}_{\text{DIL}}(\theta; \mathcal{D}) = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right] \tag{27}
\]<p>This is identical to DPO&rsquo;s objective. Thus, DIL can reinterpret DPO. Specifically, researchers demonstrate that DPO also conforms to the imitation learning objective in Equation (16) and essentially uses the CPC method for density ratio reward estimation.</p><p>In summary, DPO is equivalent to DIL if it uses the CPC method for density ratio reward estimation.</p><p><strong>Key Discussion: Why the BT Assumption Reduces the Likelihood of Preferred Responses</strong></p><ol><li><p><strong>Oversimplifying Preference Structures</strong>:<br>The BT model assumes that preferences between each pair of options can be compared independently and follow a specific probabilistic form. However, in practical applications, especially complex language generation tasks, this assumption may oversimplify the true preference structure. For example, preferences may be based on a combination of multiple complex factors, not just a simple comparison between two options. This may prevent the model from accurately capturing the factors that determine high-quality responses, thereby reducing the likelihood of preferred responses.</p></li><li><p><strong>Data Bias and Noise</strong>:<br>When using the BT model for preference estimation, if the training data contains bias or noise, the learned preference relationships may be inaccurate. For example, if certain types of responses are over-sampled or under-sampled due to biases in the data collection process, the BT model trained on such data may incorrectly estimate true preferences, leading to lower scores for preferred responses.</p></li><li><p><strong>Limitations of the Optimization Objective</strong>:<br>Using the BT model as the optimization objective may guide the model to optimize toward maximizing pairwise preference probabilities rather than directly optimizing the ability to generate high-quality responses. This may cause the model to sacrifice overall quality to improve win rates in specific comparisons, especially for responses with high intrinsic quality that are not easily highlighted in pairwise comparisons, whose likelihood may thus decrease.</p></li></ol><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Xiao, Teng, et al. On a Connection Between Imitation Learning and RLHF. arXiv:2503.05079, arXiv, 7 Mar. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2503.05079>https://doi.org/10.48550/arXiv.2503.05079</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://zhuanlan.zhihu.com/p/1910382777079165403>https://zhuanlan.zhihu.com/p/1910382777079165403</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/llm/>LLM</a></li><li><a href=https://congchan.github.io/tags/rlhf/>RLHF</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/><span class=title>« Prev</span><br><span>Awesome Large Language Model (LLM) Post-training - [2025 Update]</span>
</a><a class=next href=https://congchan.github.io/posts/multi-token-prediction/><span class=title>Next »</span><br><span>Multi-token Prediction</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Connection Between Imitation Learning and RLHF on x" href="https://x.com/intent/tweet/?text=Connection%20Between%20Imitation%20Learning%20and%20RLHF&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fconnection-between-imitation-learning-and-rlhf%2f&amp;hashtags=LLM%2cRLHF"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Connection Between Imitation Learning and RLHF on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fconnection-between-imitation-learning-and-rlhf%2f&amp;title=Connection%20Between%20Imitation%20Learning%20and%20RLHF&amp;summary=Connection%20Between%20Imitation%20Learning%20and%20RLHF&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fconnection-between-imitation-learning-and-rlhf%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Connection Between Imitation Learning and RLHF on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fconnection-between-imitation-learning-and-rlhf%2f&title=Connection%20Between%20Imitation%20Learning%20and%20RLHF"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Connection Between Imitation Learning and RLHF on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fconnection-between-imitation-learning-and-rlhf%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Connection Between Imitation Learning and RLHF on whatsapp" href="https://api.whatsapp.com/send?text=Connection%20Between%20Imitation%20Learning%20and%20RLHF%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fconnection-between-imitation-learning-and-rlhf%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Connection Between Imitation Learning and RLHF on telegram" href="https://telegram.me/share/url?text=Connection%20Between%20Imitation%20Learning%20and%20RLHF&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fconnection-between-imitation-learning-and-rlhf%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Connection Between Imitation Learning and RLHF on ycombinator" href="https://news.ycombinator.com/submitlink?t=Connection%20Between%20Imitation%20Learning%20and%20RLHF&u=https%3a%2f%2fcongchan.github.io%2fposts%2fconnection-between-imitation-learning-and-rlhf%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>