<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Survey - Pre-Trained Models - Past, Present and Future | Cong's Log</title><meta name=keywords content="NLP,Survey,2021,Pre-Trained Models"><meta name=description content="Links: https://arxiv.org/abs/2106.07139
最新出炉的 Pre-Trained Models 综述速览。

先确定综述中的一些名词的定义

Transfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的
Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。

一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题
而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题


Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法

2 Background 脉络图谱
Pre-training 可分为两大类：

2.1 Transfer Learning and Supervised Pre-Training

此类可进一步细分为 feature transfer 和 parameter transfer.


2.2 Self-Supervised Learning and Self-Supervised Pre-Training


Transfer learning 可细分为四个子类

inductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007),
transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006),
self-taught learning (Raina et al., 2007; Dai et al., 2008)
unsupervised transfer learning (Wang et al., 2008).

inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域"><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/survey-pre-trained-models-past-present-and-future/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/survey-pre-trained-models-past-present-and-future/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/survey-pre-trained-models-past-present-and-future/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Survey - Pre-Trained Models - Past, Present and Future"><meta property="og:description" content="Links: https://arxiv.org/abs/2106.07139
最新出炉的 Pre-Trained Models 综述速览。
先确定综述中的一些名词的定义
Transfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的 Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。 一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题 而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题 Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法 2 Background 脉络图谱 Pre-training 可分为两大类：
2.1 Transfer Learning and Supervised Pre-Training 此类可进一步细分为 feature transfer 和 parameter transfer. 2.2 Self-Supervised Learning and Self-Supervised Pre-Training Transfer learning 可细分为四个子类
inductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007), transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006), self-taught learning (Raina et al., 2007; Dai et al., 2008) unsupervised transfer learning (Wang et al., 2008). inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-06-19T00:00:00+00:00"><meta property="article:modified_time" content="2021-06-19T00:00:00+00:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Survey"><meta property="article:tag" content="2021"><meta property="article:tag" content="Pre-Trained Models"><meta name=twitter:card content="summary"><meta name=twitter:title content="Survey - Pre-Trained Models - Past, Present and Future"><meta name=twitter:description content="Links: https://arxiv.org/abs/2106.07139
最新出炉的 Pre-Trained Models 综述速览。

先确定综述中的一些名词的定义

Transfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的
Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。

一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题
而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题


Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法

2 Background 脉络图谱
Pre-training 可分为两大类：

2.1 Transfer Learning and Supervised Pre-Training

此类可进一步细分为 feature transfer 和 parameter transfer.


2.2 Self-Supervised Learning and Self-Supervised Pre-Training


Transfer learning 可细分为四个子类

inductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007),
transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006),
self-taught learning (Raina et al., 2007; Dai et al., 2008)
unsupervised transfer learning (Wang et al., 2008).

inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Survey - Pre-Trained Models - Past, Present and Future","item":"https://congchan.github.io/posts/survey-pre-trained-models-past-present-and-future/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Survey - Pre-Trained Models - Past, Present and Future","name":"Survey - Pre-Trained Models - Past, Present and Future","description":"Links: https://arxiv.org/abs/2106.07139\n最新出炉的 Pre-Trained Models 综述速览。\n先确定综述中的一些名词的定义\nTransfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的 Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。 一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题 而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题 Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法 2 Background 脉络图谱 Pre-training 可分为两大类：\n2.1 Transfer Learning and Supervised Pre-Training 此类可进一步细分为 feature transfer 和 parameter transfer. 2.2 Self-Supervised Learning and Self-Supervised Pre-Training Transfer learning 可细分为四个子类\ninductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007), transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006), self-taught learning (Raina et al., 2007; Dai et al., 2008) unsupervised transfer learning (Wang et al., 2008). inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域\n","keywords":["NLP","Survey","2021","Pre-Trained Models"],"articleBody":"Links: https://arxiv.org/abs/2106.07139\n最新出炉的 Pre-Trained Models 综述速览。\n先确定综述中的一些名词的定义\nTransfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的 Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。 一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题 而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题 Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法 2 Background 脉络图谱 Pre-training 可分为两大类：\n2.1 Transfer Learning and Supervised Pre-Training 此类可进一步细分为 feature transfer 和 parameter transfer. 2.2 Self-Supervised Learning and Self-Supervised Pre-Training Transfer learning 可细分为四个子类\ninductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007), transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006), self-taught learning (Raina et al., 2007; Dai et al., 2008) unsupervised transfer learning (Wang et al., 2008). inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域\nself-taught learning 和 unsupervised transfer learning 则主要集中于NLP领域，由于NLP领域的数据标注难度更大，所以主要以无监督的语言模型训练为主，2013年到2017年主要是词向量这类Feature transfer应用为主，把训练好的词表示作为下游模型的输入，但feature是固定的（ELMO是作为往可修正的feature方向发展的跳板），2018年开始有了BERT和GPT这种基于上下文的表示，把预训练的模型参数迁移到下游任务。\n3 Transformer and Representative PTMs 这部分主要介绍基于Transformer的各种表征学习PTMs， 如GPT和BERT，以及后续的家族图谱\nTransformer家族的四大优化方向：\nSome work improves the model architectures and explores novel pre- training tasks, such as XLNet (Yang et al., 2019), UniLM (Dong et al., 2019), MASS (Song et al., 2019), SpanBERT (Joshi et al., 2020) and ELEC- TRA (Clark et al., 2020). Besides, incorporating rich data sources is also an important direction, such as utilizing multilingual corpora, knowledge graphs, and images. Since the model scale is a crucial success factor of PTMs, researchers also explore to build larger models to reach over hundreds of billions of parameters, such as the series of GPT (Radford et al., 2019; Brown et al., 2020), Switch Transformer (Fedus et al., 2021), mean- while conduct computational efficiency optimization for training PTMs (Shoeybi et al., 2019; Ra- jbhandari et al., 2020; Ren et al., 2021). 4 Designing Effective Architectures two motivations\n统一NLU和NLG任务 从人类cognitive science角度切入 4.1 Unified Sequence Modeling NLP的三类versatile downstream tasks and applications：\nNatural language understanding: grammatical analysis, syntactic analysis, word/sentence/paragraph classification, ques- tion answering, factual/commonsense knowl- edge inference and etc Open-ended language generation: includes dialog generation, story generation, data-to- text generation and etc. Non-open-ended language generation: includes machine translation, abstract summarizing, blank filling and etc. understanding tasks 可以转换为 generation tasks (Schick and Schütze, 2020)。同时生成式的GPT在一些理解类任务上也可以达到甚至超过BERT的效果，因此The boundary between understanding and generation is vague. 基于此观察有如下一些研究方向：\nCombining Autoregressive and Autoencoding Modeling: 就是把GPT的单向生成和BERT的双向理解结合起来, 先驱就是XLNet permutated language modeling: XLNet (Yang et al., 2019), MPNet (Song et al., 2020) Multi-task training: UniLM (Dong et al., 2019) Mask上面做文章: GLM (Du et al., 2021), fill in blanks with variable lengths Applying Generalized Encoder-Decoder: 为了生成可变长的目标序列, 采用encoder-decoder architectures MASS (Song et al.,2019): introduces the masked-prediction strategy into the encoder-decoder structure. T5 (Raffel et al., 2020), : masking a variable-length of span in text with only one mask token and asks the decoder to recover the whole masked sequence. BART (Lewis et al., 2020a): corrupting the source sequence with multiple operations such as truncation, deletion, re- placement, shuffling, and masking, instead of mere masking. Encoder-Decoder架构导致参数更大, 虽然可以通过参数共享减轻, 但效率仍堪忧. Seq2seq的结构在NLU任务上表现不好，低于RoBERTa和GLM\n4.2 Cognitive-Inspired Architectures Transformer的注意力机制利用了人的视觉感知, 但是对于人的decision making, logical reasoning, counterfactual reasoning and working memory (Baddeley, 1992) 没有很好的模拟. 因此就有基于cognitive science的改进方向\nMaintainable Working Memory: 人的注意力机制和Transformer还是不一样的, 人的注意力机制没有Transformer那么long-range, 而是维护一个working memory(Baddeley, 1992; Brown, 1958; Barrouillet et al., 2004; Wharton et al., 1994), 负责记忆, 重组和选择性遗忘, 也就是LSTM所希望达到的目的. Transformer-XL (Dai et al., 2019) : introduce segment-level recurrence and relative positional encoding CogQA (Ding et al., 2019): maintain a cognitive graph in the multi-hop reading, the System 1 based on PTMs and the System 2 based on GNNs to model the cognitive graph for multi-hop understanding. CogLTX (Ding et al., 2020): leverages a MemRecall language model to select sen- tences that should be maintained in the working memory and another model for answering or clas- sificatio Sustainable Long-Term Memory: GPT-3 (Brown et al., 2020)表明Transformer有记忆能力, 那么就有动力去进一步挖掘Transformer的记忆能力. Lample et al. (2019)表示feed-forward networks in Transformers is equivalent to memory networks. 但记忆能力有限. REALM (Guu et al., 2020) : explore how to construct a sustainable external memory for Transformers. tensorize the whole Wikipedia sentence by sentence, and retrieve relevant sentences as context for masked pre-training. RAG (Lewis et al., 2020b) extends the masked pre-training to autoregressive generation, which could be better than extractive question answering. (Vergaet al., 2020; Févry et al., 2020) propose to tensorize entities and triples in existing knowledge bases, replace entity tokens’ embedding in an internal Transformer layer with the embedding from outer memory networks. (Dhingra et al., 2020; Sun et al., 2021) maintain a virtual knowledge from scratch, and propose a differentiable reasoning training objective over it. 4.3 其他 More Variants of Existing PTMs focus on optimizing BERT’s architecture to boost language models’ performance on natural language understanding.\nimproving the masking strategy: 可以视为一种数据增强 Span- BERT (Joshi et al., 2020): masking a continuous random-length span of tokens with a span boundary objective (SBO) could improve BERT’s performance ERNIE (Sun et al., 2019b,c): entity masking NEZHA (Wei et al., 2019) Whole Word Masking (Cui et al., 2019) change masked-prediction objective to GAN: ELECTRA (Clark et al., 2020) transform MLM to a replace token detection (RTD) objective, in which a generator will replace tokens in original sequences and a discriminator will predict whether a token is replaced. 5 Utilizing Multi-Source Data 5.1 多语言 Multilingual Pre-Training Language Multilingual masked language modeling (MMLM): multilingual BERT (mBERT) released by Devlin et al. (2019) is pre- trained with the MMLM task using non-parallel multilingual Wikipedia corpora in 104 languages. Translation language modeling (TLM) : MMLM task 无法利用 parallel corpora. 因此有XLM (Lample and Conneau, 2019) leverages bilingual sentence pairs to perform the translation language modeling (TLM) task. Unicoder (Huang et al., 2019a): Cross-lingual word recovery (CLWR), Cross-lingual paraphrase classification (CLPC) Generative models for multilingual PTMs: MASS (Song et al., 2019) extends MLM to language genera- tion. mBART (Liu et al., 2020c) extends DAE to support multiple languages by adding special symbols. 5.2 多模态 Multimodal Pre-Training Modalities can all be classified as vision and language (V\u0026L),\nViLBERT (Lu et al., 2019) is a model to learn task-agnostic joint representations of images and languages. two streams of input, by preprocessing textual and visual information separately. LXMERT (Tan and Bansal, 2019) has similar architecture compared to Vil- BERT but uses more pre-training tasks VisualBERT (Li et al., 2019), on the other side, extends the BERT architecture at the minimum. The Transformer layers of VisualBERT implicitly align elements in the input text and image regions. Unicoder-VL (Li et al., 2020a) moves the offsite visual detector in VisualBERT into an end-to-end version: It designs the image token for Transformers as the sum of the bounding box and object label features. VL-BERT(Su et al., 2020) also uses a similar architecture to VisualBERT. each input element is either a token from the input sentence or a region-of-interest (RoI) from the input image. UNITER (Chen et al., 2020e) learns unified representations between the two modali- ties. DALLE (Ramesh et al., 2021) : A bigger step towards conditional zero-shot image generation: transformer-based text-to-image zero- shot pre-trained model with around 10 billiion pa- rameters. CLIP (Radford et al., 2021) and Wen-Lan (Huo et al., 2021) explore enlarging web-scale data for V\u0026L pre-training with big success. Com- 5.3 Knowledge-Enhanced Pre-Training PTMs 可以从大量语料中中提取统计信息. 同时外部知识(such as knowledge graphs, domain- specific data and extra annotations of pre-training data) 可以作为很好的统计先验.\n6 Improving Computational Efficiency 6.1 Sstem-Level Optimization 系统层的优化是 model-agnostic and do not change underlying learning algorithms.\n单机优化: half-precision floating-point format (FP16), may fail because of the floating-point truncation and overflow mixed- precision training methods: which preserve some critical weights in FP32 to avoid the floating-point overflow and use dynamic loss scaling operations to get rid of the floating-point truncation. gradient checkpointing methods(Rasley et al., 2020) have been used to save memory by storing only a part of the activation states after forward pass. 如果模型参数太大无法塞入显存, store model parameters and activation states with the CPU memory, ZeRO-Offload (Ren et al., 2021) design delicate strategies to schedule the swap between the CPU memory and the GPU memory so that memory swap and device computation can be over- lapped as much as possible. 多机优化 数据并行 Data parallelism (Li et al., 2020d),\n模型并行, Model parallelism: Megatron- LM (Shoeybi et al., 2019) splits self-attention heads as well as feed-forward layers into differ- ent GPUs\nModel pipeline parallelism: partitions a deep neural network into multiple lay- ers and then puts different layers onto different nodes.\nGPipe (Huang et al., 2019b) which can send smaller parts of samples within a mini-batch to different nodes TeraPipe (Li et al., 2021) which can apply token-level pipeline mechanisms for Transformer-based models to make each token in a sequence be processed by different nodes. 6.2 Efficient Pre-Training 训练方法优化：改进BERT低效的mask机制 selectively mask tokens based on their importance (Gu et al., 2020) or gra- dients (Chen et al., 2020b) in back-propagation to speed up model training. ELECTRA需要识别所有token所以效率更高 warmup strategy different layers can share similar self-attention patterns, 先训练浅层的神经网络, 再复制到更深的网络中 Some layers can also be dropped during training to reduce the complexity of back-propagation and weight update (Zhang and He, 2020) 对不同层使用不同学习率 模型结构优化 减小模型复杂度，设计low-rank kernels to theoretically approximate the original attention weights and result in linear complexity 在attention mechanisms中引入稀疏性，by limiting the view of each token to a fixed size and separating tokens into several chunks so that the computation of attention weights takes place in every single chunk rather than a complete sequence Switch Transformers使用的Mix-of-experts to each layer of Transformers 6.3 模型压缩 参数共享 Parameter Sharing: ALBERT (Lan et al., 2019) uses factorized embedding parameterization and cross-layer parameter sharing 模型剪枝 Model Pruning 知识蒸馏 Knowledge Distillation: DistillBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019), BERT- PKD (Sun et al., 2019a) and MiniLM (Wang et al., 2020d). Model Quantization: Q8BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2020a), Ternary- BERT (Zhang et al., 2020b) applies 7.1 Knowledge of PTMs 知识分为linguistic knowledge and world knowledge.，\nLinguistic Knowledge\nRepresentation Probing, 通过额外的线性层在下游任务探测Representation 中是否含有语言知识 Representation Analysis：Use the hidden representations of PTMs to compute some statistics such as distances or similarities, 如 BERT visualization Attention analysis：同上 Generation Analysis：预测单词或句子的分布 construct analysis tasks based on generation: Perturbed Masking (Wu et al., 2020) recovers syntactic trees from PTMs without any extra parameter and the structure given by PTMs are competitive with a human-designed dependency schema in some downstream tasks. 在11个 linguistic tasks上的结果表明PTMs可以学习到tokens, chunks, and pairwise relations. 通过设计新的任务可以发现PTM编码了syntactic, semantic, local, and long- range information。\nWorld Knowledge\ncommonsense knowledge: Davison et al. (2019) propose to first transform relational triples into masked sen- tences and then rank these sentences according to the mutual information given by PTMs. In the ex- periments, the PTM-based extraction method with- out further training even generalizes better than current supervised approaches. factual knowledge: Petroni et al. (2019) propose to formulate the relational knowledge generation as the completion of fill-in-the-blank statements. LPAQA (Jiang et al., 2020b) search better statements/prompts through mining- based and paraphrasing-based methods. Auto - Prompt (Shin et al., 2020) proposes to train discrete prompts for knowledge probing. In P-tuning (Liu et al., 2021b), the authors discover that the bet- ter prompts lie in continuous embedding space, rather than discrete space. 7.2 Robustness of PTMs 用Adversarial attacks检验模型鲁棒性，\nPTMs can be easily fooled by synonym replacement (Jin et al., 2020; Zang et al., 2020). Irrelevant artifacts such as form words can mislead the PTMs into making wrong predic- tions (Niven and Kao, 2019; Wallace et al., 2019a). 如何生成对抗样本\nutilize the model prediction, prediction probabilities, and model gradients of the models, 但难以保证质量 human-in-the-loop methods (Wallace et al., 2019b; Nie et al., 2020) generate more natural, valid, and diverse adversarial examples。 7.3 Structural Sparsity of PTMs The multi-head attention structures are redundant in the tasks of machine translation (Michel et al., 2019), abstractive summarization (Baan et al., 2019), and language understanding (Kovaleva et al., 2019). 部分研究移除head反而得到更好的表现，一些head的注意力pattern也是相似的。 Sparsity of parameters: Gordon et al. (2020) show low levels of pruning (30-40%) do not affect pre-training loss or the performance on downstream tasks at all. Prasanna et al. (2020) validate the lottery ticket hypothesis on PTMs and find that it is possible to find sub-networks achieving per- formance that is comparable with that of the full model. 7.4 Theoretical Analysis of PTMs Erhan et al. (2010) propose two hypotheses\nbetter optimization：更接近全局最优 better regularization：更好的泛化能力 Saunshi et al. (2019) conduct a theoretical analysis of contrastive unsupervised representation learning. they prove that the loss of contrastive learning is the upper bound of the downstream loss.\n8 Future Directions 8.1 Architectures and Pre-Training Methods New Architectures New Pre-Training Tasks Beyond Fine-Tuning：An improved solution is to fix the original parameters of PTMs and add small fine-tunable adaption modules for specific tasks. Reliability 8.2 Multilingual and Multimodal Pre-Training More Modalities: image, text, video and audio More Insightful Interpretation: why bridging vision and language works More Downstream Applications: 现有的image-text retrieval, image-to-text generation, text-to-image generation 等并不是现实迫切需要的应用. Transfer Learning. 8.3 Computational Efficiency Data Movement: 设备通信瓶颈 Parallelism Strategies 设计自动化 Large-Scale Training ","wordCount":"2054","inLanguage":"en","datePublished":"2021-06-19T00:00:00Z","dateModified":"2021-06-19T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/survey-pre-trained-models-past-present-and-future/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Survey - Pre-Trained Models - Past, Present and Future</h1><div class=post-meta><span title='2021-06-19 00:00:00 +0000 UTC'>2021-06-19</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/paper-Pre-Trained-Models-Past-Present-and-Future.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#2-background-%e8%84%89%e7%bb%9c%e5%9b%be%e8%b0%b1 aria-label="2 Background 脉络图谱">2 Background 脉络图谱</a></li><li><a href=#3-transformer-and-representative-ptms aria-label="3 Transformer and Representative PTMs">3 Transformer and Representative PTMs</a></li><li><a href=#4-designing-effective-architectures aria-label="4 Designing Effective Architectures">4 Designing Effective Architectures</a><ul><li><a href=#41-unified-sequence-modeling aria-label="4.1 Unified Sequence Modeling">4.1 Unified Sequence Modeling</a></li><li><a href=#42-cognitive-inspired-architectures aria-label="4.2 Cognitive-Inspired Architectures">4.2 Cognitive-Inspired Architectures</a></li><li><a href=#43-%e5%85%b6%e4%bb%96-more-variants-of-existing-ptms aria-label="4.3 其他 More Variants of Existing PTMs">4.3 其他 More Variants of Existing PTMs</a></li></ul></li><li><a href=#5-utilizing-multi-source-data aria-label="5 Utilizing Multi-Source Data">5 Utilizing Multi-Source Data</a><ul><li><a href=#51-%e5%a4%9a%e8%af%ad%e8%a8%80-multilingual-pre-training-language aria-label="5.1 多语言 Multilingual Pre-Training Language">5.1 多语言 Multilingual Pre-Training Language</a></li><li><a href=#52-%e5%a4%9a%e6%a8%a1%e6%80%81-multimodal-pre-training aria-label="5.2 多模态 Multimodal Pre-Training">5.2 多模态 Multimodal Pre-Training</a></li><li><a href=#53-knowledge-enhanced-pre-training aria-label="5.3 Knowledge-Enhanced Pre-Training">5.3 Knowledge-Enhanced Pre-Training</a></li></ul></li><li><a href=#6-improving-computational-efficiency aria-label="6 Improving Computational Efficiency">6 Improving Computational Efficiency</a><ul><li><a href=#61-sstem-level-optimization aria-label="6.1 Sstem-Level Optimization">6.1 Sstem-Level Optimization</a></li><li><a href=#62-efficient-pre-training aria-label="6.2 Efficient Pre-Training">6.2 Efficient Pre-Training</a></li><li><a href=#63-%e6%a8%a1%e5%9e%8b%e5%8e%8b%e7%bc%a9 aria-label="6.3 模型压缩">6.3 模型压缩</a></li><li><a href=#71-knowledge-of-ptms aria-label="7.1 Knowledge of PTMs">7.1 Knowledge of PTMs</a></li><li><a href=#72-robustness-of-ptms aria-label="7.2 Robustness of PTMs">7.2 Robustness of PTMs</a></li><li><a href=#73-structural-sparsity-of-ptms aria-label="7.3 Structural Sparsity of PTMs">7.3 Structural Sparsity of PTMs</a></li><li><a href=#74-theoretical-analysis-of-ptms aria-label="7.4 Theoretical Analysis of PTMs">7.4 Theoretical Analysis of PTMs</a></li></ul></li><li><a href=#8-future-directions aria-label="8 Future Directions">8 Future Directions</a><ul><li><a href=#81-architectures-and-pre-training-methods aria-label="8.1 Architectures and Pre-Training Methods">8.1 Architectures and Pre-Training Methods</a></li><li><a href=#82-multilingual-and-multimodal-pre-training aria-label="8.2 Multilingual and Multimodal Pre-Training">8.2 Multilingual and Multimodal Pre-Training</a></li><li><a href=#83-computational-efficiency aria-label="8.3 Computational Efficiency">8.3 Computational Efficiency</a></li></ul></li></ul></div></details></div><div class=post-content><p>Links: <a href=https://arxiv.org/abs/2106.07139>https://arxiv.org/abs/2106.07139</a></p><p>最新出炉的 Pre-Trained Models 综述速览。</p><p>先确定综述中的一些名词的定义</p><ul><li>Transfer learning：迁移学习，一种用于应对机器学习中的data hungry问题的方法，是有监督的</li><li>Self-Supervised Learning：自监督学习，也用于应对机器学习中的data hungry问题，特别是针对完全没有标注的数据，可以通过某种方式以数据自身为标签进行学习（比如language modeling）。所以和无监督学习有异曲同工之处。<ul><li>一般我们说无监督主要集中于clustering, community discovery, and anomaly detection等模式识别问题</li><li>而self-supervised learning还是在监督学习的范畴，集中于classification and generation等问题</li></ul></li><li>Pre-trained models (PTMs) ：预训练模型，Pre-training是一种具体的训练方案，可以采用transfer learning或者Self-Supervised Learning方法</li></ul><h1 id=2-background-脉络图谱>2 Background 脉络图谱<a hidden class=anchor aria-hidden=true href=#2-background-脉络图谱>#</a></h1><p>Pre-training 可分为两大类：</p><ul><li>2.1 <strong>Transfer Learning</strong> and <strong>Supervised</strong> Pre-Training<ul><li>此类可进一步细分为 feature transfer 和 parameter transfer.</li></ul></li><li>2.2 <strong>Self-Supervised Learning</strong> and Self-Supervised Pre-Training</li></ul><p><img loading=lazy src=/images/papers/paper11.png></p><p>Transfer learning 可细分为四个子类</p><ul><li>inductive transfer learning (Lawrence and Platt, 2004; Mihalkova et al., 2007; Evgeniou and Pontil, 2007),</li><li>transductive transfer learning (Shimodaira, 2000; Zadrozny,2004; Daume III and Marcu, 2006),</li><li>self-taught learning (Raina et al., 2007; Dai et al., 2008)</li><li>unsupervised transfer learning (Wang et al., 2008).</li></ul><p>inductive transfer learning 和 transductive transfer learning 的研究进展主要集中以imageNet为labeled source data资源的图像领域</p><p>self-taught learning 和 unsupervised transfer learning 则主要集中于NLP领域，由于NLP领域的数据标注难度更大，所以主要以无监督的语言模型训练为主，2013年到2017年主要是词向量这类Feature transfer应用为主，把训练好的词表示作为下游模型的输入，但feature是固定的（<strong>ELMO</strong>是作为往可修正的feature方向发展的跳板），2018年开始有了<strong>BERT和GPT</strong>这种基于上下文的表示，把预训练的模型<strong>参数</strong>迁移到下游任务。</p><h1 id=3-transformer-and-representative-ptms>3 Transformer and Representative PTMs<a hidden class=anchor aria-hidden=true href=#3-transformer-and-representative-ptms>#</a></h1><p>这部分主要介绍基于Transformer的各种表征学习PTMs， 如GPT和BERT，以及后续的家族图谱</p><p><img alt=/images/papers/paper11-1.png loading=lazy src=/images/papers/paper11-1.png></p><p>Transformer家族的四大优化方向：</p><ul><li>Some work <strong>improves the model architectures and explores novel pre- training tasks</strong>, such as XLNet (Yang et al., 2019), UniLM (Dong et al., 2019), MASS (Song et al., 2019), SpanBERT (Joshi et al., 2020) and ELEC- TRA (Clark et al., 2020).</li><li>Besides, <strong>incorporating rich data sources</strong> is also an important direction, such as utilizing multilingual corpora, knowledge graphs, and images.</li><li>Since the model scale is a crucial success factor of PTMs, researchers also explore to <strong>build larger models</strong> to reach over hundreds of billions of parameters, such as the series of GPT (Radford et al., 2019; Brown et al., 2020), Switch Transformer (Fedus et al., 2021),</li><li>mean- while conduct <strong>computational efficiency optimization</strong> for training PTMs (Shoeybi et al., 2019; Ra- jbhandari et al., 2020; Ren et al., 2021).</li></ul><h1 id=4-designing-effective-architectures>4 Designing Effective Architectures<a hidden class=anchor aria-hidden=true href=#4-designing-effective-architectures>#</a></h1><p>two motivations</p><ul><li>统一NLU和NLG任务</li><li>从人类cognitive science角度切入</li></ul><h2 id=41-unified-sequence-modeling>4.1 Unified Sequence Modeling<a hidden class=anchor aria-hidden=true href=#41-unified-sequence-modeling>#</a></h2><p>NLP的三类versatile downstream tasks and applications：</p><ul><li>Natural language understanding: grammatical analysis, syntactic analysis, word/sentence/paragraph classification, ques- tion answering, factual/commonsense knowl- edge inference and etc</li><li>Open-ended language generation: includes dialog generation, story generation, data-to- text generation and etc.</li><li>Non-open-ended language generation: includes machine translation, abstract summarizing, blank filling and etc.</li></ul><p>understanding tasks 可以转换为 generation tasks (Schick and Schütze, 2020)。同时生成式的GPT在一些理解类任务上也可以达到甚至超过BERT的效果，因此The boundary between understanding and generation is vague. 基于此观察有如下一些研究方向：</p><ul><li>Combining Autoregressive and Autoencoding Modeling: 就是把GPT的单向生成和BERT的双向理解结合起来, 先驱就是XLNet<ul><li>permutated language modeling: <strong>XLNet</strong> (Yang et al., 2019), MPNet (Song et al., 2020)</li><li>Multi-task training: <strong>UniLM</strong> (Dong et al., 2019)</li><li>Mask上面做文章: GLM (Du et al., 2021), fill in blanks with variable lengths</li></ul></li><li>Applying Generalized Encoder-Decoder: 为了生成可变长的目标序列, 采用encoder-decoder architectures<ul><li><strong>MASS</strong> (Song et al.,2019): introduces the masked-prediction strategy into the encoder-decoder structure.</li><li><strong>T5</strong> (Raffel et al., 2020), : masking a variable-length of span in text with only one mask token and asks the decoder to recover the whole masked sequence.</li><li><strong>BART</strong> (Lewis et al., 2020a): corrupting the source sequence with multiple operations such as truncation, deletion, re- placement, shuffling, and masking, instead of mere masking.</li></ul></li></ul><p>Encoder-Decoder架构导致参数更大, 虽然可以通过参数共享减轻, 但效率仍堪忧. Seq2seq的结构在NLU任务上表现不好，低于RoBERTa和GLM</p><p><img alt=/images/papers/paper11-2.png loading=lazy src=/images/papers/paper11-2.png></p><h2 id=42-cognitive-inspired-architectures>4.2 Cognitive-Inspired Architectures<a hidden class=anchor aria-hidden=true href=#42-cognitive-inspired-architectures>#</a></h2><p>Transformer的注意力机制利用了人的视觉感知, 但是对于人的decision making, logical reasoning, counterfactual reasoning and working memory (Baddeley, 1992) 没有很好的模拟. 因此就有基于cognitive science的改进方向</p><ul><li><strong>Maintainable Working Memory</strong>: 人的注意力机制和Transformer还是不一样的, 人的注意力机制没有Transformer那么long-range, 而是维护一个working memory(Baddeley, 1992; Brown, 1958; Barrouillet et al., 2004; Wharton et al., 1994), 负责记忆, 重组和选择性遗忘, 也就是LSTM所希望达到的目的.<ul><li><strong>Transformer-XL</strong> (Dai et al., 2019) : introduce segment-level recurrence and relative positional encoding</li><li>CogQA (Ding et al., 2019): maintain a cognitive graph in the multi-hop reading, the System 1 based on PTMs and the System 2 based on GNNs to model the cognitive graph for multi-hop understanding.</li><li>CogLTX (Ding et al., 2020): leverages a MemRecall language model to select sen- tences that should be maintained in the working memory and another model for answering or clas- sificatio</li></ul></li><li><strong>Sustainable Long-Term Memory</strong>: GPT-3 (Brown et al., 2020)表明Transformer有记忆能力, 那么就有动力去进一步挖掘Transformer的记忆能力. Lample et al. (2019)表示feed-forward networks in Transformers is equivalent to memory networks. 但记忆能力有限.<ul><li>REALM (Guu et al., 2020) : explore how to construct a sustainable external memory for Transformers. <strong>tensorize the whole Wikipedia sentence by sentence</strong>, and retrieve relevant sentences as context for masked pre-training.</li><li>RAG (Lewis et al., 2020b) extends the masked pre-training to autoregressive generation, which could be better than extractive question answering.</li><li>(Vergaet al., 2020; Févry et al., 2020) propose to <strong>tensorize entities and triples</strong> in existing knowledge bases, replace entity tokens’ embedding in an internal Transformer layer with the embedding from outer memory networks.</li><li>(Dhingra et al., 2020; Sun et al., 2021) maintain a virtual knowledge from scratch, and propose a differentiable reasoning training objective over it.</li></ul></li></ul><h2 id=43-其他-more-variants-of-existing-ptms>4.3 其他 More Variants of Existing PTMs<a hidden class=anchor aria-hidden=true href=#43-其他-more-variants-of-existing-ptms>#</a></h2><p>focus on optimizing BERT’s architecture to boost language models’ performance on natural language understanding.</p><ul><li>improving the <strong>masking strategy</strong>: 可以视为一种数据增强<ul><li><strong>Span- BERT</strong> (Joshi et al., 2020): masking a continuous random-length span of tokens with a span boundary objective (SBO) could improve BERT’s performance</li><li><strong>ERNIE</strong> (Sun et al., 2019b,c): entity masking</li><li><strong>NEZHA</strong> (Wei et al., 2019)</li><li><strong>Whole Word Masking</strong> (Cui et al., 2019)</li></ul></li><li>change masked-prediction objective to <strong>GAN</strong>: <strong>ELECTRA</strong> (Clark et al., 2020) transform MLM to a replace token detection (RTD) objective, in which a generator will replace tokens in original sequences and a discriminator will predict whether a token is replaced.</li></ul><h1 id=5-utilizing-multi-source-data>5 Utilizing Multi-Source Data<a hidden class=anchor aria-hidden=true href=#5-utilizing-multi-source-data>#</a></h1><h2 id=51-多语言-multilingual-pre-training-language>5.1 多语言 Multilingual Pre-Training Language<a hidden class=anchor aria-hidden=true href=#51-多语言-multilingual-pre-training-language>#</a></h2><ul><li>M<strong>ultilingual masked language modeling (MMLM):</strong> multilingual BERT (mBERT) released by Devlin et al. (2019) is pre- trained with the <strong>MMLM</strong> task using non-parallel multilingual Wikipedia corpora in 104 languages.</li><li><strong>Translation language modeling (TLM)</strong> : MMLM task 无法利用 parallel corpora. 因此有<strong>XLM</strong> (Lample and Conneau, 2019) leverages bilingual sentence pairs to perform the <strong>translation language modeling (TLM)</strong> task.</li><li>Unicoder (Huang et al., 2019a): <strong>Cross-lingual word recovery (CLWR), Cross-lingual paraphrase classification (CLPC)</strong></li><li>Generative models for multilingual PTMs: <strong>MASS</strong> (Song et al., 2019) extends MLM to language genera- tion.</li><li><strong>mBART</strong> (Liu et al., 2020c) extends <strong>DAE</strong> to support multiple languages by adding special symbols.</li></ul><h2 id=52-多模态-multimodal-pre-training>5.2 多模态 Multimodal Pre-Training<a hidden class=anchor aria-hidden=true href=#52-多模态-multimodal-pre-training>#</a></h2><p>Modalities can all be classified as vision and language (V&amp;L),</p><ul><li><strong>ViLBERT</strong> (Lu et al., 2019) is a model to learn task-agnostic joint representations of images and languages. two streams of input, by preprocessing textual and visual information separately.</li><li><strong>LXMERT</strong> (Tan and Bansal, 2019) has similar architecture compared to Vil- BERT but uses more pre-training tasks</li><li><strong>VisualBERT</strong> (Li et al., 2019), on the other side, extends the BERT architecture at the minimum. The Transformer layers of VisualBERT implicitly align elements in the input text and image regions.</li><li><strong>Unicoder-VL</strong> (Li et al., 2020a) moves the offsite visual detector in VisualBERT into an end-to-end version: It designs the image token for Transformers as the sum of the bounding box and object label features.</li><li><strong>VL-BERT</strong>(Su et al., 2020) also uses a similar architecture to VisualBERT. each input element is either a token from the input sentence or a region-of-interest (RoI) from the input image.</li><li><strong>UNITER</strong> (Chen et al., 2020e) learns unified representations between the two modali- ties.</li><li><strong>DALLE</strong> (Ramesh et al., 2021) : A bigger step towards <strong>conditional zero-shot image generation:</strong> transformer-based text-to-image zero- shot pre-trained model with around 10 billiion pa- rameters.</li><li><strong>CLIP</strong> (Radford et al., 2021) and Wen-Lan (Huo et al., 2021) explore enlarging web-scale data for V&amp;L pre-training with big success. Com-</li></ul><h2 id=53-knowledge-enhanced-pre-training>5.3 Knowledge-Enhanced Pre-Training<a hidden class=anchor aria-hidden=true href=#53-knowledge-enhanced-pre-training>#</a></h2><p>PTMs 可以从大量语料中中提取统计信息. 同时外部知识(such as knowledge graphs, domain- specific data and extra annotations of pre-training data) 可以作为很好的统计先验.</p><h1 id=6-improving-computational-efficiency>6 Improving Computational Efficiency<a hidden class=anchor aria-hidden=true href=#6-improving-computational-efficiency>#</a></h1><h2 id=61-sstem-level-optimization>6.1 Sstem-Level Optimization<a hidden class=anchor aria-hidden=true href=#61-sstem-level-optimization>#</a></h2><p>系统层的优化是 model-agnostic and do not change underlying learning algorithms.</p><ul><li>单机优化:<ul><li>half-precision floating-point format (FP16), may fail because of the floating-point truncation and overflow</li><li>mixed- precision training methods: which preserve some critical weights in FP32 to avoid the floating-point overflow and use dynamic loss scaling operations to get rid of the floating-point truncation.</li><li>gradient checkpointing methods(Rasley et al., 2020) have been used to save memory by <strong>storing only a part of the activation states after forward pass</strong>.</li><li>如果模型参数太大无法塞入显存, store model parameters and activation states with the CPU memory, ZeRO-Offload (Ren et al., 2021) design delicate strategies to schedule the swap between the CPU memory and the GPU memory so that memory swap and device computation can be over- lapped as much as possible.</li></ul></li><li>多机优化<ul><li><p>数据并行 Data parallelism (Li et al., 2020d),</p><p><img alt=/images/papers/paper11-3.png loading=lazy src=/images/papers/paper11-3.png></p></li><li><p>模型并行, Model parallelism: Megatron- LM (Shoeybi et al., 2019) splits self-attention heads as well as feed-forward layers into differ- ent GPUs</p><p><img alt=/images/papers/paper11-4.png loading=lazy src=/images/papers/paper11-4.png></p></li><li><p>Model pipeline parallelism: partitions a deep neural network into multiple lay- ers and then puts different layers onto different nodes.</p><ul><li>GPipe (Huang et al., 2019b) which can send smaller parts of samples within a mini-batch to different nodes</li><li>TeraPipe (Li et al., 2021) which can apply token-level pipeline mechanisms for Transformer-based models to make each token in a sequence be processed by different nodes.</li></ul></li></ul></li></ul><h2 id=62-efficient-pre-training>6.2 Efficient Pre-Training<a hidden class=anchor aria-hidden=true href=#62-efficient-pre-training>#</a></h2><ul><li>训练方法优化：改进BERT低效的mask机制<ul><li>selectively mask tokens based on their <strong>importance</strong> (Gu et al., 2020) or gra- dients (Chen et al., 2020b) in back-propagation to speed up model training.</li><li>ELECTRA需要识别所有token所以效率更高</li><li>warmup strategy</li><li>different layers can share similar self-attention patterns, 先训练浅层的神经网络, 再复制到更深的网络中</li><li>Some layers can also be dropped during training to reduce the complexity of back-propagation and weight update (Zhang and He, 2020)</li><li>对不同层使用不同学习率</li></ul></li><li>模型结构优化<ul><li>减小模型复杂度，设计low-rank kernels to theoretically approximate the original attention weights and result in linear complexity</li><li>在attention mechanisms中引入稀疏性，by limiting the view of each token to a fixed size and separating tokens into several chunks so that the computation of attention weights takes place in every single chunk rather than a complete sequence</li><li>Switch Transformers使用的Mix-of-experts to each layer of Transformers</li></ul></li></ul><h2 id=63-模型压缩>6.3 模型压缩<a hidden class=anchor aria-hidden=true href=#63-模型压缩>#</a></h2><ul><li>参数共享 Parameter Sharing: ALBERT (Lan et al., 2019) uses factorized embedding parameterization and cross-layer parameter sharing</li><li>模型剪枝 Model Pruning</li><li>知识蒸馏 Knowledge Distillation: DistillBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019), BERT- PKD (Sun et al., 2019a) and MiniLM (Wang et al., 2020d).</li><li>Model Quantization: Q8BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2020a), Ternary- BERT (Zhang et al., 2020b) applies</li></ul><h2 id=71-knowledge-of-ptms>7.1 Knowledge of PTMs<a hidden class=anchor aria-hidden=true href=#71-knowledge-of-ptms>#</a></h2><p>知识分为linguistic knowledge and world knowledge.，</p><p>Linguistic Knowledge</p><ul><li>Representation Probing, 通过额外的线性层在下游任务探测Representation 中是否含有语言知识</li><li>Representation Analysis：Use the hidden representations of PTMs to compute some statistics such as distances or similarities, 如 <strong>BERT visualization</strong></li><li>Attention analysis：同上</li><li>Generation Analysis：预测单词或句子的分布</li><li>construct analysis tasks based on generation: <strong>Perturbed Masking</strong> (Wu et al., 2020) recovers syntactic trees from PTMs without any extra parameter and the structure given by PTMs are competitive with a human-designed dependency schema in some downstream tasks.</li></ul><p>在11个 linguistic tasks上的结果表明PTMs可以学习到tokens, chunks, and pairwise relations. 通过设计新的任务可以发现PTM编码了syntactic, semantic, local, and long- range information。</p><p>World Knowledge</p><ul><li>commonsense knowledge: Davison et al. (2019) propose to first transform relational triples into masked sen- tences and then rank these sentences according to the mutual information given by PTMs. In the ex- periments, the PTM-based extraction method with- out further training even generalizes better than current supervised approaches.</li><li>factual knowledge: Petroni et al. (2019) propose to formulate the relational knowledge generation as the completion of <strong>fill-in-the-blank</strong> statements.<ul><li>LPAQA (Jiang et al., 2020b) search better statements/prompts through mining- based and paraphrasing-based methods.</li><li>Auto - Prompt (Shin et al., 2020) proposes to train discrete prompts for knowledge probing.</li><li>In P-tuning (Liu et al., 2021b), the authors discover that the bet- ter prompts lie in continuous embedding space, rather than discrete space.</li></ul></li></ul><h2 id=72-robustness-of-ptms>7.2 Robustness of PTMs<a hidden class=anchor aria-hidden=true href=#72-robustness-of-ptms>#</a></h2><p>用Adversarial attacks检验模型鲁棒性，</p><ul><li>PTMs can be easily fooled by <strong>synonym replacement</strong> (Jin et al., 2020; Zang et al., 2020).</li><li>I<strong>rrelevant artifacts</strong> such as form words can mislead the PTMs into making wrong predic- tions (Niven and Kao, 2019; Wallace et al., 2019a).</li></ul><p>如何生成对抗样本</p><ul><li>utilize the model prediction, prediction probabilities, and model gradients of the models, 但难以保证质量</li><li>human-in-the-loop methods (Wallace et al., 2019b; Nie et al., 2020) generate more natural, valid, and diverse adversarial examples。</li></ul><h2 id=73-structural-sparsity-of-ptms>7.3 Structural Sparsity of PTMs<a hidden class=anchor aria-hidden=true href=#73-structural-sparsity-of-ptms>#</a></h2><ul><li><strong>The multi-head attention structures are redundant</strong> in the tasks of machine translation (Michel et al., 2019), abstractive summarization (Baan et al., 2019), and language understanding (Kovaleva et al., 2019). 部分研究移除head反而得到更好的表现，一些head的注意力pattern也是相似的。</li><li><strong>Sparsity of parameters</strong>:<ul><li>Gordon et al. (2020) show <strong>low levels of pruning</strong> (30-40%) do not affect pre-training loss or the performance on downstream tasks at all.</li><li>Prasanna et al. (2020) validate <strong>the lottery ticket hypothesis on PTMs</strong> and find that it is possible to find sub-networks achieving per- formance that is comparable with that of the full model.</li></ul></li></ul><h2 id=74-theoretical-analysis-of-ptms>7.4 Theoretical Analysis of PTMs<a hidden class=anchor aria-hidden=true href=#74-theoretical-analysis-of-ptms>#</a></h2><p>Erhan et al. (2010) propose two hypotheses</p><ul><li>better optimization：更接近全局最优</li><li>better regularization：更好的泛化能力</li></ul><p>Saunshi et al. (2019) conduct a theoretical analysis of c<strong>ontrastive unsupervised representation learning</strong>. they prove that <strong>the loss of contrastive learning is the upper bound of the downstream loss</strong>.</p><h1 id=8-future-directions>8 Future Directions<a hidden class=anchor aria-hidden=true href=#8-future-directions>#</a></h1><h2 id=81-architectures-and-pre-training-methods>8.1 Architectures and Pre-Training Methods<a hidden class=anchor aria-hidden=true href=#81-architectures-and-pre-training-methods>#</a></h2><ul><li>New Architectures</li><li>New Pre-Training Tasks</li><li>Beyond Fine-Tuning：An improved solution is to fix the original parameters of PTMs and add small fine-tunable adaption modules for specific tasks.</li><li>Reliability</li></ul><h2 id=82-multilingual-and-multimodal-pre-training>8.2 Multilingual and Multimodal Pre-Training<a hidden class=anchor aria-hidden=true href=#82-multilingual-and-multimodal-pre-training>#</a></h2><ul><li>More Modalities: image, text, video and audio</li><li>More Insightful Interpretation: why bridging vision and language works</li><li>More Downstream Applications: 现有的image-text retrieval, image-to-text generation, text-to-image generation 等并不是现实迫切需要的应用.</li><li>Transfer Learning.</li></ul><h2 id=83-computational-efficiency>8.3 Computational Efficiency<a hidden class=anchor aria-hidden=true href=#83-computational-efficiency>#</a></h2><ul><li>Data Movement: 设备通信瓶颈</li><li>Parallelism Strategies 设计自动化</li><li>Large-Scale Training</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/nlp/>NLP</a></li><li><a href=https://congchan.github.io/tags/survey/>Survey</a></li><li><a href=https://congchan.github.io/tags/2021/>2021</a></li><li><a href=https://congchan.github.io/tags/pre-trained-models/>Pre-Trained Models</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/mixture-of-experts-moe/><span class=title>« Prev</span><br><span>Mixture of Experts (MOE)</span>
</a><a class=next href=https://congchan.github.io/posts/corefqa-coreference-resolution-as-query-based-span-prediction/><span class=title>Next »</span><br><span>CorefQA - Coreference resolution as query-based span prediction</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey - Pre-Trained Models - Past, Present and Future on x" href="https://x.com/intent/tweet/?text=Survey%20-%20Pre-Trained%20Models%20-%20Past%2c%20Present%20and%20Future&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fsurvey-pre-trained-models-past-present-and-future%2f&amp;hashtags=NLP%2cSurvey%2c2021%2cPre-TrainedModels"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey - Pre-Trained Models - Past, Present and Future on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fsurvey-pre-trained-models-past-present-and-future%2f&amp;title=Survey%20-%20Pre-Trained%20Models%20-%20Past%2c%20Present%20and%20Future&amp;summary=Survey%20-%20Pre-Trained%20Models%20-%20Past%2c%20Present%20and%20Future&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fsurvey-pre-trained-models-past-present-and-future%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey - Pre-Trained Models - Past, Present and Future on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fsurvey-pre-trained-models-past-present-and-future%2f&title=Survey%20-%20Pre-Trained%20Models%20-%20Past%2c%20Present%20and%20Future"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey - Pre-Trained Models - Past, Present and Future on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fsurvey-pre-trained-models-past-present-and-future%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey - Pre-Trained Models - Past, Present and Future on whatsapp" href="https://api.whatsapp.com/send?text=Survey%20-%20Pre-Trained%20Models%20-%20Past%2c%20Present%20and%20Future%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fsurvey-pre-trained-models-past-present-and-future%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey - Pre-Trained Models - Past, Present and Future on telegram" href="https://telegram.me/share/url?text=Survey%20-%20Pre-Trained%20Models%20-%20Past%2c%20Present%20and%20Future&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fsurvey-pre-trained-models-past-present-and-future%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey - Pre-Trained Models - Past, Present and Future on ycombinator" href="https://news.ycombinator.com/submitlink?t=Survey%20-%20Pre-Trained%20Models%20-%20Past%2c%20Present%20and%20Future&u=https%3a%2f%2fcongchan.github.io%2fposts%2fsurvey-pre-trained-models-past-present-and-future%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>