<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点 | Cong's Log</title><meta name=keywords content="LLM,Alignment,RLHF,Truthfulness,Behavior Cloning(BC)"><meta name=description content="Cong Chen
University of Edinburgh
John Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点1，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展2，同时南大的俞扬教授也对BC和RL的对比进行了观点分享3。
归纳的核心观点有三个：

Behavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决：

核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。
除此之外，RL还允许负反馈，而 negative feedback is much more powerful


基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易
未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing

详细分享 - by John Schulman
Why there is Hallucination


Is “if a model know something” a meaningful question?

RL is the correct ways


Long form QA (LFQA)  is much difficult that short QA"><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点"><meta property="og:description" content="Cong Chen
University of Edinburgh
John Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点1，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展2，同时南大的俞扬教授也对BC和RL的对比进行了观点分享3。
归纳的核心观点有三个：
Behavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决： 核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。 除此之外，RL还允许负反馈，而 negative feedback is much more powerful 基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易 未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing 详细分享 - by John Schulman Why there is Hallucination Is “if a model know something” a meaningful question? RL is the correct ways Long form QA (LFQA) is much difficult that short QA"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-30T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-30T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Alignment"><meta property="article:tag" content="RLHF"><meta property="article:tag" content="Truthfulness"><meta property="article:tag" content="Behavior Cloning(BC)"><meta name=twitter:card content="summary"><meta name=twitter:title content="John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点"><meta name=twitter:description content="Cong Chen
University of Edinburgh
John Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点1，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展2，同时南大的俞扬教授也对BC和RL的对比进行了观点分享3。
归纳的核心观点有三个：

Behavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决：

核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。
除此之外，RL还允许负反馈，而 negative feedback is much more powerful


基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易
未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing

详细分享 - by John Schulman
Why there is Hallucination


Is “if a model know something” a meaningful question?

RL is the correct ways


Long form QA (LFQA)  is much difficult that short QA"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点","item":"https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点","name":"John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点","description":"Cong Chen\nUniversity of Edinburgh\nJohn Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点1，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展2，同时南大的俞扬教授也对BC和RL的对比进行了观点分享3。\n归纳的核心观点有三个：\nBehavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决： 核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。 除此之外，RL还允许负反馈，而 negative feedback is much more powerful 基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易 未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing 详细分享 - by John Schulman Why there is Hallucination Is “if a model know something” a meaningful question? RL is the correct ways Long form QA (LFQA) is much difficult that short QA\n","keywords":["LLM","Alignment","RLHF","Truthfulness","Behavior Cloning(BC)"],"articleBody":"Cong Chen\nUniversity of Edinburgh\nJohn Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点1，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展2，同时南大的俞扬教授也对BC和RL的对比进行了观点分享3。\n归纳的核心观点有三个：\nBehavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决： 核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。 除此之外，RL还允许负反馈，而 negative feedback is much more powerful 基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易 未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing 详细分享 - by John Schulman Why there is Hallucination Is “if a model know something” a meaningful question? RL is the correct ways Long form QA (LFQA) is much difficult that short QA\nA rising challenge in NLP is long-form question-answering (LFQA), in which a paragraph-length answer is generated in response to an open-ended question. LFQA systems have the potential to become one of the main ways people learn about the world, but currently lag behind human performance.\nBut ChatGPT has been trained via RL, why does it still Hallucinate / make false claims? Model has to guess sometimes: when it has to output a lot of details, sometimes it has to hedge Ranking based reward model doesn’t impose correct penalty: only measure if one is better than the other, but does not measure how much better, and how confident the model is. label errors: not always guarantee to provide enough information to the labelers when labeling. Such as coding problems. Avoid Hallucinate via Retrieval Why we need retrieval:\nup to date events and knowledge that happens after the models were trained. Information not in the pre-training (e.g., private corpus) verifiability Open Problems Let multiple agents collaborate with each other\nJohn Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges - YouTube ↩︎\nhttps://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81 ↩︎\nhttps://www.zhihu.com/question/596230048/answer/2990254878 ↩︎\n","wordCount":"252","inLanguage":"en","datePublished":"2023-04-30T00:00:00Z","dateModified":"2023-04-30T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点</h1><div class=post-meta><span title='2023-04-30 00:00:00 +0000 UTC'>2023-04-30</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/llm-John-Schulman-Yoav-Goldberg-thoughts-about-Behavior-Cloning-RL-and-Truthfulness.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e8%af%a6%e7%bb%86%e5%88%86%e4%ba%ab---by-john-schulman aria-label="详细分享 - by John Schulman">详细分享 - by John Schulman</a><ul><li><a href=#why-there-is-hallucination aria-label="Why there is Hallucination">Why there is Hallucination</a></li><li><a href=#is-if-a-model-know-something-a-meaningful-question aria-label="Is “if a model know something” a meaningful question?">Is “if a model know something” a meaningful question?</a></li><li><a href=#rl-is-the-correct-ways aria-label="RL is the correct ways">RL is the correct ways</a></li><li><a href=#but-chatgpt-has-been-trained-via-rl-why-does-it-still-hallucinate--make-false-claims aria-label="But ChatGPT has been trained via RL, why does it still Hallucinate / make false claims?">But ChatGPT has been trained via RL, why does it still Hallucinate / make false claims?</a></li><li><a href=#avoid-hallucinate-via-retrieval aria-label="Avoid Hallucinate via Retrieval">Avoid Hallucinate via Retrieval</a></li></ul></li><li><a href=#open--problems aria-label="Open  Problems">Open Problems</a></li></ul></div></details></div><div class=post-content><p><a href=https://congchan.github.io/>Cong Chen</a><br>University of Edinburgh</p><p>John Schulman最近在Berkeley分享了关于BC、RLHF and Truthfulness的观点<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，Yoav Goldberg也针对John Schulman的观点进行了总结和扩展<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>，同时南大的俞扬教授也对BC和RL的对比进行了观点分享<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>。</p><p>归纳的核心观点有三个：</p><ul><li>Behavior Cloning（BC, learning from demonstrations, or SFT）是最Effective的方法。RLHF过程中重度使用了BC，包括冷启动和奖励模型训练都用了BC。虽然BC更有效，相比RL也更容易work，但BC因为自身局限性，有一些固有的问题无法解决：<ul><li>核心问题是，BC训练越泛化意味着LLM越会Hallucination和撒谎；而我们想鼓励LLM根据它的内部知识来回答，问题是我们不知道其内部知识包含什么，所以要利用RLHF让LLM知道什么问题是超过自己的知识范围的（让模型知道自己不知道）。</li><li>除此之外，RL还允许负反馈，而 negative feedback is much more powerful</li></ul></li><li>基于 Ranking 的 Reward学习虽然不够好，但是实践起来更容易</li><li>未来优化方向：当LLM知道自己不知道时，目前更多的是诚实地表达“I dont know”来拒识，OpenAI的方向是让LLM尝试去搜索外部知识，生成更可信、带citing source的回答，也就是从Honest进化到Truthfulness。参考下面的 ChatGPT Browsing</li></ul><h2 id=详细分享---by-john-schulman>详细分享 - by John Schulman<a hidden class=anchor aria-hidden=true href=#详细分享---by-john-schulman>#</a></h2><h3 id=why-there-is-hallucination>Why there is Hallucination<a hidden class=anchor aria-hidden=true href=#why-there-is-hallucination>#</a></h3><p><img alt=language-model-hallucination loading=lazy src=/images/John-Schulman/John-Schulman-0-language-model-hallucination.png></p><p><img alt=Hallucination-and-Behavior-Cloning loading=lazy src=/images/John-Schulman/John-Schulman-1-Hallucination-and-Behavior-Cloning.png></p><h3 id=is-if-a-model-know-something-a-meaningful-question>Is “if a model know something” a meaningful question?<a hidden class=anchor aria-hidden=true href=#is-if-a-model-know-something-a-meaningful-question>#</a></h3><p><img alt=Does-Model-Know-About-Its-Uncertainty loading=lazy src=/images/John-Schulman/John-Schulman-2-Does-Model-Know-About-Its-Uncertainty.png></p><h3 id=rl-is-the-correct-ways>RL is the correct ways<a hidden class=anchor aria-hidden=true href=#rl-is-the-correct-ways>#</a></h3><p><img alt=John-Schulman-3 loading=lazy src=/images/John-Schulman/John-Schulman-3.png></p><p><img alt=John-Schulman-4 loading=lazy src=/images/John-Schulman/John-Schulman-4.png></p><p>Long form QA (LFQA) is much difficult that short QA</p><blockquote><p>A rising challenge in NLP is long-form question-answering (LFQA), in which a paragraph-length answer is generated in response to an open-ended question. LFQA systems have the potential to become one of the main ways people learn about the world, but currently lag behind human performance.</p></blockquote><p><img alt=John-Schulman-5 loading=lazy src=/images/John-Schulman/John-Schulman-5.png></p><h3 id=but-chatgpt-has-been-trained-via-rl-why-does-it-still-hallucinate--make-false-claims>But ChatGPT has been trained via RL, why does it still Hallucinate / make false claims?<a hidden class=anchor aria-hidden=true href=#but-chatgpt-has-been-trained-via-rl-why-does-it-still-hallucinate--make-false-claims>#</a></h3><ul><li>Model has to guess sometimes: when it has to output a lot of details, sometimes it has to hedge</li><li>Ranking based reward model doesn’t impose correct penalty: only measure if one is better than the other, but does not measure <strong>how much</strong> better, and how confident the model is.</li><li>label errors: not always guarantee to provide enough information to the labelers when labeling. Such as coding problems.</li></ul><h3 id=avoid-hallucinate-via-retrieval>Avoid Hallucinate via Retrieval<a hidden class=anchor aria-hidden=true href=#avoid-hallucinate-via-retrieval>#</a></h3><p>Why we need retrieval:</p><ul><li>up to date events and knowledge that happens after the models were trained.</li><li>Information not in the pre-training (e.g., private corpus)</li><li>verifiability</li></ul><p><img alt=John-Schulman-6 loading=lazy src=/images/John-Schulman/John-Schulman-6.png></p><p><img alt=John-Schulman-7 loading=lazy src=/images/John-Schulman/John-Schulman-7.png></p><p><img alt=John-Schulman-8 loading=lazy src=/images/John-Schulman/John-Schulman-8.png></p><p><img alt=John-Schulman-9 loading=lazy src=/images/John-Schulman/John-Schulman-9.png></p><p><img alt=John-Schulman-10 loading=lazy src=/images/John-Schulman/John-Schulman-10.png></p><p><img alt=John-Schulman-11 loading=lazy src=/images/John-Schulman/John-Schulman-11.png></p><h2 id=open--problems>Open Problems<a hidden class=anchor aria-hidden=true href=#open--problems>#</a></h2><p><img alt=John-Schulman-12 loading=lazy src=/images/John-Schulman/John-Schulman-12.png></p><p>Let multiple agents collaborate with each other</p><p><img alt=John-Schulman-13 loading=lazy src=/images/John-Schulman/John-Schulman-13.png></p><p><img alt=John-Schulman-14 loading=lazy src=/images/John-Schulman/John-Schulman-14.png></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href="https://www.youtube.com/watch?v=hhiLw5Q_UFg">John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges - YouTube</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81>https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://www.zhihu.com/question/596230048/answer/2990254878>https://www.zhihu.com/question/596230048/answer/2990254878</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/llm/>LLM</a></li><li><a href=https://congchan.github.io/tags/alignment/>Alignment</a></li><li><a href=https://congchan.github.io/tags/rlhf/>RLHF</a></li><li><a href=https://congchan.github.io/tags/truthfulness/>Truthfulness</a></li><li><a href=https://congchan.github.io/tags/behavior-cloningbc/>Behavior Cloning(BC)</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/paper-reading-lets-verify-step-by-step/><span class=title>« Prev</span><br><span>Paper Reading - Let’s Verify Step by Step</span>
</a><a class=next href=https://congchan.github.io/posts/paper-reading-complexity-based-prompting-for-multi-step-reasoning/><span class=title>Next »</span><br><span>Paper Reading - Complexity-Based Prompting for Multi-Step Reasoning</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点 on x" href="https://x.com/intent/tweet/?text=John%20Schulman%e5%92%8cYoav%20Goldberg%e5%85%b3%e4%ba%8eBehavior%20Cloning%28BC%29%e3%80%81RL%20and%20Truthfulness%e7%9a%84%e8%a7%82%e7%82%b9&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fjohn-schulman%25E5%2592%258Cyoav-goldberg%25E5%2585%25B3%25E4%25BA%258Ebehavior-cloningbcrl-and-truthfulness%25E7%259A%2584%25E8%25A7%2582%25E7%2582%25B9%2f&amp;hashtags=LLM%2cAlignment%2cRLHF%2cTruthfulness%2cBehaviorCloning%28BC%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fjohn-schulman%25E5%2592%258Cyoav-goldberg%25E5%2585%25B3%25E4%25BA%258Ebehavior-cloningbcrl-and-truthfulness%25E7%259A%2584%25E8%25A7%2582%25E7%2582%25B9%2f&amp;title=John%20Schulman%e5%92%8cYoav%20Goldberg%e5%85%b3%e4%ba%8eBehavior%20Cloning%28BC%29%e3%80%81RL%20and%20Truthfulness%e7%9a%84%e8%a7%82%e7%82%b9&amp;summary=John%20Schulman%e5%92%8cYoav%20Goldberg%e5%85%b3%e4%ba%8eBehavior%20Cloning%28BC%29%e3%80%81RL%20and%20Truthfulness%e7%9a%84%e8%a7%82%e7%82%b9&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fjohn-schulman%25E5%2592%258Cyoav-goldberg%25E5%2585%25B3%25E4%25BA%258Ebehavior-cloningbcrl-and-truthfulness%25E7%259A%2584%25E8%25A7%2582%25E7%2582%25B9%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fjohn-schulman%25E5%2592%258Cyoav-goldberg%25E5%2585%25B3%25E4%25BA%258Ebehavior-cloningbcrl-and-truthfulness%25E7%259A%2584%25E8%25A7%2582%25E7%2582%25B9%2f&title=John%20Schulman%e5%92%8cYoav%20Goldberg%e5%85%b3%e4%ba%8eBehavior%20Cloning%28BC%29%e3%80%81RL%20and%20Truthfulness%e7%9a%84%e8%a7%82%e7%82%b9"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fjohn-schulman%25E5%2592%258Cyoav-goldberg%25E5%2585%25B3%25E4%25BA%258Ebehavior-cloningbcrl-and-truthfulness%25E7%259A%2584%25E8%25A7%2582%25E7%2582%25B9%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点 on whatsapp" href="https://api.whatsapp.com/send?text=John%20Schulman%e5%92%8cYoav%20Goldberg%e5%85%b3%e4%ba%8eBehavior%20Cloning%28BC%29%e3%80%81RL%20and%20Truthfulness%e7%9a%84%e8%a7%82%e7%82%b9%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fjohn-schulman%25E5%2592%258Cyoav-goldberg%25E5%2585%25B3%25E4%25BA%258Ebehavior-cloningbcrl-and-truthfulness%25E7%259A%2584%25E8%25A7%2582%25E7%2582%25B9%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点 on telegram" href="https://telegram.me/share/url?text=John%20Schulman%e5%92%8cYoav%20Goldberg%e5%85%b3%e4%ba%8eBehavior%20Cloning%28BC%29%e3%80%81RL%20and%20Truthfulness%e7%9a%84%e8%a7%82%e7%82%b9&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fjohn-schulman%25E5%2592%258Cyoav-goldberg%25E5%2585%25B3%25E4%25BA%258Ebehavior-cloningbcrl-and-truthfulness%25E7%259A%2584%25E8%25A7%2582%25E7%2582%25B9%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点 on ycombinator" href="https://news.ycombinator.com/submitlink?t=John%20Schulman%e5%92%8cYoav%20Goldberg%e5%85%b3%e4%ba%8eBehavior%20Cloning%28BC%29%e3%80%81RL%20and%20Truthfulness%e7%9a%84%e8%a7%82%e7%82%b9&u=https%3a%2f%2fcongchan.github.io%2fposts%2fjohn-schulman%25E5%2592%258Cyoav-goldberg%25E5%2585%25B3%25E4%25BA%258Ebehavior-cloningbcrl-and-truthfulness%25E7%259A%2584%25E8%25A7%2582%25E7%2582%25B9%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>