<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Paper Reading - Let’s Verify Step by Step | Cong's Log</title><meta name=keywords content="Readings,2023,Large Language Model,Reward Modeling"><meta name=description content="TLDR
In order to train more dependable models, there are two known options: outcome supervision, which gives feedback on the final result, and process supervision, which provides feedback on each intermediate reasoning step.
This papers provides two finding:

The use of process supervision yields significantly better results than outcome supervision when training models to solve problems from the challenging MATH dataset.
The efficacy of process supervision is significantly improved by active learning.

The exclusive focus of this paper is to provide insights on training the most reliable reward model."><meta name=author content="Cong"><link rel=canonical href=https://congchan.github.io/posts/paper-reading-lets-verify-step-by-step/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/paper-reading-lets-verify-step-by-step/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/paper-reading-lets-verify-step-by-step/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Paper Reading - Let’s Verify Step by Step"><meta property="og:description" content="TLDR In order to train more dependable models, there are two known options: outcome supervision, which gives feedback on the final result, and process supervision, which provides feedback on each intermediate reasoning step.
This papers provides two finding:
The use of process supervision yields significantly better results than outcome supervision when training models to solve problems from the challenging MATH dataset. The efficacy of process supervision is significantly improved by active learning. The exclusive focus of this paper is to provide insights on training the most reliable reward model."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-18T00:00:00+00:00"><meta property="article:tag" content="Readings"><meta property="article:tag" content="2023"><meta property="article:tag" content="Large Language Model"><meta property="article:tag" content="Reward Modeling"><meta name=twitter:card content="summary"><meta name=twitter:title content="Paper Reading - Let’s Verify Step by Step"><meta name=twitter:description content="TLDR
In order to train more dependable models, there are two known options: outcome supervision, which gives feedback on the final result, and process supervision, which provides feedback on each intermediate reasoning step.
This papers provides two finding:

The use of process supervision yields significantly better results than outcome supervision when training models to solve problems from the challenging MATH dataset.
The efficacy of process supervision is significantly improved by active learning.

The exclusive focus of this paper is to provide insights on training the most reliable reward model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Paper Reading - Let’s Verify Step by Step","item":"https://congchan.github.io/posts/paper-reading-lets-verify-step-by-step/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Paper Reading - Let’s Verify Step by Step","name":"Paper Reading - Let’s Verify Step by Step","description":"TLDR In order to train more dependable models, there are two known options: outcome supervision, which gives feedback on the final result, and process supervision, which provides feedback on each intermediate reasoning step.\nThis papers provides two finding:\nThe use of process supervision yields significantly better results than outcome supervision when training models to solve problems from the challenging MATH dataset. The efficacy of process supervision is significantly improved by active learning. The exclusive focus of this paper is to provide insights on training the most reliable reward model.\n","keywords":["Readings","2023","Large Language Model","Reward Modeling"],"articleBody":"TLDR In order to train more dependable models, there are two known options: outcome supervision, which gives feedback on the final result, and process supervision, which provides feedback on each intermediate reasoning step.\nThis papers provides two finding:\nThe use of process supervision yields significantly better results than outcome supervision when training models to solve problems from the challenging MATH dataset. The efficacy of process supervision is significantly improved by active learning. The exclusive focus of this paper is to provide insights on training the most reliable reward model.\nPapers: Lightman, Hunter, et al. Let’s Verify Step by Step. arXiv:2305.20050, arXiv, 31 May 2023. arXiv.org, http://arxiv.org/abs/2305.20050.\nWhy Even the most advanced models are susceptible to generating false information, as they tend to create facts when they are uncertain (Bubeck et al., 2023). These hallucinations (Maynez et al., 2020) can be especially problematic in areas that involve multi-step reasoning, as a single logical mistake can disrupt a much larger solution.\nMethods To deal with hallucinations, one effective approach is to train reward models to differentiate between desirable and undesirable outputs. These reward models can then be utilized in a reinforcement learning pipeline or to conduct search via rejection sampling.\nOutcome-supervised reward models (ORMs) are trained solely on the final result of the model’s chain-of-thought. On the other hand, process-supervised reward models (PRMs) receive feedback for each step in the chain-of-thought.\nThis paper prefer PRMs for several reasons:\nProcess supervision provides more accurate feedback as it identifies the exact location of any errors that may occur. It is easier for humans to interpret. It directly incentivizes models to follow a human-endorsed chain-of-thought. In the field of logical reasoning, models trained with outcome supervision often use incorrect reasoning to arrive at the correct final answer (Zelikman et al., 2022; Creswell et al., 2022). Process supervision has been demonstrated to alleviate this misaligned behavior (Uesato et al., 2022). How to do PRMs The process supervision approach relies on human data-labelers to provide supervision by labeling the correctness of each step in the model-generated solutions.\nTo collect the process supervision data, present step-by-step solutions to MATH problems sampled by the large-scale generator to human data-labelers. Their task is to assign each step in the solution a label of positive, negative, or neutral. A positive label indicates that the step is correct and reasonable, while a negative label indicates that the step is either incorrect or unreasonable. A neutral label indicates ambiguity. They aim to surface solutions that are more likely to deceive their best reward model. To achieve this, they strategically select which solutions to show data-labelers. Specifically, they choose to surface convincing wrong-answer solutions. They use the term convincing to refer to solutions that are rated highly by current best PRM, and they use wrong-answer to refer to solutions that reach an incorrect final answer. The PRMs are trained to predict the correctness of each step after the last token in each step. This prediction takes the form of a single token, and maximize the log-likelihood of these target tokens during training. Define the PRM score for a solution as the probability that every step is correct under the PRM. This can be implemented as the product of the correctness probabilities for each step. When providing process supervision, they deliberately choose to supervise only up to the first incorrect step. This simplifies the comparison between outcome and process supervision. To reduce the reliance on expensive human feedback, they employ a large-scale model to oversee the training of smaller-scale models.\nActive Learning Generally, the active learning method’s performance is not stable or predictable. However, it is still worth taking a closer look. They first train a small-scale reward model, PRMselector, using a single sample from each problem, and then use this model to evaluate 1000 samples per problem. For training larger reward models, they select N samples per problem, with 80% being the most convincing wrong-answer samples (according to PRMselector), and 20% being the most convincing samples that remain (right or wrong-answer). They score the selected samples using PRMlarge and use those scores for training. This process ensures that all samples are relatively convincing under PRMselector, that a large fraction are known to contain at least one mistake, and that the dataset is not heavily biased towards wrong-answer solutions.\nModels All the large-scale models are fine-tuned from the base GPT-4 model (OpenAI, 2023), which was pre-trained solely to predict the next token and not with any Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017). The small-scale base models are designed similarly to GPT-4, but they were pre-trained with approximately 200 times less compute. Additionally, they fine-tune all models on a dataset of around 1.5 billion math-relevant tokens, which they refer to as MathMix. Yet this dataset are not open-sourced. To simplify parsing individual steps, they train the generator to produce solutions in a step-by-step format separated by newlines. Specifically, they few-shot generate solutions to MATH training problems, filter to those that reach the correct final answer, and fine-tune the base model on this dataset for a single epoch. This step is not intended to teach the generator new skills, but rather to train it to produce solutions in the desired format. Results/Analysis/Findings Task/Dataset: The process supervision dataset, PRM800K, which includes 800K step-level labels across 75K solutions to 12K problems.\nEvaluation: To evaluate the effectiveness of a reward model, they test its ability to perform a best-of-N search over uniformly sampled solutions from the generator. For each test problem, they select the solution with the highest rank determined by the reward model, automatically grade it based on its final answer, and report the fraction of correct solutions. A more reliable reward model will select the correct solution more frequently. Out-of-distribution generalization (OOD): the PRM model can tolerate a moderate amount of distribution shift, and its strong performance remains consistent even when tested on new and unfamiliar questions. Alignment Impact: Process supervision is more likely to generate interpretable reasoning since it encourages models to follow a process that is endorsed by humans. Process supervision is also inherently safer as it directly rewards an aligned chain of thought, rather than relying on outcomes as a proxy for aligned behavior.\nIn conclusion:\nprocess supervision can train reward models that are much more reliable than those trained with outcome supervision. Their state-of-the-art PRM model can solve 78.2% of problems from a representative subset of the MATH test set. A large reward model can accurately approximate human supervision for smaller reward models, and it can be used to efficiently conduct large-scale data collection ablations. Reference S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. A. Cotra. Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover. https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/ without-specific-countermeasures-the-easiest-path-to, 2022. A. Creswell, M. Shanahan, and I. Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022. T. Everitt, V. Krakovna, L. Orseau, M. Hutter, and S. Legg. Reinforcement learning with a corrupted reward channel. arXiv preprint arXiv:1705.08417, 2017. L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint arXiv:2210.10760, 2022. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022. Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022. J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020. R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332, 2021. E. Nichols, L. Gao, and R. Gomez. Collaborative storytelling with large-scale neural language models. In Proceedings of the 13th ACM SIGGRAPH Conference on Motion, Interaction and Games, pages 1–10, 2020. M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. J. Shen, Y. Yin, L. Li, L. Shang, X. Jiang, M. Zhang, and Q. Liu. Generate \u0026 rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021. N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020. A. Stuhlm ̈ uller and J. Byun. Supervise process, not outcomes. https://ought. org/updates/2022-04-06-process, 2022. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35: 15476–15488, 2022. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. ","wordCount":"1783","inLanguage":"en","datePublished":"2023-06-18T00:00:00Z","dateModified":"2023-06-18T00:00:00Z","author":{"@type":"Person","name":"Cong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/paper-reading-lets-verify-step-by-step/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Paper Reading - Let’s Verify Step by Step</h1><div class=post-meta><span title='2023-06-18 00:00:00 +0000 UTC'>2023-06-18</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Cong&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/paper-Let%e2%80%99s-Verify-Step-by-Step.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tldr aria-label=TLDR>TLDR</a></li><li><a href=#why aria-label=Why>Why</a></li><li><a href=#methods aria-label=Methods>Methods</a><ul><li><a href=#how-to-do-prms aria-label="How to do PRMs">How to do PRMs</a></li><li><a href=#active-learning aria-label="Active Learning">Active Learning</a></li><li><a href=#models aria-label=Models>Models</a></li></ul></li><li><a href=#resultsanalysisfindings aria-label=Results/Analysis/Findings>Results/Analysis/Findings</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><h1 id=tldr>TLDR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h1><p>In order to train more dependable models, there are two known options: outcome supervision, which gives feedback on the final result, and process supervision, which provides feedback on each intermediate reasoning step.</p><p>This papers provides two finding:</p><ol><li>The use of process supervision yields significantly better results than outcome supervision when training models to solve problems from the challenging MATH dataset.</li><li>The efficacy of process supervision is significantly improved by active learning.</li></ol><p>The exclusive focus of this paper is to provide insights on training the most reliable reward model.</p><p>Papers: Lightman, Hunter, et al. Let’s Verify Step by Step. arXiv:2305.20050, arXiv, 31 May 2023. arXiv.org, <a href=http://arxiv.org/abs/2305.20050>http://arxiv.org/abs/2305.20050</a>.</p><h1 id=why>Why<a hidden class=anchor aria-hidden=true href=#why>#</a></h1><p>Even the most advanced models are susceptible to generating false information, as they tend to create facts when they are uncertain (Bubeck et al., 2023). These hallucinations (Maynez et al., 2020) can be especially problematic in areas that involve multi-step reasoning, as a single logical mistake can disrupt a much larger solution.</p><h1 id=methods>Methods<a hidden class=anchor aria-hidden=true href=#methods>#</a></h1><p>To deal with hallucinations, one effective approach is to train reward models to differentiate between desirable and undesirable outputs. These reward models can then be utilized in a reinforcement learning pipeline or to conduct search via rejection sampling.</p><p>Outcome-supervised reward models (ORMs) are trained solely on the final result of the model&rsquo;s chain-of-thought. On the other hand, process-supervised reward models (PRMs) receive feedback for each step in the chain-of-thought.</p><p>This paper prefer PRMs for several reasons:</p><ul><li>Process supervision provides more accurate feedback as it identifies the exact location of any errors that may occur.</li><li>It is easier for humans to interpret.</li><li>It directly incentivizes models to follow a human-endorsed chain-of-thought.</li><li>In the field of logical reasoning, models trained with outcome supervision often use incorrect reasoning to arrive at the correct final answer (Zelikman et al., 2022; Creswell et al., 2022). Process supervision has been demonstrated to alleviate this misaligned behavior (Uesato et al., 2022).</li></ul><h2 id=how-to-do-prms>How to do PRMs<a hidden class=anchor aria-hidden=true href=#how-to-do-prms>#</a></h2><p><img loading=lazy src=/images/papers/paper13.png></p><p>The process supervision approach relies on human data-labelers to provide supervision by labeling the correctness of each step in the model-generated solutions.</p><ul><li>To collect the process supervision data, present step-by-step solutions to MATH problems sampled by the large-scale generator to human data-labelers. Their task is to assign each step in the solution a label of positive, negative, or neutral. A positive label indicates that the step is correct and reasonable, while a negative label indicates that the step is either incorrect or unreasonable. A neutral label indicates ambiguity.</li><li>They aim to surface solutions that are more likely to deceive their best reward model. To achieve this, they strategically select which solutions to show data-labelers. Specifically, they choose to surface <strong>convincing wrong-answer</strong> solutions. They use the term <strong>convincing</strong> to refer to solutions that are rated highly by current best PRM, and they use <strong>wrong-answer</strong> to refer to solutions that reach an incorrect final answer.</li><li>The PRMs are trained to predict the correctness of each step after the last token in each step. This prediction takes the form of a single token, and maximize the log-likelihood of these target tokens during training.</li><li>Define the PRM score for a solution as the probability that every step is correct under the PRM. This can be implemented as the product of the correctness probabilities for each step.</li><li>When providing process supervision, they deliberately choose to supervise only up to the first incorrect step. This simplifies the comparison between outcome and process supervision.</li></ul><p>To reduce the reliance on expensive human feedback, they employ a large-scale model to oversee the training of smaller-scale models.</p><h2 id=active-learning>Active Learning<a hidden class=anchor aria-hidden=true href=#active-learning>#</a></h2><p>Generally, the active learning method&rsquo;s performance is not stable or predictable. However, it is still worth taking a closer look. They first train a small-scale reward model, PRMselector, using a single sample from each problem, and then use this model to evaluate 1000 samples per problem. For training larger reward models, they select N samples per problem, with 80% being the most convincing wrong-answer samples (according to PRMselector), and 20% being the most convincing samples that remain (right or wrong-answer). They score the selected samples using PRMlarge and use those scores for training. This process ensures that all samples are relatively convincing under PRMselector, that a large fraction are known to contain at least one mistake, and that the dataset is not heavily biased towards wrong-answer solutions.</p><h2 id=models>Models<a hidden class=anchor aria-hidden=true href=#models>#</a></h2><ul><li>All the large-scale models are fine-tuned from the base GPT-4 model (OpenAI, 2023), which was pre-trained solely to predict the next token and not with any Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017).</li><li>The small-scale base models are designed similarly to GPT-4, but they were pre-trained with approximately 200 times less compute.</li><li>Additionally, they fine-tune all models on a dataset of around 1.5 billion math-relevant tokens, which they refer to as <strong>MathMix</strong>. Yet this dataset are not open-sourced.</li><li>To simplify parsing individual steps, they train the generator to produce solutions in a step-by-step format separated by newlines. Specifically, they few-shot generate solutions to MATH training problems, filter to those that reach the correct final answer, and fine-tune the base model on this dataset for a single epoch. This step is not intended to teach the generator new skills, but rather to train it to produce solutions in the desired format.</li></ul><h1 id=resultsanalysisfindings>Results/Analysis/Findings<a hidden class=anchor aria-hidden=true href=#resultsanalysisfindings>#</a></h1><p>Task/Dataset: The process supervision dataset, PRM800K, which includes 800K step-level labels across 75K solutions to 12K problems.</p><p>Evaluation: To evaluate the effectiveness of a reward model, they test its ability to perform a best-of-N search over uniformly sampled solutions from the generator. For each test problem, they select the solution with the highest rank determined by the reward model, automatically grade it based on its final answer, and report the fraction of correct solutions. A more reliable reward model will select the correct solution more frequently.
<img loading=lazy src=/images/papers/paper13-1.png></p><p>Out-of-distribution generalization (OOD): the PRM model can tolerate a moderate amount of distribution shift, and its strong performance remains consistent even when tested on new and unfamiliar questions.
<img loading=lazy src=/images/papers/paper13-2.png></p><p>Alignment Impact: Process supervision is more likely to generate interpretable reasoning since it encourages models to follow a process that is endorsed by humans. Process supervision is also inherently safer as it directly rewards an aligned chain of thought, rather than relying on outcomes as a proxy for aligned behavior.</p><p>In conclusion:</p><ul><li>process supervision can train reward models that are much more reliable than those trained with outcome supervision. Their state-of-the-art PRM model can solve 78.2% of problems from a representative subset of the MATH test set.</li><li>A large reward model can accurately approximate human supervision for smaller reward models, and it can be used to efficiently conduct large-scale data collection ablations.</li></ul><h1 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h1><ul><li>S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al.</li><li>A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.</li><li>P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.</li><li>K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</li><li>A. Cotra. Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover. <a href=https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/>https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/</a> without-specific-countermeasures-the-easiest-path-to, 2022.</li><li>A. Creswell, M. Shanahan, and I. Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.</li><li>T. Everitt, V. Krakovna, L. Orseau, M. Hutter, and S. Legg. Reinforcement learning with a corrupted reward channel. arXiv preprint arXiv:1705.08417, 2017.</li><li>L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint arXiv:2210.10760, 2022.</li><li>D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</li><li>T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</li><li>A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</li><li>Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.</li><li>J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.</li><li>R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</li><li>E. Nichols, L. Gao, and R. Gomez. Collaborative storytelling with large-scale neural language models. In Proceedings of the 13th ACM SIGGRAPH Conference on Motion, Interaction and Games, pages 1–10, 2020.</li><li>M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</li><li>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</li><li>L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</li><li>J. Shen, Y. Yin, L. Li, L. Shang, X. Jiang, M. Zhang, and Q. Liu. Generate & rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.</li><li>N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.</li><li>A. Stuhlm ̈ uller and J. Byun. Supervise process, not outcomes. https://ought. org/updates/2022-04-06-process, 2022.</li><li>J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.</li><li>X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</li><li>J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</li><li>E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35: 15476–15488, 2022.</li><li>D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/readings/>Readings</a></li><li><a href=https://congchan.github.io/tags/2023/>2023</a></li><li><a href=https://congchan.github.io/tags/large-language-model/>Large Language Model</a></li><li><a href=https://congchan.github.io/tags/reward-modeling/>Reward Modeling</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/paper-reading-constitutional-ai/><span class=title>« Prev</span><br><span>Paper Reading - Constitutional AI</span>
</a><a class=next href=https://congchan.github.io/posts/john-schulman%E5%92%8Cyoav-goldberg%E5%85%B3%E4%BA%8Ebehavior-cloningbcrl-and-truthfulness%E7%9A%84%E8%A7%82%E7%82%B9/><span class=title>Next »</span><br><span>John Schulman和Yoav Goldberg关于Behavior Cloning(BC)、RL and Truthfulness的观点</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Paper Reading - Let’s Verify Step by Step on x" href="https://x.com/intent/tweet/?text=Paper%20Reading%20-%20Let%e2%80%99s%20Verify%20Step%20by%20Step&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fpaper-reading-lets-verify-step-by-step%2f&amp;hashtags=Readings%2c2023%2cLargeLanguageModel%2cRewardModeling"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Paper Reading - Let’s Verify Step by Step on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fpaper-reading-lets-verify-step-by-step%2f&amp;title=Paper%20Reading%20-%20Let%e2%80%99s%20Verify%20Step%20by%20Step&amp;summary=Paper%20Reading%20-%20Let%e2%80%99s%20Verify%20Step%20by%20Step&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fpaper-reading-lets-verify-step-by-step%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Paper Reading - Let’s Verify Step by Step on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fpaper-reading-lets-verify-step-by-step%2f&title=Paper%20Reading%20-%20Let%e2%80%99s%20Verify%20Step%20by%20Step"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Paper Reading - Let’s Verify Step by Step on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fpaper-reading-lets-verify-step-by-step%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Paper Reading - Let’s Verify Step by Step on whatsapp" href="https://api.whatsapp.com/send?text=Paper%20Reading%20-%20Let%e2%80%99s%20Verify%20Step%20by%20Step%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fpaper-reading-lets-verify-step-by-step%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Paper Reading - Let’s Verify Step by Step on telegram" href="https://telegram.me/share/url?text=Paper%20Reading%20-%20Let%e2%80%99s%20Verify%20Step%20by%20Step&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fpaper-reading-lets-verify-step-by-step%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Paper Reading - Let’s Verify Step by Step on ycombinator" href="https://news.ycombinator.com/submitlink?t=Paper%20Reading%20-%20Let%e2%80%99s%20Verify%20Step%20by%20Step&u=https%3a%2f%2fcongchan.github.io%2fposts%2fpaper-reading-lets-verify-step-by-step%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>