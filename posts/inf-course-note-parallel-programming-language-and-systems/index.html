<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Inf Course Note - Parallel Programming Language and Systems | Cong's Log</title><meta name=keywords content="Software Engineer,Parallelism & Concurrency,Java,C,course-note,Inf Course Note"><meta name=description content="爱丁堡大学信息学院课程笔记 Parallel Programming Language and Systems, Informatics, University of Edinburgh
Reference:
http://www.inf.ed.ac.uk/teaching/courses/ppls/
CMU 15213: Introduction to Computer Systems (ICS)
Computer Systems: A Programmer&rsquo;s Perspective
A Comprehensive MPI Tutorial Resource
A chapter on MPI from Ian Foster&rsquo;s online Book Designing and Building Parallel Programs

Introduction to parallel computer architecture
Covering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model."><meta name=author content="Cong Chan"><link rel=canonical href=https://congchan.github.io/posts/inf-course-note-parallel-programming-language-and-systems/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/inf-course-note-parallel-programming-language-and-systems/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/inf-course-note-parallel-programming-language-and-systems/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Inf Course Note - Parallel Programming Language and Systems"><meta property="og:description" content="爱丁堡大学信息学院课程笔记 Parallel Programming Language and Systems, Informatics, University of Edinburgh
Reference: http://www.inf.ed.ac.uk/teaching/courses/ppls/ CMU 15213: Introduction to Computer Systems (ICS) Computer Systems: A Programmer’s Perspective A Comprehensive MPI Tutorial Resource A chapter on MPI from Ian Foster’s online Book Designing and Building Parallel Programs
Introduction to parallel computer architecture Covering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-06-30T00:00:00+00:00"><meta property="article:modified_time" content="2018-06-30T00:00:00+00:00"><meta property="article:tag" content="Software Engineer"><meta property="article:tag" content="Parallelism & Concurrency"><meta property="article:tag" content="Java"><meta property="article:tag" content="C"><meta property="article:tag" content="Course-Note"><meta property="article:tag" content="Inf Course Note"><meta name=twitter:card content="summary"><meta name=twitter:title content="Inf Course Note - Parallel Programming Language and Systems"><meta name=twitter:description content="爱丁堡大学信息学院课程笔记 Parallel Programming Language and Systems, Informatics, University of Edinburgh
Reference:
http://www.inf.ed.ac.uk/teaching/courses/ppls/
CMU 15213: Introduction to Computer Systems (ICS)
Computer Systems: A Programmer&rsquo;s Perspective
A Comprehensive MPI Tutorial Resource
A chapter on MPI from Ian Foster&rsquo;s online Book Designing and Building Parallel Programs

Introduction to parallel computer architecture
Covering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Inf Course Note - Parallel Programming Language and Systems","item":"https://congchan.github.io/posts/inf-course-note-parallel-programming-language-and-systems/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Inf Course Note - Parallel Programming Language and Systems","name":"Inf Course Note - Parallel Programming Language and Systems","description":"爱丁堡大学信息学院课程笔记 Parallel Programming Language and Systems, Informatics, University of Edinburgh\nReference: http://www.inf.ed.ac.uk/teaching/courses/ppls/ CMU 15213: Introduction to Computer Systems (ICS) Computer Systems: A Programmer\u0026rsquo;s Perspective A Comprehensive MPI Tutorial Resource A chapter on MPI from Ian Foster\u0026rsquo;s online Book Designing and Building Parallel Programs\nIntroduction to parallel computer architecture Covering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model.\n","keywords":["Software Engineer","Parallelism \u0026 Concurrency","Java","C","course-note","Inf Course Note"],"articleBody":"爱丁堡大学信息学院课程笔记 Parallel Programming Language and Systems, Informatics, University of Edinburgh\nReference: http://www.inf.ed.ac.uk/teaching/courses/ppls/ CMU 15213: Introduction to Computer Systems (ICS) Computer Systems: A Programmer’s Perspective A Comprehensive MPI Tutorial Resource A chapter on MPI from Ian Foster’s online Book Designing and Building Parallel Programs\nIntroduction to parallel computer architecture Covering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model.\nBridging the gap between the parallel applications and algorithms which we can design and describe in abstract terms and the parallel computer architectures (and their lowest level programming interfaces) which it is practical to construct.\nThe ability to express parallelism (a.k.a concurrency) concisely, correctly and efficiently is important in several contexts: • Performance Computing: parallelism is the means by which the execution time of computationally demanding applications can be reduced. In the era of static (or even falling) clock speeds and increasing core count, this class is entering the computing mainstream. • Distributed Computing: when concurrency is inherent in the nature of the system and we have no choice but to express and control it. • Systems Programming: when it is conceptually simpler to think of a system as being composed of concurrent components, even though these will actually be executed by time-sharing a single processor.\nParallel Architecture Two types (mainstream):\nShared Memory architectures: in which all processors can physically address the whole memory, usually with support for cache coherency (for example, a quad or oct core chip, or more expensive machines with tens or hundreds of cores) Multicomputer architectures: in which processors can only physically address their “own” memory (for example, a networked cluster of PCs), which interact with messages across the network. Increasingly, systems will span both classes (e.g. cluster of manycore, or network-onchip manycores like the Intel SCC), and incorporate other specialized, constrained parallel hardware such as GPUs.\nReal parallel machines are complex, with unforseen semantic and performance traps. We need to provide programming tools which simplify things, but without sacrificing too much performance.\nShared Memory Architectures Uniform Memory Access (UMA) architectures have all memory “equidistant” from all CPUs. For NUMA performance varies with data location. NUMA is also confusingly called Distributed Shared Memory as memory is physically distributed but logically shared. Memory consistency challenge: when, and in what order should one processor public updates to the shared memory? Exactly what and when it is permissible for each processor to see is defined by the Consistency Model, which is effectively a contract between hardware and software, must be respected by application programmers (and compiler/library writers) to ensure program correctness.\nDifferent consistency models trade off conceptual simplicity against cost (time/hardware complexity):\nSequential consistency: every processor “sees” the same sequential interleaving of the basic reads and writes. This is very intuitive, but expensive to implement. Release consistency: writes are only guaranteed to be visible after program specified synchronization points (triggered by special machine instructions). This is less intuitive, but allows faster implementations. Shared memory architectures also raise tricky performance issues: The unit of transfer between memory and cache is a cache-line or block, containing several words. False sharing occurs when two logically unconnected variables share the same cache-line. Updates to one cause remote copies of the line, including the other variable, to be invalidated.\nMulticomputer architectures Lack of any hardware integration between cache/memory system and the interconnect. Each processor only accesses its own physical address space, so no consistency issues. Information is shared by explicit, co-operative message passing Performance/correctness issues include the semantics of synchronization and constraints on message ordering.\nParallel Applications and Algorithms Three well-known parallel patterns: Bag of Tasks, Pipeline and Interacting Peers.\nHere using the co, \u003c \u003e, await notation.\n在co oc内的代码, 顺序是任意的.\n# 这里暂时用 // 表示并行的代码 co a=1; // a=2; // a=3; ## all happen at the same time, What is a in the end? oc To answer the above question, we need to define Atomic Actions: Reads and writes of single variables as being atomic. For more than one statements, if they appear to execute as a single indivisible step with no visible intermediate states, they are atomic, must be enclosed in \u003c \u003e.\na=0; co a=1; // a=2; // b=a+a; ## what is b? oc The above code has no \u003c \u003e, each value accessed in an expression is a read. Each assignment is a write. Thus, b could be 0, 1, 2, 3, or 4.\na=0; co a=1; // a=2; // ","wordCount":"13272","inLanguage":"en","datePublished":"2018-06-30T00:00:00Z","dateModified":"2018-06-30T00:00:00Z","author":{"@type":"Person","name":"Cong Chan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/inf-course-note-parallel-programming-language-and-systems/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Inf Course Note - Parallel Programming Language and Systems</h1><div class=post-meta><span title='2018-06-30 00:00:00 +0000 UTC'>2018-06-30</span>&nbsp;·&nbsp;63 min&nbsp;·&nbsp;Cong Chan&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/UoE-ppls.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction-to-parallel-computer-architecture aria-label="Introduction to parallel computer architecture">Introduction to parallel computer architecture</a></li><li><a href=#parallel-architecture aria-label="Parallel Architecture">Parallel Architecture</a><ul><li><a href=#shared-memory-architectures aria-label="Shared Memory Architectures">Shared Memory Architectures</a></li><li><a href=#multicomputer-architectures aria-label="Multicomputer architectures">Multicomputer architectures</a></li></ul></li><li><a href=#parallel-applications-and-algorithms aria-label="Parallel Applications and Algorithms">Parallel Applications and Algorithms</a><ul><li><a href=#the-bag-of-tasks aria-label="The Bag-of-Tasks">The Bag-of-Tasks</a></li><li><a href=#pipeline-patterns aria-label="Pipeline Patterns.">Pipeline Patterns.</a></li><li><a href=#interacting-peers-pattern aria-label="Interacting Peers Pattern">Interacting Peers Pattern</a></li><li><a href=#other-patterns aria-label="Other Patterns">Other Patterns</a></li></ul></li><li><a href=#shared-variable-programming aria-label="Shared Variable Programming">Shared Variable Programming</a><ul><li><a href=#mutual-exclusion aria-label="Mutual Exclusion">Mutual Exclusion</a><ul><li><a href=#critical-sections-problem aria-label="Critical Sections problem">Critical Sections problem</a><ul><li><a href=#test-and-set-ts-instruction aria-label="Test-and-Set (TS) instruction">Test-and-Set (TS) instruction</a></li><li><a href=#lamport aria-label="Lamport&rsquo;s Bakery Algorithm">Lamport&rsquo;s Bakery Algorithm</a></li></ul></li></ul></li><li><a href=#condition-synchronization aria-label="Condition Synchronization">Condition Synchronization</a><ul><li><a href=#barrier-synchronization aria-label="Barrier synchronization">Barrier synchronization</a><ul><li><a href=#sense-reversing-barrier aria-label="Sense Reversing Barrier">Sense Reversing Barrier</a></li><li><a href=#symmetric-barriers aria-label="Symmetric Barriers">Symmetric Barriers</a></li><li><a href=#dissemination-barriers aria-label="Dissemination Barriers">Dissemination Barriers</a></li></ul></li></ul></li></ul></li><li><a href=#structured-primitives aria-label="Structured Primitives">Structured Primitives</a><ul><li><a href=#semaphores-%e4%bf%a1%e5%8f%b7%e7%81%af aria-label="Semaphores 信号灯">Semaphores 信号灯</a><ul><li><a href=#semaphores-for-critical-section-mutual-exclusion aria-label="Semaphores for Critical Section (mutual exclusion)">Semaphores for Critical Section (mutual exclusion)</a></li><li><a href=#semaphores-for-barrier-synchronisation aria-label="Semaphores for Barrier Synchronisation">Semaphores for Barrier Synchronisation</a></li><li><a href=#semaphores-for-producer-consumer-buffering aria-label="Semaphores for Producer-Consumer Buffering">Semaphores for Producer-Consumer Buffering</a></li></ul></li><li><a href=#monitors aria-label=Monitors>Monitors</a></li></ul></li><li><a href=#real-shared-variable-programming-systems aria-label="Real Shared Variable Programming Systems">Real Shared Variable Programming Systems</a></li><li><a href=#posix-threads-pthread aria-label="POSIX Threads (Pthread)">POSIX Threads (Pthread)</a><ul><li><a href=#pthreads-semaphores aria-label="Pthreads semaphores">Pthreads semaphores</a></li><li><a href=#pthreads-monitors aria-label="Pthreads Monitors">Pthreads Monitors</a></li><li><a href=#memory-consistency-in-pthreads aria-label="Memory Consistency in Pthreads">Memory Consistency in Pthreads</a></li></ul></li><li><a href=#java-concurrency aria-label="Java Concurrency">Java Concurrency</a><ul><li><a href=#java-threads aria-label="Java Threads">Java Threads</a></li><li><a href=#java-monitors aria-label="Java &ldquo;Monitors&rdquo;">Java &ldquo;Monitors&rdquo;</a></li><li><a href=#the-javautilconcurrent-package aria-label="The java.util.concurrent package">The java.util.concurrent package</a></li></ul></li><li><a href=#message-passing-programming aria-label="Message Passing Programming">Message Passing Programming</a></li><li><a href=#message-passing-interface-mpi aria-label="Message Passing Interface (MPI)">Message Passing Interface (MPI)</a><ul><li><a href=#mpi-primitives-6-basics-functions aria-label="MPI Primitives (6 basics functions)">MPI Primitives (6 basics functions)</a></li><li><a href=#mpi-task-farm aria-label="MPI Task Farm">MPI Task Farm</a></li><li><a href=#send-in-standard-mode aria-label="Send in standard mode">Send in standard mode</a></li><li><a href=#receive-in-standard-mode aria-label="Receive in standard mode">Receive in standard mode</a></li><li><a href=#prime-sieve-generator aria-label="Prime Sieve Generator">Prime Sieve Generator</a></li><li><a href=#synchronization-in-mpi aria-label="Synchronization in MPI">Synchronization in MPI</a></li><li><a href=#blocking-communication-semantics-in-mpi aria-label="Blocking Communication Semantics in MPI">Blocking Communication Semantics in MPI</a></li><li><a href=#probing-for-messages aria-label="Probing for Messages">Probing for Messages</a></li><li><a href=#collective-operations aria-label="Collective Operations">Collective Operations</a></li><li><a href=#communicators aria-label=Communicators>Communicators</a></li></ul></li><li><a href=#task-and-pattern-based-models aria-label="Task and Pattern Based Models">Task and Pattern Based Models</a></li><li><a href=#threading-building-blocks aria-label="Threading Building Blocks">Threading Building Blocks</a><ul><li><a href=#tbb-parallel_for aria-label="TBB parallel_for">TBB parallel_for</a><ul><li><a href=#range-class aria-label="Range Class">Range Class</a></li><li><a href=#body-class aria-label="Body Class">Body Class</a></li></ul></li><li><a href=#tbb-partitioners aria-label="TBB Partitioners">TBB Partitioners</a></li><li><a href=#tbb-parallel_reduce-template aria-label="TBB parallel_reduce Template">TBB parallel_reduce Template</a></li><li><a href=#the-task-scheduler aria-label="The Task Scheduler">The Task Scheduler</a></li></ul></li><li><a href=#linda aria-label=Linda>Linda</a><ul><li><a href=#tuple-space aria-label="Tuple Space">Tuple Space</a></li></ul></li></ul></div></details></div><div class=post-content><p>爱丁堡大学信息学院课程笔记 Parallel Programming Language and Systems, Informatics, University of Edinburgh</p><p>Reference:
<a href=http://www.inf.ed.ac.uk/teaching/courses/ppls/>http://www.inf.ed.ac.uk/teaching/courses/ppls/</a>
<a href=http://www.cs.cmu.edu/~213/>CMU 15213: Introduction to Computer Systems (ICS)</a>
<a href=http://csapp.cs.cmu.edu/>Computer Systems: A Programmer&rsquo;s Perspective</a>
<a href=http://mpitutorial.com/>A Comprehensive MPI Tutorial Resource</a>
<a href=http://www.mcs.anl.gov/~itf/dbpp/text/node94.html#SECTION03500000000000000000>A chapter on MPI from Ian Foster&rsquo;s online Book Designing and Building Parallel Programs</a></p><h2 id=introduction-to-parallel-computer-architecture>Introduction to parallel computer architecture<a hidden class=anchor aria-hidden=true href=#introduction-to-parallel-computer-architecture>#</a></h2><p>Covering some of the nasty issues presented by the shared memory model, including weak consistency models and false sharing in the cache, and some architectural issues for the multicomputer model.</p><p>Bridging the gap between the parallel applications and algorithms which we can design and describe in abstract terms and the parallel computer architectures (and their lowest level programming interfaces) which it is practical to construct.</p><p>The ability to express parallelism (a.k.a concurrency) concisely, correctly and efficiently is important in several contexts:
• Performance Computing: parallelism is the means by which the execution time of computationally demanding applications can be reduced. In the era of static (or even falling) clock speeds and increasing core count, this class is entering the computing mainstream.
• Distributed Computing: when concurrency is inherent in the nature of the system and we have no choice but to express and control it.
• Systems Programming: when it is conceptually simpler to think of a system as being composed of concurrent components, even though these will actually be executed by time-sharing a single processor.</p><h2 id=parallel-architecture>Parallel Architecture<a hidden class=anchor aria-hidden=true href=#parallel-architecture>#</a></h2><p>Two types (mainstream):</p><ul><li>Shared Memory architectures: in which all processors can physically address the whole memory, usually with support for cache coherency (for example, a quad or oct core chip, or more expensive machines with tens or hundreds of cores)</li><li>Multicomputer architectures: in which processors can only physically address their &ldquo;own&rdquo; memory (for example, a networked cluster of PCs), which interact with messages across the network.</li></ul><p>Increasingly, systems will span both classes (e.g. cluster of manycore, or network-onchip manycores like the Intel SCC), and incorporate other specialized, constrained parallel hardware such as GPUs.</p><p>Real parallel machines are complex, with unforseen semantic and performance traps. We need to provide programming tools which simplify things, but without sacrificing too much performance.</p><h3 id=shared-memory-architectures>Shared Memory Architectures<a hidden class=anchor aria-hidden=true href=#shared-memory-architectures>#</a></h3><p>Uniform Memory Access (UMA) architectures have all memory &ldquo;equidistant&rdquo; from all CPUs.
For NUMA performance varies with data location. NUMA is also confusingly called Distributed Shared Memory as memory is physically distributed but logically shared.
<img loading=lazy src=/images/Shared_Memory_Architectures.png title="image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></p><p>Memory consistency challenge: when, and in what order should one processor public updates to the shared memory? Exactly what and when it is permissible for each processor to see is defined by the <strong>Consistency Model</strong>, which is effectively a contract between hardware and software, must be respected by application programmers (and compiler/library writers) to ensure program correctness.</p><p>Different consistency models trade off conceptual <strong>simplicity against cost</strong> (time/hardware complexity):</p><ul><li><strong>Sequential consistency</strong>: every processor &ldquo;sees&rdquo; the same sequential interleaving of the basic reads and writes. This is very intuitive, but expensive to implement.</li><li><strong>Release consistency</strong>: writes are only guaranteed to be visible after program specified synchronization points (triggered by special machine instructions). This is less intuitive, but allows faster implementations.</li></ul><p>Shared memory architectures also raise tricky performance issues: The unit of transfer between memory and cache is a cache-line or block, containing several words. <strong>False sharing</strong> occurs when two logically unconnected variables share the same cache-line. Updates to one cause remote copies of the line, including the other variable, to be invalidated.</p><h3 id=multicomputer-architectures>Multicomputer architectures<a hidden class=anchor aria-hidden=true href=#multicomputer-architectures>#</a></h3><p>Lack of any hardware integration between cache/memory system and the interconnect. Each processor only accesses its <strong>own physical address space</strong>, so no consistency issues. Information is shared by explicit, co-operative message passing
<img loading=lazy src=/images/Multicomputer_Architectures.png title="image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf">
Performance/correctness issues include the semantics of <strong>synchronization</strong> and constraints on <strong>message ordering</strong>.</p><h2 id=parallel-applications-and-algorithms>Parallel Applications and Algorithms<a hidden class=anchor aria-hidden=true href=#parallel-applications-and-algorithms>#</a></h2><p>Three well-known parallel patterns: <strong>Bag of Tasks, Pipeline and Interacting Peers</strong>.</p><p>Here using the <code>co</code>, <code>&lt; ></code>, <code>await</code> notation.</p><p>在<code>co oc</code>内的代码, 顺序是任意的.</p><pre tabindex=0><code># 这里暂时用 // 表示并行的代码
co
    a=1; // a=2; // a=3; ## all happen at the same time, What is a in the end?
oc
</code></pre><p>To answer the above question, we need to define <strong>Atomic Actions</strong>: Reads and writes of single variables as being atomic. For more than one statements, if they appear to execute as a single indivisible step with no visible intermediate states, they are atomic, must be enclosed in <code>&lt; ></code>.</p><pre tabindex=0><code>a=0;
co
    a=1; // a=2; // b=a+a; ## what is b?
oc
</code></pre><p>The above code has no <code>&lt; ></code>, each value accessed in an expression is a read. Each assignment is <code>a</code> write. Thus, <code>b</code> could be 0, 1, 2, 3, or 4.</p><pre tabindex=0><code>a=0;
co
    a=1; // a=2; // &lt;b=a+a;&gt;
oc
</code></pre><p>Now the only outcomes for b are 0, 2 or 4.</p><p><strong>Sequential memory consistency (SC)</strong>
To make agreement on such inconsistency, we define the sequential memory consistency (SC), to be consistent with the following rules:</p><ol><li>ordering of atomic actions (particularly reads and writes to memory) from any one thread have to occur in normal program order</li><li>atomic actions from different threads are interleaved arbitrarily (ie in an unpredictable sequential order, subject only to rule 1)</li></ol><p><strong>It doesn&rsquo;t mean that SC programs have to be executed sequentially</strong>!
It only means that the results we get must be the same as if the program had been executed in this way.</p><p><strong>Await</strong>
The await notation <code>&lt; await (B) S ></code> allows us to indicate that <code>S</code> must appear to be delayed until <code>B</code> is true, and must be executed within the same atomic action as a successful check of <code>B</code>.</p><pre tabindex=0><code>a=0; flag=0;
co
{a=25; flag=1;}
//
&lt;await (flag==1) x=a;&gt; ## x = 25
oc
</code></pre><p>However, it is not guaranteed that, an await statement is executed right after its condition becomes true. If other atomic actions make the condition false again, before the await executes, it will have to wait for another chance.</p><h3 id=the-bag-of-tasks>The Bag-of-Tasks<a hidden class=anchor aria-hidden=true href=#the-bag-of-tasks>#</a></h3><p>Example: Adaptive Quadrature, compute an approximation to the shaded integral by partitioning until the 梯形 trapezoidal approximation is &ldquo;good enough&rdquo;, compared with the sum of its two sub-divided trapezoidals&rsquo;s area.
<code>area = quad (a, b, f(a), f(b), (f(a)+f(b))*(b-a)/2);</code>
The recursive calls to <code>quad</code> do not interfere with each other. So we can parallelize the program by changing the calls to</p><pre tabindex=0><code># 简单地并行
co
    larea = quad(left, mid, f(left), f(mid), larea); //
    rarea = quad(mid, right, f(mid), f(right), rarea);
oc
</code></pre><p>In practice, there is very little work directly involved in each call to <code>quad</code>. The work involved in <strong>creating and scheduling a process or thread is substantial</strong> (much worse than a simple function call), program may be swamped by this overhead.</p><p>Using the Bag of Tasks pattern: a <strong>fixed number of worker processes/threads</strong> maintain and process a dynamic collection of homogeneous &ldquo;tasks&rdquo;. Execution of a particular task may lead to the <strong>creation</strong> of more task instances.</p><pre tabindex=0><code># Bag of Tasks pattern
co [w = 1 to P] {
    while (all tasks not done) {
        get a task;
        execute the task;
        possibly add new tasks to the bag;
    }
}
</code></pre><p>1, Shared bag: contains <code>task(a, b, f(a), f(b), area)</code>
2, Get a task: remove a record from the bag, either:
• adds its local area approximation to the total
• or creates two more tasks for a better approximation (by adding them to the bag).</p><p>Advantage:
1, It constraints the number of processes/threads to avoid overhead.
2, Useful for independent tasks and to implement recursive parallelism
3, <strong>Naturally load-balanced</strong>: each worker will probably complete a different number of tasks, but will do roughly the same amount of work overall.</p><p>Bag of Tasks <strong>Implementation</strong>: The challenge is to make accessing the bag much cheaper than creating a new thread. With a shared address space, a simple implementation would make the bag an atomically accessed shared data structure.</p><pre tabindex=0><code>shared int size = 1, idle = 0;
shared double total = 0.0;
bag.insert (a, b, f(a), f(b), approxarea);
co [w = 1 to P] {
    while (true) {
        &lt; idle++; &gt;
        &lt; await ( size &gt; 0 || idle == P )             ## 检测 termination
          if (size &gt; 0) {                             ## get a task
              bag.remove (left, right ...); size--; idle--;
          } else break; &gt;                             ## the work is done
        mid = (left + right)/2; ..etc..               ## compute larea, etc
        if (fabs(larea + rarea - lrarea) &gt; EPSILON) { ## create new tasks
            &lt; bag.insert (left, mid, fleft, fmid, larea);
              bag.insert (mid, right, fmid, fright, rarea);
              size = size + 2; &gt;
        } else &lt; total = total + larea + rarea; &gt;
    }
}
</code></pre><p><a href=https://www2.cs.arizona.edu/~greg/mpdbook/lectures/lec09.html>Detecting termination</a>:
不能仅仅因为 bag 空了就认为可以结束了, 因为还可能有还在工作的 workers 未来会产生新的任务. 所以需要让 workers 有能力把自己的工作完成状况告知 bag. When bag is empty AND all tasks are done; All tasks are done when all workers are waiting to get a new task.</p><p>If a bag of tasks algorithm has terminated, there are no tasks left. However, the inverse is not true. I.e. no tasks in a bag could mean that one of the workers is still processing a task which can lead to creation of multiple new tasks.
To solve this problem, workers could have the ability to notify the master/bag once they finish the current task. As a result, an implementation of bag of tasks can then contain a count of idle and active works to prevent early termination</p><p>A more sophisticated implementation (with less contention) might internally have a collection of bags, perhaps one per worker, with task-stealing to distribute the load as necessary.</p><p>With <strong>message passing</strong>, a simple scheme might allocate an explicit &ldquo;farmer&rdquo; node to maintain the bag. Again, a more sophisticated implementation could distribute the bag and the farmer, with task-stealing and termination checking via messages.</p><h3 id=pipeline-patterns>Pipeline Patterns.<a hidden class=anchor aria-hidden=true href=#pipeline-patterns>#</a></h3><p>Example: <a href=https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes>The Sieve of Eratosthenes</a> algorithms for finding all prime numbers.</p><p>To find all prime numbers in the range 2 to N. The algorithm write down all integers in the range, then repeatedly remove all multiples of the smallest remaining number. Before each removal phase, the new smallest remaining number is guaranteed to be prime.</p><p>Notice that, it is not necessarily to wait one Sieve completed then start another. As long as one Sieve stage finds out one candidate number could not be divided exactly by the sieve number, it could generate a new stage with this candidate number as Sieve. And different sieve just remove the multiples of its own Sieve number.</p><pre tabindex=0><code># a message-passing style pipeline pseudocode
main() {                                # the generator
    spawn the first sieve process;
    for (i=2; i&lt;=N; i++) {
        send i to first sieve;
    }
    send -1 to first sieve;             # a &#34;stop&#34; signal
}

sieve() {
    int myprime, candidate;
    receive myprime from predecessor and record it;
    do {
        receive candidate from predecessor;
        if (candidate == -1) {send -1 to successor if it exists}
        else if (myprime doesn&#39;t divide candidate exactly) {
            if (no successor yet) spawn successor sieve process;
            send candidate to successor sieve process;
        }
    } while (candidate != -1)
}
</code></pre><p>每一个数(2-N)都可能作为筛子, 筛掉能整除这个筛子的其他数，而筛子之间是互相独立的，所以可以以<a href="http://www.informit.com/articles/article.aspx?p=366887&amp;seqNum=8">流水线模式 pipeline patterns</a>来并行操作，动态生成筛子。最开始最小的数字<code>2</code>会成为筛子。筛子可以理解为不同的工序，其余数字从小到大逐一通过这些工序加工（在 Sieve of Eratosthenes 问题中变为筛选排查），无法被筛子整除的数字会被传递到下个筛子（如果没有下一个筛子，则以这个数字创建新的筛子），这样保证生成的筛子就都是素数了。虽然工序是按顺序过的，但是所有工序可以同时对不同的产品（数字）开工，从而达到并行目的。</p><p>For pipeline patterns, the potential concurrency can be exploited by assigning each operation (stage of the pipeline) to a different worker and having them work simultaneously, with the data elements passing from one worker to the next as operations are completed. Despite the dependencies (order constraints) of the processing steps, the pipeline threads can work in parallel by applying their processing step to different data (products).</p><p>Think of pipeline patterns as the factory assembly line. We need to pick out prime number from a range of numbers N, each number is passed into a sequence of stages, each stages checks a pass in number based on the stages&rsquo;s Sieve. The numbers that finally pass all stages without being removed is a prime number.</p><p>Pipelines are composed of a sequence of threads, in which each thread&rsquo;s input is the previous thread&rsquo;s output, (<strong>Producer-Consumer relationships</strong>).</p><p>The advantages of pipeline patterns is that construction of pipeline stages is dynamic and data-dependent.</p><p>To allow production and consumption to be loosely synchronized, we will need some buffering in the system.</p><p>The programming challenges are to ensure that no producer overwrites a buffer entry before a consumer has used it, and that no consumer tries to consume an entry which doesn&rsquo;t really exist (or re-use an already consumed entry)</p><h3 id=interacting-peers-pattern>Interacting Peers Pattern<a hidden class=anchor aria-hidden=true href=#interacting-peers-pattern>#</a></h3><p>Models of physical phenomena are often expressed as a system of partial differential equations. These can be approximately solved by &ldquo;finite difference methods&rdquo; which involve iteration on a matrix of points, in an interacting peers pattern. The &ldquo;compute&rdquo; step usually involves only a small number of neighbouring points. The termination test looks for convergence.</p><p>We could use a duplicate grid and <strong>barriers</strong> to enforce correct synchronization between iterations:</p><pre tabindex=0><code>shared real grid[n+2, n+2], newgrid[n+2, n+2];
shared bool converged; local real diff;
co [i = 1 to n, j = 1 to n] {
    initialise grid;
    do {
        barrier();                                  ## before resetting test
        converged = true;                           ## provisionally
        newgrid[i,j] = (grid[i-1,j] + grid[i+1,j] +
            grid[i,j-1] + grid[i,j+1])/4;           ## compute new value
        diff = abs (newgrid[i,j] - grid[i,j]);      ## compute local change
        barrier();                                  ## before converged update
        if (diff &gt; EPSILON) converged = false;      ## any one will do
        grid[i,j] = newgrid[i,j];                   ## copy back to real grid
        barrier();                                  ## before global check
    } while (not converged);
}
</code></pre><p>A <code>barrier()</code> in ppls makes any thread that arrive here has to wait all the other threads arriving here.</p><p>以方腔热对流的模拟计算模型为例，每个网格节点$(i,j)_{t+1}$ 的更新依赖于上一个迭代时间点的$(i,j)_t$以及其临近几个点的值，创建最多跟网格点数量一样的threads，然后并行地计算网格点的新值，更新的值用一个buffer层来缓存，用<code>barrier()</code>来保证所有网格点的更新值都计算完毕，再检查收敛情况，再用一个<code>barrier()</code>保证所有buffer层的值都更新到原网格上，再决定是否进行下一次计算。</p><p>Single Program Multiple Data (SPMD): A programming style, all processes execute more or less the same code, but on distinct partitions of the data.</p><h3 id=other-patterns>Other Patterns<a hidden class=anchor aria-hidden=true href=#other-patterns>#</a></h3><p>Other candidate patterns include MapReduce (championed by Google), Scan, Divide & Conquer, Farm as well as application domain specific operations.</p><h2 id=shared-variable-programming>Shared Variable Programming<a hidden class=anchor aria-hidden=true href=#shared-variable-programming>#</a></h2><p>In the <strong>shared-memory programming model</strong>, tasks share a common address space, which they read and write asynchronously. An advantage of this model from the programmer&rsquo;s point is that the notion of data &ldquo;ownership&rdquo; is lacking, so there is no need to specify explicitly the communication of data between tasks. Program development can often be simplified.</p><p>There are two fundamentally different synchronization in shared variable programming. <strong>Mutual Exclusion</strong> and <strong>Condition Synchronization</strong>.</p><h3 id=mutual-exclusion>Mutual Exclusion<a hidden class=anchor aria-hidden=true href=#mutual-exclusion>#</a></h3><p>Atomic actions, at most one thread is executing the critical section at a time. Prevent two or more threads from being active concurrently for some period, because their actions may interfere incorrectly. For example, we might require updates to a shared counter (e.g., count++) to execute with mutual exclusion.</p><h4 id=critical-sections-problem>Critical Sections problem<a hidden class=anchor aria-hidden=true href=#critical-sections-problem>#</a></h4><p>A simple pattern of mutual exclusion occurs in the <strong>critical section problem</strong> - when n threads execute code of the following form, in which it is essential that at most one thread is executing statements in the critical section at a time (because of potentially unsafe access to shared variables)</p><pre tabindex=0><code>co [i = 1 to n] {
    while (something) {
        lock(l);      #entry section
        critical section;
        unlock(l);    #exit section
        non-critical section;
    }
}
</code></pre><p>Design code to execute before (<strong>entry protocol</strong>) and after (<strong>exit protocol</strong>) the critical section to make the critical section <strong>atomic</strong>. If one thread lock the critical section, no one(thread) else could lock it or unlock it anymore, until the thread unlock it.</p><p>Important properties:</p><ol><li><strong>Mutual exclusion</strong>: When a thread is executing in its critical section, no other threads can be executing in their critical sections.</li><li><strong>Absence of Deadlock</strong> (or Livelock): If two or more threads are trying to enter the critical section, <strong>at least one succeeds</strong>.</li></ol><blockquote><p>A deadlock is a state in which each member of a group is waiting for some other member to take action, such as sending a message or more commonly releasing a lock, so that neither of them take action.
类似两个人相遇互不相让, 没人肯挪动.
Livelock is a condition that takes place when two or more programs change their state continuously, with neither program making progress.
类似两个人相遇同时往相同方向避让.</p></blockquote><ol start=3><li><strong>Absence of Unnecessary Delay</strong>: If a thread is trying to enter its critical section and the other threads are executing their non-critical sections, or have terminated, the first thread is not prevented from entering its critical section.</li><li><strong>Eventual Entry</strong> (No Starvation): A thread that is attempting to enter its critical section will eventually succeed. May not matter in some &ldquo;performance parallel&rdquo; programs - as long as we are making progress elsewhere.</li></ol><p>Simple implementation of each lock with a <strong>shared boolean variable</strong>: if <code>false</code>, then one locking thread can set it to <code>true</code> and be allowed to proceed. Other attempted locks must be forced to wait.</p><pre tabindex=0><code># model assumes that the l = false;
# write is already atomic
# This might fail if the model is more relaxed than SC.
lock_t l = false;
co [i = 1 to n] {
    while (something) {
        &lt; await (!l) l = true; &gt; # guarantee the others waiting
        critical section;
        l = false; # unlock the lock, open the critical section
        non-critical section;
    }
}
</code></pre><p>To implement the <code>&lt; await (!l) l = true; ></code>, we rely on some <strong>simpler atomic primitive</strong>, implemented with hardware support. There are many possibilities, including &ldquo;Fetch-and-Add&rdquo;, &ldquo;Test-and-Set&rdquo; and the &ldquo;Load-Linked, Store-Conditional&rdquo; pairing.</p><h5 id=test-and-set-ts-instruction>Test-and-Set (TS) instruction<a hidden class=anchor aria-hidden=true href=#test-and-set-ts-instruction>#</a></h5><p>Behaving like a call-by-reference function, so that the variable passed in is read from and written to, but in reality it is a single machine instruction. The key feature is that this happens (or at least, appears to happen) atomically.</p><pre tabindex=0><code># A Test-and-Set (TS) instructionW
bool TS (bool v) {
    &lt; bool initial = v;
    v = true;
    return initial; &gt;
}

lock_t l = false;
co [i = 1 to n] {
    while (something) {
        while (TS(l)) ;  ## spin lock
        critical section;
        l = false;
        non-critical section;
    }
}
</code></pre><p>This is called <strong>spin lock</strong>,</p><p>Simple spin locks don&rsquo;t make good use of the cache (those spinning Test-And-Sets play havoc with contention and coherence performance). A pragmatically better spin locks is known as <strong>Test-and-Test-and-Set</strong> - mainly spinning on a read rather than a read-write function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=n>something</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>while</span> <span class=p>(</span><span class=n>l</span> <span class=o>||</span> <span class=nf>TS</span><span class=p>(</span><span class=n>l</span><span class=p>));</span> <span class=cm>/* only TS() if l was false*/</span>
</span></span><span class=line><span class=cl>        <span class=n>critical</span> <span class=n>section</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>...</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>...</span>
</span></span></code></pre></div><p>Simply read <code>l</code> until there is a chance that a Test-and-Set might succeed.</p><p><strong>Spin lock</strong> guarantees mutual exclusion, absence of deadlock and absence of delay, but does <strong>not guarantee eventual entry</strong>.</p><h5 id=lamport><a href=https://en.wikipedia.org/wiki/Lamport%27s_bakery_algorithm>Lamport&rsquo;s Bakery Algorithm</a><a hidden class=anchor aria-hidden=true href=#lamport>#</a></h5><p>Implement critical sections using only simple atomic read and simple atomic write instructions (i.e. no need for atomic read-modify-write).</p><p>采用商店结账排队机制，顾客就是一个个threads，根据排队码，越小的优先级越高（0 除外，0 代表没有结账需求），最小的可以进入critical section。</p><p>The challenge is entry protocal, if a thread intends to access the critical section:</p><ol><li>排队取号：It sets its turn <code>turn[i] = max(turn[:])+1</code> (Threads not at or intend to access the critical section have a turn of 0)</li><li>等待叫号：This thread waits until its turn comes up (until it has the smallest turn).</li></ol><pre tabindex=0><code>int turn[n] = [0, 0, ... 0];
co [i=1 to n] {
    while (true) {
        turn[i] = max (turn[1..n]) + 1;
        for (j = 1 to n except i) {
            while ((turn[j]!=0 and (turn[i] &gt; (turn[j])) skip;
        }
        critical section;
        turn[i] = 0;
        noncritical section;
    }
}
</code></pre><p>因为<code>max (turn[1..n]) + 1</code>不是atomic的, 所以会出现问题.</p><p>问题一: if turn setting is not atomic then two (or more) threads may claim the same turn.</p><blockquote><p>两个threads在取号阶段<code>turn[i] = max(turn[:])+1</code>出现并发，两个都先<code>max</code>, 之后再<code>+1</code>.</p></blockquote><p>问题二: there is possibility that a thread can claim a lower turn than another thread which enters the critical section before it!</p><blockquote><p>两个threads在取号阶段<code>turn[i] = max(turn[:])+1</code>出现并发, 并且在两个threads分别进行<code>max</code>的间隙, 刚好在CS中的thread完成并退出CS，导致两个thread看到的<code>max</code>值不一样了. 前者比后者看到的大, 但前者却因为更早进行<code>+1</code>操作而提前进入了CS.</p></blockquote><p>举例：假如同时有三个thread A B C, A 已经在CS中(turn(A)>0)：</p><ol><li>B 先运行max比较(<code>max = turn(A)</code>),</li><li>C 在 A 退出后(<code>turn(A) = 0</code>)才进行比较(<code>max = 0</code>),</li><li>B 先进行<code>+1</code>操作(<code>turn(B) = turn(A)+1 > 1</code>),</li><li>B 进行比较后允许进入CS (<strong>此时turn(C)还是0</strong>, 0是被忽略的);</li><li>之后C才 <code>+1</code>(<code>turn(C) = 0 + 1 = 1</code>);</li><li>这样导致B的值虽然比C大, 但B还是比C先进入CS; 之后因为 C 的 turn 比较小， 所以 C 也跟着进入 CS。</li></ol><p>问题一解决方案 - 使用线程ID（绝不相同）做二次区分, 在相同 turn 的情况下，具有较低ID的 thread 有限。</p><p>问题二解决方案 - 在<code>max (turn[1..n]) + 1</code>之前先<code>turn[i] = 1;</code>.
• 这样，任何 threads 想取号都要先标记为 1
• 标记后，才有资格跟其他 thread 比较
• 以<code>max+1</code>作为号码进入队列，这样任何的可能的 turn 值都必定大于 1
• B 无法提前进入CS (<strong>此时turn(C)不再是被忽略的0, 而是最小正整数1</strong>).</p><pre tabindex=0><code>#  (x, a) &gt; (y,b) means (x&gt;y) || (x==y &amp;&amp; a&gt;b).
while (true) {
    turn[i] = 1; turn[i] = max (turn[1..n]) + 1;
    for (j = 1 to n except i) {
        while ((turn[j]!=0 and (turn[i], i) &gt; (turn[j], j)) skip;
    }
    ...
}
</code></pre><p>Lamport&rsquo;s algorithm has the strong property of guaranteeing eventual entry (unlike our spin lock versions). The algorithm is too inefficient to be practical if spin-locks are available.</p><h3 id=condition-synchronization>Condition Synchronization<a hidden class=anchor aria-hidden=true href=#condition-synchronization>#</a></h3><p>Delay an action until some condition (on the shared variables such as in producer-consumer, or with respect to the progress of other threads such as in a <strong>Barrier</strong>) becomes true.</p><h4 id=barrier-synchronization>Barrier synchronization<a hidden class=anchor aria-hidden=true href=#barrier-synchronization>#</a></h4><p><strong>Barrier synchronization</strong> is a particular pattern of condition synchronization, a kind of computation-wide waiting:</p><pre tabindex=0><code>co [i = 1 to n] {
    while (something) {
        do some work;
        wait for all n workers to get here;
    }
}
</code></pre><p>A <strong>Counter Barriers</strong></p><pre tabindex=0><code>&lt;count = count + 1;&gt;
&lt;await (count == n);&gt;
</code></pre><p>is fine as a single-use barrier, but things get more complex if (as is more likely) we need the barrier to be <strong>reusable</strong>.</p><p>改良为<code>&lt;await (count == n); count = 0;></code>也不行: an inter-iteration race, 假如<code>count == n</code>, 那么n个threads都完成了前面的statements并准备执行<code>await</code>, 但其中任何一个 thread 先执行完整个代码都使<code>count = 0</code>,这样剩余的threads就无法通过await条件了.</p><h5 id=sense-reversing-barrier>Sense Reversing Barrier<a hidden class=anchor aria-hidden=true href=#sense-reversing-barrier>#</a></h5><p>A shared variable <code>sense</code> is <strong>flipped after each use</strong> of the barrier to indicate that all threads may proceed. 关键每个 thread 都有自己的 private variable <code>mySense</code> 和 while spin lock。解决了前面的死锁问题。</p><pre tabindex=0><code>shared int count = 0; shared boolean sense = false;
co [i = 1 to n] {
    private boolean mySense = !sense; ## one per thread
    while (something) {
        do some work;
        &lt; count = count + 1;
          if (count == n) { count = 0; sense = mySense; }   ## flip sense
        &gt;
        while (sense != mySense);                           ## wait or pass
        mySense = !mySense;                                 ## flip mySense
        // 或者使用 &lt; await (sense==mySense) mySense = !sense;&gt;
    }
}
</code></pre><p>所有thread的local variable <code>mySense</code>开始都被赋值为<code>!sense</code>(<code>true</code>)，前面n-1个thread都得在内循环<code>while</code>那里等着；直到最后一个thread完成工作后, <code>if</code>条件才满足, <code>count</code>重置为<code>0</code>, <strong>反转</strong><code>sense</code>(被赋值为<code>mySense</code>也即是<code>true</code>), 之后所有threads才能结束内部<code>while</code>循环，接着再次<strong>反转</strong><code>sense</code>(被赋值为<code>!mySense</code>也即是<code>false</code>), 然后进行下一轮大循环，借此达到重复利用barrier的目的. <code>sense</code>初始值是什么无所谓, 反转才是关键.</p><p>缺点：$O(n)$效率，count次数（同步次数）正比于thread数量。</p><h5 id=symmetric-barriers>Symmetric Barriers<a hidden class=anchor aria-hidden=true href=#symmetric-barriers>#</a></h5><p>Symmetric barriers are designed to avoid the bottleneck at the counter.
通过 pair-threads barriers 多轮同步来构建一个完整的 n-threads barriers，让所有threads都知道大家已经完成任务。总共是$\log_2n$ 轮同步。每个thread在完成必要工作后, 开始进入下面的pairwise同步环节，自己(myid)的初始arrive状态为0:</p><pre tabindex=0><code># arrive[i] == 1 means arrive barrier
# there will be log_2 #threads stages,
# 每个stage代表一次pairwise同步
for [s = 0 to stages-1] {
    &lt;await (arrive[myid] == 0);&gt;   # 1
    arrive[myid] = 1;              # 2
    work out who my friend is at stage s;
    &lt;await (arrive[friend] == 1);&gt; # 3
    arrive[friend] = 0;            # 4
}
</code></pre><p>这样保证了，每个thread需要先把自己的arrive状态标记为1(#1，#2)，才可以去看同伴的状态（#3），假如同伴也是1，那么表明自己这一组是都到达了barrier状态（大家都是1），那么就会把对方的状态初始化为0 （#4），进入下一阶段，更换同伴，继续同步比较。
<img loading=lazy src=/images/symmetric_barrier.jpg title="Butterfly barrier for 8 processe. Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf">
When used as <strong>a step within a multistage symmetric barrier</strong>, 会出现问题：假如有四个thread，那么就会有两个stages：第一次是1和2同步，3和4同步。2一直没到barrier，1一直卡在#3。而3和4 同步完后开始检查1的状况，发现<code>arrive[1] = 1</code>，就运行Lines (3) and (4), 结果1就被初始化了，而2还没是没到barrier。</p><p>解决办法是给每个stage分配新的arrive变量。</p><pre tabindex=0><code>for [s = 0 to stages-1] {
    &lt;await (arrive[myid][s] == 0);&gt;
    arrive[myid][s] = 1;
    work out who my friend is at this stage;
    &lt;await (arrive[friend][s] == 1);&gt;
    arrive[friend][s] = 0;
}
</code></pre><p>这样假如出现2一直没到barrier的情况, 那么1会卡在当前stage, 1的stage+1的arrive状态就无法更新为1.</p><h5 id=dissemination-barriers>Dissemination Barriers<a hidden class=anchor aria-hidden=true href=#dissemination-barriers>#</a></h5><p>If n isn&rsquo;t a power of 2, instead of pairwise synchs, we have two partners at each stage for each thread, one incoming and one outgoing.
<img loading=lazy src=/images/dissemination_barrier.png title=" Dissemination barrier for 6 processes. Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></p><h2 id=structured-primitives>Structured Primitives<a hidden class=anchor aria-hidden=true href=#structured-primitives>#</a></h2><p>Instead of implementing directly in the user-address space, a number of more structured primitives have been devised for <strong>implementation with the assistance of the operating system</strong>, so that threads can be directly suspended and resumed by the OS&rsquo;s scheduler.</p><blockquote><p>• Machine code, instructions and data directly understandable by a CPU;
• Language primitive, the simplest element provided by a programming language;
• Primitive data type, a datatype provided by a programming language.</p></blockquote><h3 id=semaphores-信号灯>Semaphores 信号灯<a hidden class=anchor aria-hidden=true href=#semaphores-信号灯>#</a></h3><p>A semaphore is a special shared variable, accessible only through two atomic operations, <strong>P(try to decrease)</strong> and <strong>V(increase)</strong>, defined by:
P(s): <code>&lt;await (s>0) s=s-1;></code>
V(s): <code>&lt;s=s+1;></code></p><p>Property: A thread executing <code>P()</code> on a 0 valued semaphore will be suspended on a queue until after some other thread has executed a <code>V()</code>.</p><p>Application: A semaphore appears to be a simple integer. A thread waits for permission to proceed a <strong>critical section</strong>, and then signals that it has proceeded by performing a <code>P()</code> operation on the semaphore.</p><p><strong>Binary semaphore</strong>: A semaphore whose usage is organised to only ever take the value (0, 1) as a mutex 互斥.
<strong>Counting(split) semaphore</strong>: can take on arbitrary nonnegative values.</p><p>Semaphores still require careful programming: there is no explicit connection in the program source between &ldquo;matching&rdquo; semaphore operations. It is easy to get things wrong.</p><p>Similarly, there is no obvious indication of how semaphores are being used - some may be for mutual exclusion, others for condition synchronization. Again confusion is possible.</p><h4 id=semaphores-for-critical-section-mutual-exclusion>Semaphores for Critical Section (mutual exclusion)<a hidden class=anchor aria-hidden=true href=#semaphores-for-critical-section-mutual-exclusion>#</a></h4><pre tabindex=0><code>sem mutex = 1;
co [i = 1 to n] {
    while (whatever) {
        P(mutex);
        critical section;
        V(mutex);
        noncritical section;
    }
}
</code></pre><h4 id=semaphores-for-barrier-synchronisation>Semaphores for Barrier Synchronisation<a hidden class=anchor aria-hidden=true href=#semaphores-for-barrier-synchronisation>#</a></h4><p>实现 symmetric barrier: an array of arrive semaphores for each stage</p><pre tabindex=0><code>for [s = 1 to stages] {
    V(arrive[myid][s]);
    work out who my friend is at stage s;
    P(arrive[friend][s]);
}
</code></pre><h4 id=semaphores-for-producer-consumer-buffering>Semaphores for Producer-Consumer Buffering<a hidden class=anchor aria-hidden=true href=#semaphores-for-producer-consumer-buffering>#</a></h4><p>针对单个producer和consumer，控制其接触单个容量的buffer权限：一个semaphores标识buffer已满<code>full</code>，一个标识空<code>empty</code>。这种情况下，只能有一个semaphore是<code>1</code>，故称之为<strong>split binary semaphore</strong>。 <code>P(full)</code> 执行 <code>wait full > 0 : full -= 1</code>, <code>V(empty)</code>执行<code>empty += 1</code></p><pre tabindex=0><code>T buf; sem empty = 1, full = 0;
co
  co [i = 1 to M] {
      while (whatever) {
          ...produce new data locally
          P(empty);
          buf = data;                # producer
          V(full);
  }   }
//
  co [j = 1 to N] {
      while (whatever) {
          P(full);
          result = buf;              # consumer
          V(empty);
          ... handle result locally
  }   }
oc
</code></pre><p><strong>Bounded Buffer</strong>: Control access to a multi-space buffer (the producer can run ahead of the consumer up to some limit)</p><ul><li>Implement the buffer itself with an array (circular),</li><li>and two integer indices, indicating the current front and rear of the buffer and use arithmetic modulo <code>n</code> (the buffer size), so that the buffer conceptually becomes circular</li><li>For a single producer and consumer, we protect the buffer with a split &ldquo;counting&rdquo; semaphore, initialised according to the buffer size.</li><li>Think of full as counting how many space in the buffer are full, and empty as counting how many are empty</li></ul><pre tabindex=0><code>T buf[n]; int front = 0, rear = 0;
sem empty = n, full = 0;
co ## Producer
    while (whatever) {
        ...produce new data locally
        P(empty);                 # empty&gt;0, 才能生产, empty-=1
        buf[rear] = data; rear = (rear + 1) % n;
        V(full);
    }
// ## Consumer
    while (whatever) {
        P(full);                  # full&gt;0, 才能消耗, full-=1
        result = buf[front]; front = (front + 1) % n;
        V(empty);
        ... handle result locally
    }
oc
</code></pre><p><strong>Multiple Producers/Consumers</strong>: Because each producer may access the same pointer to overide each other, so as consumer. Thus we need two levels of protection.</p><ul><li>Use a split counting semaphore to avoid buffer overflow (or underflow), as previously.</li><li>Use a mutual exclusion semaphores to prevent interference between producers (and another to prevent interference between consumers). This allows up to one consumer and one producer to be actively simultaneously within a non-empty, non-full buffer.</li></ul><pre tabindex=0><code>T buf[n]; int front = 0, rear = 0; 86
sem empty = n, full = 0, mutexP = 1, mutexC = 1;
co
  co [i = 1 to M] {
      while (whatever) {
          ...produce new data locally
          P(empty);
            P(mutexP); # stop the other producers from accessing the buffer
              buf[rear] = data; rear = (rear + 1) % n;
            V(mutexP);
          V(full);
} }
//
  co [j = 1 to N] {
      while (whatever) {
          P(full);
            P(mutexC);
              result = buf[front]; front = (front + 1) % n;
            V(mutexC);
          V(empty);
          ... handle result locally
    } }
oc
</code></pre><p><strong>Extending Multiple Producers/Consumers</strong>: If the buffered items are large and take a long time to read/write, we would like to relax this solution to allow several producers and/or consumers to be active within the buffer simultaneously.</p><ul><li>We need to ensure that these workers accesse distinct buffer locations, which require the index arithmetic to be kept atomic.</li><li>Make sure that the producer/consumers wait for that element to be empty/full before actually proceeding.
![](/images/Multiple_Producers_Consumers.png &ldquo;The producers are filling distinct slots, but not necessarily completing these fills in strict order - slot i+1 might finish filling before slot i. However, consumers only know that a slot has been filled and assume, possibly incorrectly, that it is the &ldquo;next&rdquo; one. Image from: <a href=http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf%22>http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"</a>)</li></ul><p>The solution is to have <strong>extra semaphores pair for each buffer location</strong>.</p><h3 id=monitors>Monitors<a hidden class=anchor aria-hidden=true href=#monitors>#</a></h3><p>The monitor is a more structured mechanism which allows threads to have both <strong>mutual exclusion</strong> and the ability to <strong>wait</strong> (block) for a certain condition to become true. It has a mechanism for <strong>signaling</strong> other threads that their condition has been met. A monitor consists of a <strong>mutex (lock)</strong> object and <strong>condition variables</strong> (cv). A condition variable is basically a container of threads that are waiting for a certain condition.</p><p>For <strong>Mutual Exclusion</strong>: i.e. a mutex (lock) object, ensures that <strong>at most one thread is active within the monitor at each point in time</strong>. 不同线程的下一条即将执行的指令 (suspended) 可能是来自同一个 monitor (由os自行分配), 但同一时间内，至多只能有一个线程执行下一条指令，但可能不同线程各自收到了来自这个 monitor 代码的不同指令. It is as if the body of each monitor method is implicitly surrounded with <code>P()</code> and <code>V()</code> operations on a single hidden binary semaphore, shared by all methods.</p><p>For <strong>Condition Synchronization</strong>, using a cv with a monitor to control <strong>a queue of delayed threads</strong> by a kind of <strong>Signal and Continue (SC)</strong> scheme.![](/images/signal_and_continue.png &ldquo;State transition diagram for &ldquo;signal-and-continue&rdquo; monitors. Image from: <a href=http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf%22>http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"</a>)
For a <code>condition_variables x;</code></p><ul><li><code>wait(x)</code>: Release lock; wait for the condition to become true; reacquire lock upon return (Java wait())</li><li><code>Signal(x)</code>: Wake up a waiter, if any (Java notify())</li><li><code>signal-all(x)</code>or<code>Broadcast(x)</code>: Wake up all the waiters (Java notifyAll())</li></ul><p>For the thread active inside a monitor method - <strong>executing in monitor state</strong></p><ul><li>If the thread could not proceed, it may call the <code>wait(cv)</code> operation to <strong>give up the (implicit) lock</strong> it holds on the monitor, and being <strong>suspended</strong> (push to the end of CV queue). Each CV has its unique block queue.</li><li>Or the thread could calls the operation <code>signal(cv)</code> to release the lock. This allow one previously blocked thread (normally chosen by a FIFO discipline) to <strong>become ready for scheduling</strong> again (only one will be allowed to enter the <strong>monitor entry queue</strong> at a time). The signalling thread <strong>continues</strong> uninterrupted.</li><li>Or <code>return()</code>.</li></ul><p>If no threads are waiting, then a <code>signal()</code> is <strong>&ldquo;lost&rdquo;</strong> or &ldquo;forgotten&rdquo;, whereas a <code>V()</code> in Semaphores allows a subsequent <code>P()</code> to proceed.</p><p>Monitor semantics mean that when a thread which was previously blocked on a condition is actually awakened again in the monitor.</p><p>The point to remember is that when the signal happened, the signalled thread was allowed to <strong>try to acquire the monitor lock</strong> again). It could be that some other thread acquires the lock first, and does something which negates the condition again (for example, it consumes the “new item” from a monitor protected buffer).</p><p>Thus it is often <strong>necessary</strong>, in all but the most tightly constrained situations, to wrap each conditional variable <code>wait()</code> call in a loop which rechecks the condition it was waiting for is still true.</p><p><strong>Single producer, single consumer bounder buffer</strong></p><pre tabindex=0><code>monitor Bounded_Buffer {
    typeT buf[n];                     # an array of some type T
    int front = 0,                    # index of first full slot
        rear = 0;                     # index of first empty slot
        count = 0;                    # number of full slots
    ## rear == (front + count) % n
    condition_variables not_full,     # signaled when count &lt; n
                        not_empty;    # signaled when count &gt; 0
    procedure deposit(typeT data) {   # 存
        while (count == n) wait(not_full);
        buf[rear] = data; rear = (rear+1) % n; count++;
        signal(not_empty);
    }
    procedure fetch(typeT &amp;result) {  # 取
        while (count == 0) wait(not_empty);
        result = buf[front]; front = (front+1) % n; count--;
        signal(not_full);
    }
}
</code></pre><p><strong>Why the while loop is necessary</strong> as a safety check on the <code>wait</code> calls (why not use if)? - 因为<code>notify()</code>只会让正在 wait queue 的 thread 进入准备状态, 但不会直接控制其恢复工作（是否马上开始，谁先开始，都是由os内部控制的）, 所以导致不同 thread 进度不同; 而<code>while</code>可以保证当即使 thread 因为受到<code>notify()</code>而结束<code>wait()</code>开始进入准备状态(entry queue)后, 继续检查 buffer 状态, 这样假如发现自己是最优先安排的那个, 就可以跳出<code>while</code>循环进入工作状态; 假如发现自己优先度不是最高的(<code>while</code>循环条件继续满足), 则继续<code>wait()</code>.</p><p>The key difference to semaphores: <code>signal()</code> on a condition variable is not &ldquo;remembered&rdquo; in the way that <code>V()</code> on a semaphore is. If no threads are waiting, then a <code>signal()</code> is &ldquo;lost&rdquo; or &ldquo;forgotten&rdquo;, whereas a <code>V()</code> will allow a subsequent <code>P()</code> to proceed.</p><h2 id=real-shared-variable-programming-systems>Real Shared Variable Programming Systems<a hidden class=anchor aria-hidden=true href=#real-shared-variable-programming-systems>#</a></h2><p>Various concepts for shared variable programming have been embedded in real programming systems. In particular C&rsquo;s <strong>Posix threads (Pthreads)</strong> library and <strong>Java&rsquo;s threads and monitors</strong>.</p><h2 id=posix-threads-pthread>POSIX Threads (Pthread)<a hidden class=anchor aria-hidden=true href=#posix-threads-pthread>#</a></h2><p>Create a new thread: Threads (type <code>pthread_t</code>) begin by executing a given function, and terminate when that function exits (or when killed off by another thread).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>pthread_create</span> <span class=p>(</span><span class=kt>pthread_t</span> <span class=o>*</span><span class=kr>thread</span><span class=p>,</span> <span class=kt>p_thread_attr_t</span> <span class=o>*</span><span class=n>attr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=kt>void</span> <span class=o>*</span><span class=p>(</span><span class=o>*</span><span class=n>function</span><span class=p>)</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=p>),</span> <span class=kt>void</span> <span class=o>*</span><span class=n>arguments</span><span class=p>);</span>
</span></span></code></pre></div><p>Wait for thread termination: <code>int pthread_join (pthread_t t, void ** result);</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>//一个简单但是有错误的例子，
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>target</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=o>*</span><span class=nf>adderthread</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>arg</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>target</span> <span class=o>=</span> <span class=n>target</span><span class=o>+</span><span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span> <span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>i</span><span class=p>;</span> <span class=kt>pthread_t</span> <span class=kr>thread</span><span class=p>[</span><span class=n>P</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>target</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>P</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>pthread_create</span><span class=p>(</span><span class=o>&amp;</span><span class=kr>thread</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=nb>NULL</span><span class=p>,</span> <span class=n>adderthread</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=p>.....</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Variable <code>target</code> is accessible to all threads (shared memory). Its increment is <strong>not atomic</strong>, so we may get unpredictable results.</p><p>POSIX provides mechanisms to coordinate accesses including <strong>semaphores</strong> and building blocks for <strong>monitors</strong>.</p><h3 id=pthreads-semaphores>Pthreads semaphores<a hidden class=anchor aria-hidden=true href=#pthreads-semaphores>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>//用 pthread semaphores 改写前面的代码
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>sem_t</span> <span class=n>lock</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=o>*</span><span class=nf>adderthread</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>arg</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>sem_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>lock</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>target</span> <span class=o>=</span> <span class=n>target</span><span class=o>+</span><span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=nf>sem_post</span><span class=p>(</span><span class=o>&amp;</span><span class=n>lock</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span> <span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>target</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nf>sem_init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>lock</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>.....</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ol><li><code>sem_init(&amp;sem, share, init)</code>, where init is the initial value and share is a &ldquo;boolean&rdquo; (in the C sense) indicating whether the semaphore will be shared between processes (true) or just threads within a process (false).</li><li><code>sem_wait(s)</code>, which is the Posix name for P(s)</li><li><code>sem_post(s)</code>, which is the Posix name for V(s)</li></ol><p>A Producers & Consumers:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>sem_t</span> <span class=n>empty</span><span class=p>,</span> <span class=n>full</span><span class=p>;</span> <span class=c1>// the global semaphores
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>data</span><span class=p>;</span> <span class=c1>// shared buffer
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span> <span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>pthread_t</span> <span class=n>pid</span><span class=p>,</span> <span class=n>cid</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>....</span>
</span></span><span class=line><span class=cl>    <span class=nf>sem_init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>empty</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>);</span> <span class=c1>// sem empty = 1
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>sem_init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>full</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span> <span class=c1>// sem full = 0
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>pthread_create</span><span class=p>(</span><span class=o>&amp;</span><span class=n>pid</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>attr</span><span class=p>,</span> <span class=n>Producer</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>pthread_create</span><span class=p>(</span><span class=o>&amp;</span><span class=n>cid</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>attr</span><span class=p>,</span> <span class=n>Consumer</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>pthread_join</span><span class=p>(</span><span class=n>pid</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>pthread_join</span><span class=p>(</span><span class=n>cid</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=o>*</span><span class=nf>Producer</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>arg</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>produced</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>produced</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>produced</span> <span class=o>&lt;</span> <span class=n>numIters</span><span class=p>;</span> <span class=n>produced</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>sem_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>empty</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=n>produced</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=nf>sem_post</span><span class=p>(</span><span class=o>&amp;</span><span class=n>full</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=o>*</span><span class=nf>Consumer</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>arg</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=n>consumed</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>consumed</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>consumed</span> <span class=o>&lt;</span> <span class=n>numIters</span><span class=p>;</span> <span class=n>consumed</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>sem_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>full</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>total</span> <span class=o>=</span> <span class=n>total</span><span class=o>+</span><span class=n>data</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=nf>sem_post</span><span class=p>(</span><span class=o>&amp;</span><span class=n>empty</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;after %d iterations, the total is %d (should be %d)</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>numIters</span><span class=p>,</span> <span class=n>total</span><span class=p>,</span> <span class=n>numIters</span><span class=o>*</span><span class=p>(</span><span class=n>numIters</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span><span class=o>/</span><span class=mi>2</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=pthreads-monitors>Pthreads Monitors<a hidden class=anchor aria-hidden=true href=#pthreads-monitors>#</a></h3><p>Pthreads provides <strong>locks</strong>, of type <code>pthread_mutex_t m;</code>. These can be</p><ul><li><strong>Initialized</strong> with <code>pthread_mutex_init(&amp;m, attr)</code>, where attr are attributes concerning scope (as with semaphore creation). If attr is <code>NULL</code>, the default mutex attributes (NONRECURSIVE) are used;</li><li><strong>Locked</strong> with <code>pthread_mutex_lock(&amp;m)</code>, which blocks the locking thread if <code>m</code> is already locked. There is also a non-blocking version <code>pthread_mutex_trylock(&amp;m)</code>.</li><li><strong>Unlocked</strong> with <code>pthread_mutex_unlock(&amp;m)</code>. Only a thread which holds a given lock, should unlock it!</li></ul><p>Pthreads provides <strong>condition variables</strong> <code>pthread_cond_t</code>. As well as the usual initialization, these can be:</p><ul><li><strong>Waited</strong> on with <code>pthread_cond_wait(&amp;cv, &amp;mut)</code> where <code>cv</code> is a condition variable, and <code>mut</code> must be a lock already held by this thread, and which is implictly released.</li><li><strong>Signalled</strong> with <code>pthread_cond_signal(&amp;cv)</code> by a thread which should (but doesn&rsquo;t strictly have to) hold the associated mutex. The semantics are &ldquo;Signal-and-Continue&rdquo; as previously discussed.</li><li><strong>Signalled all</strong> with <code>pthread_cond_broadcast(&amp;cv)</code>. This is &ldquo;signal-all&rdquo;</li></ul><p>A simple Jacobi grid-iteration program with a re-usable Counter Barrier. To avoid copying between &ldquo;new&rdquo; and &ldquo;old&rdquo; grids, each iteration performs two Jacobi steps. Convergence testing could be added as before.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>pthread_mutex_t</span> <span class=n>barrier</span><span class=p>;</span> <span class=c1>// mutex semaphore for the barrier
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>pthread_cond_t</span> <span class=n>go</span><span class=p>;</span>       <span class=c1>// condition variable for leaving
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>numArrived</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>Barrier</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>pthread_mutex_lock</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>numArrived</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>numArrived</span> <span class=o>==</span> <span class=n>numWorkers</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>numArrived</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=nf>pthread_cond_broadcast</span><span class=p>(</span><span class=o>&amp;</span><span class=n>go</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>pthread_cond_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>go</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>barrier</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=nf>pthread_mutex_unlock</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>pthread_t</span> <span class=n>workerid</span><span class=p>[</span><span class=n>MAXWORKERS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=nf>pthread_mutex_init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>pthread_cond_init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>go</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>InitializeGrids</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>numWorkers</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nf>pthread_create</span><span class=p>(</span><span class=o>&amp;</span><span class=n>workerid</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=o>&amp;</span><span class=n>attr</span><span class=p>,</span> <span class=n>Worker</span><span class=p>,</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=p>)</span> <span class=n>i</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>numWorkers</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nf>pthread_join</span><span class=p>(</span><span class=n>workerid</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=nb>NULL</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=o>*</span><span class=nf>Worker</span><span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>arg</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>myid</span> <span class=o>=</span> <span class=p>(</span><span class=kt>int</span><span class=p>)</span> <span class=n>arg</span><span class=p>,</span> <span class=n>rowA</span> <span class=o>=</span> <span class=n>myid</span><span class=o>*</span><span class=n>rowshare</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>rowB</span> <span class=o>=</span> <span class=n>rowA</span><span class=o>+</span><span class=n>rowshare</span><span class=o>-</span><span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>iters</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>iters</span> <span class=o>&lt;=</span> <span class=n>numIters</span><span class=p>;</span> <span class=n>iters</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>=</span> <span class=n>rowA</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;=</span> <span class=n>rowB</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=n>j</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;=</span> <span class=n>gridSize</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>grid2</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>grid1</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+</span> <span class=n>grid1</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+</span> <span class=n>grid1</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>grid1</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=o>+</span><span class=mi>1</span><span class=p>])</span> <span class=o>*</span> <span class=mf>0.25</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=nf>Barrier</span><span class=p>();</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>=</span> <span class=n>rowA</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;=</span> <span class=n>rowB</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=n>j</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;=</span> <span class=n>gridSize</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>grid1</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>grid2</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+</span> <span class=n>grid2</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+</span> <span class=n>grid2</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>grid2</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=o>+</span><span class=mi>1</span><span class=p>])</span> <span class=o>*</span> <span class=mf>0.25</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=nf>Barrier</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=memory-consistency-in-pthreads>Memory Consistency in Pthreads<a hidden class=anchor aria-hidden=true href=#memory-consistency-in-pthreads>#</a></h3><p>Weak consistency models can wreck naive DIY synchronization attempts!</p><p>To enable portability, Pthreads mutex, semaphore and condition variable operations <strong>implicitly act as memory fences</strong>, executing architecture specific instructions.</p><p>In effect, the C + Pthreads combination guarantees a <strong>weak consistency memory model</strong>, with the only certainties provided at uses of Pthreads primitives.</p><p>For example, all writes by a thread which has released some <strong>mutex</strong>, are guaranteed to be seen by any thread which then acquires it. Nothing can be assumed about the visibility of writes which cannot be seen to be ordered by their relationship to uses of Pthread primitives.</p><p>The programmer must also be careful to use only <strong>thread-safe</strong> code, which works irrespective of how many threads are active.</p><blockquote><p>Thread-safe code only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specification without unintended interaction. Implementation is guaranteed to be free of race conditions when accessed by multiple threads simultaneously.</p></blockquote><p>Typical problems involve the use of non-local data. For example, imagine a non-thread safe <code>malloc</code>. Unluckily interleaved calls might break the underlying free space data structure. Some libraries will provide thread-safe versions (but of course, which pay an unnecessary performance penalty when used in a single threaded program).</p><h2 id=java-concurrency>Java Concurrency<a hidden class=anchor aria-hidden=true href=#java-concurrency>#</a></h2><p>Java是一种多线程 multi-threaded 编程语言，其同步模型是基于 monitor 概念，可用于开发多线程程序。多任务 multtasking 就是多个进程共享公共处理资源（如CPU）的时候。多线程将多任务的思想扩展到可以将单个应用程序中的特定操作细分为单独线程的应用程序。每个线程都可以并行运行。操作系统不仅在不同的应用程序之间分配处理时间，而且在应用程序内的每个线程之间分配处理时间。</p><h3 id=java-threads>Java Threads<a hidden class=anchor aria-hidden=true href=#java-threads>#</a></h3><p><img loading=lazy src=https://www.tutorialspoint.com/java/images/Thread_Life_Cycle.jpg title="Life Cycle of a Thread. image from: http://www.tutorialspoint.com/java/java_multithreading.htm">
Threads can be created from classes which extend <code>java.lang.Thread</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>class</span> <span class=nc>Simple</span><span class=w> </span><span class=kd>extends</span><span class=w> </span><span class=n>Thread</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>run</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;this is a thread&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>new</span><span class=w> </span><span class=n>Simple</span><span class=p>().</span><span class=na>start</span><span class=p>();</span><span class=w> </span><span class=c1>// implicitly calls the run() method</span><span class=w>
</span></span></span></code></pre></div><p>Or implement <code>java.lang.Runnable</code> (so we can extend some other class too).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>class</span> <span class=nc>Bigger</span><span class=w> </span><span class=kd>extends</span><span class=w> </span><span class=n>Whatever</span><span class=w> </span><span class=kd>implements</span><span class=w> </span><span class=n>Runnable</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>run</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w> </span><span class=p>....</span><span class=w> </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>new</span><span class=w> </span><span class=n>Thread</span><span class=p>(</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>Bigger</span><span class=w> </span><span class=p>(...)</span><span class=w> </span><span class=p>).</span><span class=na>start</span><span class=p>();</span><span class=w>
</span></span></span></code></pre></div><p>Wait to join with another thread</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>class</span> <span class=nc>Friend</span><span class=w> </span><span class=kd>extends</span><span class=w> </span><span class=n>Thread</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>private</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=n>me</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=nf>Friend</span><span class=w> </span><span class=p>(</span><span class=kt>int</span><span class=w> </span><span class=n>i</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w> </span><span class=n>me</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>i</span><span class=p>;</span><span class=w> </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>run</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;Hello from thread &#34;</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>me</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>class</span> <span class=nc>Hello</span><span class=w> </span><span class=kd>throws</span><span class=w> </span><span class=n>java</span><span class=p>.</span><span class=na>lang</span><span class=p>.</span><span class=na>InterruptedException</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>private</span><span class=w> </span><span class=kd>static</span><span class=w> </span><span class=kd>final</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=n>n</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>5</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kd>static</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span><span class=w> </span><span class=n>args</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=kt>int</span><span class=w> </span><span class=n>i</span><span class=p>;</span><span class=w> </span><span class=n>Friend</span><span class=w> </span><span class=n>t</span><span class=o>[]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>Friend</span><span class=o>[</span><span class=n>n</span><span class=o>]</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=w> </span><span class=p>(</span><span class=s>&#34;Hello from the main thread&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>for</span><span class=w> </span><span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=n>0</span><span class=p>;</span><span class=w> </span><span class=n>i</span><span class=o>&lt;</span><span class=n>n</span><span class=p>;</span><span class=w> </span><span class=n>i</span><span class=o>++</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>t</span><span class=o>[</span><span class=n>i</span><span class=o>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>Friend</span><span class=p>(</span><span class=n>i</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>t</span><span class=o>[</span><span class=n>i</span><span class=o>]</span><span class=p>.</span><span class=na>start</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>for</span><span class=w> </span><span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=n>0</span><span class=p>;</span><span class=w> </span><span class=n>i</span><span class=o>&lt;</span><span class=n>n</span><span class=p>;</span><span class=w> </span><span class=n>i</span><span class=o>++</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>t</span><span class=o>[</span><span class=n>i</span><span class=o>]</span><span class=p>.</span><span class=na>join</span><span class=p>();</span><span class=w> </span><span class=c1>// might throw java.lang.InterruptedException</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=w> </span><span class=p>(</span><span class=s>&#34;Goodbye from the main thread&#34;</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><h3 id=java-monitors>Java &ldquo;Monitors&rdquo;<a hidden class=anchor aria-hidden=true href=#java-monitors>#</a></h3><p>Java provides an implementation of the <strong>monitor</strong> concept (but doesn’t actually have monitor as a keyword).</p><p>Any object in a Java program can, in effect, become a monitor, simply by declaring one or more of its methods to be <strong>synchronized</strong>, or by including a synchronized block of code.</p><p>Each such object is associated with one, <strong>implicit lock</strong>. A thread executing any <code>synchronized</code> code must first acquire this lock. This happens implicitly (i.e. there is no source syntax). Similarly, upon leaving the synchronized block the lock is <strong>implicitly released</strong>.</p><p>Java&rsquo;s <strong>condition variable</strong> mechanism uses <strong>Signal-and-Continue</strong> semantics (The signalling thread continues uninterrupted). Each synchronizable object is associated with a <strong>single implicit condition variable</strong>. Manipulated with methods <code>wait()</code>, <code>notify()</code> and <code>notifyAll()</code>. We can only have <strong>one conditional variable queue per monitor</strong> (hence the absence of any explicit syntax for the condition variable itself).</p><p><code>wait()</code>: has three variance, one which waits indefinitely for any other thread to call notify or notifyAll method on the object to wake up the current thread. Other two variances puts the current thread in wait for specific amount of time before they wake up.</p><p><code>notify()</code>: <strong>wakes up only one thread</strong> waiting on the object and that thread starts execution.</p><p><code>notifyAll()</code>: <strong>wakes up all the threads</strong> waiting on the object, although which one will process first depends on the OS implementation.</p><p>These methods can be used to implement producer consumer problem where consumer threads are waiting for the objects in Queue and producer threads put object in queue and notify the waiting threads.</p><p><strong>Readers & Writers problem</strong> requires control access to some shared resource, such that there may be many concurrent readers, but only one writer (with exclusive access) at a time.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=cm>/* 2 readers and 2 writers making 5 accesses each
</span></span></span><span class=line><span class=cl><span class=cm>with concurrent read or exclusive write.  */</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>class</span> <span class=nc>ReadWrite</span><span class=w> </span><span class=p>{</span><span class=w> </span><span class=c1>// driver program -- two readers and two writers</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>static</span><span class=w> </span><span class=n>Database</span><span class=w> </span><span class=n>RW</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>Database</span><span class=p>();</span><span class=w> </span><span class=c1>// the monitor</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kd>static</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span><span class=w> </span><span class=n>arg</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=kt>int</span><span class=w> </span><span class=n>rounds</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>Integer</span><span class=p>.</span><span class=na>parseInt</span><span class=p>(</span><span class=n>arg</span><span class=o>[</span><span class=n>0</span><span class=o>]</span><span class=p>,</span><span class=n>10</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>new</span><span class=w> </span><span class=n>Reader</span><span class=p>(</span><span class=n>rounds</span><span class=p>,</span><span class=w> </span><span class=n>RW</span><span class=p>).</span><span class=na>start</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>new</span><span class=w> </span><span class=n>Reader</span><span class=p>(</span><span class=n>rounds</span><span class=p>,</span><span class=w> </span><span class=n>RW</span><span class=p>).</span><span class=na>start</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>new</span><span class=w> </span><span class=n>Writer</span><span class=p>(</span><span class=n>rounds</span><span class=p>,</span><span class=w> </span><span class=n>RW</span><span class=p>).</span><span class=na>start</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>new</span><span class=w> </span><span class=n>Writer</span><span class=p>(</span><span class=n>rounds</span><span class=p>,</span><span class=w> </span><span class=n>RW</span><span class=p>).</span><span class=na>start</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>class</span> <span class=nc>Reader</span><span class=w> </span><span class=kd>extends</span><span class=w> </span><span class=n>Thread</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kt>int</span><span class=w> </span><span class=n>rounds</span><span class=p>;</span><span class=w> </span><span class=n>Database</span><span class=w> </span><span class=n>RW</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>private</span><span class=w> </span><span class=n>Random</span><span class=w> </span><span class=n>generator</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>Random</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=nf>Reader</span><span class=p>(</span><span class=kt>int</span><span class=w> </span><span class=n>rounds</span><span class=p>,</span><span class=w> </span><span class=n>Database</span><span class=w> </span><span class=n>RW</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>this</span><span class=p>.</span><span class=na>rounds</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>rounds</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>this</span><span class=p>.</span><span class=na>RW</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>RW</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>run</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>for</span><span class=w> </span><span class=p>(</span><span class=kt>int</span><span class=w> </span><span class=n>i</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>0</span><span class=p>;</span><span class=w> </span><span class=n>i</span><span class=o>&lt;</span><span class=n>rounds</span><span class=p>;</span><span class=w> </span><span class=n>i</span><span class=o>++</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=k>try</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=n>Thread</span><span class=p>.</span><span class=na>sleep</span><span class=p>(</span><span class=n>generator</span><span class=p>.</span><span class=na>nextInt</span><span class=p>(</span><span class=n>500</span><span class=p>));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=p>}</span><span class=w> </span><span class=k>catch</span><span class=w> </span><span class=p>(</span><span class=n>java</span><span class=p>.</span><span class=na>lang</span><span class=p>.</span><span class=na>InterruptedException</span><span class=w> </span><span class=n>e</span><span class=p>)</span><span class=w> </span><span class=p>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;read: &#34;</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>RW</span><span class=p>.</span><span class=na>read</span><span class=p>());</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>class</span> <span class=nc>Writer</span><span class=w> </span><span class=kd>extends</span><span class=w> </span><span class=n>Thread</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kt>int</span><span class=w> </span><span class=n>rounds</span><span class=p>;</span><span class=w> </span><span class=n>Database</span><span class=w> </span><span class=n>RW</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>private</span><span class=w> </span><span class=n>Random</span><span class=w> </span><span class=n>generator</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>Random</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=nf>Writer</span><span class=p>(</span><span class=kt>int</span><span class=w> </span><span class=n>rounds</span><span class=p>,</span><span class=w> </span><span class=n>Database</span><span class=w> </span><span class=n>RW</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>this</span><span class=p>.</span><span class=na>rounds</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>rounds</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>this</span><span class=p>.</span><span class=na>RW</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>RW</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>run</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>for</span><span class=w> </span><span class=p>(</span><span class=kt>int</span><span class=w> </span><span class=n>i</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>0</span><span class=p>;</span><span class=w> </span><span class=n>i</span><span class=o>&lt;</span><span class=n>rounds</span><span class=p>;</span><span class=w> </span><span class=n>i</span><span class=o>++</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=k>try</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=n>Thread</span><span class=p>.</span><span class=na>sleep</span><span class=p>(</span><span class=n>generator</span><span class=p>.</span><span class=na>nextInt</span><span class=p>(</span><span class=n>500</span><span class=p>));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=p>}</span><span class=w> </span><span class=k>catch</span><span class=w> </span><span class=p>(</span><span class=n>java</span><span class=p>.</span><span class=na>lang</span><span class=p>.</span><span class=na>InterruptedException</span><span class=w> </span><span class=n>e</span><span class=p>)</span><span class=w> </span><span class=p>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=n>RW</span><span class=p>.</span><span class=na>write</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><p>Implement the &ldquo;database&rdquo;. Allowing several readers to be actively concurrently. The last reader to leave will signal a waiting writer.</p><p>Thus we need to count readers, which implies <strong>atomic update</strong> of the count. A reader needs <strong>two protected sections</strong> to achieve this.</p><p>Notice that while readers are actually reading the data they do not hold the lock.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>class</span> <span class=nc>Database</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>private</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=n>data</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>0</span><span class=p>;</span><span class=w> </span><span class=c1>// the data</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kt>int</span><span class=w> </span><span class=n>nr</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>0</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// synchronized means no more than one thread could do that</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>private</span><span class=w> </span><span class=kd>synchronized</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>startRead</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w> </span><span class=n>nr</span><span class=o>++</span><span class=p>;</span><span class=w> </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>private</span><span class=w> </span><span class=kd>synchronized</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>endRead</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>nr</span><span class=o>--</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>if</span><span class=w> </span><span class=p>(</span><span class=n>nr</span><span class=o>==</span><span class=n>0</span><span class=p>)</span><span class=w> </span><span class=n>notify</span><span class=p>();</span><span class=w> </span><span class=p>}</span><span class=c1>// awaken a waiting writer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=nf>read</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=kt>int</span><span class=w> </span><span class=n>snapshot</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>startRead</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>snapshot</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>data</span><span class=p>;</span><span class=w>      </span><span class=c1>// read data</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>endRead</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w> </span><span class=n>snapshot</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kd>synchronized</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>write</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=kt>int</span><span class=w> </span><span class=n>temp</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>while</span><span class=w> </span><span class=p>(</span><span class=n>nr</span><span class=o>&gt;</span><span class=n>0</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=k>try</span><span class=w> </span><span class=p>{</span><span class=w> </span><span class=n>wait</span><span class=p>();</span><span class=w> </span><span class=p>}</span><span class=w> </span><span class=k>catch</span><span class=w> </span><span class=p>(</span><span class=n>InterruptedException</span><span class=w> </span><span class=n>ex</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=k>return</span><span class=p>;}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>temp</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>data</span><span class=p>;</span><span class=w> </span><span class=c1>// next six lines are the ‘‘database’’ update!</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>data</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>99999</span><span class=p>;</span><span class=w> </span><span class=c1>// to simulate an inconsistent temporary state</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>try</span><span class=w> </span><span class=p>{</span><span class=w> </span><span class=n>Thread</span><span class=p>.</span><span class=na>sleep</span><span class=p>(</span><span class=n>generator</span><span class=p>.</span><span class=na>nextInt</span><span class=p>(</span><span class=n>500</span><span class=p>));</span><span class=w> </span><span class=c1>// wait a bit, for demo purposes only</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=p>}</span><span class=w> </span><span class=k>catch</span><span class=w> </span><span class=p>(</span><span class=n>java</span><span class=p>.</span><span class=na>lang</span><span class=p>.</span><span class=na>InterruptedException</span><span class=w> </span><span class=n>e</span><span class=p>)</span><span class=w> </span><span class=p>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>data</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>temp</span><span class=o>+</span><span class=n>1</span><span class=p>;</span><span class=w> </span><span class=c1>// back to a safe state</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&#34;wrote: &#34;</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>data</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>notify</span><span class=p>();</span><span class=w> </span><span class=c1>// awaken another waiting writer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><p>We could express the same effect with synchronized blocks</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>public</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=nf>read</span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kt>int</span><span class=w> </span><span class=n>snapshot</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>synchronized</span><span class=p>(</span><span class=k>this</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w> </span><span class=n>nr</span><span class=o>++</span><span class=p>;</span><span class=w> </span><span class=p>}</span><span class=w> </span><span class=c1>// this - the database object</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>snapshot</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>data</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>synchronized</span><span class=p>(</span><span class=k>this</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>nr</span><span class=o>--</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>if</span><span class=w> </span><span class=p>(</span><span class=n>nr</span><span class=o>==</span><span class=n>0</span><span class=p>)</span><span class=w> </span><span class=n>notify</span><span class=p>();</span><span class=w> </span><span class=c1>// awaken a waiting writer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>return</span><span class=w> </span><span class=n>snapshot</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><p>Would it be OK to use <code>notifyAll()</code> in <code>read()</code>? - <strong>Yes</strong>, but with extra transmission cost.</p><p><strong>Buffer for One Producer - One Consumer</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=cm>/** (borrowed from Skansholm, Java from the Beginning) */</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=kd>public</span><span class=w> </span><span class=kd>class</span> <span class=nc>Buffer</span><span class=w> </span><span class=kd>extends</span><span class=w> </span><span class=n>Vector</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kd>synchronized</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>putLast</span><span class=w> </span><span class=p>(</span><span class=n>Object</span><span class=w> </span><span class=n>obj</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>addElement</span><span class=p>(</span><span class=n>obj</span><span class=p>);</span><span class=w> </span><span class=c1>// Vectors grow implicitly</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>notify</span><span class=p>();</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=kd>public</span><span class=w> </span><span class=kd>synchronized</span><span class=w> </span><span class=n>Object</span><span class=w> </span><span class=nf>getFirst</span><span class=w> </span><span class=p>()</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>while</span><span class=w> </span><span class=p>(</span><span class=n>isEmpty</span><span class=p>())</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=k>try</span><span class=w> </span><span class=p>{</span><span class=n>wait</span><span class=p>();}</span><span class=w> </span><span class=k>catch</span><span class=w> </span><span class=p>(</span><span class=n>InterruptedException</span><span class=w> </span><span class=n>e</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=k>return</span><span class=w> </span><span class=kc>null</span><span class=p>;}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>Object</span><span class=w> </span><span class=n>obj</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>elementAt</span><span class=p>(</span><span class=n>0</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=n>removeElementAt</span><span class=p>(</span><span class=n>0</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w> </span><span class=n>obj</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><h3 id=the-javautilconcurrent-package>The <code>java.util.concurrent</code> package<a hidden class=anchor aria-hidden=true href=#the-javautilconcurrent-package>#</a></h3><p>Including a re-usable <strong>barrier</strong> and <strong>semaphores</strong> (with P() and V() called <code>acquire()</code> and <code>release()</code>). It also has some thread-safe concurrent data structures (queues, hash tables).</p><p>The <code>java.util.concurrent.atomic</code> package provides implementations of <strong>atomically accessible</strong> integers, booleans and so on, with atomic operations like <code>addAndGet</code>, <code>compareAndSet</code>.</p><p>The <code>java.util.concurrent.locks</code> package provides implementations of <strong>locks and condition variables</strong>, to allow a finer grained, more explicit control than that provided by the built-in synchronized monitors.</p><h2 id=message-passing-programming>Message Passing Programming<a hidden class=anchor aria-hidden=true href=#message-passing-programming>#</a></h2><p>When the underyling archictecture doesn&rsquo;t support physically shared memory (for example, by distributing the OS and virtual memory system, i.e. <strong>Multicomputer architectures</strong>), we can make the disjoint nature of the address spaces apparent to the programmer, who must make decisions about data distribution and invoke explicit operations to allow interaction across these.</p><p><strong>Message passing</strong>, which is a approache to abstract and implement such a model, dominates the performance-oriented parallel computing world.</p><p>Message passing is characterized as requiring the <strong>explicit participation</strong> of both interacting processes, since each address space can only be directly manipulated by its owner. The basic requirement is thus for <strong>send</strong> and <strong>receive</strong> primitives for transferring data out of and into local address spaces.</p><p>The resulting programs can seem quite fragmented: we express algorithms as a collection of local perspectives. These are often captured in a single program source using <strong>Single Program Multiple Data (SPMD)</strong> style, with different processes following different paths through the same code, branching with respect to local data values and/or to some process identifier.</p><pre tabindex=0><code>// SPMD Compare-Exchange
co [me = 0 to P-1] { // assumes P is even
  int a, temp;       // these are private to each process now
  ......

  // typical one step within a parallel sorting algorithm
  if (me%2 == 0) {
      send (me+1, a);    // send from a to process me+1
      recv (me+1, temp); // receive into temp from process me+1
      a = (a&lt;=temp) ? a : temp; // 取较小值
  } else {
      send (me-1, a);
      recv (me-1, temp);
      a = (a&gt;temp) ? a : temp; // 取较大值
  } ......
}
</code></pre><p><img loading=lazy src=/images/SPMD_Compare_Exchange.png title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf">
1, <strong>Synchronization</strong>: Must a sending process pause until a matching receive has been executed (<strong>synchronous</strong>), or not (<strong>asynchronous</strong>)? Asynchronous semantics require the implementation to buffer messages which haven&rsquo;t yet been, and indeed may never be, received. If we use synchronous semantics, the compare-exchange code above will deadlock. Can you fix it?</p><blockquote><p>One way s to make the send be a non-blocking one (<code>MPI_Isend</code>)
Another way is to reverse the order of one of the send/receive pairs:</p></blockquote><pre tabindex=0><code>} else {
    recv (me-1, temp);
    send (me-1, a);
    a = (a&gt;temp) ? a : temp; // 取较大值
} ......
</code></pre><p>2, <strong>Addressing</strong>: When we invoke a send (or receive) do we have to specify a unique destination (or source) process or can we use <strong>wild-cards</strong>? Do we require program-wide process naming, or can we create process groups and aliases?
3, <strong>Collective Operations</strong>: Do we restrict the programmer to single-source, single-destination, point-to-point messages, or do we provide abstractions of more complex data exchanges involving several partners?<img loading=lazy src=/images/Collective_Operations.jpg title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf">• Broadcast: Everyone gets a copy of the same value.
• Scatter: Data is partitioned and spread across the group.
• Gather: Data is gathered from across the group.
• Reduction: Combine the gathered values with an associative operation.
• Scan (Prefix): Reduce and also compute all the ordered partial reductions.</p><h2 id=message-passing-interface-mpi>Message Passing Interface (MPI)<a hidden class=anchor aria-hidden=true href=#message-passing-interface-mpi>#</a></h2><blockquote><p>Message Passing Interface (MPI) is a standardized and portable message-passing standard. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran.</p></blockquote><p>Processes can be created <strong>statically</strong> when the program is invoked (e.g. using the mpirun command) or spawned <strong>dynamically</strong>.</p><p>All communications take place within the context of &ldquo;communication spaces&rdquo; called <strong>communicators</strong>, which denote sets of processes, allows the MPI programmer to define <strong>modules</strong> that encapsulate internal communication structures. A process can belong to many communicators simultaneously. New communicators can be defined dynamically.</p><p>Simple send/receives operate with respect to other processes in a communicator. <strong>Send must specify a target</strong> but receive can wild card on matching sender.</p><p>Messages can be tagged with an extra value to aid disambiguation.</p><p>Message-passing programming models are by default <strong>nondeterministic</strong>: the arrival order of messages sent from two processes A and B, to a third process C, is not defined. (However, MPI does guarantee that two messages sent from one process A, to another process B, will arrive in the order sent.)</p><p>There are many <strong>synchronization modes</strong> and a range of <strong>collective operations</strong>.</p><h3 id=mpi-primitives-6-basics-functions>MPI Primitives (6 basics functions)<a hidden class=anchor aria-hidden=true href=#mpi-primitives-6-basics-functions>#</a></h3><p>1, <code>int MPI_Init(int *argc, char ***argv)</code>: Initiate an MPI computation.
2, <code>int MPI_Finalize()</code>: Terminate a computation.
These must be called <strong>once</strong> by every participating process, before/after any other MPI calls. They return <strong>MPI_SUCCESS</strong> if successful, or an error code.</p><p>Each process has a <strong>unique identifier</strong> in each communicator of which it is a member (range 0&mldr;members-1). <code>MPI_COMM_WORLD</code> is the built-in <strong>global communicator</strong>, to which all processes belong by default.</p><p>A process can find the size of a communicator, and its own rank within it:
3, <code>int MPI_Comm_Size (MPI_Comm comm, int *np)</code>: Determine number of processes (comm - communicator). The processes in a process group are identified with unique, contiguous integers numbered from 0 to <code>np-1</code>.
4, <code>int MPI_Comm_rank (MPI_Comm comm, int *me)</code>: Determine my process identifier.</p><p>5, <code>MPI_SEND</code>: Send a message.
6, <code>MPI_RECV</code>: Receive a message.</p><h3 id=mpi-task-farm>MPI Task Farm<a hidden class=anchor aria-hidden=true href=#mpi-task-farm>#</a></h3><p>A task farm is bag-of-tasks in which <strong>all the tasks are known</strong> from the beginning. The challenge is to assign them <strong>dynamically</strong> to worker processes, to allow for the possibility that some tasks may take much longer to compute than others.</p><p>To simplify the code, we assume that there are <strong>at least as many tasks as processors</strong> and that tasks and results are just integers. In a real application these would be more complex data structures.</p><p>Notice the handling of the characteristic <strong>non-determinism</strong> in the order of task completion, with tags used to identify tasks and results. We also use a special tag to indicate an &ldquo;end of tasks&rdquo; message.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cm>/** SPMD style
</span></span></span><span class=line><span class=cl><span class=cm>农场主分配任务给工人 */</span>
</span></span><span class=line><span class=cl><span class=cp>#define MAX_TASKS 100
</span></span></span><span class=line><span class=cl><span class=cp>#define NO_MORE_TASKS MAX_TASKS+1
</span></span></span><span class=line><span class=cl><span class=cp>#define FARMER 0  </span><span class=c1>// 第一个 process 是farmer,其余是worker
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>np</span><span class=p>,</span> <span class=n>rank</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>argc</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>argv</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Comm_rank</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>rank</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Comm_size</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>np</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=n>FARMER</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>farmer</span><span class=p>(</span><span class=n>np</span><span class=o>-</span><span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>worker</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Finalize</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>farmer</span> <span class=p>(</span><span class=kt>int</span> <span class=n>workers</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>i</span><span class=p>,</span> <span class=n>task</span><span class=p>[</span><span class=n>MAX_TASKS</span><span class=p>],</span> <span class=n>result</span><span class=p>[</span><span class=n>MAX_TASKS</span><span class=p>],</span> <span class=n>temp</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>who</span><span class=p>;</span> <span class=n>MPI_Status</span> <span class=n>status</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// 1, 给每个人发送任务
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>workers</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>task</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// 2, 收取任务结果, 继续发放剩余任务
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>while</span> <span class=p>(</span><span class=n>i</span><span class=o>&lt;</span><span class=n>MAX_TASKS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>temp</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_ANY_SOURCE</span><span class=p>,</span> <span class=n>MPI_ANY_TAG</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>who</span> <span class=o>=</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_SOURCE</span><span class=p>;</span> <span class=n>tag</span> <span class=o>=</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_TAG</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span><span class=p>[</span><span class=n>tag</span><span class=p>]</span> <span class=o>=</span> <span class=n>temp</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>task</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>who</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>i</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// 3, 所有任务已经完成, 收集最后一个任务结果, 并且发出结束任务信号
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>workers</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>temp</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_ANY_SOURCE</span><span class=p>,</span> <span class=n>MPI_ANY_TAG</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>who</span> <span class=o>=</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_SOURCE</span><span class=p>;</span> <span class=n>tag</span> <span class=o>=</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_TAG</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span><span class=p>[</span><span class=n>tag</span><span class=p>]</span> <span class=o>=</span> <span class=n>temp</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>task</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>who</span><span class=p>,</span> <span class=n>NO_MORE_TASKS</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Notice that the final loop, which gathers the last computed tasks, has a predetermined bound. We know that this loop begins after dispatch of the last uncomputed task, so there must be exactly as many results left to gather as there are workers.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>void</span> <span class=nf>worker</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>task</span><span class=p>,</span> <span class=n>result</span><span class=p>,</span> <span class=n>tag</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Status</span> <span class=n>status</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>task</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>FARMER</span><span class=p>,</span> <span class=n>MPI_ANY_TAG</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>tag</span> <span class=o>=</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_TAG</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=n>tag</span> <span class=o>!=</span> <span class=n>NO_MORE_TASKS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=nf>somefunction</span><span class=p>(</span><span class=n>task</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>result</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>FARMER</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>task</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>FARMER</span><span class=p>,</span> <span class=n>MPI_ANY_TAG</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>tag</span> <span class=o>=</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_TAG</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>A worker is only concerned with its interaction with the farmer. 这样速度较快的worker可以自动接更多的任务，最终整体上达成 load balance。</p><h3 id=send-in-standard-mode>Send in standard mode<a hidden class=anchor aria-hidden=true href=#send-in-standard-mode>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>MPI_Send</span><span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>buf</span><span class=p>,</span> <span class=kt>int</span> <span class=n>count</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>datatype</span><span class=p>,</span> <span class=kt>int</span> <span class=n>dest</span><span class=p>,</span>
</span></span><span class=line><span class=cl>             <span class=kt>int</span> <span class=n>tag</span><span class=p>,</span> <span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>)</span>
</span></span></code></pre></div><p>Send <code>count</code> items of given type starting in position <code>buf</code> to process <code>dest</code> in communicator <code>comm</code>, tagging the message with <code>tag</code> (which must be non-negative).</p><p>There are corresponding datatypes for each basic C type, <code>MPI_INT</code>, <code>MPI_FLOAT</code> etc, and also facilities for constructing <strong>derived types</strong> which group these together.</p><p>Are <code>MPI_Send</code> and <code>MPI_Recv</code> synchronous or asynchronous?</p><h3 id=receive-in-standard-mode>Receive in standard mode<a hidden class=anchor aria-hidden=true href=#receive-in-standard-mode>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>MPI_Recv</span><span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>buf</span><span class=p>,</span> <span class=kt>int</span> <span class=n>count</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>datatype</span><span class=p>,</span> <span class=kt>int</span> <span class=n>source</span><span class=p>,</span>
</span></span><span class=line><span class=cl>             <span class=kt>int</span> <span class=n>tag</span><span class=p>,</span> <span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>,</span> <span class=n>MPI_Status</span> <span class=o>*</span><span class=n>status</span><span class=p>)</span>
</span></span></code></pre></div><p>Receive <code>count</code> items of given type starting in position <code>buf</code>, from process <code>source</code> in communicator <code>comm</code>, tagged by <code>tag</code>. It attempts to receive a message that has an envelope corresponding to the specified <code>tag</code>, <code>source</code>, and <code>comm</code>, <strong>blocking</strong> until such a message is available. When the message arrives, elements of the specified datatype are placed into the buffer at address <code>buf</code>. This buffer is guaranteed to be large enough to contain at least <code>count</code> elements.</p><p><strong>Non-determinism</strong> (within a communicator) is achieved with &ldquo;wild cards&rdquo;, by naming <code>MPI_ANY_SOURCE</code> and/or <code>MPI_ANY_TAG</code> as the source or tag respectively.</p><p>A receive can match any available message sent to the receiver which has the specified communicator, tag and source, subject to the constraint that messages sent <strong>between any particular pair of processes</strong> are guaranteed to appear to be <strong>non-overtaking</strong>. In other words, a receive cannot match message B in preference to message A if A was sent before B by the same process, the receive will receive the first one which was sent, not the first one to arrive.</p><p>The <code>status</code> variable can be used subsequently to inquire about the <code>size</code>, <code>tag</code>, and <code>source</code> of the received message. <strong>Status information</strong> is returned in a structure with <code>status.MPI_SOURCE</code> and <code>status.MPI_TAG</code> fields. This is useful in conjunction with <strong>wild card</strong> receives, allowing the receiver to determine the actual source and tag associated with the received message.</p><h3 id=prime-sieve-generator>Prime Sieve Generator<a hidden class=anchor aria-hidden=true href=#prime-sieve-generator>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Comm</span> <span class=n>nextComm</span><span class=p>;</span> <span class=kt>int</span> <span class=n>candidate</span> <span class=o>=</span> <span class=mi>2</span><span class=p>,</span> <span class=n>N</span> <span class=o>=</span> <span class=nf>atoi</span><span class=p>(</span><span class=n>argv</span><span class=p>[</span><span class=mi>1</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>argc</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>argv</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Comm_spawn</span><span class=p>(</span><span class=s>&#34;sieve&#34;</span><span class=p>,</span> <span class=n>argv</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INFO_NULL</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>nextComm</span><span class=p>,</span> <span class=n>MPI_ERRCODES_IGNORE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=n>candidate</span><span class=o>&lt;</span><span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>candidate</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>nextComm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>candidate</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>candidate</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>candidate</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>nextComm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Finalize</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We use <code>MPI_Comm_spawn</code> to <strong>dynamically create</strong> new sieve processes as we need them, and <code>MPI_Comm_get_parent</code> to find an inter-communicator to the process group which created us.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Comm</span> <span class=n>predComm</span><span class=p>,</span> <span class=n>succComm</span><span class=p>;</span> <span class=n>MPI_Status</span> <span class=n>status</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>myprime</span><span class=p>,</span> <span class=n>candidate</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>firstoutput</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>            <span class=c1>// a C style boolean
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>MPI_Init</span> <span class=p>(</span><span class=o>&amp;</span><span class=n>argc</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>argv</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Comm_get_parent</span> <span class=p>(</span><span class=o>&amp;</span><span class=n>predComm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>myprime</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>predComm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span> <span class=p>(</span><span class=s>&#34;%d is a prime</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>myprime</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>candidate</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>predComm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=n>candidate</span><span class=o>!=-</span><span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>candidate</span><span class=o>%</span><span class=n>myprime</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>    <span class=c1>// not sieved out
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>if</span> <span class=p>(</span><span class=n>firstoutput</span><span class=p>)</span> <span class=p>{</span>      <span class=c1>// create my successor if necessary
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=nf>MPI_Comm_spawn</span><span class=p>(</span><span class=s>&#34;sieve&#34;</span><span class=p>,</span> <span class=n>argv</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INFO_NULL</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span>  <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>succComm</span><span class=p>,</span> <span class=n>MPI_ERRCODES_IGNORE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                <span class=n>firstoutput</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>candidate</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>succComm</span><span class=p>)</span> <span class=c1>// pass on the candidate
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Recv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>candidate</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>predComm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span> <span class=c1>// next candidate
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=n>firstoutput</span><span class=p>)</span> <span class=nf>MPI_Send</span><span class=p>(</span><span class=o>&amp;</span><span class=n>candidate</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>succComm</span><span class=p>);</span> <span class=c1>// candidate=-1, shut down
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>MPI_Finalize</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The message flow is insured by the method in which new processes are spawned/created. Every time a new “sieve” process is spawned, MPI creates it in a new group/communicator. succComm is a handle to this new group which always contains only one process. Therefore, when a candidate is sent to the process, there is only one process in the succComm group and it has id 0.</p><p>The Recv function works in the same way predComm is a handle of the parent group (i.e. group of the process that created this sieve). And because the parent was the only process in this group/communicator, it can be identified by id 0.</p><p>In conclusion, a process creates at most one successor. This successor is the only process in its group/communicator. The succCom and predComm are handles to the children and parent groups respectively, both of which contain a single process with id 0 which is unique in its own group/communicator.</p><p>Spawning New MPI Processes</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>MPI_Comm_spawn</span> <span class=p>(</span><span class=kt>char</span> <span class=o>*</span><span class=n>command</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[],</span> <span class=kt>int</span> <span class=n>p</span><span class=p>,</span> <span class=n>MPI_Info</span> <span class=n>info</span><span class=p>,</span>
</span></span><span class=line><span class=cl>     <span class=kt>int</span> <span class=n>root</span><span class=p>,</span> <span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>,</span> <span class=n>MPI_Comm</span> <span class=o>*</span><span class=n>intercomm</span><span class=p>,</span> <span class=kt>int</span> <span class=n>errcodes</span><span class=p>[])</span>
</span></span></code></pre></div><p>This <strong>spawns</strong> p new processes, each executing a copy of program <code>command</code>, in a new communicator returned as <code>intercomm</code>. To the new processes, <code>intercomm</code> appears as <code>MPI_COMM_WORLD</code>. It must be
called by <strong>all processes</strong> in <code>comm</code> (it is &ldquo;collective&rdquo;), with process root computing the parameters. <code>info</code> and <code>errcodes</code> are used in system dependent ways to control/monitor process placement, errors etc.</p><p><code>MPI_Comm_get_parent</code> gives the new processes a reference to the communicator which created them.</p><h3 id=synchronization-in-mpi>Synchronization in MPI<a hidden class=anchor aria-hidden=true href=#synchronization-in-mpi>#</a></h3><p>MPI uses the term <strong>blocking</strong> in a slightly unconventional way, to refer to the relationship between the <strong>caller</strong> of a communication operation and the <strong>implementation</strong> of that operation (i.e. nothing to do with any matching operation).<img loading=lazy src=/images/Synchronization_in_MPI.png title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf">
Thus, a <strong>blocking send</strong> complete only when it is safe to reuse the specified output buffer (because the data has been copied somewhere safe by the system). 注意这里跟前面提到的<strong>synchronous</strong>概念不一样，synchronous 强调<strong>接收成功</strong>才是判断发送成功与否的标识，而 blocking 只需要保证缓存可以被安全改写即可。</p><p>In contrast, a process calling a <strong>non-blocking send</strong> continues immediately with unpredictable effects on the value actually sent. Similarly, there is a <strong>non-blocking receive</strong> operation which allows the calling process to continue immediately, with similar issues concerning the values which appear in the buffer. 意义在于，当需要发送的信息字节非常巨大时，发送和接收耗时都非常久，这时候如果可以不需要等待这些巨量信息的传输而直接继续下一个任务，则能提高效率。</p><p>To manage these effects, there are MPI operations for <strong>monitoring</strong> the progress of non-blocking communications (effectively, to ask, &ldquo;is it OK to use this variable now?&rdquo;). - The idea is that with <strong>careful use</strong> these can allow the process to get on with other useful work even before the user-space buffer has been safely stored.</p><h3 id=blocking-communication-semantics-in-mpi>Blocking Communication Semantics in MPI<a hidden class=anchor aria-hidden=true href=#blocking-communication-semantics-in-mpi>#</a></h3><p>MPI provides different blocking send operations, vary <strong>in the level of synchronization</strong> they provide. Each makes different demands on the underlying communication protocol (i.e. the implementation).</p><p>1, <strong>Synchronous mode</strong> send (<code>MPI_Ssend</code>) is blocking and synchronous, only complete when a matching receive has been found.</p><p>2, <strong>Standard mode</strong> send (<code>MPI_Send</code>) is blocking. Its synchronicity depends upon the state of the implementation buffers, in that it will be <strong>asynchronous</strong> unless the relevant buffers are full, in which case it will wait for buffer space (and so may appear to behave in a &ldquo;semi&rdquo; synchronous fashion).</p><p>3, <strong>Buffered mode</strong> send (<code>MPI_Bsend</code>) is blocking and asynchronous, but the programmer must previously have made enough buffer space available (otherwise an error is reported). There are associated operations for <strong>allocating</strong> the buffer space.</p><p><strong>Receiving</strong> with <code>MPI_Recv</code> blocks until a matching message has been completely received into the buffer (so it is blocking and <strong>synchronous</strong>).</p><p>MPI also provides <strong>non-blocking</strong> sends and receives which return <strong>immediately</strong> (i.e. possibly before it is safe to use/reuse the buffer). There are immediate versions of all the blocking operations (with an extra &ldquo;I&rdquo; in the name). For example, <code>MPI_Isend</code> is the <strong>standard mode immediate send</strong>, and <code>MPI_Irecv</code> is the immediate receive.</p><p>Non-blocking operations have an extra parameter, called a &lsquo;request&rsquo; which is a <strong>handle on the communication</strong>, used with <code>MPI_Wait</code> and <code>MPI_Test</code> to <strong>wait</strong> or <strong>check</strong> for <strong>completion</strong> of the communication (in the sense of the corresponding blocking version of the operation).</p><h3 id=probing-for-messages>Probing for Messages<a hidden class=anchor aria-hidden=true href=#probing-for-messages>#</a></h3><p>A receiving process may want to <strong>check</strong> for a <strong>potential receive</strong> without actually receiving it. For example, we may not know the incoming message size, and want to create a suitable receiving buffer.</p><p><code>int MPI_Probe(int src, int tag, MPI_Comm comm, MPI_Status *status)</code> behaves like <code>MPI_Recv</code> , filling in <code>*status</code>, without actually receiving the message.</p><p>There is also a version which tests whether a message is available immediately <code>int MPI_Iprobe(int src, int tag, MPI_Comm comm, int *flag, MPI_Status *status)</code> leaving a (C-style) boolean result in <code>*flag</code> (i.e. message/no message).</p><p>We can then determine the <strong>size</strong> of the incoming message by inspecting its status information. <code>int MPI_Get_count(MPI_Status *status, MPI_Datatype t, int *count)</code> sets <code>*count</code> to the <strong>number of items</strong> of type <code>t</code> in message with status <code>*status</code>.</p><p>We could use these functions to receive (for example) a message containing an <strong>unknown number</strong> of integers from an <strong>unknown source</strong>, but with <strong>tag</strong> <code>75</code>, in a given communicator comm.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=nf>MPI_Probe</span><span class=p>(</span><span class=n>MPI_ANY_SOURCE</span><span class=p>,</span> <span class=mi>75</span><span class=p>,</span> <span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=nf>MPI_Get_count</span><span class=p>(</span><span class=o>&amp;</span><span class=n>status</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>count</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>buf</span> <span class=o>=</span> <span class=p>(</span><span class=kt>int</span> <span class=o>*</span><span class=p>)</span> <span class=nf>malloc</span><span class=p>(</span><span class=n>count</span><span class=o>*</span><span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=n>source</span> <span class=o>=</span> <span class=n>status</span><span class=p>.</span><span class=n>MPI_SOURCE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=nf>MPI_Recv</span><span class=p>(</span><span class=n>buf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>source</span><span class=p>,</span> <span class=mi>75</span><span class=p>,</span> <span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span></code></pre></div><h3 id=collective-operations>Collective Operations<a hidden class=anchor aria-hidden=true href=#collective-operations>#</a></h3><p>MPI offers a range of more complex operations which would otherwise require <strong>complex sequences</strong> of sends, receives and computations.</p><p>These are called <strong>collective</strong> operations, because they must be called by <strong>all</strong> processes in a communicator.<img loading=lazy src=/images/Collective_Operations.jpg title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></p><p>1, <code>MPI_Bcast</code> <strong>broadcasts</strong> <code>count</code> items of type <code>t</code> from <code>buf</code> in <code>root</code> to <code>buf</code> in all other processes in <code>comm</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>MPI_Bcast</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>buf</span><span class=p>,</span> <span class=kt>int</span> <span class=n>count</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>t</span><span class=p>,</span> <span class=kt>int</span> <span class=n>root</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>)</span>
</span></span></code></pre></div><p>2, <code>MPI_Scatter</code> is used to <strong>divide the contents of a buffer</strong> across all processes.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>MPI_Scatter</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>sendbuf</span><span class=p>,</span> <span class=kt>int</span> <span class=n>sendcount</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>sendt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=kt>void</span> <span class=o>*</span><span class=n>recvbuf</span><span class=p>,</span> <span class=kt>int</span> <span class=n>recvcount</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>recvt</span><span class=p>,</span> <span class=kt>int</span> <span class=n>root</span><span class=p>,</span> <span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>)</span>
</span></span></code></pre></div><p>$i^{th}$ chunk (size <code>sendcount</code>) of <code>root</code>&rsquo;s <code>sendbuf</code> is sent to <code>recvbuf</code> on process $i$ (including the root process itself). The first three parameters are only significant at the root. Counts, types, root and communicator parameters must match between root and all receivers.</p><p>3, <code>MPI_Gather</code> is the inverse of <code>MPI_Scatter</code>. Instead of spreading elements from one process to many processes, <code>MPI_Gather</code> takes elements from many processes and gathers them to one single process.</p><blockquote><p><code>MPI_Gather</code> takes elements from each process and gathers them to the root process. The elements are ordered by the rank of the process from which they were received. Only the <code>root</code> process needs to have a valid receive buffer. The <code>recv_count</code> parameter is <strong>the count of elements received per process</strong>, not the total summation of counts from all processes.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=nf>MPI_Gather</span><span class=p>(</span> <span class=kt>void</span><span class=o>*</span> <span class=n>send_data</span><span class=p>,</span> <span class=kt>int</span> <span class=n>send_count</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>send_datatype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=kt>void</span><span class=o>*</span> <span class=n>recv_data</span><span class=p>,</span> <span class=kt>int</span> <span class=n>recv_count</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>recv_datatype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>root</span><span class=p>,</span> <span class=n>MPI_Comm</span> <span class=n>communicator</span><span class=p>)</span>
</span></span></code></pre></div><p>4, <code>MPI_Allreduce</code> computes a <strong>reduction</strong>, such as adding a collection
of values together. No root, all Processes receive the reduced result.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>MPI_Allreduce</span> <span class=p>(</span><span class=kt>void</span> <span class=o>*</span><span class=n>sendbuf</span><span class=p>,</span> <span class=kt>void</span> <span class=o>*</span><span class=n>recvbuf</span><span class=p>,</span> <span class=kt>int</span> <span class=n>count</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                   <span class=n>MPI_Datatype</span> <span class=n>sendt</span><span class=p>,</span> <span class=n>MPI_Op</span> <span class=n>op</span><span class=p>,</span> <span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Reduces</strong> elements from all send buffers, point-wise, to count single values, using <code>op</code>, storing result(s) in <strong>all</strong> receive buffers. The <code>op</code> is chosen from a <strong>predefined set</strong> (<code>MPI_SUM</code>, <code>MPI_MAX</code> etc) or <strong>constructed</strong> with user code and <code>MPI_Op_create</code>. <code>MPI_Allreduce</code> is the equivalent of doing <code>MPI_Reduce</code> followed by an <code>MPI_Bcast</code>.<img loading=lazy src=http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_allreduce_1.png></p><p>Jacobi (1-dimensional wrapped), each neighour is owned by distinct process, thus could not read each other&rsquo;s data - introduce a layer of message passing, introduce halo as buffer.<img loading=lazy src=/images/1_d_jacobi_MPI.png title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// here for convenience MPI_Sendrecv combines a send and a receive.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Comm_size</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>p</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Comm_rank</span><span class=p>(</span><span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>rank</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=nf>read_problem</span><span class=p>(</span><span class=o>&amp;</span><span class=n>n</span><span class=p>,</span> <span class=n>work</span><span class=p>);</span> <span class=c1>// 数据存在 root - 0号进程
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Bcast</span><span class=p>(</span><span class=o>&amp;</span><span class=n>n</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span> <span class=c1>// 广播数据
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>mysize</span> <span class=o>=</span> <span class=n>n</span><span class=o>/</span><span class=n>p</span><span class=p>;</span>             <span class=c1>// assume p divides n, for simplicity
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>local</span> <span class=o>=</span> <span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=p>)</span> <span class=nf>malloc</span><span class=p>(</span><span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>mysize</span><span class=o>+</span><span class=mi>2</span><span class=p>));</span> <span class=c1>//include fringe/halo
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>MPI_Scatter</span><span class=p>(</span><span class=n>work</span><span class=p>,</span> <span class=n>mysize</span><span class=p>,</span> <span class=n>MPI_FLOAT</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>local</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>mysize</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>MPI_FLOAT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span> <span class=c1>// scatter 分发数据到各process主位置
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>left</span> <span class=o>=</span> <span class=p>(</span><span class=n>rank</span><span class=o>+</span><span class=n>p</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>%</span><span class=n>p</span><span class=p>;</span>      <span class=c1>// who is my left neighour?
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>right</span> <span class=o>=</span> <span class=p>(</span><span class=n>rank</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span><span class=o>%</span><span class=n>p</span><span class=p>;</span>       <span class=c1>// who is my right neighour?
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=k>do</span> <span class=p>{</span> <span class=c1>//[0]和[mysize+1]halo
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>MPI_Sendrecv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>local</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_FLOAT</span><span class=p>,</span> <span class=n>left</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span>        <span class=c1>// send this
</span></span></span><span class=line><span class=cl><span class=c1></span>                     <span class=o>&amp;</span><span class=n>local</span><span class=p>[</span><span class=n>mysize</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_FLOAT</span><span class=p>,</span> <span class=n>right</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=c1>// receive this
</span></span></span><span class=line><span class=cl><span class=c1></span>                     <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>               <span class=c1>// anti-clockwise
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>MPI_Sendrecv</span><span class=p>(</span><span class=o>&amp;</span><span class=n>local</span><span class=p>[</span><span class=n>mysize</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_FLOAT</span><span class=p>,</span> <span class=n>right</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                     <span class=o>&amp;</span><span class=n>local</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_FLOAT</span><span class=p>,</span> <span class=n>left</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                     <span class=n>MPI_COMM_WORLD</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>               <span class=c1>// clockwise
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>do_one_step</span><span class=p>(</span><span class=n>local</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>local_error</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Allreduce</span><span class=p>(</span><span class=o>&amp;</span><span class=n>local_error</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>global_error</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>MPI_FLOAT</span><span class=p>,</span> <span class=n>MPI_MAX</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>while</span> <span class=p>(</span><span class=n>global_error</span> <span class=o>&gt;</span> <span class=n>acceptable_error</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Gather</span> <span class=p>(</span><span class=o>&amp;</span><span class=n>local</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>mysize</span><span class=p>,</span> <span class=n>MPI_FLOAT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>work</span><span class=p>,</span> <span class=n>mysize</span><span class=p>,</span> <span class=n>MPI_FLOAT</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=nf>print_results</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=n>work</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>MPI_Sendrecv</span><span class=p>(</span><span class=k>const</span> <span class=kt>void</span> <span class=o>*</span><span class=n>sendbuf</span><span class=p>,</span> <span class=kt>int</span> <span class=n>sendcount</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>sendtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>dest</span><span class=p>,</span> <span class=kt>int</span> <span class=n>sendtag</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=kt>void</span> <span class=o>*</span><span class=n>recvbuf</span><span class=p>,</span> <span class=kt>int</span> <span class=n>recvcount</span><span class=p>,</span> <span class=n>MPI_Datatype</span> <span class=n>recvtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>source</span><span class=p>,</span> <span class=kt>int</span> <span class=n>recvtag</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>,</span> <span class=n>MPI_Status</span> <span class=o>*</span><span class=n>status</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=communicators>Communicators<a hidden class=anchor aria-hidden=true href=#communicators>#</a></h3><p>Communicators define contexts within which groups of processes interact. All processes belong to <code>MPI_COMM_WORLD</code> from the MPI initialisation call onwards.</p><p>Create new communicators from old ones by collectively calling
<code>MPI_Comm_split(MPI_Comm old, int colour, int key, MPI_Comm *newcomm)</code> to create new communicators based on <strong><code>colors</code></strong> and <strong><code>keys</code></strong>:
<strong><code>color</code></strong> - control of subset assignment (nonnegative integer). Processes with the same color are in the same new communicator.
<strong><code>key</code></strong> - control of rank assignment (integer).</p><p>Within each new communicator, processes are assigned a new rank in the range $0...p^{\prime} − 1$, where $p^{\prime}$ is the size of the new communicator. Ranks are ordered by (but not necessarily equal to) the value passed in as the <code>key</code> parameter, with ties broken by considering process rank in the parent communicator.</p><p>This can be helpful in expressing algorithms which contain nested structure. For example, many <strong>divide-and-conquer</strong> algorithms split the data and machine in half, process recursively within the halves, then unwind to process the recursive results back at the upper level.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>//Divide &amp; Conquer Communicators
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>void</span> <span class=nf>some_DC_algorithm</span> <span class=p>(</span> <span class=p>...,</span> <span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>MPI_Comm_size</span><span class=p>(</span><span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>p</span><span class=p>);</span> <span class=nf>MPI_Comm_rank</span><span class=p>(</span><span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>myrank</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>...</span> <span class=n>pre</span><span class=o>-</span><span class=n>recursion</span> <span class=n>work</span> <span class=p>...</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>p</span><span class=o>&gt;</span><span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>MPI_Comm_split</span> <span class=p>(</span><span class=n>comm</span><span class=p>,</span> <span class=n>myrank</span><span class=o>&lt;</span><span class=p>(</span><span class=n>p</span><span class=o>/</span><span class=mi>2</span><span class=p>),</span> <span class=mi>0</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>subcomm</span><span class=p>);</span> <span class=c1>// two sub-machines
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>some_DC_algorithm</span> <span class=p>(...,</span> <span class=n>subcomm</span><span class=p>);</span> <span class=c1>// recursive step
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// in both sub-machines
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span> <span class=k>else</span> <span class=nf>do_base_case_solution_locally</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>...</span> <span class=n>post</span><span class=o>-</span><span class=n>recursion</span> <span class=n>work</span> <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=task-and-pattern-based-models>Task and Pattern Based Models<a hidden class=anchor aria-hidden=true href=#task-and-pattern-based-models>#</a></h2><p>Programming explicitly with threads (or processes) has some drawbacks:
• Natural expression of many highly parallel algorithms involves creation of far more threads than there are cores. Thread creation and scheduling have higher overheads than simpler activities like function calls (by a factor of 50-100).
• The OS has control over the scheduling of threads to processor cores, but it does not have the application specific knowledge required to make intelligent assignments (for example to optimize cache re-use). Traditional OS concerns for fairness may be irrelevant or even counter-productive.</p><p>To avoid this, programmers resort to complex scheduling and synchronization of a smaller number of coarser grained threads. How to avoid this?</p><p>A number of languages and libraries have emerged which
• separate the responsibility for identifying potential parallelism, which remains the application programmer&rsquo;s job, from detailed scheduling of this work to threads and cores, which becomes the language/library run-time&rsquo;s job.
• provide abstractions of common patterns of parallelism, which can be specialized with application specific operations, leaving implementation of the pattern and its inherent synchronization to the system.</p><p>These are sometimes called <strong>task based</strong> approaches, in contrast to traditional threaded models. Examples include <strong>OpenMP</strong>, which is a compiler/language based model, and Intel&rsquo;s <strong>Threading Building Blocks</strong> library.</p><h2 id=threading-building-blocks>Threading Building Blocks<a hidden class=anchor aria-hidden=true href=#threading-building-blocks>#</a></h2><p>Threading Building Blocks (TBB) is a shared variable model, C++ <strong>template-based</strong> library. It uses <strong>generic programming</strong> techniques to provide a collection of <strong>parallel algorithms</strong>, each of which is an <strong>abstraction of a parallel pattern</strong>. It also provides a direct mechanism for specifying task graphs and a collection of concurrent data structures and synchronization primitives.</p><blockquote><p>泛型程序设计（generic programming）是程序设计语言的一种风格或范式，允许程序员在强类型程序设计语言中编写代码时使用一些以后才指定的类型，在实例化时作为参数指明这些类型。</p></blockquote><p>It handles <strong>scheduling</strong> of tasks, whether explicit programmed or inferred from pattern calls, to a fixed number of threads internally. In effect, this is a hidden Bag-of-Tasks, leaving the OS with almost nothing to do.</p><p>Game of Life (<a href=http://web.stanford.edu/class/archive/cs/cs106b/cs106b.1186//assn/life.html>cs106b 作业1</a>) Original Code for a Step</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>enum</span> <span class=nc>State</span> <span class=p>{</span><span class=n>DEAD</span><span class=p>,</span><span class=n>ALIVE</span><span class=p>}</span> <span class=p>;</span> <span class=c1>// cell status
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>typedef</span> <span class=n>State</span> <span class=o>**</span><span class=n>Grid</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>NextGen</span><span class=p>(</span><span class=n>Grid</span> <span class=n>oldMap</span><span class=p>,</span> <span class=n>Grid</span> <span class=n>newMap</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>,</span> <span class=n>ncount</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>State</span> <span class=n>current</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>row</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>row</span> <span class=o>&lt;=</span> <span class=n>MAXROW</span><span class=p>;</span> <span class=n>row</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=n>col</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>col</span> <span class=o>&lt;=</span> <span class=n>MAXCOL</span><span class=p>;</span> <span class=n>col</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>current</span> <span class=o>=</span> <span class=n>oldMap</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=n>ncount</span> <span class=o>=</span> <span class=n>NeighborCount</span><span class=p>(</span><span class=n>oldMap</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=n>newMap</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>CellStatus</span><span class=p>(</span><span class=n>current</span><span class=p>,</span> <span class=n>ncount</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>   <span class=p>}</span>   <span class=p>}</span>
</span></span></code></pre></div><h3 id=tbb-parallel_for>TBB <code>parallel_for</code><a hidden class=anchor aria-hidden=true href=#tbb-parallel_for>#</a></h3><p>假设我们想将上面的函数<code>NextGen</code>应用到数组(网格)的每个元素，这个例子是可以放心使用并行处理模式的。函数模板<code>tbb::parallel_for</code> 将此迭代空间(<code>Range</code>)分解为一个个块，并把每个块运行在不同的线程上。要并行化这个循环，第一步是将循环体转换为可以在一个块上运行的形式 - 一个STL风格的函数对象，称为<code>body</code>对象，其中由<code>operator()</code>中处理。
Game of Life Step Using <code>parallel_for</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>void</span> <span class=nf>NextGen</span><span class=p>(</span><span class=n>Grid</span> <span class=n>oldMap</span><span class=p>,</span> <span class=n>Grid</span> <span class=n>newMap</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>parallel_for</span> <span class=p>(</span><span class=n>blocked_range</span><span class=o>&lt;</span><span class=kt>int</span><span class=o>&gt;</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>maxrow</span><span class=o>+</span><span class=mi>1</span><span class=p>),</span> <span class=c1>// Range
</span></span></span><span class=line><span class=cl><span class=c1></span>                  <span class=n>CompNextGen</span><span class=p>(</span><span class=n>oldMap</span><span class=p>,</span> <span class=n>newMap</span><span class=p>),</span>     <span class=c1>// Body
</span></span></span><span class=line><span class=cl><span class=c1></span>                  <span class=n>affinity_partitioner</span><span class=p>());</span>         <span class=c1>// Partitioner
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p><strong>Range</strong> defines a task(iteration) space, and its sub-division (partition) technique;
<strong>Body</strong> defines the code which processes a range;
<strong>Partitioner</strong> (optional parameter) influencing partitioning and scheduling strategy.</p><p>The <code>parallel_for</code> Template:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span> <span class=n>Range</span><span class=p>,</span> <span class=k>typename</span> <span class=n>Body</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=n>parallel_for</span><span class=p>(</span><span class=k>const</span> <span class=n>Range</span><span class=o>&amp;</span> <span class=n>range</span><span class=p>,</span> <span class=k>const</span> <span class=n>Body</span> <span class=o>&amp;</span><span class=n>body</span><span class=p>);</span>
</span></span></code></pre></div><p>Requires definition of:</p><ul><li>A <code>range</code> space to iterate over<ul><li>Must define a copy constructor and a destructor</li><li>a <strong>destructor</strong> to destroy these copies</li><li>Defines <code>is_empty()</code></li><li>Defines i<code>s_divisible()</code></li><li>Defines a <strong>splitting constructor</strong>, <code>R(R &amp;r, split)</code></li></ul></li><li>A <code>body</code> type that operates on the range (or a subrange)<ul><li>Must define a <strong>copy constructor</strong>, which is invoked to create a separate copy (or copies) for each worker thread.</li><li>Defines <code>operator()</code></li></ul></li></ul><blockquote><p>In the C++ programming language, a <strong>copy constructor</strong> is a special constructor for creating a new object as a copy of an existing object.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>//通用形式
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>classname</span> <span class=p>(</span><span class=k>const</span> <span class=n>classname</span> <span class=o>&amp;</span><span class=n>obj</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=c1>// body of constructor
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//实例
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#include</span> <span class=cpf>&lt;iostream&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>using</span> <span class=k>namespace</span> <span class=n>std</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Line</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>      <span class=kt>int</span> <span class=n>getLength</span><span class=p>(</span> <span class=kt>void</span> <span class=p>);</span>
</span></span><span class=line><span class=cl>      <span class=n>Line</span><span class=p>(</span> <span class=kt>int</span> <span class=n>len</span> <span class=p>);</span>             <span class=c1>// simple constructor
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=n>Line</span><span class=p>(</span> <span class=k>const</span> <span class=n>Line</span> <span class=o>&amp;</span><span class=n>obj</span><span class=p>);</span>      <span class=c1>// copy constructor
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=o>~</span><span class=n>Line</span><span class=p>();</span>                     <span class=c1>// destructor
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>   <span class=k>private</span><span class=o>:</span>
</span></span><span class=line><span class=cl>      <span class=kt>int</span> <span class=o>*</span><span class=n>ptr</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><h4 id=range-class>Range Class<a hidden class=anchor aria-hidden=true href=#range-class>#</a></h4><p>A <code>blocked_range&lt;T></code> is a template class provided by the library. It describes a one-dimensional iteration space over type <code>T</code>. and be queried for the beginning (<code>r.begin()</code>) and end (<code>r.end()</code>) of the range.</p><p>The TBB runtime can break a <code>blocked_range</code> into two smaller ranges, each (roughly) half the size.</p><p>Note that a <code>blocked_range</code> carries no problem data. The values in the range can be used as we choose, for example to index into arrays.
<strong>Range is Generic</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>R</span><span class=o>::</span><span class=n>R</span> <span class=p>(</span><span class=k>const</span> <span class=n>R</span><span class=o>&amp;</span><span class=p>)</span> <span class=c1>// Copy constructor
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>R</span><span class=o>::~</span><span class=n>R</span><span class=p>()</span>         <span class=c1>// Destructor
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>bool</span> <span class=n>R</span><span class=o>::</span><span class=n>is_empty</span><span class=p>()</span> <span class=k>const</span> <span class=c1>// True if range is empty
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>bool</span> <span class=n>R</span><span class=o>::</span><span class=n>is_divisible</span><span class=p>()</span> <span class=k>const</span> <span class=c1>// True if range can be partitioned
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>R</span><span class=o>::</span><span class=n>R</span> <span class=p>(</span><span class=n>R</span><span class=o>&amp;</span> <span class=n>r</span><span class=p>,</span> <span class=n>split</span><span class=p>)</span> <span class=c1>// Splitting constructor; splits r into two subranges
</span></span></span></code></pre></div><p>Besides the provided <code>blocked_range</code> and <code>blocked_range2d</code>, users can define their own ranges. TBB DIY Range Example: Compute Fibonacci numbers.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>class</span> <span class=nc>FibRange</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>n_</span> <span class=p>;</span> <span class=c1>// represents the range corresponding to fib(n)
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>FibRange</span><span class=p>(</span><span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=o>:</span> <span class=n>n_</span><span class=p>(</span><span class=n>n</span><span class=p>)</span> <span class=p>{</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>FibRange</span><span class=p>(</span><span class=n>FibRange</span><span class=o>&amp;</span> <span class=n>other</span><span class=p>,</span> <span class=n>split</span><span class=p>)</span> <span class=c1>// split constructor
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=o>:</span> <span class=n>n_</span><span class=p>(</span><span class=n>other</span><span class=p>.</span><span class=n>n_</span> <span class=o>-</span> <span class=mi>2</span><span class=p>)</span> <span class=c1>// initialize the new object
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>{</span> <span class=n>other</span><span class=p>.</span><span class=n>n_</span> <span class=o>=</span> <span class=n>other</span><span class=p>.</span><span class=n>n_</span> <span class=o>-</span> <span class=mi>1</span><span class=p>;}</span> <span class=c1>// reuse the other range object
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=kt>bool</span> <span class=nf>is_divisible</span><span class=p>()</span> <span class=k>const</span> <span class=p>{</span> <span class=k>return</span> <span class=p>(</span><span class=n>n_</span> <span class=o>&gt;</span> <span class=mi>10</span><span class=p>);</span> <span class=p>}</span> <span class=c1>// sequential threshold
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=kt>bool</span> <span class=nf>is_empty</span><span class=p>()</span> <span class=k>const</span> <span class=p>{</span> <span class=k>return</span> <span class=n>n_</span> <span class=o>&lt;</span> <span class=mi>0</span><span class=p>;</span> <span class=p>};</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><h4 id=body-class>Body Class<a hidden class=anchor aria-hidden=true href=#body-class>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>class</span> <span class=nc>CompNextGen</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Grid</span> <span class=n>oldMap</span><span class=p>,</span> <span class=n>newMap</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>    <span class=n>CompNextGen</span> <span class=p>(</span><span class=n>Grid</span> <span class=n>omap</span><span class=p>,</span> <span class=n>Grid</span> <span class=n>nmap</span><span class=p>)</span> <span class=o>:</span> <span class=n>oldMap</span><span class=p>(</span><span class=n>omap</span><span class=p>),</span> <span class=n>newMap</span><span class=p>(</span><span class=n>nmap</span><span class=p>)</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// 分割迭代空间的方式多种多样
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>void</span> <span class=nf>operator</span><span class=p>()(</span> <span class=k>const</span> <span class=n>blocked_range</span><span class=o>&lt;</span><span class=kt>int</span><span class=o>&gt;&amp;</span> <span class=n>r</span> <span class=p>)</span> <span class=k>const</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=n>r</span><span class=p>.</span><span class=n>begin</span><span class=p>();</span> <span class=n>row</span> <span class=o>&lt;</span> <span class=n>r</span><span class=p>.</span><span class=n>end</span><span class=p>();</span> <span class=n>row</span><span class=o>++</span><span class=p>){</span> <span class=c1>// 这里按行分割
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>col</span> <span class=o>&lt;=</span> <span class=n>maxcol</span><span class=p>;</span> <span class=n>col</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>nState</span> <span class=n>current</span> <span class=o>=</span> <span class=n>oldMap</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>ncount</span> <span class=o>=</span> <span class=n>NeighborCount</span><span class=p>(</span><span class=n>oldMap</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                <span class=n>newMap</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>CellStatus</span><span class=p>(</span><span class=n>current</span><span class=p>,</span> <span class=n>ncount</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p><strong>Body is Generic</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>Body</span><span class=o>::</span><span class=n>Body</span><span class=p>(</span><span class=k>const</span> <span class=n>Body</span><span class=o>&amp;</span><span class=p>)</span> <span class=err>\\</span> <span class=n>Copy</span> <span class=n>constructor</span>
</span></span><span class=line><span class=cl><span class=n>Body</span><span class=o>::~</span><span class=n>Body</span><span class=p>()</span>           <span class=err>\\</span> <span class=n>Destructor</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=n>Body</span><span class=o>::</span><span class=k>operator</span><span class=p>()</span> <span class=p>(</span><span class=n>Range</span><span class=o>&amp;</span> <span class=n>subrange</span><span class=p>)</span> <span class=k>const</span>  <span class=err>\\</span> <span class=n>Apply</span> <span class=n>the</span> <span class=n>body</span> <span class=n>to</span> <span class=n>subrange</span><span class=p>.</span>
</span></span></code></pre></div><blockquote><p>Because the body object might be copied, its <code>operator()</code> should not modify the body hence should be declared <code>const</code>. Otherwise the modification might or might not become visible to the thread that invoked parallel_for, depending upon whether <code>operator()</code> is acting on the original or a copy.
Credit from <a href=https://www.threadingbuildingblocks.org/docs/help/tbb_userguide/parallel_for.html>www.threadingbuildingblocks.org</a></p></blockquote><p><code>parallel_for</code> partitions original range into subranges, and deals out subranges to worker threads in a way that: Balances load, Uses cache efficiently, and Scales.</p><p>Game of Life 1D with C++11 Lambda Function, an alternative interface to <code>parallel_for</code> allows us to use a C++ lambda expression to avoid writing a body class.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>void</span> <span class=nf>NextGen</span><span class=p>(</span><span class=n>Grid</span> <span class=n>oldMap</span><span class=p>,</span> <span class=n>Grid</span> <span class=n>newMap</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>parallel_for</span> <span class=p>(</span><span class=n>blocked_range</span><span class=o>&lt;</span><span class=kt>int</span><span class=o>&gt;</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>maxrow</span><span class=o>+</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=p>[</span><span class=o>&amp;</span><span class=p>](</span><span class=k>const</span> <span class=n>blocked_range</span><span class=o>&lt;</span><span class=kt>int</span><span class=o>&gt;&amp;</span> <span class=n>r</span><span class=p>){</span>
</span></span><span class=line><span class=cl>                    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=n>r</span><span class=p>.</span><span class=n>begin</span><span class=p>();</span> <span class=n>row</span> <span class=o>&lt;</span> <span class=n>r</span><span class=p>.</span><span class=n>end</span><span class=p>();</span> <span class=n>row</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>                        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>col</span> <span class=o>&lt;=</span> <span class=n>MAXCOL</span><span class=p>;</span> <span class=n>col</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                            <span class=n>State</span> <span class=n>current</span> <span class=o>=</span> <span class=n>oldMap</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                            <span class=kt>int</span> <span class=n>ncount</span> <span class=o>=</span> <span class=n>NeighborCount</span><span class=p>(</span><span class=n>oldMap</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                            <span class=n>newMap</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>CellStatus</span><span class=p>(</span><span class=n>current</span><span class=p>,</span> <span class=n>ncount</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                        <span class=p>}</span>                    <span class=p>}</span>                <span class=p>}</span>    <span class=p>);}</span>
</span></span></code></pre></div><p><code>[&]</code>引入 lambda 表达式. 该表达式创建一个类似于<code>CompNextGen</code>的函数对象. 当局部变量在 lambda expression 之外声明，但又在lambda表达式内使用时, 它们被"捕获"为函数对象内的字段. <code>[&]</code>指定引用，<code>[=]</code>指定按值捕获.</p><h3 id=tbb-partitioners>TBB Partitioners<a hidden class=anchor aria-hidden=true href=#tbb-partitioners>#</a></h3><p>TBB supports different partitioning strategy:
1, <code>tbb::parallel_for( range, body, tbb::simple_partitioner() );</code> <strong>forces</strong> all ranges to be <strong>fully partitioned</strong> (i.e. until <code>is_divisible()</code> fails).
2, <code>tbb::parallel_for( range, body, tbb::auto_partitioner() );</code> allows the TBB runtime to <strong>decide</strong> whether to partition the range (to improve <strong>granularity</strong>).
3, <code>tbb::parallel_for( range, body, tbb::affinity_partitioner );</code> is like <code>auto_partitioner()</code> but also, when the <code>parallel_for</code> is inside a loop, tries to allocate the same range to the same processor core across iterations to <strong>improve cache behaviour</strong>.</p><p>Game of Life Using a 2D decomposition</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>void</span> <span class=nf>NextGen</span><span class=p>(</span><span class=n>Grid</span> <span class=n>oldMap</span><span class=p>,</span> <span class=n>Grid</span> <span class=n>newMap</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>parallel_for</span> <span class=p>(</span><span class=n>blocked_range2d</span><span class=o>&lt;</span><span class=kt>int</span><span class=p>,</span> <span class=kt>int</span><span class=o>&gt;</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>maxrow</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>maxcol</span><span class=o>+</span><span class=mi>1</span><span class=p>),</span> <span class=c1>// Range
</span></span></span><span class=line><span class=cl><span class=c1></span>                  <span class=n>CompNextGen</span><span class=p>(</span><span class=n>oldMap</span><span class=p>,</span> <span class=n>newMap</span><span class=p>));</span>                  <span class=c1>// Body
</span></span></span><span class=line><span class=cl><span class=c1></span>                  <span class=n>auto_partitioner</span><span class=p>());</span>                           <span class=c1>// Partitioner
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>CompNextGen</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Grid</span> <span class=n>oldMap</span><span class=p>,</span> <span class=n>Grid</span> <span class=n>newMap</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>        <span class=n>CompNextGen</span> <span class=p>(</span><span class=n>Grid</span> <span class=n>omap</span><span class=p>,</span> <span class=n>Grid</span> <span class=n>nmap</span><span class=p>)</span> <span class=o>:</span> <span class=n>oldMap</span><span class=p>(</span><span class=n>omap</span><span class=p>),</span> <span class=n>newMap</span><span class=p>(</span><span class=n>nmap</span><span class=p>)</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// 二维分割
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>void</span> <span class=nf>operator</span><span class=p>()(</span> <span class=k>const</span> <span class=n>blocked_range2d</span><span class=o>&lt;</span><span class=kt>int</span><span class=p>,</span> <span class=kt>int</span><span class=o>&gt;&amp;</span> <span class=n>r</span> <span class=p>)</span> <span class=k>const</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=n>r</span><span class=p>.</span><span class=n>rows</span><span class=p>().</span><span class=n>begin</span><span class=p>();</span> <span class=n>row</span> <span class=o>&lt;</span> <span class=n>r</span><span class=p>.</span><span class=n>rows</span><span class=p>().</span><span class=n>end</span><span class=p>();</span> <span class=n>row</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=n>r</span><span class=p>.</span><span class=n>cols</span><span class=p>().</span><span class=n>begin</span><span class=p>();</span> <span class=n>col</span> <span class=o>&lt;</span> <span class=n>r</span><span class=p>.</span><span class=n>cols</span><span class=p>().</span><span class=n>end</span><span class=p>();</span> <span class=n>col</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>State</span> <span class=n>current</span> <span class=o>=</span> <span class=n>oldMap</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>ncount</span> <span class=o>=</span> <span class=n>NeighborCount</span><span class=p>(</span><span class=n>oldMap</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                <span class=n>newMap</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>CellStatus</span><span class=p>(</span><span class=n>current</span><span class=p>,</span> <span class=n>ncount</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>        <span class=p>}</span>    <span class=p>};}</span>
</span></span></code></pre></div><p><code>blocked_range2d</code> is partitioned in alternating dimensions, level by level.</p><h3 id=tbb-parallel_reduce-template>TBB <code>parallel_reduce</code> Template<a hidden class=anchor aria-hidden=true href=#tbb-parallel_reduce-template>#</a></h3><p>TBB <code>parallel_reduce</code> has similar structure to <code>parallel_for</code> but additionally allows bodies to <code>gather results</code> internally as they go along.</p><p>We could parallelize a loop reduction (iterations are independent), as in a Numerical Integration example, with a <code>parallel_for</code>, but we would need a <strong>critical section</strong> of some kind to accumulate the partial results. <strong><code>parallel_reduce</code></strong> structures and hides this, with one further generic operation, called <code>join</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span> <span class=n>Range</span><span class=p>,</span> <span class=k>typename</span> <span class=n>Body</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=n>parallel_reduce</span> <span class=p>(</span><span class=k>const</span> <span class=n>Range</span><span class=o>&amp;</span> <span class=n>range</span><span class=p>,</span> <span class=n>Body</span> <span class=o>&amp;</span><span class=n>body</span><span class=p>);</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>Body</span><span class=o>::</span><span class=n>Body</span><span class=p>(</span> <span class=k>const</span> <span class=n>Body</span><span class=o>&amp;</span><span class=p>,</span> <span class=n>split</span> <span class=p>)</span> <span class=c1>//Splitting constructor
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>Body</span><span class=o>::~</span><span class=n>Body</span><span class=p>()</span>                    <span class=c1>// Destructor
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>void</span> <span class=n>Body</span><span class=o>::</span><span class=k>operator</span><span class=p>()</span> <span class=p>(</span><span class=n>Range</span><span class=o>&amp;</span> <span class=n>subrange</span><span class=p>)</span> <span class=k>const</span> <span class=c1>// Accumulate results from subrange
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>void</span> <span class=n>Body</span><span class=o>::</span><span class=n>join</span><span class=p>(</span> <span class=n>Body</span><span class=o>&amp;</span> <span class=n>rhs</span> <span class=p>);</span> <span class=c1>// Merge result of rhs into the result of this.
</span></span></span></code></pre></div><p>When a worker thread is <strong>available</strong>, as decided by the task scheduler, <code>parallel_reduce</code> invokes the <code>splitting constructor</code> to create a subtask for the worker. When the subtask <strong>completes</strong>, <code>parallel_reduce</code> uses method <code>join</code> to accumulate the result of the subtask. It reuses <strong>Range</strong> concept from <code>parallel_for</code>.
<img loading=lazy src=https://www.threadingbuildingblocks.org/docs/help/tbb_userguide/Images/image009.jpg title="Graph of the Split-join Sequence. An arc indicates order in time. image from https://www.threadingbuildingblocks.org/docs/help/tbb_userguide/Images/image009.jpg">
The Fib Body Class (with <code>operator()</code>), using the DIY range example - <code>FibRange</code> from above</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>class</span> <span class=nc>Fib</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>fsum_</span> <span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>Fib</span><span class=p>()</span> <span class=o>:</span> <span class=n>fsum_</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=p>{</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>Fib</span><span class=p>(</span><span class=n>Fib</span><span class=o>&amp;</span> <span class=n>other</span><span class=p>,</span> <span class=n>split</span><span class=p>)</span> <span class=o>:</span> <span class=n>fsum_</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=p>{</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1>// use += since each body may accumulate more than one range
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=kt>void</span> <span class=nf>operator</span><span class=p>()</span> <span class=p>(</span><span class=n>FibRange</span><span class=o>&amp;</span> <span class=n>range</span><span class=p>)</span> <span class=p>{</span> <span class=n>fsum_</span> <span class=o>+=</span> <span class=n>fib</span><span class=p>(</span><span class=n>range</span><span class=p>.</span><span class=n>n_</span> <span class=p>);</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=nf>fib</span><span class=p>(</span><span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span><span class=k>if</span> <span class=p>(</span><span class=n>n</span> <span class=o>&lt;</span> <span class=mi>2</span><span class=p>)</span> <span class=k>return</span> <span class=mi>1</span><span class=p>;</span> <span class=k>else</span> <span class=k>return</span> <span class=n>fib</span><span class=p>(</span><span class=n>n</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>+</span><span class=n>fib</span><span class=p>(</span><span class=n>n</span><span class=o>-</span><span class=mi>2</span><span class=p>);}</span>
</span></span><span class=line><span class=cl>        <span class=kt>void</span> <span class=nf>join</span><span class=p>(</span><span class=n>Fib</span><span class=o>&amp;</span> <span class=n>rhs</span><span class=p>)</span> <span class=p>{</span> <span class=n>fsum_</span> <span class=o>+=</span> <span class=n>rhs</span><span class=p>.</span><span class=n>fsum_</span><span class=p>;</span> <span class=p>};</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span> <span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span><span class=o>*</span> <span class=n>argv</span><span class=p>[]</span> <span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>task_scheduler_init</span> <span class=n>init</span><span class=p>(</span><span class=mi>2</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>Fib</span> <span class=n>f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>parallel_reduce</span><span class=p>(</span><span class=n>FibRange</span><span class=p>(</span><span class=n>FIBSEED</span><span class=p>),</span> <span class=n>f</span><span class=p>,</span> <span class=n>simple_partitioner</span><span class=p>());</span>
</span></span><span class=line><span class=cl>    <span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;Fib &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>FIBSEED</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; is &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>f</span><span class=p>.</span><span class=n>fsum_</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Using a <code>simple_partitioner</code> forces full splitting of the ranges. We could use <code>auto_partitioner</code> to let the TBB run-time system control this.</p><h3 id=the-task-scheduler>The Task Scheduler<a hidden class=anchor aria-hidden=true href=#the-task-scheduler>#</a></h3><p>如果一个算法不能自然地映射到前面提到的任何其中一种 high-level loop templates，可以使用 task scheduler 直接操作任务, 可以构建新的高级模板。</p><p>All of TBB&rsquo;s parallel pattern constructs are implemented via the same underlying <strong>task scheduler</strong>, which executes a task graph representing the pattern.</p><p>TBB also allows the programmer to (carefully!) <strong>create task graphs directly</strong>. This allows expression of unstructured task graphs, or the implementation and abstraction of further patterns.</p><p>There are functions to create new tasks as children of existing tasks and to specify the control dependencies between them.</p><p>How to code Fibonacci using tasks directly? The key method is <code>task::execute</code>, which we override with our application specific behaviour.</p><p>Recursion is typically used to calculate Fibonacci number but leads to unbalanced task graph.</p><p>Fibonacci - Task Spawning Solution - Use TBB tasks to thread creation and execution of task graph:</p><ol><li>Allocate space for the task by a special &ldquo;overloaded new&rdquo; and method <code>task::allocate_root</code> - Create root of a task tree. Tasks must be allocated by special methods so that the space can be efficiently recycled when the task completes.</li><li>Construct task with the constructor <code>FibTask(n, &amp;sum)</code> invoked by <code>new</code>. When the task is run in step 3, it computes the nth Fibonacci number and stores it into <code>*sum</code>.</li><li>Run the task and wait for completion with <code>task::spawn_root_and_wait</code>.</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&#34;tbb/task.h&#34;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=kt>long</span> <span class=n>ParallelFib</span><span class=p>(</span> <span class=kt>long</span> <span class=n>n</span> <span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>long</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>FibTask</span><span class=o>&amp;</span> <span class=n>a</span> <span class=o>=</span> <span class=o>*</span><span class=k>new</span><span class=p>(</span><span class=n>task</span><span class=o>::</span><span class=n>allocate_root</span><span class=p>())</span> <span class=n>FibTask</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>sum</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>task</span><span class=o>::</span><span class=n>spawn_root_and_wait</span><span class=p>(</span><span class=n>a</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FibTask</span><span class=o>:</span> <span class=k>public</span> <span class=n>task</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=kt>long</span> <span class=n>n</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>long</span><span class=o>*</span> <span class=k>const</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>FibTask</span><span class=p>(</span> <span class=kt>long</span> <span class=n>n_</span><span class=p>,</span> <span class=kt>long</span><span class=o>*</span> <span class=n>sum_</span> <span class=p>)</span> <span class=o>:</span> <span class=n>n</span><span class=p>(</span><span class=n>n_</span><span class=p>),</span> <span class=n>sum</span><span class=p>(</span><span class=n>sum_</span><span class=p>)</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=n>task</span><span class=o>*</span> <span class=nf>execute</span><span class=p>()</span> <span class=p>{</span> <span class=c1>// Overrides virtual function task::execute
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>if</span><span class=p>(</span> <span class=n>n</span> <span class=o>&lt;</span> <span class=n>CutOff</span> <span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=o>*</span><span class=n>sum</span> <span class=o>=</span> <span class=n>SerialFib</span><span class=p>(</span><span class=n>n</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=kt>long</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>                <span class=n>FibTask</span><span class=o>&amp;</span> <span class=n>a</span> <span class=o>=</span> <span class=o>*</span><span class=k>new</span><span class=p>(</span> <span class=n>allocate_child</span><span class=p>()</span> <span class=p>)</span> <span class=n>FibTask</span><span class=p>(</span><span class=n>n</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=o>&amp;</span><span class=n>x</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                <span class=n>FibTask</span><span class=o>&amp;</span> <span class=n>b</span> <span class=o>=</span> <span class=o>*</span><span class=k>new</span><span class=p>(</span> <span class=n>allocate_child</span><span class=p>()</span> <span class=p>)</span> <span class=n>FibTask</span><span class=p>(</span><span class=n>n</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span><span class=o>&amp;</span><span class=n>y</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                <span class=n>set_ref_count</span><span class=p>(</span><span class=mi>3</span><span class=p>);</span> <span class=c1>// Set to 3  = 2 children + 1 for wait
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=n>spawn</span><span class=p>(</span> <span class=n>b</span> <span class=p>);</span> <span class=c1>// Start b running.
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=c1>// Start a running and wait for all children (a and b).
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=n>spawn_and_wait_for_all</span><span class=p>(</span><span class=n>a</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                <span class=o>*</span><span class=n>sum</span> <span class=o>=</span> <span class=n>x</span><span class=o>+</span><span class=n>y</span><span class=p>;</span> <span class=c1>// Do the sum
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><p>The TBB scheduler runs tasks in a way that tends to minimize both memory demands and cross-thread communication. The intuition is that a balance must be reached between depth-first and breadth-first execution.</p><p>At any point in execution, the collection of known tasks is maintained as a <strong>shared</strong> graph. Each thread maintains its own <strong>double-ended queue</strong> of tasks (roughly, as pointers into the shared graph).</p><p>Newly <strong>spawned</strong> tasks are added to the front of the local queue.</p><p>当一条线程参与 task graph 时，它会不断按照优先原则执行下面的规则来获取任务:</p><ol><li>looks at the <strong>front of its local queue</strong>, which encourages locality within one thread&rsquo;s work; 如果 deque 为空，则此规则不适用；</li><li>假如失败, steal a task from the <strong>back of one other (randomly chosen) thread&rsquo;s queue</strong>, which encourages stealing of big tasks, and discourages locality across threads.
<img loading=lazy src=/images/TBB_Scheduler.png title="Image from: http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf"></li></ol><h2 id=linda>Linda<a hidden class=anchor aria-hidden=true href=#linda>#</a></h2><p>Linda presents an alternative conceptual model for parallelism, based around a small library of operations. The Linda model saw something of a revival in distributed java systems programming, under the name <strong>JavaSpaces</strong>.</p><p>The key concept is that processes interact through <strong>tuple space</strong>, a global, <strong>content addressable</strong> memory, which is thread safe, with no race conditions, therefore does not require explicit <strong>locks</strong>. Each tuple is an <strong>ordered</strong> collection of typed data fields. Duplicate tuples are allowed.</p><p>The tuple space itself acts like a <strong>monitor</strong>. If a process tries to access a tuplen, it is blocked until a matching tuple becomes available.</p><p><strong>Semaphore</strong> - Linda have tuple (or a set of tuples for a counting semaphore) that represent the locks. If someone needs to enter the lock, it waits until a tuple is available in the bag, pull it out of the bag and inserts it back into the tuple space.</p><p>Processes run <strong>asynchronously</strong> and can operate on tuple space with six operations.</p><p>1, Add new tuple to tuple space: <code>out(exp1, exp2, ...., expN);</code> - evaluates the expressions in the parameter list before <strong>atomically</strong> placing a copy of the results as a new tuple in tuple space. It could be considered as an <strong>asynchronous send with a wild-card destination</strong> in message-passing. <code>out("sum", 2, 3)</code>, <code>out("Green", x*y, square(2));</code></p><p>2, To take a tuple from tuple space: <code>in(tuple-template);</code> - <strong>atomically removes</strong> from tuple space a tuple which <strong>matches the template</strong>. <code>template</code> contains actual values and formal parameters (indicated by <code>?</code>) to be assigned during the match. 匹配包含与实际值的匹配，以及与形式参数类型 types 相匹配. <code>in</code> is <strong>blocking</strong>, in the sense that the caller is <strong>suspended</strong> until a matching tuple becomes available. For example: <code>in("sum",?i,?j)</code> matches <code>"sum"</code>, assigns <code>2</code> to <code>i</code> and <code>3</code> to <code>j</code> and the tuple is removed from the tuple space. <code>in("Green", ?y, ?r, FALSE);</code>. We could think of <code>in</code> as a <strong>blocking, asynchronous receive, with wild-card source</strong>, but with additional constraints implied by the pattern matching.</p><p>3, <strong>Atomically</strong> read a tuple from tuple space with <code>rd(tuple-template);</code></p><p>4, Tuples may also be created with <code>eval(expr, expr, ...)</code> which is like <code>out</code>, but <strong>dynamically creates new processes</strong> to evaluate each field of the tuple which has been expressed as a function call. The calling process continues immediately, and the resulting tuple enters tuple space atomically when all the newly sparked processes have terminated</p><p>5, Finally, there are <strong>non-blocking</strong> forms <code>inp</code>, <code>rdp</code> (<code>p</code> for &ldquo;predicate&rdquo;) which complete &ldquo;immediately&rdquo;, returning a boolean indicating whether or not a match occurred. This allow a process to carry on with a different task and then try again later.</p><p><strong>Bag of Tasks</strong> Implementation:
同样以前面的 Adaptive Quadrature 为例. Use a <code>("counts", x, y)</code> tuple, in effect as a shared variable, to count the number of tasks and number of idle workers. The final field in a task tuple indicates whether this is a real task or a &ldquo;no more tasks&rdquo; signal.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span> <span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>out</span><span class=p>(</span><span class=s>&#34;total&#34;</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>);</span> <span class=nf>out</span><span class=p>(</span><span class=s>&#34;counts&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>P</span><span class=p>);</span> <span class=c1>// set initial #task and #idle
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>out</span><span class=p>(</span><span class=s>&#34;task&#34;</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=nf>f</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=nf>f</span><span class=p>(</span><span class=n>b</span><span class=p>),</span> <span class=n>approxarea</span><span class=p>,</span> <span class=n>FALSE</span><span class=p>);</span> <span class=c1>// make initial task
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>P</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=nf>eval</span><span class=p>(</span><span class=nf>worker</span><span class=p>());</span>             <span class=c1>// create P workers
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>in</span> <span class=p>(</span><span class=s>&#34;counts&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>P</span><span class=p>);</span>              <span class=c1>// no tasks left, and P workers idle
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>in</span> <span class=p>(</span><span class=s>&#34;total&#34;</span><span class=p>,</span> <span class=o>?</span><span class=n>total</span><span class=p>);</span>                             <span class=c1>// get the result
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>out</span> <span class=p>(</span><span class=s>&#34;task&#34;</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=n>TRUE</span><span class=p>);</span>      <span class=c1>// indicate no more tasks
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>...</span>                                               <span class=c1>//use total
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>worker</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=nb>true</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>in</span><span class=p>(</span><span class=s>&#34;task&#34;</span><span class=p>,</span> <span class=o>?</span><span class=n>left</span><span class=p>,</span> <span class=o>?</span><span class=n>right</span><span class=p>,</span> <span class=o>?</span><span class=n>fleft</span><span class=p>,</span> <span class=o>?</span><span class=n>fright</span><span class=p>,</span> <span class=o>?</span><span class=n>lrarea</span><span class=p>,</span> <span class=o>?</span><span class=n>gameOver</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>gameOver</span><span class=p>)</span> <span class=p>{</span>  <span class=c1>// if gameOver == TRUE
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>out</span> <span class=p>(</span><span class=s>&#34;task&#34;</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=n>TRUE</span><span class=p>);</span> <span class=c1>// for others to see
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>break</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=nf>in</span><span class=p>(</span><span class=s>&#34;counts&#34;</span><span class=p>,</span> <span class=o>?</span><span class=n>size</span><span class=p>,</span> <span class=o>?</span><span class=n>idle</span><span class=p>);</span> <span class=nf>out</span><span class=p>(</span><span class=s>&#34;counts&#34;</span><span class=p>,</span> <span class=n>size</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>idle</span><span class=o>-</span><span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>...</span> <span class=n>usual</span> <span class=n>task</span> <span class=n>calculations</span> <span class=p>...</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=nf>abs</span> <span class=p>(</span><span class=n>larea</span> <span class=o>+</span> <span class=n>rarea</span> <span class=o>-</span> <span class=n>lrarea</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>EPSILON</span><span class=p>)</span> <span class=p>{</span> <span class=c1>// create new tasks
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>out</span><span class=p>(</span><span class=s>&#34;task&#34;</span><span class=p>,</span> <span class=n>left</span><span class=p>,</span> <span class=n>mid</span><span class=p>,</span> <span class=n>fleft</span><span class=p>,</span> <span class=n>fmid</span><span class=p>,</span> <span class=n>larea</span><span class=p>,</span> <span class=n>FALSE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=nf>out</span><span class=p>(</span><span class=s>&#34;task&#34;</span><span class=p>,</span> <span class=n>mid</span><span class=p>,</span> <span class=n>right</span><span class=p>,</span> <span class=n>fmid</span><span class=p>,</span> <span class=n>fright</span><span class=p>,</span> <span class=n>rarea</span><span class=p>,</span> <span class=n>FALSE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=nf>in</span><span class=p>(</span><span class=s>&#34;counts&#34;</span><span class=p>,</span> <span class=o>?</span><span class=n>size</span><span class=p>,</span> <span class=o>?</span><span class=n>idle</span><span class=p>);</span> <span class=nf>out</span><span class=p>(</span><span class=s>&#34;counts&#34;</span><span class=p>,</span> <span class=n>size</span><span class=o>+</span><span class=mi>2</span><span class=p>,</span> <span class=n>idle</span><span class=o>+</span><span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=nf>in</span> <span class=p>(</span><span class=s>&#34;total&#34;</span><span class=p>,</span> <span class=o>?</span><span class=n>total</span><span class=p>);</span> <span class=nf>out</span> <span class=p>(</span><span class=s>&#34;total&#34;</span><span class=p>,</span> <span class=n>total</span><span class=o>+</span><span class=n>larea</span><span class=o>+</span><span class=n>rarea</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=nf>in</span><span class=p>(</span><span class=s>&#34;counts&#34;</span><span class=p>,</span> <span class=o>?</span><span class=n>size</span><span class=p>,</span> <span class=o>?</span><span class=n>idle</span><span class=p>);</span> <span class=nf>out</span><span class=p>(</span><span class=s>&#34;counts&#34;</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>idle</span><span class=o>+</span><span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>  <span class=p>}</span>    <span class=p>}</span>
</span></span></code></pre></div><p><strong>Pipeline</strong> Implementation:
Use <code>eval()</code> to create the sieve processes <strong>dynamically</strong> as we need them. The sieve processes eventually <strong>turn into</strong> part of an &ldquo;array&rdquo; of primes in tuple space. We ensure <strong>pipelined message flow by tagging</strong> tuples with their destination and position in the sequence.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>void</span> <span class=nf>main</span> <span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span> <span class=kt>char</span> <span class=o>*</span><span class=n>argv</span><span class=p>[])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nf>eval</span><span class=p>(</span><span class=s>&#34;prime&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=nf>sieve</span><span class=p>(</span><span class=mi>1</span><span class=p>));</span> <span class=c1>// the 1st prime number, the 1st worker
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=mi>2</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>LIMIT</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>out</span><span class=p>(</span><span class=s>&#34;number&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>);</span> <span class=c1>// send number to sieve
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>sieve</span> <span class=p>(</span><span class=kt>int</span> <span class=n>me</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span><span class=p>,</span> <span class=n>in_seq</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>out_seq</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stop</span><span class=o>=</span><span class=n>FALSE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nf>in</span><span class=p>(</span><span class=s>&#34;number&#34;</span><span class=p>,</span> <span class=n>me</span><span class=p>,</span> <span class=n>in_seq</span><span class=p>,</span> <span class=o>?</span><span class=n>p</span><span class=p>);</span>   <span class=c1>// in_seq = 1, first arrival is prime
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>while</span> <span class=p>(</span><span class=o>!</span><span class=n>stop</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>in_seq</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=nf>in</span><span class=p>(</span><span class=s>&#34;number&#34;</span><span class=p>,</span> <span class=n>me</span><span class=p>,</span> <span class=n>in_seq</span><span class=p>,</span> <span class=o>?</span><span class=n>n</span><span class=p>);</span>      <span class=c1>// get the next candidate
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>if</span> <span class=p>(</span><span class=n>n</span><span class=o>==</span><span class=n>LIMIT</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>stop</span> <span class=o>=</span> <span class=n>TRUE</span><span class=p>;</span> <span class=nf>out</span><span class=p>(</span><span class=s>&#34;number&#34;</span><span class=p>,</span> <span class=n>me</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>out_seq</span><span class=p>,</span> <span class=n>n</span><span class=p>);</span> <span class=c1>// pass on the signal
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span> <span class=k>else</span> <span class=k>if</span> <span class=p>(</span><span class=n>n</span><span class=o>%</span><span class=n>p</span> <span class=o>!=</span><span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// if never created a successor before
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>if</span> <span class=p>(</span><span class=n>out_seq</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=nf>eval</span><span class=p>(</span><span class=s>&#34;prime&#34;</span><span class=p>,</span> <span class=n>me</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=nf>sieve</span><span class=p>(</span><span class=n>me</span><span class=o>+</span><span class=mi>1</span><span class=p>));</span> <span class=c1>// new sieve
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=nf>out</span><span class=p>(</span><span class=s>&#34;number&#34;</span><span class=p>,</span> <span class=n>me</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>out_seq</span><span class=p>,</span> <span class=n>n</span><span class=p>);</span>       <span class=c1>// and its first input
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>out_seq</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>p</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=tuple-space>Tuple Space<a hidden class=anchor aria-hidden=true href=#tuple-space>#</a></h3><p>Linda&rsquo;s powerful matching model sets a demanding implementation challenge, way beyond the associative memory hardware used in on-chip caches.</p><p><strong>Indexing</strong> and <strong>hashing</strong> techniques adapted from relational database technology can help (e.g. Linda <code>rd</code> and SQL <code>select</code>).</p><p>Advanced Linda implementations perform considerable compile-time analysis of program specific tuple usage. For example, possible tuples (in a given program) can be categorised into a set of classes by <strong>type signature</strong>, and stored separately.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/software-engineer/>Software Engineer</a></li><li><a href=https://congchan.github.io/tags/parallelism--concurrency/>Parallelism & Concurrency</a></li><li><a href=https://congchan.github.io/tags/java/>Java</a></li><li><a href=https://congchan.github.io/tags/c/>C</a></li><li><a href=https://congchan.github.io/tags/course-note/>Course-Note</a></li><li><a href=https://congchan.github.io/tags/inf-course-note/>Inf Course Note</a></li></ul><nav class=paginav><a class=prev href=https://congchan.github.io/posts/inf-course-note-natural-language-understanding/><span class=title>« Prev</span><br><span>Inf Course Note - Natural Language Understanding</span>
</a><a class=next href=https://congchan.github.io/posts/inf-course-note-software-architecture-process-and-management/><span class=title>Next »</span><br><span>Inf Course Note - Software Architecture, Process, and Management</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Parallel Programming Language and Systems on x" href="https://x.com/intent/tweet/?text=Inf%20Course%20Note%20-%20Parallel%20Programming%20Language%20and%20Systems&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-parallel-programming-language-and-systems%2f&amp;hashtags=SoftwareEngineer%2cParallelism%26Concurrency%2cJava%2cC%2ccourse-note%2cInfCourseNote"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Parallel Programming Language and Systems on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-parallel-programming-language-and-systems%2f&amp;title=Inf%20Course%20Note%20-%20Parallel%20Programming%20Language%20and%20Systems&amp;summary=Inf%20Course%20Note%20-%20Parallel%20Programming%20Language%20and%20Systems&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-parallel-programming-language-and-systems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Parallel Programming Language and Systems on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-parallel-programming-language-and-systems%2f&title=Inf%20Course%20Note%20-%20Parallel%20Programming%20Language%20and%20Systems"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Parallel Programming Language and Systems on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-parallel-programming-language-and-systems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Parallel Programming Language and Systems on whatsapp" href="https://api.whatsapp.com/send?text=Inf%20Course%20Note%20-%20Parallel%20Programming%20Language%20and%20Systems%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-parallel-programming-language-and-systems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Parallel Programming Language and Systems on telegram" href="https://telegram.me/share/url?text=Inf%20Course%20Note%20-%20Parallel%20Programming%20Language%20and%20Systems&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-parallel-programming-language-and-systems%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Inf Course Note - Parallel Programming Language and Systems on ycombinator" href="https://news.ycombinator.com/submitlink?t=Inf%20Course%20Note%20-%20Parallel%20Programming%20Language%20and%20Systems&u=https%3a%2f%2fcongchan.github.io%2fposts%2finf-course-note-parallel-programming-language-and-systems%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>