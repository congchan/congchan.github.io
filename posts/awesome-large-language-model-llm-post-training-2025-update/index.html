<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Awesome Large Language Model (LLM) Post-training - [2025 Update] | Cong's Log</title><meta name=keywords content="2025,Large Language Model,Post-training"><meta name=description content="In the race to build truly helpful AI assistants, we&rsquo;ve discovered a fundamental truth: raw intelligence isn&rsquo;t enough. A model that masters calculus but can&rsquo;t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.
This is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, Reinforcement Learning from Human Feedback (RLHF) reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF&rsquo;s complexity was showing:"><meta name=author content="Cong"><link rel=canonical href=https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/><link crossorigin=anonymous href=/assets/css/stylesheet.1f908d890a7e84b56b73a7a0dc6591e6e3f782fcba048ce1eb46319195bedaef.css integrity="sha256-H5CNiQp+hLVrc6eg3GWR5uP3gvy6BIzh60YxkZW+2u8=" rel="preload stylesheet" as=style><link rel=icon href=https://congchan.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://congchan.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://congchan.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://congchan.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://congchan.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6T0DPR6SMC"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6T0DPR6SMC")}</script><meta property="og:url" content="https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/"><meta property="og:site_name" content="Cong's Log"><meta property="og:title" content="Awesome Large Language Model (LLM) Post-training - [2025 Update]"><meta property="og:description" content="In the race to build truly helpful AI assistants, we’ve discovered a fundamental truth: raw intelligence isn’t enough. A model that masters calculus but can’t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.
This is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, Reinforcement Learning from Human Feedback (RLHF) reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF’s complexity was showing:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-30T00:00:00+00:00"><meta property="article:tag" content="2025"><meta property="article:tag" content="Large Language Model"><meta property="article:tag" content="Post-Training"><meta name=twitter:card content="summary"><meta name=twitter:title content="Awesome Large Language Model (LLM) Post-training - [2025 Update]"><meta name=twitter:description content="In the race to build truly helpful AI assistants, we&rsquo;ve discovered a fundamental truth: raw intelligence isn&rsquo;t enough. A model that masters calculus but can&rsquo;t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.
This is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, Reinforcement Learning from Human Feedback (RLHF) reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF&rsquo;s complexity was showing:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://congchan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Awesome Large Language Model (LLM) Post-training - [2025 Update]","item":"https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Awesome Large Language Model (LLM) Post-training - [2025 Update]","name":"Awesome Large Language Model (LLM) Post-training - [2025 Update]","description":"In the race to build truly helpful AI assistants, we\u0026rsquo;ve discovered a fundamental truth: raw intelligence isn\u0026rsquo;t enough. A model that masters calculus but can\u0026rsquo;t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.\nThis is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, Reinforcement Learning from Human Feedback (RLHF) reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF\u0026rsquo;s complexity was showing:\n","keywords":["2025","Large Language Model","Post-training"],"articleBody":"In the race to build truly helpful AI assistants, we’ve discovered a fundamental truth: raw intelligence isn’t enough. A model that masters calculus but can’t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.\nThis is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, Reinforcement Learning from Human Feedback (RLHF) reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF’s complexity was showing:\nThe 3-stage training treadmill (SFT → Reward Modeling → RL tuning) Prohibitively expensive human preference labeling Reward hacking vulnerabilities where models “game” the system Enter the new generation of alignment techniques. There are three trends of directions:\nEliminating reward modeling stages. Or use Rule-based rewards to incentivize LLM intelligence. Using AI-generated preferences. Enabling single-step optimization I am currently following the most cutting-edge LLM alignment methods, and this blog will be updated periodically.\nI. RL*F (Reinforcement Learning from X Feedback) with Proximal Policy Optimization (PPO) Proximal Policy Optimization Algorithms.1 (PPO) is the mose widely used reinforcment learning algorithm in Post-training. PPO is a policy gradient algorithm that optimizes policies by maximizing a clipped surrogate objective to balance exploration and exploitation. It is widely used in RLHF due to its stability and sample efficiency.\nCore Innovation: Uses clipped objective function to limit policy updates, balancing stability and performance. Dominates RLHF pipelines. Application: OpenAI’s ChatGPT, Claude series. Limitations: Requires separate reward model training; unstable with large batches. Reinforcement Learning from Human Feedback (RLHF) Training Language Models to Follow Instructions with Human Feedback. 2 (RLHF) combines reinforcement learning with human preferences to train LLMs, to produce outputs that are more aligned with human preferences and expectations.\nRLHF combines supervised fine-tuning (SFT) with PPO, using human ranked dataset to train a reward model (RM) to model human preferences, and guides policy updates. RLHF aligns LLMs with human values but is costly due to manual labeling.\nBase Model Training Start with a pre-trained language model (like GPT) The model is initially trained on large datasets using supervised learning SFT: Supervised fine-tuning on high-quality data. Human Feedback Collection Human evaluators compare pairs of model outputs They rank which response is better based on criteria like: Helpfulness,Harmlessness, Honesty Reward Modeling: A separate neural network (reward model) is trained on human preference rankings, to predict human preferences This model learns to score outputs based on the collected human feedback It essentially learns to mimic human judgment Reinforcement Learning Optimization The original language model as policy is optimized against RM using PPO. The reward model provides feedback signals Techniques like Proximal Policy Optimization (PPO) are commonly used The model learns to generate responses that maximize the predicted human preference score Benefits\nBetter Alignment: Models produce outputs more consistent with human values Reduced Harmful Content: Helps minimize toxic, biased, or dangerous responses Improved Quality: Responses become more helpful and relevant Scalability: Once trained, the reward model can provide feedback without constant human intervention Challenges\nScalability of Human Feedback: Collecting sufficient high-quality human feedback is expensive and time-consuming Reward Hacking: Models might find ways to maximize reward scores without actually improving quality Bias in Human Feedback: Human evaluators may introduce their own biases Complexity: The multi-stage training process is computationally intensive Reinforcement Learning from AI Feedback (RLAIF) In Constitutional AI: Harmlessness from AI Feedback. 3, researchers introduced a paradigm that replaces human labels for harmfulness with AI-generated feedback. The approach uses a “constitution” of principles to guide self-critique and revision, enabling models to learn harmless behavior on a hybrid of human and AI preferences. This paper was the first effort to explore RLAIF.\nKey Innovation: The framework combines supervised learning (critique → revision cycles) and RL from AI Feedback (RLAIF), where a preference model (PM) is trained on AI-generated comparisons. For example, models generate pairs of responses and evaluate which aligns better with constitutional principles (e.g., “avoid harmful advice”). Impact: As shown in Figure 2 of the paper, Constitutional AI achieves a Pareto improvement in harmlessness and helpfulness, outperforming RLHF models that trade off these traits. The approach reduces reliance on human labeling, a critical step toward scalable supervision. This work laid the groundwork for self-supervised RM, demonstrating that models can learn to evaluate their own behavior using explicit principles.\nIn RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback 4, RLAIF achieved comparable performance to RLHF.\nII. Improve Value Functioning and Eliminating Critic Model The PPO algorithm necessitates loading four models, each of substantial size, which introduces considerable engineering complexity in the design of multi-model training, inference, and real-time parameter updates. This process demands a significant amount of GPU resources.\nFor instance, during RLHF training, when the Actor, Critic, Reward, and Ref Models are of identical scale, such as 70B, employing vLLM/TensorRT-llm for PPO sample generation acceleration and DeepSpeed/Megatron for training acceleration results in roughly equal computational resource consumption between inference and training stages. Consequently, the Critic model accounts for approximately one-quarter of the total computational resource usage.\nIn the context of LLMs, it is common for only the final token to receive a reward score from the reward model. This practice can complicate the training of a value function that accurately reflects each token’s contribution. To address this challenge, numerous studies focus on optimizing the calculation of the value function, incidentally simplifying or potentially eliminating the need for the Critic model in the process.\nGroup Relative Policy Optimization (GRPO) Group Relative Policy Optimization (GRPO)5 is an efficient training algorithm proposed in the DeepSeekMath paper to enhance mathematical reasoning in language models.\nGRPO is a variant of PPO that eliminates the need for a critic model (value function), instead estimating the baseline from group-averaged rewards. This reduces memory and computational costs significantly, making it more resource-efficient.\nKey Differences from PPO\nNo Value Function: Unlike PPO, which uses a learned value function to compute advantages, GRPO calculates advantages using relative rewards within a group of sampled outputs for the same question. Group-based Baseline: For each question $q$, GRPO samples $ G $ outputs from the old policy. The baseline is the average reward of these outputs, and advantages are normalized within the group. Simplified Objective: GRPO optimizes the policy by maximizing a objective that uses group-relative advantages, avoiding the complexity of value function training. How GRPO Works Sampling: For each question $q$, sample $G$ outputs $\\{o_1, o_2, \\dots, o_G\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$.\nOutcome Reward Scoring and Normalization: Use a reward model to score each output, yielding $\\{r_1, r_2, \\dots, r_G\\}$. Outcome supervision provides the normalized reward at the end of each output $o_i$ and sets the advantages $\\hat{A}_{i,t}$ of all tokens in the output as the normalized reward by subtracting the group mean and dividing by the standard deviation: $$ \\hat{A}_{i,t} = \\tilde{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)} $$ Policy Update: Maximize the GRPO objective, which includes a KL divergence term to regularize against a reference model. optimizes the policy model by maximizing the following objective: $$ \\begin{aligned} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[ q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q) \\right] \\\\\\\\ \u0026 \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left\\{ \\min \\left[ \\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,\\lt t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\\lt t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,\\lt t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\\lt t})}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_{i,t} \\right] - \\beta \\mathbb{D}_{\\text{KL}} \\left[ \\pi_{\\theta} \\| \\pi_{\\text{ref}} \\right] \\right\\} \\end{aligned} $$where $\\epsilon$ and $\\beta$ are hyper-parameters, and $\\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only, which will be detailed in the following subsections.\nAlso note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}$. PPO approach: # Modify the reward with KL penalty reward_with_kl = reward - beta * kl_divergence(current_policy, ref_policy) advantage = compute_advantage(values, reward_with_kl, dones) GRPO Approach: # Compute advantage using original reward advantage = compute_advantage(values, reward, dones) # Policy loss with direct KL regularization policy_loss = -(log_probs * advantage).mean() kl_loss = kl_divergence(current_policy, ref_policy).mean() total_loss = policy_loss + beta * kl_loss And different from the KL penalty term used in PPO, GRPO estimate the KL divergence with the following unbiased estimator6: $$ \\mathbb{D}_{KL} \\left[ \\pi_{\\theta} \\| \\pi_{ref} \\right] = \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,\\lt t})}{\\pi_{\\theta}(o_{i,t}|q, o_{i,\\lt t})} - \\log \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,\\lt t})}{\\pi_{\\theta}(o_{i,t}|q, o_{i,\\lt t})} - 1 $$ which is guaranteed to be positive.\nExperimental Results\nDeepSeekMath-RL (7B) using GRPO surpasses all open-source models on MATH and approaches closed-source models like GPT-4. Iterative GRPO (updating the reward model incrementally) further boosts performance, especially in the first iteration. GRPO with process supervision (step-wise rewards) outperforms outcome supervision, highlighting the value of fine-grained feedback. RLHF with GRPO - Deepseek-R1 DeepSeek-R17 is the first public model who make use of both rule-based rewards and general RLHF via GRPO.\nDeepSeek-R1-Zero: Pure RL Training\nRL Algorithm: Uses Group Relative Policy Optimization (GRPO) to optimize policies without a critic model, reducing training costs. Reward Modeling: Relies on rule-based rewards for accuracy (e.g., math problem correctness) and format (enforcing CoT within tags), avoiding neural reward models to prevent reward hacking. Self-Evolution: Through RL, the model autonomously develops complex reasoning behaviors, such as reflecting on mistakes and exploring alternative solutions, leading to significant performance gains. For example, AIME 2024 pass@1 improves from 15.6% to 71.0%, and to 86.7% with majority voting. Limitation: Suffering from readability issues, such as language mixing and unstructured outputs. And Lack of non-reasoning tasks such as writing and factual QA is suboptimal. Experiences an unstable early training process. DeepSeek-R1: Cold-Start and Multi-Stage Refinement DeepSeek-R1 is an attempts to address the limitations of DeepSeek-R1-Zero. 1. Cold-Start Data: Collect thousands of long CoT (chain-of-thought) examples through few-shot prompting, model-generated outputs, and human annotation. These cold-start data are used to fine-tune the DeepSeek-V3-Base to create an initial RL actor that prioritizes readable formats (e.g., summaries and structured CoT) and reducing language mixing.\n2. Reasoning-Oriented RL: Apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. Incorporates language consistency rewards to mitigate mixed-language outputs, balancing performance with human readability.\n3. Rejection Sampling \u0026 SFT: After RL convergence, new SFT data is collected from RL checkpoints, combining reasoning and non-reasoning tasks (e.g., writing, factual QA) to enhance general capabilities.\nFor Reasoning Data Collection: Use rejection sampling on the RL checkpoint to curate ~600K reasoning samples, filtering out mixed-language and unreadable CoT. Include generative reward models (using DeepSeek-V3) for evaluation. For Non-Reasoning Data: Reuse SFT data from DeepSeek-V3 for tasks like writing, factual QA, and translation, collecting ~200K samples. Fine-Tuning: Train DeepSeek-V3-Base on the combined ~800K samples for two epochs to enhance general capabilities. 4. Scenario-Agnostic RL: A final RL stage aligns the model with human preferences for helpfulness and harmlessness, using a mix of rule-based and neural reward models.\nReasoning Data: Use rule-based rewards for math, code, and logic tasks, as in previous stages. General Data: Employ reward models to capture human preferences in complex scenarios (e.g., writing, role-playing), building on DeepSeek-V3’s pipeline . Evaluation Focus: For helpfulness, assess the final summary; for harmlessness, evaluate the entire response (CoT and summary) to mitigate risks . Key Advantages of Iterative RL Training\nPerformance Enhancement: DeepSeek-R1 achieves comparable results to OpenAI-o1-1217 on reasoning benchmarks (e.g., 79.8% pass@1 on AIME 2024) . Readability and Consistency: Cold-start data and language rewards reduce language mixing and improve output structure . Generalization: SFT with diverse data enables competence in non-reasoning tasks like creative writing and factual QA . Performance Benchmarks DeepSeek-R1 and distilled models excel on reasoning tasks:\nMath/Coding: AIME 2024 pass@1 of 79.8% (vs. OpenAI-o1-1217’s 79.2%), MATH-500 pass@1 of 97.3%, and Codeforces rating of 2029 (top 3.7% of human participants). Knowledge Tasks: MMLU score of 90.8%, GPQA Diamond of 71.5%, slightly below o1-1217 but surpassing other closed-source models. General Tasks: Strong performance in creative writing, summarization, and long-context understanding, with win rates of 87.6% on AlpacaEval 2.0 and 92.3% on ArenaHard. RLOO (REINFORCE Leave One-Out) PPO suffers from high computational costs and sensitive hyperparameter tuning. RLOO8, as a simpler RL methods, specifically REINFORCE-style optimization, can preserve or even enhance performance while reducing complexity.\nImage from https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo 9\nKey Insights:\nPPO’s Limitations in RLHF: PPO was designed for traditional RL environments with high variance and random policy initializations. In RLHF, pre-trained LLMs provide a strong policy initialization, concentrating probability mass on a small subset of tokens. This stability makes PPO’s complexity (e.g., clipping, value networks) unnecessary. REINFORCE for RLHF: By modeling the entire sequence generation as a single action (vs. PPO’s token-level actions), REINFORCE directly optimizes the full trajectory reward with unbiased baselines. This avoids the bias introduced by PPO’s bootstrapped value functions. REINFORCE Leave-One-Out (RLOO): A multi-sample extension of REINFORCE, RLOO uses online samples to create dynamic baselines, reducing variance without bias. It outperforms PPO and RL-free methods by fully leveraging all generated samples. REINFORCE REINFORCE loss, which applies the vanilla policy gradient to the entire sequence, using a moving average reward as a baseline.It basically multiplies the (reward - baseline) by the logprob of actions.\nCore Idea: In LLM applications, since the reward $r(x, y)$ is only available at the end of the full sequence, REINFORCE models the entire generation as a single action rather than each token. This aligns with the bandit problem formulation, where the Markov Decision Process (MDP) includes only the initial state (prompt) and the terminal state (completed sequence). Estimator: It uses the REINFORCE estimator to backpropagate through the discrete action space (generation) and directly optimize the KL-shaped reward objective for the entire sequence. The update rule is $$ \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}(. | x)}\\left[R(y, x) \\nabla_{\\theta} \\log \\pi_{\\theta}(y | x)\\right] \\tag{6} $$ The intuition here is related to the likelihood principle. If an action (generation of \\(y\\)) leads to a high reward, we want to increase the probability of taking that action in the future, and vice versa. The REINFORCE estimator is used to update the parameters \\(\\theta\\) of the policy \\(\\pi_{\\theta}\\). The expectation in the formula combines the reward and the policy gradient. Essentially, it tells us how to adjust the policy parameters \\(\\theta\\) to maximize the expected reward. Mathematically, when we take the expectation of the product of the reward \\(R(y, x)\\) and the policy gradient \\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y | x)\\), we are computing a quantity that, when used to update \\(\\theta\\) (using gradient ascent, for example), will tend to increase the expected reward. If \\(R(y, x)\\) is positive, the update will push the policy in the direction of increasing the probability of generating \\(y\\) given \\(x\\), and if \\(R(y, x)\\) is negative, it will push the policy away from generating \\(y\\) given \\(x\\). Baseline: To improve learning, one can reduce the variance of the REINFORCE estimator, while keeping it unbiased, by subtracting a baseline $b$ that has high covariance with the stochastic gradient estimate of the Eq.6: $$ \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(.|x)} \\big[ (R(y, x) - b) \\nabla_\\theta \\log \\pi_\\theta(y|x) \\big] \\tag{7} $$ The moving average of all rewards throughout training (Williams, 1992) is a strong parameter-free choice for the baseline: $$ b_{\\text{MA}} = \\frac{1}{S} \\sum_{s} R(x^s, y^s) \\tag{8} $$ Where $S$ is the number of training steps, and $(x^s, y^s)$ is the prompt-completion pair at the step $s$. This baseline is simple, computationally cheap, and parameter-free. Noticed that REINFORCE is a special case of PPO. PPO uses importance sampling to update the policy without re-collecting data, with a clipped objective to bound policy updates: $\\mathcal{L}^{\\text{PPO}}(\\theta) = \\mathbb{E} \\left[ \\min \\left( r_t(\\theta) \\cdot A_t, \\text{clip}(r_t(\\theta), 1-\\varepsilon, 1+\\varepsilon) \\cdot A_t \\right) \\right]$, where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$ is the importance ratio, $A_t$ is the advantage estimate (e.g., from TD or MC), and $\\varepsilon$ is the clipping parameter. To reduce PPO’s objective to REINFORCE’s with specific parameters set:\nRemoving Clipping ($\\varepsilon \\to \\infty$): When $\\varepsilon$ is infinitely large, the $\\text{clip}(\\cdot)$ operation becomes irrelevant, and the PPO objective simplifies to: $\\mathcal{L}(\\theta) = \\mathbb{E} \\left[ r_t(\\theta) \\cdot A_t \\right]$. If we further assume the old policy $\\pi_{\\theta_{\\text{old}}}$ is the same as the current policy $\\pi_\\theta$ (i.e., no importance sampling, or a single update without old policy), then $r_t(\\theta) = 1$, and the objective becomes: $\\mathcal{L}(\\theta) = \\mathbb{E} \\left[ A_t \\right]$ Using Monte Carlo(MC) Returns as Advantage ($A_t = G_t - b(s_t)$): If $A_t$ is defined as the MC return minus a baseline (as in REINFORCE), and the baseline $b(s_t) = 0$ (or ignored), then $A_t = G_t$. Substituting into the objective: $\\mathcal{L}(\\theta) = \\mathbb{E} \\left[ G_t \\right]$, whose gradient is exactly the REINFORCE update rule without a baseline. If a baseline is included ($b(s_t) \\neq 0$), it aligns with REINFORCE with a baseline, which is the standard practice to reduce variance. — A2C Is a Special Case of PPO10\nEven though the logprob is explicitly in the REINFORCE loss, it is also implicitly in the PPO loss.\nREINFORCE Leave-One-Out (RLOO) Leverages multiple online samples to further reduce variance in the REINFORCE estimator while keeping it unbiased.\nFor each prompt, generates k samples and uses the average reward of k-1 samples as a baseline for the remaining one, creating a variance-reduced gradient estimate.\nThe baseline in Eq. 8 is simple to implement and computationally cheap. However, it can be improved upon if we have access to multiple online samples, that can be used for further unbiased variance reduction:\nThe rewards for each sample can serve all other samples as a baseline. Policy updates can be done on an average of gradient estimates for each sample, resulting in a variance-reduced multi-sample Monte-Carlo (MC) estimate. This is the intuition behind the REINFORCE Leave-One-Out (RLOO) estimator, proposed by (Kool et al., 2019): $$ \\frac{1}{k} \\sum_{i=1}^{k} \\left[ R(y_{(i)}, x) - \\frac{1}{k - 1} \\sum_{ j \\neq i} R(y_{(j)}, x) \\right] \\nabla \\log \\pi(y_{(i)} | x) \\text{ for } y_{(1)}, \\ldots, y_{(k)} \\stackrel{i.i.d}{\\sim} \\pi_{\\theta}(. | x) $$ Where \\(k\\) refers to the number of online samples generated, \\(\\text{RLOO}_k\\) considers each \\( y_{(i)} \\) individually and uses the remaining \\( k - 1 \\) samples to create an unbiased estimate of the expected return for the prompt, akin to a parameter-free value-function, but estimated at each training step.\nThis is a much more effective baseline (as the paper’s experiments showed) than \\( b_{\\text{MA}} \\) since it’s created on-the-fly for each sample and at each training step, but comes at a cost of increased sampling time during training.\nNoted that generating extra samples as a means of variance reduction has been proposed by concurrent work - Remax 11, but RLOO focus on the efficiency benefits of fully utilizing all samples.\nResults:\nPerformance: REINFORCE outperforms PPO by 3.2–20.3% in win-rate. RLOO further improves performance, surpassing DPO and RAFT across all datasets. Sample Efficiency: RLOO with k=2 matches or exceeds RAFT with k=4, demonstrating better use of online samples. Robustness: RLOO is less sensitive to KL penalty and reward noise compared to RAFT, maintaining stable performance under varying conditions. Alignment Tax: RLOO preserves language fluency (perplexity) and diversity better than PPO, with lower reward variance—a key factor for safety-critical applications. ReMax PPO introduces significant computational overhead for LLMs due to its complex architecture: it requires training a value model, leading to heavy memory usage, tedious hyperparameter tuning, and slow training. To make RLHF efficient, ReMax 11 leverages 3 properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. ReMax does not require training an additional value model as in PPO and is further enhanced with a new variance reduction technique.\nKey limitations of PPO for RLHF:\nValue model overhead: Consumes ~46% of GPU memory for a 7B model. Hyperparameter complexity: Requires tuning 4+ parameters (e.g., clipping, GAE coefficient). Slow convergence: Training with PPO can be 4× slower than earlier RLHF steps. Key Insights: Unique Properties of RLHF for LLMs ReMax leverages three properties of RLHF that PPO overlooks:\nFast simulation: Generating a complete LLM response (trajectory) is rapid (e.g., \u003c10s for 7B models). Deterministic transitions: Text generation depends only on past tokens, with no stochastic environment dynamics. Trajectory-level rewards: Rewards are given only after the full response, not at each step. The ReMax Algorithm ReMax is built on the REINFORCE algorithm but introduces a variance reduction technique:\nGreedy baseline: For each prompt, compute the reward of a greedy (deterministic) response and use it to normalize the gradient, reducing variance. For a prompt, sample a stochastic response and a greedy response. Compute the reward difference between the two responses. No value model: Directly optimizes the policy to maximize the log-likelihood of high-reward responses, weighted by the reward difference. Pseudo-code Core Steps:\nfor prompt in dataset: seq = lm.sample(prompt, greedy=False) # Stochastic response seq_max = lm.sample(prompt, greedy=True) # Greedy response rew = rm(prompt, seq) - rm(prompt, seq_max) # Reward difference logp = lm.inference(prompt, seq) # Log-likelihood loss = - (logp.sum() * rew).mean() # Loss for optimization lm.minimize(loss) REINFORCE++ RLOO and GRPO increase inference costs to trade for eliminating the critic model. The blog 12 argues that eliminating the critic model may inadvertently lower training efficiency due to increased inference costs:\nWhen all models (Actor, Critic, Reward, Reference) are similar in scale (e.g., 70B parameters), inference and training consume roughly equal computational resources (1:1 ratio). Eliminating the critic model may actually reduce training efficiency due to increased inference costs System complexity remains largely unchanged since multiple models still need to operate together Performance Analysis: REINFORCE-based methods (e.g., RLOO, ReMax, GRPO) eliminate the critic but struggle with accurate advantage estimation, often overfitting to simple prompts and being vulnerable to reward hacking. These methods estimate advantages per prompt, leading to instability and poor generalization. GRPO and RLOO don’t provide significant theoretical improvements over PPO The claimed advantages (like “10x” efficiency gains) are often exaggerated PPO can address critic model issues by initializing critics with actor weights Alternative solution: Pre-train the critic model by freezing actor weights during PPO training Technical Issues with GRPO: Numerical instability: Small differences in sampled rewards can be amplified during normalization Example: rewards of 1.001 vs 1.00 become -0.7070 vs 0.7072 after normalization Convergence problems: When all sampled rewards are equal, GRPO provides zero learning signal Under Process Reward Model (PRM) settings, GRPO essentially becomes REINFORCE with mean baseline Bottom Line:: Both GRPO and RLOO are most beneficial when critic/reward models are significantly larger than actors, but even then, PPO remains a viable alternative with proper initialization strategies. The computational and complexity advantages are often overstated. REINFORCE++ 13 is a critic-free RLHF algorithm that uses the global batch mean reward as a baseline instead of prompt-specific baselines, preventing overfitting and improving robustness.\nThe overall algorithm flow of REINFORCE++: Sample one response per prompt, compute rewards, normalize advantages, and update the policy using a clipped objective (similar to PPO but without the critic).\nAdvantages Normalization: Normalizes advantages across the entire batch to stabilize training and enhance out-of-distribution (OOD) generalization. REINFORCE++ replaces prompt-specific baselines with the mean reward of a global batch, reducing overfitting to individual prompts. The Advantage is calculated as: $$ A_{q, o_t} = r(o_{1:t}, q) - \\beta \\cdot \\sum_{i=t}^T KL(i), \\quad \\text{with } KL(t) = \\log\\left(\\frac{\\pi_{\\theta_{\\text{old}}}^{RL}(o_t | q, o_{\\lt t})}{\\pi^{\\text{SFT}}(o_t | q, o_{\\lt t})}\\right) $$ The token-level KL penalty avoids the need for a critic network while achieving comparable stability. The gradient of the token-level KL penalty has been theoretically proven to be unbiased concerning the $k_3$ loss of GRPO in RLHF.\nThe advantage is normalized globally: $$ A_{q, o_t}^{\\text{norm}} = \\frac{A_{q, o_t} - \\text{mean}(A_{q, o_t})}{\\text{std}(A_{q, o_t})} $$Experimental Results:\nBradley-Terry Reward Model: REINFORCE++ matches or exceeds performance of GRPO, RLOO, and ReMax on OOD benchmarks (e.g., GSM8K, MATH, code generation), with higher per-token efficiency. Long CoT Tasks: Small-Scale Datasets: GRPO overfits to training prompts (e.g., AIME-24), achieving near-perfect scores but failing on OOD test sets (AIME-25), while REINFORCE++ shows stable generalization. Logical Reasoning (Knights and Knaves): REINFORCE++ outperforms GRPO in complex and OOD scenarios (e.g., 8-character puzzles), with higher Pass@1 scores (36 vs. 20) and longer, more reasoned responses. Mathematical Reasoning: From scratch or fine-tuned models, REINFORCE++ demonstrates better OOD generalization on MATH and AIME datasets. DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization) DAPO 14 decouples policy clipping and dynamic sampling to enhance training efficiency, reducing variance in gradient estimates.\nDAPO addresses critical challenges in RL training—such as entropy collapse, reward noise, and instability—with four key techniques.\nClip-Higher: Decouples lower $\\varepsilon_{low}$ and higher $\\varepsilon_{high}$ clipping ranges in policy optimization to prevent entropy collapse. By increasing $\\varepsilon_{high}$, low-probability “exploration” tokens gain more room for probability increases, enhancing diversity. Dynamic Sampling: Over-samples and filters out prompts with all-correct or all-wrong outputs to maintain effective gradient signals. This mitigates gradient-decreasing issues from zero-advantage batches, improving training efficiency. Token-Level Policy Gradient Loss: Shifts from sample-level to token-level loss calculation, balancing the influence of long and short responses. This prevents low-quality, overly long generations and stabilizes training. Overlong Reward Shaping: Introduces soft punishment for truncated responses to reduce reward noise. Instead of harsh penalties, it uses a length-aware function to guide models toward optimal response lengths. Dataset and Implementation The DAPO-Math-17K dataset transforms math problems into integer answers for reliable rule-based reward signals. The system is built on the verl framework, with training details including AdamW optimization, group reward normalization, and dynamic sampling hyperparameters.\nExperiments and Results AIME 2024 Performance: DAPO achieves 50 points on AIME with Qwen2.5-32B, outperforming DeepSeek-R1-Zero-Qwen-32B (47 points) with half the training steps (Figure 1). Ablation Study: Each technique contributes significantly: Overlong Filtering (+6), Clip-Higher (+2), Soft Overlong Punishment (+3), Token-Level Loss (+1), and Dynamic Sampling (+8) (Table 1). Training Dynamics: Metrics like response length, reward, and entropy show stable improvement, with Clip-Higher specifically combating entropy collapse (Figure 7). Dr. GRPO Dr. GRPO 15 claims that:\nDeepSeek-V3-Base already exhibit “Aha moment”, while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. There is an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. Dr. GRPO is an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Key Insights on Base Models\nTemplate Impact on Question-Answering Ability\nBase models like Llama and DeepSeek require prompt templates (e.g., R1 template) to elicit question-answering behavior, while Qwen2.5 models excel without templates. This suggests Qwen2.5 might be pretrained on question-answer pairs, acting like supervised fine-tuned (SFT) models even in their base form. For example, Qwen2.5-Math-7B achieves 69.0% accuracy on MATH500 without templates, outperforming traditional prompting methods. Preexisting “Aha Moment” in Base Models\nThe “Aha moment” (self-reflection behaviors) often attributed to RL emergence is already present in base models like DeepSeek-V3-Base. Experiments show these models generate self-reflection keywords (e.g., “Aha,” “wait”) in responses to math problems without RL tuning. Analysis of Reinforcement Learning\nBiases in GRPO\nGRPO introduces two key biases: Response-length bias: Dividing by response length ($|o_i|$) penalizes short correct responses and favors longer incorrect ones. Question-difficulty bias: Normalizing by reward standard deviation prioritizes easy/hard questions, skewing optimization. These biases lead to unnecessarily long incorrect responses, as observed in training dynamics. Dr. GRPO: Unbiased Optimization\nThe authors propose Dr. GRPO, which removes $|o_i|$ and standard deviation normalization from GRPO. This fixes the biases, improving token efficiency while maintaining reasoning performance. Experiments show Dr. GRPO reduces the length of incorrect responses and matches the accuracy of GRPO with fewer tokens. Group-in-Group Policy Optimization (GiGPO) for LLM Agent Training GiGPO16 is designed to enhance long-horizon training for LLM agents. Unlike existing group-based RL methods (e.g., GRPO) that struggle with fine-grained credit assignment in multi-step tasks, GiGPO achieves hierarchical advantage estimation while retaining key benefits: being critic-free, memory-efficient, and computationally lightweight.\nKey Limitations of Existing Group-based RL:\nTraditional group-based RL (e.g., GRPO, RLOO) works well for single-turn tasks (e.g., math reasoning) where rewards are immediate but fails in long-horizon scenarios. In multi-step environments (e.g., embodied navigation, web shopping), rewards are sparse/delayed, making it hard to assign credit to individual steps. Naive extensions of group-based RL collapse step-level distinctions, reducing effectiveness in agent training. GiGPO’s hierarchical “group-in-group” design:\nEpisode-Level Grouping:\nObjective: Capture holistic trajectory quality. Method: Samples a group of complete trajectories under identical initial conditions and task descriptions. Computes macro relative advantages using total episode returns, normalized by the group’s mean and a scaling factor (either standard deviation or 1 for stability). $$ A^E(\\tau_i) = \\frac{R(\\tau_i) - \\text{mean}(\\{R(\\tau_j)\\})}{F_{\\text{norm}}(\\{R(\\tau_j)\\})} $$ where \\( R(\\tau_i) \\) is the total reward of trajectory \\( \\tau_i \\) Step-Level Grouping:\nObjective: Evaluate local step effectiveness. Method: Identifies anchor states (repeated environment states across trajectories) to form step-level groups. Computes micro relative advantages using discounted future rewards for actions taken at these shared states. $$ A^S(a_t^{(i)}) = \\frac{R_t^{(i)} - \\text{mean}(\\{R_t^{(j)}\\})}{F_{\\text{norm}}(\\{R_t^{(j)}\\})} $$ where \\( R_t^{(i)} \\) is the discounted return from step \\( t \\) in trajectory \\( i \\) GiGPO Combined Advantage Signal: The final advantage for each action merges both levels, $$ A(a_t^{(i)}) = A^E(\\tau_i) + \\omega \\cdot A^S(a_t^{(i)}) $$ where \\( \\omega \\) balances the two signals.\nGiGPO was evaluated on two challenging benchmarks using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct:\nBenchmark Improvement Over GRPO Key Findings ALFWorld 12-13% higher success Superior performance in embodied household task planning. WebShop \u003e9% higher success Better at goal-driven web navigation and shopping tasks. Efficiency: GiGPO matches GRPO’s GPU memory usage and rollout costs, with \u003c0.002% additional computation time. Ablation Studies: Removing either episode-level or step-level advantages significantly degrades performance, confirming the value of hierarchy. III. Optimization without Reward Model Direct Preference Optimization (DPO) DPO17 is a RL-free training method for LLMs that directly optimizes for human preferences without requiring a separate reward model. DPO eliminates the need for a reward model by directly optimizing a policy using pairwise preference data. It minimizes the KL divergence between the policy and a reference model while maximizing the Bradley-Terry loss. DPO derives a closed-form solution that directly optimizes the policy using preference data. The key insight is that the optimal policy under the RLHF objective can be expressed analytically in terms of the reward function and reference policy.\nThe DPO objective is based on the Bradley-Terry preference model, the optimal RLHF policy $π^∗$ under the Bradley-Terry model satisfies the preference model::\n$$P(y_w \\succ y_l | x) = \\frac{1}{1 + \\exp(\\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} - \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)})}$$Where:\n$y_w$ is the preferred (winning) response $y_l$ is the less preferred (losing) response $x$ is the input prompt $\\pi_\\theta$ is the policy being optimized $\\pi_{ref}$ is the reference policy $\\beta$ is a temperature parameter For a static dataset of comparisons $\\mathcal{D} = \\left\\{ x^{(i)}, y_w^{(i)}, y_l^{(i)} \\right\\}_{i=1}^N$, the reward modeling approach works by defining: $$ \\mathcal{L}_R(r_\\phi, \\mathcal{D}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( r_\\phi(x, y_w) - r_\\phi(x, y_l) \\right) \\right] $$Analogous to reward modeling approach, DPO’s policy objective becomes: $$ \\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\text{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)} \\right) \\right] $$ The gradient with respect to the parameters $\\theta$ can be written as: $$ \\nabla_\\theta \\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = - \\beta \\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\bigg[ \\underbrace{\\sigma\\bigl(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta(x, y_w)\\bigr)}_{\\text{higher weight when reward estimate is wrong}} \\, \\bigg[ \\underbrace{\\nabla_\\theta \\log \\pi(y_w \\mid x)}_{\\text{increase likelihood of } y_w} - \\underbrace{\\nabla_\\theta \\log \\pi(y_l \\mid x)}_{\\text{decrease likelihood of } y_l} \\bigg] \\bigg] $$where $\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)}$ is the reward implicitly defined by the language model $\\pi_\\theta$ and reference model $\\pi_{\\text{ref}}$.\nA mechanistic understanding of DPO The gradient of the loss function $\\mathcal{L}_{\\text{DPO}}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. The examples are weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. The papers’s experiments suggest the importance of this weighting, as a naïve version of this method without the weighting coefficient can cause the language model to degenerate. DPO’s Training Process Data Collection: Gather preference pairs $(x, y_w, y_l)$ where $y_w$ is preferred over $y_l$ for prompt $x$ Direct Optimization: Minimize the DPO loss $\\mathcal{L}_{\\text{DPO}}$ Regularization: The KL divergence constraint from RLHF is implicitly maintained through the reference policy terms Advantages of DPO Simplicity Eliminates the need for reward model training Reduces the training pipeline from 3 stages to 2 stages Avoids the complexities of reinforcement learning Stability More stable training compared to PPO-based RLHF No issues with reward model overoptimization Direct gradient-based optimization Efficiency Requires less computational resources Faster convergence Easier hyperparameter tuning Practical Implementation # Simplified DPO loss computation def dpo_loss(policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, beta=0.1): policy_logratios = policy_chosen_logps - policy_rejected_logps reference_logratios = reference_chosen_logps - reference_rejected_logps logits = beta * (policy_logratios - reference_logratios) loss = -torch.nn.functional.logsigmoid(logits).mean() return loss Limitations and Considerations Data Quality: Heavily dependent on high-quality preference data Distribution Shift: May struggle with significant shifts from reference policy Preference Complexity: Works best with clear preference distinctions Hyperparameter Sensitivity: The β parameter requires careful tuning Comparison with RLHF\nAspect RLHF DPO Complexity High (3 stages) Medium (2 stages) Stability Can be unstable More stable Computational Cost High Lower Flexibility High Moderate Online DPO - Direct Language Model Alignment from Online AI Feedback Direct Alignment from Preferences (DAP) methods like DPO have emerged as efficient alternatives to RLHF, but they rely on pre-collected offline preference data. This leads to two key issues:\nOffline Feedback: Preferences are static and not updated during training. Off-Policy Learning: Responses in the dataset are generated by a different model, causing distribution shift as the target model evolves. The proposed Online AI Feedback (OAIF) framework18 makes Online DAP methods online and on-policy by:\nSampling two responses from the current model for each prompt. Using an LLM annotator (e.g., PaLM 2) to provide real-time preference feedback by choosing the better response. Updating the model using standard DAP losses (DPO, IPO, SLiC) with this online feedback. Key advantages:\nAvoids distribution shift by using on-policy generations. Eliminates the need for a separate Reward Model (RM), unlike RLHF. Enables controllable feedback via prompt engineering for the LLM annotator. Experiments and Results\nEffectiveness vs. Offline DAP:\nOnline DAP methods (DPO, IPO, SLiC) achieved an average 66% win rate over their offline counterparts in human evaluations. Online DPO outperformed SFT baselines, RLHF, and RLAIF 58% of the time on the TL;DR task. Generalization to Other DAP Methods:\nOAIF improved all three DAP methods, with online SLiC showing a 71.43% win rate over offline SLiC in TL;DR. Comparison with RLHF/RLAIF:\nOnline DPO was preferred 58% of the time in 4-way comparisons (vs. offline DPO, RLAIF, RLHF). RLHF relies on static RMs, which struggle as the model evolves, while OAIF’s LLM annotator adapts dynamically. Controllability via Prompts:\nInstructing the LLM annotator to prefer shorter responses reduced average token length from ~120 to ~40, while maintaining quality above SFT baselines. Impact of Annotator Size:\nLarger annotators (e.g., PaLM 2-L) improved performance, but even smaller annotators (PaLM 2-XS) outperformed RLHF in some cases. OAIF addresses the offline and off-policy limitations of DAP methods, achieving better alignment with reduced human annotation. The approach paves the way for scalable LLM alignment using AI feedback, with potential for real-time user adaptation and qualitative objective control.\nMethod No RM Needed On-Policy Online Feedback Offline DAP ✓ ✗ ✗ RLHF/RLAIF ✗ ✓ ✓ OAIF (Proposed) ✓ ✓ ✓ TDPO: Token-level Direct Preference Optimization DPO optimize models at the sentence level, evaluating full responses. However, LLMs generate text token-by-token in an auto-regressive manner, which creates a mismatch between evaluation and generation processes. DPO uses KL divergence to constrain models to a reference LLM but struggles with divergence efficiency: the KL divergence of dispreferred responses grows too quickly, limiting diversity. This motivates the need for a more granular, token-level optimization approach.\nTDPO19 optimizes token-level preferences to improve sequence generation quality, addressing limitations of instance-level DPO in long-chain reasoning.\nTDPO models text generation as an MDP, where:\nState \\( s_t = [x, y^{\\lt t}] \\) (prompt + partial response) Action \\( a_t = y^t \\) (next token) Reward \\( R_t := R(s_t, a_t) = R\\bigl([x, y^{\\lt t}], y^t\\bigr). \\) (token-wise reward) The objective function combines the advantage function of a reference model with forward KL divergence: \\[ \\max_{\\pi_\\theta} \\mathbb{E}\\left[ A_{\\pi_{ref}}(s_t, z) - \\beta D_{KL}(\\pi_\\theta(\\cdot|s_t) \\| \\pi_{ref}(\\cdot|s_t)) \\right] \\] where \\( A_{\\pi_{ref}} \\) is the advantage function, and \\( \\beta \\) weights the KL penalty.\nTDPO transforms the sentence-level Bradley-Terry model into a token-level preference model, relating it to the Regret Preference Model. The key is expressing human preference probability as: \\[ P_{BT}(y_1 \\succ y_2|x) = \\sigma(u(x, y_1, y_2) - \\delta(x, y_1, y_2)) \\] where \\( u \\) is the reward difference from DPO, represented as, $$ u(x, y_1, y_2) = \\beta \\log \\frac{\\pi_\\theta(y_1 \\mid x)}{\\pi_{\\text{ref}}(y_1 \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_2 \\mid x)}{\\pi_{\\text{ref}}(y_2 \\mid x)} $$and \\( \\delta \\) is the weighted SeqKL difference between responses. $$ \\delta(x, y_1, y_2) = \\beta D_{\\text{SeqKL}}\\left(x, y_2; \\pi_{\\text{ref}} \\parallel \\pi_\\theta\\right) - \\beta D_{\\text{SeqKL}}\\left(x, y_1; \\pi_{\\text{ref}} \\parallel \\pi_\\theta\\right) $$Reformulate the Bradley-Terry model into a structure solely relevant to the policy. This formulate a likelihood maximization objective for a parametrized policy $\\pi_\\theta$, leading to the derivation of the loss function for the initial version of TDPO, $\\text{TDPO}_1$: $$ \\mathcal{L}_{\\text{TDPO}_1}\\left( \\pi_\\theta; \\pi_{\\text{ref}} \\right) = - \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left( u(x, y_w, y_l) - \\delta(x, y_w, y_l) \\right) \\right] $$In $\\text{TDPO}_1$, \\( \\delta \\) depends on \\( \\pi_\\theta \\) (the current policy). During backpropagation, this causes gradient coupling: Updates to \\( \\pi_\\theta \\) affect both \\( u \\) and \\( \\delta \\), leading to unstable training (e.g., gradient conflicts or explosions).\nThis issue is addressed by decoupling the gradient flow of \\( \\delta \\) from \\( \\pi_\\theta \\) using a stop-gradient operation. Specifically:\nThe SeqKL term in \\( \\delta \\) is wrapped with a stop-gradient (e.g., detach() in PyTorch), so its gradient no longer propagates back to \\( \\pi_\\theta \\). The updated loss function for ${\\text{TDPO}_2}$ becomes: $$ \\mathcal{L}_{\\text{TDPO}_2}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}} \\left[ \\log \\sigma\\left( u(x,y_w,y_l) - \\alpha \\cdot \\text{stop\\_grad}\\left( \\delta(x,y_w,y_l) \\right) \\right) \\right] $$ By isolating \\( \\delta \\)’s gradient, TDPO₂ avoids feedback loops between \\( u \\) and \\( \\delta \\), reducing training oscillations. The SeqKL term acts as a “soft regularizer” to control token-level divergence, while the main optimization focuses on aligning with human preferences (via \\( u \\)). Define the loss function for $\\text{TDPO}_2$ as: $$ \\mathcal{L}_{\\text{TDPO}_2}\\left( \\pi_\\theta; \\pi_{\\text{ref}} \\right) = - \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left( u(x, y_w, y_l) - \\alpha \\delta_2(x, y_w, y_l) \\right) \\right] $$ where $\\alpha$ is a parameter, and $$ \\delta_2(x, y_1, y_2) = \\beta D_{\\text{SeqKL}}\\left( x, y_2; \\pi_{\\text{ref}} \\parallel \\pi_\\theta \\right) - sg\\left( \\beta D_{\\text{SeqKL}}\\left( x, y_1; \\pi_{\\text{ref}} \\parallel \\pi_\\theta \\right) \\right) $$The $sg$ represents the stop-gradient operator, which blocks the propagation of gradients.\nImplementation from: https://github.com/Vance0124/Token-level-Direct-Preference-Optimization/blob/2a736ecb285394a8419b461816bce1ba3b093cc9/trainers.py#L46\nclass BasicTrainer(object): ... def tdpo_concatenated_forward(self, model: nn.Module, reference_model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]): \"\"\"Run the policy model and the reference model on the given batch of inputs, concatenating the chosen and rejected inputs together. We do this to avoid doing two forward passes, because it's faster for FSDP. \"\"\" concatenated_batch = concatenated_inputs(batch) all_logits = model(concatenated_batch['concatenated_input_ids'], attention_mask=concatenated_batch['concatenated_attention_mask']).logits.to(torch.float32) with torch.no_grad(): reference_all_logits = reference_model(concatenated_batch['concatenated_input_ids'], attention_mask=concatenated_batch[ 'concatenated_attention_mask']).logits.to(torch.float32) all_logps_margin, all_position_kl, all_logps = _tdpo_get_batch_logps(all_logits, reference_all_logits, concatenated_batch['concatenated_labels'], average_log_prob=False) chosen_logps_margin = all_logps_margin[:batch['chosen_input_ids'].shape[0]] rejected_logps_margin = all_logps_margin[batch['chosen_input_ids'].shape[0]:] chosen_position_kl = all_position_kl[:batch['chosen_input_ids'].shape[0]] rejected_position_kl = all_position_kl[batch['chosen_input_ids'].shape[0]:] chosen_logps = all_logps[:batch['chosen_input_ids'].shape[0]].detach() rejected_logps = all_logps[batch['chosen_input_ids'].shape[0]:].detach() return chosen_logps_margin, rejected_logps_margin, chosen_position_kl, rejected_position_kl, \\ chosen_logps, rejected_logps def tdpo_loss(chosen_logps_margin: torch.FloatTensor, rejected_logps_margin: torch.FloatTensor, chosen_position_kl: torch.FloatTensor, rejected_position_kl: torch.FloatTensor, beta: float, alpha: float = 0.5, if_tdpo2: bool = True) -\u003e Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]: \"\"\"Compute the TDPO loss for a batch of policy and reference model log probabilities. Args: chosen_logps_margin: The difference of log probabilities between the policy model and the reference model for the chosen responses. Shape: (batch_size,) rejected_logps_margin: The difference of log probabilities between the policy model and the reference model for the rejected responses. Shape: (batch_size,) chosen_position_kl: The difference of sequential kl divergence between the policy model and the reference model for the chosen responses. Shape: (batch_size,) rejected_position_kl: The difference of sequential kl divergence between the policy model and the reference model for the rejected responses. Shape: (batch_size,) beta: Temperature parameter for the TDPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -\u003e 0. alpha: Temperature parameter for the TDPO loss, used to adjust the impact of sequential kl divergence. if_tdpo2: Determine whether to use method TDPO2, default is True; if False, then use method TDPO1. Returns: A tuple of two tensors: (losses, rewards). The losses tensor contains the TDPO loss for each example in the batch. The rewards tensors contain the rewards for response pair. \"\"\" chosen_values = chosen_logps_margin + chosen_position_kl rejected_values = rejected_logps_margin + rejected_position_kl chosen_rejected_logps_margin = chosen_logps_margin - rejected_logps_margin if not if_tdpo2: logits = chosen_rejected_logps_margin - (rejected_position_kl - chosen_position_kl) # tdpo1 else: logits = chosen_rejected_logps_margin - alpha * (rejected_position_kl - chosen_position_kl.detach()) # tdpo2 losses = -F.logsigmoid(beta * logits) chosen_rewards = beta * chosen_values.detach() rejected_rewards = beta * rejected_values.detach() return losses, chosen_rewards, rejected_rewards Trust Region DPO (TR-DPO) Offline alignment methods (e.g., DPO) fine-tune LLMs using pre-constructed datasets without needing explicit reward models. However, they suffer from overoptimization: as the trained policy (\\(\\pi_\\theta\\)) deviates too far from the fixed reference policy (\\(\\pi_{ref}\\), typically a supervised fine-tuned model), the quality of generated outputs degrades. This is linked to increased probabilities of out-of-domain (OOD) data and a decline in in-domain (ID) data probabilities.\nOveroptimization in offline methods arises due to vanishing curvature in the loss landscape (analyzed via Hessian dynamics), making it hard to reverse declining ID data probabilities. Updating \\(\\pi_{ref}\\) “resets” the optimization process, restoring curvature and preventing OOD drift.\nTR-DPO20 address the overoptimization via dynamically updating the reference policy (\\(\\pi_{ref}\\)) during training to mitigate overoptimization. This approach, inspired by trust region optimization, ensures the model stays within a “trustworthy” range of behavior while allowing beneficial divergence from the initial reference. Unlike game-theoretical approaches (which rely on online sampling), TR methods work entirely offline, using pre-existing datasets.\nTwo update strategies are introduced:\nSoft Update: Gradually merges the current policy into the reference policy using a weighted average:\n$$\\pi_{ref} \\leftarrow \\alpha \\pi_\\theta + (1-\\alpha) \\pi_{ref_{prev}}$$\nwhere \\(\\alpha\\) controls the update rate, and stop-gradient (\\(sg\\)) prevents backpropagating through \\(\\pi_{ref}\\)\nHard Update: Periodically replaces the reference policy with the current policy after a fixed number of steps (\\(\\tau\\)):\n$$\\pi_{ref} \\leftarrow \\pi_\\theta$$ These strategies are applied to existing methods, creating TR-DPO, TR-IPO, and TR-KTO.\nThe authors evaluate TR methods on both task-specific and general benchmarks, using models like Pythia (2.8B–12B) and Llama3 (8B):\nTask-Specific Tasks:\nOn Anthropic-HH (helpful/harmless dialogue) and Reddit TL;DR (summarization), TR methods outperformed vanilla DPO/IPO/KTO. For example, TR-DPO with \\(\\alpha=0.6\\) or \\(\\tau=512\\) achieved 8.4–15% higher win rates. General Benchmarks:\nOn AlpacaEval 2 and Arena-Hard, TR methods showed significant gains. TR-IPO with hard updates improved win rates by 15.1 points on Arena-Hard, while TR-DPO improved by 9.5 points. Overoptimization Mitigation:\nAt equivalent KL divergence (distance from the initial reference), TR methods maintained higher human-centric (HC) metrics (coherence, helpfulness, etc.) compared to vanilla methods, indicating reduced overoptimization. TR alignment methods (TR-DPO, TR-IPO, TR-KTO) effectively reduce overoptimization by dynamically updating the reference policy. They outperform traditional offline methods across tasks and model sizes, enabling LLMs to diverge beneficially from initial references while maintaining high quality. Future work will explore broader applicability and adaptive update strategies.\nStep-DPO: Step-wise Preference Optimization for Long-chain Reasoning LLMs struggle with long-chain mathematical reasoning due to the need for precise step-by-step correctness. Traditional DPO fails to improve such reasoning effectively because it evaluates entire answers rather than individual steps, making it hard to identify subtle errors in intermediate reasoning.\nStep-DPO treats each reasoning step as the unit for preference optimization (instead of holistic answers in DPO). By focusing on the first erroneous step in a chain, Step-DPO provides fine-grained supervision, enabling LLMs to locate and correct mistakes more accurately.\nPaper: Lai, Xin, et al. Step-DPO: Step-Wise Preference Optimization for Long-Chain Reasoning of LLMs. arXiv:2406.18629, arXiv, 26 June 2024. arXiv.org, https://doi.org/10.48550/arXiv.2406.18629.\nSpecifically, the answer \\( y \\) can be decomposed into a sequence of reasoning steps \\( y = s_1, \\ldots, s_n \\), where \\( s_i \\) is the \\( i \\)-th reasoning step.\nGiven a prompt \\( x \\) and a series of initial correct reasoning steps \\( s_{1\\sim k-1} = s_1, \\ldots, s_{k-1} \\), Step-DPO aims to maximize the probability of the correct next reasoning step \\( s_{\\textit{win}} \\) and minimize the probability of the incorrect one \\( s_{\\textit{lose}} \\). This objective can be formulated as:\n$$ \\mathcal{L}(\\theta) = -\\mathbb{E}_{(x, s_{1\\sim k-1}, s_{\\textit{win}}, s_{\\textit{lose}}) \\sim D} \\left[ \\log \\sigma\\left( \\beta \\log \\frac{\\pi_\\theta(s_{\\textit{win}} \\mid x; s_{1\\sim k-1})}{\\pi_{\\textit{ref}}(s_{\\textit{win}} \\mid x; s_{1\\sim k-1})} - \\beta \\log \\frac{\\pi_\\theta(s_{\\textit{lose}} \\mid x; s_{1\\sim k-1})}{\\pi_{\\textit{ref}}(s_{\\textit{lose}} \\mid x; s_{1\\sim k-1})} \\right) \\right] $$\nThe authors propose a 3-step pipeline to build high-quality step-wise preference data (10K pairs):\nError Collection: Use a reference model to generate incorrect answers for math problems, retaining cases where the final answer differs from the ground truth. Step Localization: Identify the first erroneous step in each incorrect answer manually or with GPT-4. Rectification: Generate correct next steps by sampling from the reference model given the initial correct steps, ensuring in-distribution data (self-generated) over out-of-distribution data (human/GPT-4-generated), which proves more effective. Performance:\nStep-DPO achieves up to 3% accuracy gain on MATH with as few as 10K data pairs and \u003c500 training steps for 70B+ parameter models. Qwen2-72B-Instruct + Step-DPO reaches 70.8% on MATH and 94.0% on GSM8K, outperforming closed-source models like GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Ablation Studies:\nStep-DPO outperforms DPO by 1.8–2.6% on MATH. In-distribution data yields 0.7% higher accuracy than out-of-distribution data. Identity Preference Optimization (IPO) RLHF and DPO can overfit deterministic preferences because the logit transformation (Ψ) amplifies small preference differences near 1, weakening KL regularization. This leads to policies deviating from the reference policy, even with large regularization parameters.\nΨPO is defined as maximizing a non-linear function of preference probabilities (Ψ) balanced by KL regularization to a reference policy. RLHF and DPO are shown to be special cases when Ψ is the logit function, relying on the Bradley-Terry model.\nIdentity-PO (IPO) is an approach setting Ψ to the identity function, bypassing the Bradley-Terry model. IPO optimizes total preferences directly, maintaining effective regularization even with deterministic preferences. An empirical sampled loss is derived for practical implementation, avoiding reward modeling.\nPaper: A General Theoretical Paradigm to Understand Learning from Human Preferences.\nIPO Loss function:\n$$ \\mathbb{E}_{(y_w, y_l) \\sim D} \\left[ \\left( h_\\pi(y_w, y_l) - \\frac{\\tau^{-1}}{2} \\right)^2 \\right] $$IPO learns from preferences dataset simply by regressing the gap between log-likelihood ratios $\\log(\\pi(y_w)/\\pi(y_l))$ and $\\log(\\pi_{\\text{ref}}(y_w)/\\pi_{\\text{ref}}(y_l))$ to $\\frac{\\tau^{-1}}{2}$.\nSo the weaker the regularisation becomes, the higher would be the log-likelihood ratio of $y_w$ to $y_l$.\nIn other words IPO, unlike DPO, always regularizes its solution towards $\\pi_{\\text{ref}}$ by controlling the gap between the log-likelihood ratios $\\log(\\pi(y_w)/\\pi(y_l))$ and $\\log(\\pi_{\\text{ref}}(y_w)/\\pi_{\\text{ref}}(y_l))$, thus avoiding the over-fitting to the preference dataset.\n# ... calculate logits the same as DPO # https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/dpo_trainer.py#L974 # eqn (17) of the paper where beta is the regularization parameter for the IPO loss, denoted by tau in the paper. losses = (logits - 1 / (2 * self.beta)) ** 2 SPPO - Self-Play Preference Optimization for Language Model Alignment Paper: Wu, Yue, et al. Self-Play Preference Optimization for Language Model Alignment. arXiv:2405.00675, arXiv, 4 Oct. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2405.00675.\nDMPO: Direct Multi-Turn Preference Optimization Extends DPO to multi-turn dialogue by optimizing preferences across conversation history, improving coherence and relevance in multi-step interactions.\nPaper: Shi, Wentao, et al. Direct Multi-Turn Preference Optimization for Language Agents. arXiv:2406.14868, arXiv, 23 Feb. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2406.14868.\nWPO - Enhancing RLHF with Weighted Preference Optimization IV. Reference-Free Optimization SimPO: Simple Preference Optimization with a Reference-Free Reward SimPO aligns the reward function with the generation metric, eliminates the need for a reference model, and introduces a target reward margin to enhance performance.\nIn DPO, for any triple $(x, y_w, y_l)$, satisfying the reward ranking $r(x, y_w) \u003e r(x, y_l)$ does not necessarily gaurantee the likelihood ranking $p_\\theta(y_w \\mid x) \u003e p_\\theta(y_l \\mid x)$. The paper found that only roughly 50% of the triples from a held-out set satisfy this condition when trained with DPO.\nPaper: SimPO: Simple Preference Optimization with a Reference-Free Reward.\nLength-Normalized Reward formulation: Replacing the reward formulation in DPO with $p_\\theta$ in the average log likelihood $$ p_\\theta(y \\mid x) = \\frac{1}{|y|} \\log \\pi_\\theta(y \\mid x) = \\frac{1}{|y|} \\sum_{i=1}^{|y|} \\log \\pi_\\theta(y_i \\mid x, y_{\\lt i}). $$ so that it aligns with the likelihood metric that guides generation, results in a length-normalized reward: $$ r_{\\text{SimPO}}(x, y) = \\frac{\\beta}{|y|} \\log \\pi_\\theta(y \\mid x) = \\frac{\\beta}{|y|} \\sum_{i=1}^{|y|} \\log \\pi_\\theta(y_i \\mid x, y_{\\lt i}) $$where $\\beta$ is a constant that controls the scaling of the reward difference.\nThis reward eliminates the need for a reference model, enhancing memory and computational efficiency.\nRemoving the length normalization term from the reward formulation results in a bias toward generating longer but lower-quality sequences.\nTarget Reward Margin: SimPO incorporates a target margin $\\gamma$ into the Bradley-Terry objective to ensure the reward difference between winning ($y_w$) and losing ($y_l$) responses exceeds $\\gamma$: $$ \\mathcal{L}_{\\text{SimPO}} = -\\mathbb{E}\\left[\\log \\sigma\\left(\\frac{\\beta}{|y_w|}\\log\\pi_{\\theta}(y_w|x) - \\frac{\\beta}{|y_l|}\\log\\pi_{\\theta}(y_l|x) - \\gamma\\right)\\right] $$This margin enhances the model’s ability to distinguish between high-quality and low-quality responses, improving generalization.\nImplementation: https://docs.pytorch.org/torchtune/0.3/_modules/torchtune/rlhf/loss/dpo.html#SimPOLoss.forward\ndef simpo_loss( policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, simpo_gamma, beta ): logits = (policy_chosen_logps - policy_rejected_logps) gamma_logratios = simpo_gamma / beta logits = logits - gamma_logratios losses = ( -F.logsigmoid(beta * logits) * (1 - label_smoothing) - F.logsigmoid(beta * logits) * label_smoothing ) return losses Odds Ratio Preference Optimization (ORPO) ORPO (Odds Ratio Preference Optimization) combines SFT and preference optimization in a single monolithic training stage, eliminating reference model requirements. This approach streamlines training while maintaining competitive performance across multiple benchmarks.\nPaper: Hong, S., et al. (2024). ORPO: Monolithic Preference Optimization without Reference Model.\nExisting methods like RLHF and DPO often require:\nA supervised fine-tuning (SFT) warm-up stage. A reference model for comparison. Complex multi-stage processes, which are resource-intensive. The core insight: SFT can be enhanced to directly incorporate preference alignment by penalizing undesired generation styles, eliminating the need for extra stages or reference models.\nORPO Algorithm ORPO integrates preference alignment into SFT using an odds ratio to contrast favored (\\(y_w\\)) and disfavored (\\(y_l\\)) responses. The key components are:\nObjective Function: \\(\\mathcal{L}_{ORPO} = \\mathcal{L}_{SFT} + \\lambda \\cdot \\mathcal{L}_{OR}\\), where: \\(\\mathcal{L}_{SFT}\\) is the standard negative log-likelihood (NLL) loss for SFT. \\(\\mathcal{L}_{OR}\\) is a new loss term that maximizes the odds ratio between \\(y_w\\) and \\(y_l\\), defined as: \\[ \\mathcal{L}_{OR} = -\\log \\sigma\\left(\\log \\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)}\\right) \\] where \\(odds_\\theta(y|x) = \\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)}\\) and \\(\\sigma\\) is the sigmoid function. Gradient Analysis: The gradient of \\(\\mathcal{L}_{OR}\\) dynamically penalizes disfavored responses, accelerating adaptation to desired styles while preserving domain knowledge from SFT. Implementation from https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/orpo_trainer.py#L623\ndef odds_ratio_loss( self, policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, ) -\u003e tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]: \"\"\"Compute ORPO's odds ratio (OR) loss for a batch of policy and reference model log probabilities. Args: policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,) policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,) Returns: A tuple of three tensors: (losses, chosen_rewards, rejected_rewards). The losses tensor contains the ORPO loss for each example in the batch. The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively. The log odds ratio of the chosen responses over the rejected responses ratio for logging purposes. The `log(sigmoid(log_odds_chosen))` for logging purposes. \"\"\" # Derived from Eqs. (4) and (7) from https://huggingface.co/papers/2403.07691 by using log identities and exp(log(P(y|x)) = P(y|x) log_odds = (policy_chosen_logps - policy_rejected_logps) - ( torch.log1p(-torch.exp(policy_chosen_logps)) - torch.log1p(-torch.exp(policy_rejected_logps)) ) ratio = F.logsigmoid(log_odds) losses = self.beta * ratio chosen_rewards = self.beta * (policy_chosen_logps.to(self.accelerator.device)).detach() rejected_rewards = self.beta * (policy_rejected_logps.to(self.accelerator.device)).detach() return losses, chosen_rewards, rejected_rewards, torch.mean(ratio), torch.mean(log_odds) Advantages over Existing Methods**\nMonolithic Design: ORPO performs preference alignment in a single stage during SFT, unlike RLHF/DPO’s multi-stage workflows. No Reference Model: Avoids the need for a frozen SFT model, reducing memory usage and computational cost (half the forward passes per batch). Stability with Odds Ratio: Compared to probability ratios, the odds ratio provides a milder penalty, preventing over-suppression of disfavored responses (Figure 6). V. Non-preference-based methods KTO - Model Alignment as Prospect Theoretic Optimization The paper introduces Kahneman-Tversky Optimization (KTO), a approach maximizes the utility of generations instead of maximizing the log-likelihood of preferences(as DPO does).\nTheoretically, KTO align LLM with human preferences by framing alignment as a problem of prospect theoretic optimization. Drawing from Kahneman and Tversky’s prospect theory, which describes how humans make decisions under uncertainty with biases like loss aversion, the authors argue that existing alignment methods (e.g., DPO, PPO) implicitly incorporate these biases. They formalize these as Human-Aware Losses (HALOs) and propose KTO as a principled HALO that directly maximizes human utility rather than the log-likelihood of preferences.\nPaper: KTO: Model Alignment as Prospect Theoretic Optimization.\nKTO Method:\nDerives a HALO using Kahneman-Tversky’s value function, replacing exponents with logistic functions for stability. Requires only binary feedback (desirable/undesirable) instead of preference pairs, making data collection cheaper and more scalable. Introduces hyperparameters β (risk aversion) and λ_D/λ_U (loss aversion for desirable/undesirable outputs) to model human decision biases. Prospect Theory in Alignment:\nValue Function: Models human utility relative to a reference point, with concavity in gains and steeper slopes in losses (loss aversion). Reference Point: In KTO, the reference is the KL divergence between the current policy and the reference model, estimated via microbatch mismatches for stability. Loss Function:\n$\\lambda_y$ denotes $\\lambda_D$ ($\\lambda_U$) when $y$ is desirable(undesirable) respectively, the default KTO loss is: $$ \\mathcal{L}_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) \\triangleq \\mathbb{E}_{x,y \\sim D} \\left[ \\lambda_y - v(x,y) \\right] \\tag{8} $$ where $$ r_\\theta(x,y) = \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}, $$ $$ z_0 = \\text{KL}\\bigl(\\pi_\\theta(y'|x) \\big\\| \\pi_{\\text{ref}}(y'|x)\\bigr), $$ $$ v(x,y) = \\begin{cases} \\lambda_D \\sigma\\bigl(\\beta(r_\\theta(x,y) - z_0)\\bigr) \u0026 \\text{if } y \\sim y_{\\text{desirable}}|x, \\\\ \\lambda_U \\sigma\\bigl(\\beta(z_0 - r_\\theta(x,y))\\bigr) \u0026 \\text{if } y \\sim y_{\\text{undesirable}}|x. \\end{cases} $$Implementation from https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/kto_trainer.py#L1090:\ndef kto_loss( policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, policy_KL_logps: torch.FloatTensor, reference_chosen_logps: torch.FloatTensor, reference_rejected_logps: torch.FloatTensor, reference_KL_logps: torch.FloatTensor, ): kl = (policy_KL_logps - reference_KL_logps).mean().detach() # Chosen losses if policy_chosen_logps.shape[0] != 0 or reference_chosen_logps.shape[0] != 0: chosen_logratios = policy_chosen_logps - reference_chosen_logps chosen_losses = 1 - F.sigmoid(beta * (chosen_logratios - kl)) chosen_rewards = beta * chosen_logratios.detach() else: chosen_losses = torch.Tensor([]) chosen_rewards = torch.Tensor([]) # Rejected losses if policy_rejected_logps.shape[0] != 0 or reference_rejected_logps.shape[0] != 0: rejected_logratios = policy_rejected_logps - reference_rejected_logps rejected_losses = 1 - F.sigmoid(beta * (kl - rejected_logratios)) rejected_rewards = beta * rejected_logratios.detach() else: rejected_losses = torch.Tensor([]) rejected_rewards = torch.Tensor([]) losses = torch.cat( (chosen_losses, rejected_losses), 0, ) return losses VI. Others SPO - A Minimaximalist Approach to Reinforcement Learning from Human Feedback Uses self-play to generate preference data, where models compete to maximize rewards. It employs a minimax approach to balance exploration and exploitation. Paper: OpenAI. (2024). A Minimaximalist Approach to Reinforcement Learning from Human Feedback.\nVII. Key Trends and Challenges []: https://github.com/volcengine/verl\nScalability: Methods like RLHF and DPO face challenges with large models (e.g., 405B parameters) due to memory and computational costs. Frameworks like Verl address this via asynchronous distributed training . Data Efficiency: SimPO and ORPO reduce reliance on reference models and pairwise data, while Step-DPO and KTO focus on fine-grained optimization . Generalization: SPO and DMPO aim to improve generalization across tasks (e.g., dialogue, math) by leveraging self-play and multi-turn optimization . Theoretical Foundations: KTO and SimPO provide theoretical insights into preference learning, linking RL to prospect theory and information theory . VIII. Future Directions Multi-Modality: IPO and DMPO highlight the need for alignment across text, video, and dialogue. Self-Supervised Learning: RLAIF and SPO explore AI-generated feedback to reduce human labeling. Efficiency: Schulman, John, et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, arXiv, 28 Aug. 2017. arXiv.org, http://arxiv.org/abs/1707.06347. ↩︎\nOuyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, http://arxiv.org/abs/2203.02155. ↩︎\nBai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2212.08073. ↩︎\nLee, Harrison, et al. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. arXiv:2309.00267, arXiv, 3 Sept. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2309.00267. ↩︎\nShao, Zhihong, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, arXiv, 27 Apr. 2024. arXiv.org, http://arxiv.org/abs/2402.03300. ↩︎\nJ. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-approx.html. ↩︎\nDeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948. ↩︎\nAhmadian, Arash, et al. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs. arXiv:2402.14740, arXiv, 26 Feb. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2402.14740. ↩︎\nhttps://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo ↩︎\nHuang, Shengyi, et al. A2C Is a Special Case of PPO. arXiv:2205.09123, arXiv, 18 May 2022. arXiv.org, https://doi.org/10.48550/arXiv.2205.09123. ↩︎\nLi, Ziniu, et al. ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. arXiv:2310.10505, arXiv, 16 May 2024. arXiv.org, https://doi.org/10.48550/arXiv.2310.10505. ↩︎ ↩︎\nhttps://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights ↩︎\nHu, Jian, et al. REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models. arXiv:2501.03262, arXiv, 6 Apr. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.03262. ↩︎\nYu, Qiying, et al. DAPO: An Open-Source LLM Reinforcement Learning System at Scale. arXiv:2503.14476, arXiv, 20 May 2025. arXiv.org, https://doi.org/10.48550/arXiv.2503.14476. ↩︎\nLiu, Zichen, et al. Understanding R1-Zero-Like Training: A Critical Perspective. arXiv:2503.20783, arXiv, 26 Mar. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2503.20783. ↩︎\nFeng, Lang, et al. Group-in-Group Policy Optimization for LLM Agent Training. arXiv:2505.10978, arXiv, 16 May 2025. arXiv.org, https://doi.org/10.48550/arXiv.2505.10978. ↩︎\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model. ↩︎\nGuo, Shangmin, et al. Direct Language Model Alignment from Online AI Feedback. arXiv:2402.04792, arXiv, 29 Feb. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2402.04792. ↩︎\nToken-level Direct Preference Optimization. ↩︎\nGorbatovski, Alexey, et al. Learn Your Reference Model for Real Good Alignment. arXiv:2404.09656, arXiv, 25 Feb. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2404.09656. ↩︎\n","wordCount":"9520","inLanguage":"en","datePublished":"2025-05-30T00:00:00Z","dateModified":"2025-05-30T00:00:00Z","author":{"@type":"Person","name":"Cong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://congchan.github.io/posts/awesome-large-language-model-llm-post-training-2025-update/"},"publisher":{"@type":"Organization","name":"Cong's Log","logo":{"@type":"ImageObject","url":"https://congchan.github.io/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://congchan.github.io/ accesskey=h title="Cong's Log (Alt + H)">Cong's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://congchan.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://congchan.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://congchan.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://congchan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://congchan.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Awesome Large Language Model (LLM) Post-training - [2025 Update]</h1><div class=post-meta><span title='2025-05-30 00:00:00 +0000 UTC'>2025-05-30</span>&nbsp;·&nbsp;45 min&nbsp;·&nbsp;Cong&nbsp;|&nbsp;<a href=https://github.com/%3cgitlab%20user%3e/%3crepo%20name%3e/tree/%3cbranch%20name%3e/%3cpath%20to%20content%3e//posts/llm-Post-training-Updating.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#i-rlf-reinforcement-learning-from-x-feedback-with-proximal-policy-optimization-ppo aria-label="I. RL*F (Reinforcement Learning from X Feedback) with Proximal Policy Optimization (PPO)">I. RL*F (Reinforcement Learning from X Feedback) with Proximal Policy Optimization (PPO)</a><ul><li><a href=#reinforcement-learning-from-human-feedback-rlhf aria-label="Reinforcement Learning from Human Feedback (RLHF)">Reinforcement Learning from Human Feedback (RLHF)</a></li><li><a href=#reinforcement-learning-from-ai-feedback-rlaif aria-label="Reinforcement Learning from AI Feedback (RLAIF)">Reinforcement Learning from AI Feedback (RLAIF)</a></li></ul></li><li><a href=#ii-improve-value-functioning-and-eliminating-critic-model aria-label="II. Improve Value Functioning and Eliminating Critic Model">II. Improve Value Functioning and Eliminating Critic Model</a><ul><li><a href=#group-relative-policy-optimization-grpo aria-label="Group Relative Policy Optimization (GRPO)">Group Relative Policy Optimization (GRPO)</a><ul><li><a href=#how-grpo-works aria-label="How GRPO Works">How GRPO Works</a></li><li><a href=#rlhf-with-grpo---deepseek-r1 aria-label="RLHF with GRPO - Deepseek-R1">RLHF with GRPO - Deepseek-R1</a></li></ul></li><li><a href=#rloo-reinforce-leave-one-out aria-label="RLOO (REINFORCE Leave One-Out)">RLOO (REINFORCE Leave One-Out)</a><ul><li><a href=#reinforce aria-label=REINFORCE>REINFORCE</a></li><li><a href=#reinforce-leave-one-out-rloo aria-label="REINFORCE Leave-One-Out (RLOO)">REINFORCE Leave-One-Out (RLOO)</a></li></ul></li><li><a href=#remax aria-label=ReMax>ReMax</a><ul><li><a href=#key-insights-unique-properties-of-rlhf-for-llms aria-label="Key Insights: Unique Properties of RLHF for LLMs">Key Insights: Unique Properties of RLHF for LLMs</a></li><li><a href=#the-remax-algorithm aria-label="The ReMax Algorithm">The ReMax Algorithm</a></li></ul></li><li><a href=#reinforce-1 aria-label=REINFORCE++>REINFORCE++</a></li><li><a href=#dapo-decoupled-clip-and-dynamic-sampling-policy-optimization aria-label="DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization)">DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization)</a><ul><li><a href=#dataset-and-implementation aria-label="Dataset and Implementation">Dataset and Implementation</a></li><li><a href=#experiments-and-results aria-label="Experiments and Results">Experiments and Results</a></li></ul></li><li><a href=#dr-grpo aria-label="Dr. GRPO">Dr. GRPO</a></li><li><a href=#group-in-group-policy-optimization-gigpo-for-llm-agent-training aria-label="Group-in-Group Policy Optimization (GiGPO) for LLM Agent Training">Group-in-Group Policy Optimization (GiGPO) for LLM Agent Training</a></li></ul></li><li><a href=#iii-optimization-without-reward-model aria-label="III. Optimization without Reward Model">III. Optimization without Reward Model</a><ul><li><a href=#direct-preference-optimization-dpo aria-label="Direct Preference Optimization (DPO)">Direct Preference Optimization (DPO)</a><ul><li><a href=#a-mechanistic-understanding-of-dpo aria-label="A mechanistic understanding of DPO">A mechanistic understanding of DPO</a></li><li><a href=#dpos-training-process aria-label="DPO&rsquo;s Training Process">DPO&rsquo;s Training Process</a></li><li><a href=#advantages-of-dpo aria-label="Advantages of DPO">Advantages of DPO</a></li><li><a href=#practical-implementation aria-label="Practical Implementation">Practical Implementation</a></li><li><a href=#limitations-and-considerations aria-label="Limitations and Considerations">Limitations and Considerations</a></li></ul></li><li><a href=#online-dpo---direct-language-model-alignment-from-online-ai-feedback aria-label="Online DPO - Direct Language Model Alignment from Online AI Feedback">Online DPO - Direct Language Model Alignment from Online AI Feedback</a></li><li><a href=#tdpo-token-level-direct-preference-optimization aria-label="TDPO: Token-level Direct Preference Optimization">TDPO: Token-level Direct Preference Optimization</a></li><li><a href=#trust-region-dpo-tr-dpo aria-label="Trust Region DPO (TR-DPO)">Trust Region DPO (TR-DPO)</a></li><li><a href=#step-dpo-step-wise-preference-optimization-for-long-chain-reasoning aria-label="Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning">Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning</a></li><li><a href=#identity-preference-optimization-ipo aria-label="Identity Preference Optimization (IPO)">Identity Preference Optimization (IPO)</a></li><li><a href=#sppo---self-play-preference-optimization-for-language-model-alignment aria-label="SPPO - Self-Play Preference Optimization for Language Model Alignment">SPPO - Self-Play Preference Optimization for Language Model Alignment</a></li><li><a href=#dmpo-direct-multi-turn-preference-optimization aria-label="DMPO: Direct Multi-Turn Preference Optimization">DMPO: Direct Multi-Turn Preference Optimization</a></li><li><a href=#wpo---enhancing-rlhf-with-weighted-preference-optimization aria-label="WPO - Enhancing RLHF with Weighted Preference Optimization">WPO - Enhancing RLHF with Weighted Preference Optimization</a></li></ul></li><li><a href=#iv-reference-free-optimization aria-label="IV. Reference-Free Optimization">IV. Reference-Free Optimization</a><ul><li><a href=#simpo-simple-preference-optimization-with-a-reference-free-reward aria-label="SimPO: Simple Preference Optimization with a Reference-Free Reward">SimPO: Simple Preference Optimization with a Reference-Free Reward</a></li><li><a href=#odds-ratio-preference-optimization-orpo aria-label="Odds Ratio Preference Optimization (ORPO)">Odds Ratio Preference Optimization (ORPO)</a><ul><li><a href=#orpo-algorithm aria-label="ORPO Algorithm">ORPO Algorithm</a></li></ul></li></ul></li><li><a href=#v-non-preference-based-methods aria-label="V. Non-preference-based methods">V. Non-preference-based methods</a><ul><li><a href=#kto---model-alignment-as-prospect-theoretic-optimization aria-label="KTO - Model Alignment as Prospect Theoretic Optimization">KTO - Model Alignment as Prospect Theoretic Optimization</a></li></ul></li><li><a href=#vi-others aria-label="VI. Others">VI. Others</a><ul><li><a href=#spo---a-minimaximalist-approach-to-reinforcement-learning-from-human-feedback aria-label="SPO - A Minimaximalist Approach to Reinforcement Learning from Human Feedback">SPO - A Minimaximalist Approach to Reinforcement Learning from Human Feedback</a></li></ul></li><li><a href=#vii-key-trends-and-challenges aria-label="VII. Key Trends and Challenges">VII. Key Trends and Challenges</a></li><li><a href=#viii-future-directions aria-label="VIII. Future Directions">VIII. Future Directions</a></li></ul></div></details></div><div class=post-content><p>In the race to build truly helpful AI assistants, we&rsquo;ve discovered a fundamental truth: raw intelligence isn&rsquo;t enough. A model that masters calculus but can&rsquo;t refuse harmful requests is like a library with no librarian - overflowing with knowledge but dangerously uncurated.</p><p>This is the alignment problem: how do we transform raw language models into trustworthy collaborators? For years, <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> reigned supreme. Its PPO-based approach taught ChatGPT to decline malicious requests and helped Claude write harmless poetry. But beneath the surface, RLHF&rsquo;s complexity was showing:</p><ol><li><strong>The 3-stage training treadmill</strong> (SFT → Reward Modeling → RL tuning)</li><li><strong>Prohibitively expensive</strong> human preference labeling</li><li><strong>Reward hacking</strong> vulnerabilities where models &ldquo;game&rdquo; the system</li></ol><p>Enter the new generation of alignment techniques. There are three trends of directions:</p><ol><li>Eliminating reward modeling stages. Or use Rule-based rewards to incentivize LLM intelligence.</li><li>Using AI-generated preferences.</li><li>Enabling single-step optimization</li></ol><p>I am currently following the most cutting-edge LLM alignment methods, and this blog will be updated periodically.</p><h1 id=i-rlf-reinforcement-learning-from-x-feedback-with-proximal-policy-optimization-ppo><strong>I. RL*F (Reinforcement Learning from X Feedback) with Proximal Policy Optimization (PPO)</strong><a hidden class=anchor aria-hidden=true href=#i-rlf-reinforcement-learning-from-x-feedback-with-proximal-policy-optimization-ppo>#</a></h1><p><cite><a href=https://arxiv.org/abs/1707.06347>Proximal Policy Optimization Algorithms</a>.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite> (PPO) is the mose widely used reinforcment learning algorithm in Post-training. PPO is a policy gradient algorithm that optimizes policies by maximizing a clipped surrogate objective to balance exploration and exploitation. It is widely used in RLHF due to its stability and sample efficiency.</p><ul><li><strong>Core Innovation</strong>: Uses clipped objective function to limit policy updates, balancing stability and performance. Dominates RLHF pipelines.</li><li><strong>Application</strong>: OpenAI&rsquo;s ChatGPT, Claude series.</li><li><strong>Limitations</strong>: Requires separate reward model training; unstable with large batches.</li></ul><h2 id=reinforcement-learning-from-human-feedback-rlhf><strong>Reinforcement Learning from Human Feedback (RLHF)</strong><a hidden class=anchor aria-hidden=true href=#reinforcement-learning-from-human-feedback-rlhf>#</a></h2><p><cite><a href=https://arxiv.org/abs/2203.02155>Training Language Models to Follow Instructions with Human Feedback</a>. <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></cite> (RLHF) combines reinforcement learning with human preferences to train LLMs, to produce outputs that are more aligned with human preferences and expectations.</p><p>RLHF combines supervised fine-tuning (SFT) with PPO, using human ranked dataset to train a reward model (RM) to model human preferences, and guides policy updates. RLHF aligns LLMs with human values but is costly due to manual labeling.</p><p><img loading=lazy src=/images/RLHF.png></p><ol><li><strong>Base Model Training</strong><ul><li>Start with a pre-trained language model (like GPT)</li><li>The model is initially trained on large datasets using supervised learning</li></ul></li><li><strong>SFT</strong>: Supervised fine-tuning on high-quality data.</li><li><strong>Human Feedback Collection</strong><ul><li>Human evaluators compare pairs of model outputs</li><li>They rank which response is better based on criteria like: Helpfulness,Harmlessness, Honesty</li></ul></li><li><strong>Reward Modeling</strong>:<ul><li>A separate neural network (reward model) is trained on human preference rankings, to predict human preferences</li><li>This model learns to score outputs based on the collected human feedback</li><li>It essentially learns to mimic human judgment</li></ul></li><li><strong>Reinforcement Learning Optimization</strong><ul><li>The original language model as policy is optimized against RM using PPO.</li><li>The reward model provides feedback signals</li><li>Techniques like Proximal Policy Optimization (PPO) are commonly used</li><li>The model learns to generate responses that maximize the predicted human preference score</li></ul></li></ol><p><strong>Benefits</strong></p><ul><li>Better Alignment: Models produce outputs more consistent with human values</li><li>Reduced Harmful Content: Helps minimize toxic, biased, or dangerous responses</li><li>Improved Quality: Responses become more helpful and relevant</li><li>Scalability: Once trained, the reward model can provide feedback without constant human intervention</li></ul><p><strong>Challenges</strong></p><ul><li>Scalability of Human Feedback: Collecting sufficient high-quality human feedback is expensive and time-consuming</li><li>Reward Hacking: Models might find ways to maximize reward scores without actually improving quality</li><li>Bias in Human Feedback: Human evaluators may introduce their own biases</li><li>Complexity: The multi-stage training process is computationally intensive</li></ul><h2 id=reinforcement-learning-from-ai-feedback-rlaif><strong>Reinforcement Learning from AI Feedback (RLAIF)</strong><a hidden class=anchor aria-hidden=true href=#reinforcement-learning-from-ai-feedback-rlaif>#</a></h2><p>In <cite><a href=https://doi.org/10.48550/arXiv.2212.08073>Constitutional AI: Harmlessness from AI Feedback</a>. <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></cite>, researchers introduced a paradigm that replaces human labels for harmfulness with AI-generated feedback. The approach uses a &ldquo;constitution&rdquo; of principles to guide self-critique and revision, enabling models to learn harmless behavior on a hybrid of human and AI preferences. This paper was the first effort to explore RLAIF.</p><ul><li><strong>Key Innovation</strong>: The framework combines supervised learning (critique → revision cycles) and RL from AI Feedback (RLAIF), where a preference model (PM) is trained on AI-generated comparisons. For example, models generate pairs of responses and evaluate which aligns better with constitutional principles (e.g., &ldquo;avoid harmful advice&rdquo;).</li><li><strong>Impact</strong>: As shown in Figure 2 of the paper, Constitutional AI achieves a Pareto improvement in harmlessness and helpfulness, outperforming RLHF models that trade off these traits. The approach reduces reliance on human labeling, a critical step toward scalable supervision.</li></ul><p>This work laid the groundwork for self-supervised RM, demonstrating that models can learn to evaluate their own behavior using explicit principles.</p><p>In <cite><a href=https://arxiv.org/abs/2309.00267>RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</a> <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></cite>, RLAIF achieved comparable performance to RLHF.</p><h1 id=ii-improve-value-functioning-and-eliminating-critic-model><strong>II. Improve Value Functioning and Eliminating Critic Model</strong><a hidden class=anchor aria-hidden=true href=#ii-improve-value-functioning-and-eliminating-critic-model>#</a></h1><p>The PPO algorithm necessitates loading four models, each of substantial size, which introduces considerable engineering complexity in the design of multi-model training, inference, and real-time parameter updates. This process demands a significant amount of GPU resources.</p><p>For instance, during RLHF training, when the Actor, Critic, Reward, and Ref Models are of identical scale, such as 70B, employing vLLM/TensorRT-llm for PPO sample generation acceleration and DeepSpeed/Megatron for training acceleration results in roughly equal computational resource consumption between inference and training stages. Consequently, the Critic model accounts for approximately one-quarter of the total computational resource usage.</p><p>In the context of LLMs, it is common for only the final token to receive a reward score from the reward model. This practice can complicate the training of a value function that accurately reflects each token&rsquo;s contribution. To address this challenge, numerous studies focus on optimizing the calculation of the value function, incidentally simplifying or potentially eliminating the need for the Critic model in the process.</p><h2 id=group-relative-policy-optimization-grpo><strong>Group Relative Policy Optimization (GRPO)</strong><a hidden class=anchor aria-hidden=true href=#group-relative-policy-optimization-grpo>#</a></h2><p><cite>Group Relative Policy Optimization (GRPO)<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></cite> is an efficient training algorithm proposed in the DeepSeekMath paper to enhance mathematical reasoning in language models.</p><p><img loading=lazy src=/images/GRPO.png></p><p>GRPO is a variant of PPO that eliminates the need for a critic model (value function), instead estimating the baseline from <strong>group-averaged rewards</strong>. This reduces memory and computational costs significantly, making it more resource-efficient.</p><p><strong>Key Differences from PPO</strong></p><ul><li><strong>No Value Function</strong>: Unlike PPO, which uses a learned value function to compute advantages, GRPO calculates advantages using relative rewards within a group of sampled outputs for the same question.</li><li><strong>Group-based Baseline</strong>: For each question $q$, GRPO samples $ G $ outputs from the old policy. The baseline is the average reward of these outputs, and advantages are normalized within the group.</li><li><strong>Simplified Objective</strong>: GRPO optimizes the policy by maximizing a objective that uses group-relative advantages, avoiding the complexity of value function training.</li></ul><h3 id=how-grpo-works><strong>How GRPO Works</strong><a hidden class=anchor aria-hidden=true href=#how-grpo-works>#</a></h3><ol><li><p><strong>Sampling</strong>: For each question $q$, sample $G$ outputs $\{o_1, o_2, \dots, o_G\}$ from the old policy $\pi_{\theta_{\text{old}}}$.</p></li><li><p><strong>Outcome Reward Scoring and Normalization</strong>: Use a reward model to score each output, yielding $\{r_1, r_2, \dots, r_G\}$. Outcome supervision provides the normalized reward at the end of each output $o_i$ and sets the advantages $\hat{A}_{i,t}$ of all tokens in the output as the normalized reward by subtracting the group mean and dividing by the standard deviation:</p>$$
\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \text{mean}(r)}{\text{std}(r)}
$$</li><li><p><strong>Policy Update</strong>: Maximize the GRPO objective, which includes a KL divergence term to regularize against a reference model. optimizes the policy model by maximizing the following objective:</p>$$
\begin{aligned} \mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[ q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q) \right] \\\\
& \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left\{ \min \left[ \frac{\pi_{\theta}(o_{i,t}|q, o_{i,\lt t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,\lt t})} \hat{A}_{i,t}, \text{clip} \left( \frac{\pi_{\theta}(o_{i,t}|q, o_{i,\lt t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,\lt t})}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}_{i,t} \right] - \beta \mathbb{D}_{\text{KL}} \left[ \pi_{\theta} \| \pi_{\text{ref}} \right] \right\} \end{aligned}
$$<p>where $\epsilon$ and $\beta$ are hyper-parameters, and $\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only, which will be detailed in the following subsections.</p><ul><li>Also note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\hat{A}_{i,t}$.<ul><li>PPO approach:<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Modify the reward with KL penalty</span>
</span></span><span class=line><span class=cl><span class=n>reward_with_kl</span> <span class=o>=</span> <span class=n>reward</span> <span class=o>-</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>kl_divergence</span><span class=p>(</span><span class=n>current_policy</span><span class=p>,</span> <span class=n>ref_policy</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>advantage</span> <span class=o>=</span> <span class=n>compute_advantage</span><span class=p>(</span><span class=n>values</span><span class=p>,</span> <span class=n>reward_with_kl</span><span class=p>,</span> <span class=n>dones</span><span class=p>)</span>
</span></span></code></pre></div></li><li>GRPO Approach:<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Compute advantage using original reward</span>
</span></span><span class=line><span class=cl><span class=n>advantage</span> <span class=o>=</span> <span class=n>compute_advantage</span><span class=p>(</span><span class=n>values</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>dones</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Policy loss with direct KL regularization</span>
</span></span><span class=line><span class=cl><span class=n>policy_loss</span> <span class=o>=</span> <span class=o>-</span><span class=p>(</span><span class=n>log_probs</span> <span class=o>*</span> <span class=n>advantage</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>kl_loss</span> <span class=o>=</span> <span class=n>kl_divergence</span><span class=p>(</span><span class=n>current_policy</span><span class=p>,</span> <span class=n>ref_policy</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>total_loss</span> <span class=o>=</span> <span class=n>policy_loss</span> <span class=o>+</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>kl_loss</span>
</span></span></code></pre></div></li></ul></li><li>And different from the KL penalty term used in PPO, GRPO estimate the KL divergence with the following <cite>unbiased estimator<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></cite>:</li></ul>$$
\mathbb{D}_{KL} \left[ \pi_{\theta} \| \pi_{ref} \right] = \frac{\pi_{ref}(o_{i,t}|q, o_{i,\lt t})}{\pi_{\theta}(o_{i,t}|q, o_{i,\lt t})} - \log \frac{\pi_{ref}(o_{i,t}|q, o_{i,\lt t})}{\pi_{\theta}(o_{i,t}|q, o_{i,\lt t})} - 1
$$<p>which is guaranteed to be positive.</p></li></ol><p><strong>Experimental Results</strong></p><ul><li>DeepSeekMath-RL (7B) using GRPO surpasses all open-source models on MATH and approaches closed-source models like GPT-4.</li><li>Iterative GRPO (updating the reward model incrementally) further boosts performance, especially in the first iteration.</li><li>GRPO with process supervision (step-wise rewards) outperforms outcome supervision, highlighting the value of fine-grained feedback.</li></ul><h3 id=rlhf-with-grpo---deepseek-r1><strong>RLHF with GRPO - Deepseek-R1</strong><a hidden class=anchor aria-hidden=true href=#rlhf-with-grpo---deepseek-r1>#</a></h3><p><cite>DeepSeek-R1<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> </cite>is the first public model who make use of both rule-based rewards and general RLHF via GRPO.</p><p><strong>DeepSeek-R1-Zero: Pure RL Training</strong></p><ul><li><strong>RL Algorithm</strong>: Uses Group Relative Policy Optimization (GRPO) to optimize policies without a critic model, reducing training costs.</li><li><strong>Reward Modeling</strong>: Relies on rule-based rewards for accuracy (e.g., math problem correctness) and format (enforcing CoT within tags), avoiding neural reward models to prevent reward hacking.</li><li><strong>Self-Evolution</strong>: Through RL, the model autonomously develops complex reasoning behaviors, such as reflecting on mistakes and exploring alternative solutions, leading to significant performance gains. For example, AIME 2024 pass@1 improves from 15.6% to 71.0%, and to 86.7% with majority voting.</li><li><strong>Limitation</strong>:<ul><li>Suffering from readability issues, such as language mixing and unstructured outputs. And</li><li>Lack of non-reasoning tasks such as writing and factual QA is suboptimal.</li><li>Experiences an unstable early training process.</li></ul></li></ul><p><strong>DeepSeek-R1: Cold-Start and Multi-Stage Refinement</strong>
DeepSeek-R1 is an attempts to address the limitations of DeepSeek-R1-Zero.
<img loading=lazy src=/images/deepseek_r1.png></p><ul><li><p><strong>1. Cold-Start Data</strong>: Collect thousands of long CoT (chain-of-thought) examples through few-shot prompting, model-generated outputs, and human annotation. These cold-start data are used to fine-tune the DeepSeek-V3-Base to create an initial RL actor that prioritizes readable formats (e.g., summaries and structured CoT) and reducing language mixing.</p></li><li><p><strong>2. Reasoning-Oriented RL</strong>: Apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. Incorporates language consistency rewards to mitigate mixed-language outputs, balancing performance with human readability.</p></li><li><p><strong>3. Rejection Sampling & SFT</strong>: After RL convergence, new SFT data is collected from RL checkpoints, combining reasoning and non-reasoning tasks (e.g., writing, factual QA) to enhance general capabilities.</p><ul><li>For <strong>Reasoning Data Collection</strong>: Use rejection sampling on the RL checkpoint to curate ~600K reasoning samples, filtering out mixed-language and unreadable CoT. Include generative reward models (using DeepSeek-V3) for evaluation.</li><li>For <strong>Non-Reasoning Data</strong>: Reuse SFT data from DeepSeek-V3 for tasks like writing, factual QA, and translation, collecting ~200K samples.</li><li><strong>Fine-Tuning</strong>: Train DeepSeek-V3-Base on the combined ~800K samples for two epochs to enhance general capabilities.</li></ul></li><li><p><strong>4. Scenario-Agnostic RL</strong>: A final RL stage aligns the model with human preferences for helpfulness and harmlessness, using a mix of rule-based and neural reward models.</p><ul><li><strong>Reasoning Data</strong>: Use rule-based rewards for math, code, and logic tasks, as in previous stages.</li><li><strong>General Data</strong>: Employ reward models to capture human preferences in complex scenarios (e.g., writing, role-playing), building on DeepSeek-V3’s pipeline .</li><li><strong>Evaluation Focus</strong>: For helpfulness, assess the final summary; for harmlessness, evaluate the entire response (CoT and summary) to mitigate risks .</li></ul></li></ul><p><strong>Key Advantages of Iterative RL Training</strong></p><ul><li><strong>Performance Enhancement</strong>: DeepSeek-R1 achieves comparable results to OpenAI-o1-1217 on reasoning benchmarks (e.g., 79.8% pass@1 on AIME 2024) .</li><li><strong>Readability and Consistency</strong>: Cold-start data and language rewards reduce language mixing and improve output structure .</li><li><strong>Generalization</strong>: SFT with diverse data enables competence in non-reasoning tasks like creative writing and factual QA .</li></ul><p><strong>Performance Benchmarks</strong>
DeepSeek-R1 and distilled models excel on reasoning tasks:</p><ul><li><strong>Math/Coding</strong>: AIME 2024 pass@1 of 79.8% (vs. OpenAI-o1-1217’s 79.2%), MATH-500 pass@1 of 97.3%, and Codeforces rating of 2029 (top 3.7% of human participants).</li><li><strong>Knowledge Tasks</strong>: MMLU score of 90.8%, GPQA Diamond of 71.5%, slightly below o1-1217 but surpassing other closed-source models.</li><li><strong>General Tasks</strong>: Strong performance in creative writing, summarization, and long-context understanding, with win rates of 87.6% on AlpacaEval 2.0 and 92.3% on ArenaHard.</li></ul><h2 id=rloo-reinforce-leave-one-out><strong>RLOO (REINFORCE Leave One-Out)</strong><a hidden class=anchor aria-hidden=true href=#rloo-reinforce-leave-one-out>#</a></h2><p>PPO suffers from high computational costs and sensitive hyperparameter tuning. <cite>RLOO<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup></cite>, as a simpler RL methods, specifically REINFORCE-style optimization, can preserve or even enhance performance while reducing complexity.</p><p><img alt="Image from https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo" loading=lazy src=/images/RLOO.png>
<img alt="Image from https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo" loading=lazy src=/images/RLOO-1.png>
Image from <cite><a href=https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo>https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo</a> <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup></cite></p><p>Key Insights:</p><ol><li>PPO&rsquo;s Limitations in RLHF: PPO was designed for traditional RL environments with high variance and random policy initializations. In RLHF, pre-trained LLMs provide a strong policy initialization, concentrating probability mass on a small subset of tokens. This stability makes PPO&rsquo;s complexity (e.g., clipping, value networks) unnecessary.</li><li>REINFORCE for RLHF: By modeling the entire sequence generation as a single action (vs. PPO&rsquo;s token-level actions), REINFORCE directly optimizes the full trajectory reward with unbiased baselines. This avoids the bias introduced by PPO&rsquo;s bootstrapped value functions.</li><li>REINFORCE Leave-One-Out (RLOO): A multi-sample extension of REINFORCE, RLOO uses online samples to create dynamic baselines, reducing variance without bias. It outperforms PPO and RL-free methods by fully leveraging all generated samples.</li></ol><h3 id=reinforce>REINFORCE<a hidden class=anchor aria-hidden=true href=#reinforce>#</a></h3><p>REINFORCE loss, which applies the vanilla policy gradient to the entire sequence, using a moving average reward as a baseline.It basically multiplies the (reward - baseline) by the logprob of actions.</p><ul><li>Core Idea: In LLM applications, since the reward $r(x, y)$ is only available at the end of the full sequence, REINFORCE models the entire generation as a single action rather than each token. This aligns with the bandit problem formulation, where the Markov Decision Process (MDP) includes only the initial state (prompt) and the terminal state (completed sequence).</li><li>Estimator: It uses the REINFORCE estimator to backpropagate through the discrete action space (generation) and directly optimize the KL-shaped reward objective for the entire sequence. The update rule is
$$
\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(. | x)}\left[R(y, x) \nabla_{\theta} \log \pi_{\theta}(y | x)\right]
\tag{6}
$$
The intuition here is related to the likelihood principle. If an action (generation of \(y\)) leads to a high reward, we want to increase the probability of taking that action in the future, and vice versa. The REINFORCE estimator is used to update the parameters \(\theta\) of the policy \(\pi_{\theta}\). The expectation in the formula combines the reward and the policy gradient. Essentially, it tells us how to adjust the policy parameters \(\theta\) to maximize the expected reward. Mathematically, when we take the expectation of the product of the reward \(R(y, x)\) and the policy gradient \(\nabla_{\theta} \log \pi_{\theta}(y | x)\), we are computing a quantity that, when used to update \(\theta\) (using gradient ascent, for example), will tend to increase the expected reward. If \(R(y, x)\) is positive, the update will push the policy in the direction of increasing the probability of generating \(y\) given \(x\), and if \(R(y, x)\) is negative, it will push the policy away from generating \(y\) given \(x\).</li><li>Baseline: To improve learning, one can reduce the variance of the REINFORCE estimator, while keeping it unbiased, by subtracting a baseline $b$ that has high covariance with the stochastic gradient estimate of the Eq.6:
$$
\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(.|x)} \big[ (R(y, x) - b) \nabla_\theta \log \pi_\theta(y|x) \big]
\tag{7}
$$
The moving average of all rewards throughout training (Williams, 1992) is a strong parameter-free choice for the baseline:
$$
b_{\text{MA}} = \frac{1}{S} \sum_{s} R(x^s, y^s)
\tag{8}
$$
Where $S$ is the number of training steps, and $(x^s, y^s)$ is the prompt-completion pair at the step $s$. This baseline is simple, computationally cheap, and parameter-free.</li></ul><blockquote><p>Noticed that REINFORCE is a special case of PPO. PPO uses <strong>importance sampling</strong> to update the policy without re-collecting data, with a clipped objective to bound policy updates: $\mathcal{L}^{\text{PPO}}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) \cdot A_t, \text{clip}(r_t(\theta), 1-\varepsilon, 1+\varepsilon) \cdot A_t \right) \right]$, where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ is the importance ratio, $A_t$ is the advantage estimate (e.g., from TD or MC), and $\varepsilon$ is the clipping parameter. To reduce PPO’s objective to REINFORCE’s with specific parameters set:</p><ol><li>Removing Clipping ($\varepsilon \to \infty$): When $\varepsilon$ is infinitely large, the $\text{clip}(\cdot)$ operation becomes irrelevant, and the PPO objective simplifies to: $\mathcal{L}(\theta) = \mathbb{E} \left[ r_t(\theta) \cdot A_t \right]$.</li></ol></blockquote><blockquote><ol start=2><li>If we further assume the old policy $\pi_{\theta_{\text{old}}}$ is the same as the current policy $\pi_\theta$ (i.e., no importance sampling, or a single update without old policy), then $r_t(\theta) = 1$, and the objective becomes: $\mathcal{L}(\theta) = \mathbb{E} \left[ A_t \right]$</li></ol></blockquote><blockquote><ol start=3><li>Using Monte Carlo(MC) Returns as Advantage ($A_t = G_t - b(s_t)$): If $A_t$ is defined as the MC return minus a baseline (as in REINFORCE), and the baseline $b(s_t) = 0$ (or ignored), then $A_t = G_t$. Substituting into the objective: $\mathcal{L}(\theta) = \mathbb{E} \left[ G_t \right]$, whose gradient is exactly the REINFORCE update rule without a baseline. If a baseline is included ($b(s_t) \neq 0$), it aligns with REINFORCE with a baseline, which is the standard practice to reduce variance.</li></ol></blockquote><blockquote><p>— <cite>A2C Is a Special Case of PPO<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup></cite></p></blockquote><p>Even though the logprob is explicitly in the REINFORCE loss, it is also implicitly in the PPO loss.</p><h3 id=reinforce-leave-one-out-rloo>REINFORCE Leave-One-Out (RLOO)<a hidden class=anchor aria-hidden=true href=#reinforce-leave-one-out-rloo>#</a></h3><p>Leverages multiple online samples to further reduce variance in the REINFORCE estimator while keeping it unbiased.</p><p>For each prompt, generates k samples and uses the average reward of k-1 samples as a baseline for the remaining one, creating a variance-reduced gradient estimate.</p><p>The baseline in Eq. 8 is simple to implement and computationally cheap. However, it can be improved upon if we have access to multiple online samples, that can be used for further unbiased variance reduction:</p><ol><li>The rewards for each sample can serve all other samples as a baseline.</li><li>Policy updates can be done on an average of gradient estimates for each sample, resulting in a variance-reduced multi-sample Monte-Carlo (MC) estimate.</li></ol><p>This is the intuition behind the REINFORCE Leave-One-Out (RLOO) estimator, proposed by (Kool et al., 2019):</p>$$
\frac{1}{k} \sum_{i=1}^{k} \left[ R(y_{(i)}, x) - \frac{1}{k - 1} \sum_{ j \neq i} R(y_{(j)}, x) \right] \nabla \log \pi(y_{(i)} | x) \text{ for } y_{(1)}, \ldots, y_{(k)} \stackrel{i.i.d}{\sim} \pi_{\theta}(. | x)
$$<p>Where \(k\) refers to the number of online samples generated, \(\text{RLOO}_k\) considers each \( y_{(i)} \) individually and uses the remaining \( k - 1 \) samples to create an unbiased estimate of the expected return for the prompt, akin to a parameter-free value-function, but estimated at each training step.</p><p>This is a much more effective baseline (as the paper&rsquo;s experiments showed) than \( b_{\text{MA}} \) since it&rsquo;s created on-the-fly for each sample and at each training step, but comes at a cost of increased sampling time during training.</p><p>Noted that generating extra samples as a means of variance reduction has been proposed by concurrent work - <cite>Remax <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup></cite>, but RLOO focus on the efficiency benefits of fully utilizing all samples.</p><p>Results:</p><ol><li>Performance: REINFORCE outperforms PPO by 3.2–20.3% in win-rate. RLOO further improves performance, surpassing DPO and RAFT across all datasets.</li><li>Sample Efficiency: RLOO with k=2 matches or exceeds RAFT with k=4, demonstrating better use of online samples.</li><li>Robustness: RLOO is less sensitive to KL penalty and reward noise compared to RAFT, maintaining stable performance under varying conditions.</li><li>Alignment Tax: RLOO preserves language fluency (perplexity) and diversity better than PPO, with lower reward variance—a key factor for safety-critical applications.</li></ol><h2 id=remax>ReMax<a hidden class=anchor aria-hidden=true href=#remax>#</a></h2><p>PPO introduces significant computational overhead for LLMs due to its complex architecture: it requires training a value model, leading to heavy memory usage, tedious hyperparameter tuning, and slow training. To make RLHF efficient, <cite>ReMax <sup id=fnref1:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup></cite> leverages 3 properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. ReMax does not require training an additional value model as in PPO and is further enhanced with a new variance reduction technique.</p><p>Key limitations of PPO for RLHF:</p><ul><li>Value model overhead: Consumes ~46% of GPU memory for a 7B model.</li><li>Hyperparameter complexity: Requires tuning 4+ parameters (e.g., clipping, GAE coefficient).</li><li>Slow convergence: Training with PPO can be 4× slower than earlier RLHF steps.</li></ul><p><img loading=lazy src=/images/ReMax.png></p><h3 id=key-insights-unique-properties-of-rlhf-for-llms>Key Insights: Unique Properties of RLHF for LLMs<a hidden class=anchor aria-hidden=true href=#key-insights-unique-properties-of-rlhf-for-llms>#</a></h3><p>ReMax leverages three properties of RLHF that PPO overlooks:</p><ol><li>Fast simulation: Generating a complete LLM response (trajectory) is rapid (e.g., &lt;10s for 7B models).</li><li>Deterministic transitions: Text generation depends only on past tokens, with no stochastic environment dynamics.</li><li>Trajectory-level rewards: Rewards are given only after the full response, not at each step.</li></ol><h3 id=the-remax-algorithm><strong>The ReMax Algorithm</strong><a hidden class=anchor aria-hidden=true href=#the-remax-algorithm>#</a></h3><p>ReMax is built on the REINFORCE algorithm but introduces a variance reduction technique:</p><ul><li>Greedy baseline: For each prompt, compute the reward of a greedy (deterministic) response and use it to normalize the gradient, reducing variance.<ol><li>For a prompt, sample a stochastic response and a greedy response.</li><li>Compute the reward difference between the two responses.</li></ol></li><li>No value model: Directly optimizes the policy to maximize the log-likelihood of high-reward responses, weighted by the reward difference.</li></ul><p>Pseudo-code Core Steps:</p><pre tabindex=0><code>for prompt in dataset:
    seq = lm.sample(prompt, greedy=False)         # Stochastic response
    seq_max = lm.sample(prompt, greedy=True)      # Greedy response
    rew = rm(prompt, seq) - rm(prompt, seq_max)   # Reward difference
    logp = lm.inference(prompt, seq)              # Log-likelihood
    loss = - (logp.sum() * rew).mean()            # Loss for optimization
    lm.minimize(loss)
</code></pre><h2 id=reinforce-1>REINFORCE++<a hidden class=anchor aria-hidden=true href=#reinforce-1>#</a></h2><p>RLOO and GRPO increase inference costs to trade for eliminating the critic model. The <cite>blog <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup></cite> argues that eliminating the critic model may inadvertently lower training efficiency due to increased inference costs:</p><blockquote><ul><li>When all models (Actor, Critic, Reward, Reference) are similar in scale (e.g., 70B parameters), inference and training consume roughly equal computational resources (1:1 ratio). Eliminating the critic model may actually <strong>reduce training efficiency</strong> due to increased inference costs</li><li>System complexity remains largely unchanged since multiple models still need to operate together</li><li><strong>Performance Analysis:</strong><ul><li>REINFORCE-based methods (e.g., RLOO, ReMax, GRPO) eliminate the critic but struggle with accurate advantage estimation, often overfitting to simple prompts and being vulnerable to reward hacking. These methods estimate advantages per prompt, leading to instability and poor generalization.</li><li>GRPO and RLOO don&rsquo;t provide significant theoretical improvements over PPO</li><li>The claimed advantages (like &ldquo;10x&rdquo; efficiency gains) are often exaggerated</li><li>PPO can address critic model issues by initializing critics with actor weights</li><li><strong>Alternative solution</strong>: Pre-train the critic model by freezing actor weights during PPO training</li></ul></li><li><strong>Technical Issues with GRPO:</strong><ul><li><strong>Numerical instability</strong>: Small differences in sampled rewards can be amplified during normalization</li><li>Example: rewards of 1.001 vs 1.00 become -0.7070 vs 0.7072 after normalization</li><li><strong>Convergence problems</strong>: When all sampled rewards are equal, GRPO provides zero learning signal</li></ul></li><li>Under Process Reward Model (PRM) settings, GRPO essentially becomes REINFORCE with mean baseline</li><li><strong>Bottom Line:</strong>: Both GRPO and RLOO are most beneficial when critic/reward models are significantly larger than actors, but even then, PPO remains a viable alternative with proper initialization strategies. The computational and complexity advantages are often overstated.</li></ul></blockquote><p><cite>REINFORCE++ <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup></cite> is a critic-free RLHF algorithm that uses the <strong>global batch mean reward as a baseline</strong> instead of prompt-specific baselines, preventing overfitting and improving robustness.</p><p><img loading=lazy src=/images/REINFORCE++.png>
<strong>The overall algorithm flow of REINFORCE++</strong>: Sample one response per prompt, compute rewards, normalize advantages, and update the policy using a clipped objective (similar to PPO but without the critic).</p><p><strong>Advantages Normalization</strong>: Normalizes advantages across the entire batch to stabilize training and enhance out-of-distribution (OOD) generalization. REINFORCE++ replaces prompt-specific baselines with the mean reward of a global batch, reducing overfitting to individual prompts. The Advantage is calculated as:</p>$$
A_{q, o_t} = r(o_{1:t}, q) - \beta \cdot \sum_{i=t}^T KL(i), \quad \text{with } KL(t) = \log\left(\frac{\pi_{\theta_{\text{old}}}^{RL}(o_t | q, o_{\lt t})}{\pi^{\text{SFT}}(o_t | q, o_{\lt t})}\right)
$$<p>The token-level KL penalty avoids the need for a critic network while achieving comparable stability. The gradient of the token-level KL penalty has been theoretically proven to be unbiased concerning the $k_3$ loss of GRPO in RLHF.</p><p>The advantage is normalized globally:</p>$$
A_{q, o_t}^{\text{norm}} = \frac{A_{q, o_t} - \text{mean}(A_{q, o_t})}{\text{std}(A_{q, o_t})}
$$<p>Experimental Results:</p><ul><li>Bradley-Terry Reward Model: REINFORCE++ matches or exceeds performance of GRPO, RLOO, and ReMax on OOD benchmarks (e.g., GSM8K, MATH, code generation), with higher per-token efficiency.</li><li>Long CoT Tasks:<ul><li>Small-Scale Datasets: GRPO overfits to training prompts (e.g., AIME-24), achieving near-perfect scores but failing on OOD test sets (AIME-25), while REINFORCE++ shows stable generalization.</li><li>Logical Reasoning (Knights and Knaves): REINFORCE++ outperforms GRPO in complex and OOD scenarios (e.g., 8-character puzzles), with higher Pass@1 scores (36 vs. 20) and longer, more reasoned responses.</li><li>Mathematical Reasoning: From scratch or fine-tuned models, REINFORCE++ demonstrates better OOD generalization on MATH and AIME datasets.</li></ul></li></ul><h2 id=dapo-decoupled-clip-and-dynamic-sampling-policy-optimization><strong>DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization)</strong><a hidden class=anchor aria-hidden=true href=#dapo-decoupled-clip-and-dynamic-sampling-policy-optimization>#</a></h2><p><cite>DAPO <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup></cite> decouples policy clipping and dynamic sampling to enhance training efficiency, reducing variance in gradient estimates.</p><p>DAPO addresses critical challenges in RL training—such as entropy collapse, reward noise, and instability—with four key techniques.</p><ol><li>Clip-Higher: Decouples lower $\varepsilon_{low}$ and higher $\varepsilon_{high}$ clipping ranges in policy optimization to prevent entropy collapse. By increasing $\varepsilon_{high}$, low-probability &ldquo;exploration&rdquo; tokens gain more room for probability increases, enhancing diversity. <img loading=lazy src=/images/DAPO-1.png></li><li>Dynamic Sampling: Over-samples and filters out prompts with all-correct or all-wrong outputs to maintain effective gradient signals. This mitigates gradient-decreasing issues from zero-advantage batches, improving training efficiency. <img loading=lazy src=/images/DAPO-2.png></li><li>Token-Level Policy Gradient Loss: Shifts from sample-level to token-level loss calculation, balancing the influence of long and short responses. This prevents low-quality, overly long generations and stabilizes training. <img loading=lazy src=/images/DAPO-3.png></li><li>Overlong Reward Shaping: Introduces soft punishment for truncated responses to reduce reward noise. Instead of harsh penalties, it uses a length-aware function to guide models toward optimal response lengths. <img loading=lazy src=/images/DAPO-4.png></li></ol><h3 id=dataset-and-implementation>Dataset and Implementation<a hidden class=anchor aria-hidden=true href=#dataset-and-implementation>#</a></h3><p>The DAPO-Math-17K dataset transforms math problems into integer answers for reliable rule-based reward signals. The system is built on the verl framework, with training details including AdamW optimization, group reward normalization, and dynamic sampling hyperparameters.</p><h3 id=experiments-and-results>Experiments and Results<a hidden class=anchor aria-hidden=true href=#experiments-and-results>#</a></h3><ul><li>AIME 2024 Performance: DAPO achieves 50 points on AIME with Qwen2.5-32B, outperforming DeepSeek-R1-Zero-Qwen-32B (47 points) with half the training steps (Figure 1).</li><li>Ablation Study: Each technique contributes significantly: Overlong Filtering (+6), Clip-Higher (+2), Soft Overlong Punishment (+3), Token-Level Loss (+1), and Dynamic Sampling (+8) (Table 1).</li><li>Training Dynamics: Metrics like response length, reward, and entropy show stable improvement, with Clip-Higher specifically combating entropy collapse (Figure 7).</li></ul><h2 id=dr-grpo>Dr. GRPO<a hidden class=anchor aria-hidden=true href=#dr-grpo>#</a></h2><p><cite>Dr. GRPO <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup></cite> claims that:</p><ul><li>DeepSeek-V3-Base already exhibit “Aha moment”, while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases.</li><li>There is an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training.</li><li>Dr. GRPO is an unbiased optimization method that improves token efficiency while maintaining reasoning performance.</li></ul><p><img loading=lazy src=/images/Dr.GRPO.png></p><p>Key Insights on Base Models</p><ol><li><p>Template Impact on Question-Answering Ability</p><ul><li>Base models like Llama and DeepSeek require prompt templates (e.g., R1 template) to elicit question-answering behavior, while Qwen2.5 models excel without templates. This suggests Qwen2.5 might be pretrained on question-answer pairs, acting like supervised fine-tuned (SFT) models even in their base form.</li><li>For example, Qwen2.5-Math-7B achieves 69.0% accuracy on MATH500 without templates, outperforming traditional prompting methods.</li></ul></li><li><p>Preexisting &ldquo;Aha Moment&rdquo; in Base Models</p><ul><li>The &ldquo;Aha moment&rdquo; (self-reflection behaviors) often attributed to RL emergence is already present in base models like DeepSeek-V3-Base. Experiments show these models generate self-reflection keywords (e.g., &ldquo;Aha,&rdquo; &ldquo;wait&rdquo;) in responses to math problems without RL tuning.</li></ul></li></ol><p>Analysis of Reinforcement Learning</p><ol><li><p>Biases in GRPO</p><ul><li>GRPO introduces two key biases:<ul><li>Response-length bias: Dividing by response length ($|o_i|$) penalizes short correct responses and favors longer incorrect ones.</li><li>Question-difficulty bias: Normalizing by reward standard deviation prioritizes easy/hard questions, skewing optimization.</li></ul></li><li>These biases lead to unnecessarily long incorrect responses, as observed in training dynamics.</li></ul></li><li><p>Dr. GRPO: Unbiased Optimization</p><ul><li>The authors propose Dr. GRPO, which removes $|o_i|$ and standard deviation normalization from GRPO. This fixes the biases, improving token efficiency while maintaining reasoning performance.</li><li>Experiments show Dr. GRPO reduces the length of incorrect responses and matches the accuracy of GRPO with fewer tokens.</li></ul></li></ol><h2 id=group-in-group-policy-optimization-gigpo-for-llm-agent-training>Group-in-Group Policy Optimization (GiGPO) for LLM Agent Training<a hidden class=anchor aria-hidden=true href=#group-in-group-policy-optimization-gigpo-for-llm-agent-training>#</a></h2><p><img loading=lazy src=/images/GiGPO.png></p><p><cite>GiGPO<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup></cite> is designed to enhance long-horizon training for LLM agents. Unlike existing group-based RL methods (e.g., GRPO) that struggle with fine-grained credit assignment in multi-step tasks, GiGPO achieves hierarchical advantage estimation while retaining key benefits: being critic-free, memory-efficient, and computationally lightweight.</p><p>Key Limitations of Existing Group-based RL:</p><ul><li>Traditional group-based RL (e.g., GRPO, RLOO) works well for single-turn tasks (e.g., math reasoning) where rewards are immediate but fails in long-horizon scenarios.</li><li>In multi-step environments (e.g., embodied navigation, web shopping), rewards are sparse/delayed, making it hard to assign credit to individual steps.</li><li>Naive extensions of group-based RL collapse step-level distinctions, reducing effectiveness in agent training.</li></ul><p>GiGPO&rsquo;s hierarchical &ldquo;group-in-group&rdquo; design:</p><ol><li><p>Episode-Level Grouping:</p><ul><li>Objective: Capture holistic trajectory quality.</li><li>Method: Samples a group of complete trajectories under identical initial conditions and task descriptions. Computes <strong>macro relative advantages</strong> using total episode returns, normalized by the group&rsquo;s mean and a scaling factor (either standard deviation or 1 for stability).
$$
A^E(\tau_i) = \frac{R(\tau_i) - \text{mean}(\{R(\tau_j)\})}{F_{\text{norm}}(\{R(\tau_j)\})}
$$
where \( R(\tau_i) \) is the total reward of trajectory \( \tau_i \)</li></ul></li><li><p>Step-Level Grouping:</p><ul><li>Objective: Evaluate local step effectiveness.</li><li>Method: Identifies <strong>anchor states</strong> (repeated environment states across trajectories) to form step-level groups. Computes <strong>micro relative advantages</strong> using discounted future rewards for actions taken at these shared states.
$$
A^S(a_t^{(i)}) = \frac{R_t^{(i)} - \text{mean}(\{R_t^{(j)}\})}{F_{\text{norm}}(\{R_t^{(j)}\})}
$$
where \( R_t^{(i)} \) is the discounted return from step \( t \) in trajectory \( i \)</li></ul></li></ol><p>GiGPO Combined Advantage Signal:
The final advantage for each action merges both levels,</p>$$
A(a_t^{(i)}) = A^E(\tau_i) + \omega \cdot A^S(a_t^{(i)})
$$<p>where \( \omega \) balances the two signals.</p><p>GiGPO was evaluated on two challenging benchmarks using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct:</p><table><thead><tr><th>Benchmark</th><th>Improvement Over GRPO</th><th>Key Findings</th></tr></thead><tbody><tr><td>ALFWorld</td><td>12-13% higher success</td><td>Superior performance in embodied household task planning.</td></tr><tr><td>WebShop</td><td>>9% higher success</td><td>Better at goal-driven web navigation and shopping tasks.</td></tr></tbody></table><ul><li>Efficiency: GiGPO matches GRPO&rsquo;s GPU memory usage and rollout costs, with &lt;0.002% additional computation time.</li><li>Ablation Studies: Removing either episode-level or step-level advantages significantly degrades performance, confirming the value of hierarchy.</li></ul><h1 id=iii-optimization-without-reward-model><strong>III. Optimization without Reward Model</strong><a hidden class=anchor aria-hidden=true href=#iii-optimization-without-reward-model>#</a></h1><h2 id=direct-preference-optimization-dpo><strong>Direct Preference Optimization (DPO)</strong><a hidden class=anchor aria-hidden=true href=#direct-preference-optimization-dpo>#</a></h2><p><cite>DPO<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup></cite> is a RL-free training method for LLMs that directly optimizes for human preferences without requiring a separate reward model. DPO eliminates the need for a reward model by directly optimizing a policy using pairwise preference data. It minimizes the KL divergence between the policy and a reference model while maximizing the Bradley-Terry loss.
<img loading=lazy src=/images/DPO.png></p><p>DPO derives a closed-form solution that directly optimizes the policy using preference data. The key insight is that the optimal policy under the RLHF objective can be expressed analytically in terms of the reward function and reference policy.</p><p>The DPO objective is based on the Bradley-Terry preference model, the optimal RLHF policy $π^∗$ under the Bradley-Terry model satisfies the preference model::</p>$$P(y_w \succ y_l | x) = \frac{1}{1 + \exp(\beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} - \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)})}$$<p>Where:</p><ul><li>$y_w$ is the preferred (winning) response</li><li>$y_l$ is the less preferred (losing) response</li><li>$x$ is the input prompt</li><li>$\pi_\theta$ is the policy being optimized</li><li>$\pi_{ref}$ is the reference policy</li><li>$\beta$ is a temperature parameter</li></ul><p>For a static dataset of comparisons $\mathcal{D} = \left\{ x^{(i)}, y_w^{(i)}, y_l^{(i)} \right\}_{i=1}^N$, the reward modeling approach works by defining:</p>$$
\mathcal{L}_R(r_\phi, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( r_\phi(x, y_w) - r_\phi(x, y_l) \right) \right]
$$<p>Analogous to reward modeling approach, DPO&rsquo;s policy objective becomes:</p>$$
\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
$$<p>The gradient with respect to the parameters $\theta$ can be written as:</p>$$
\nabla_\theta \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = - \beta \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \bigg[ \underbrace{\sigma\bigl(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w)\bigr)}_{\text{higher weight when reward estimate is wrong}} \, \bigg[ \underbrace{\nabla_\theta \log \pi(y_w \mid x)}_{\text{increase likelihood of } y_w} - \underbrace{\nabla_\theta \log \pi(y_l \mid x)}_{\text{decrease likelihood of } y_l} \bigg] \bigg]
$$<p>where $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$ is the reward implicitly defined by the language model $\pi_\theta$ and reference model $\pi_{\text{ref}}$.</p><h3 id=a-mechanistic-understanding-of-dpo>A mechanistic understanding of DPO<a hidden class=anchor aria-hidden=true href=#a-mechanistic-understanding-of-dpo>#</a></h3><ul><li>The gradient of the loss function $\mathcal{L}_{\text{DPO}}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$.</li><li>The examples are weighed by how much higher the implicit reward model $\hat{r}_\theta$ rates the dispreferred completions, scaled by $\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint.</li><li>The papers&rsquo;s experiments suggest the importance of this weighting, as a naïve version of this method without the weighting coefficient can cause the language model to degenerate.</li></ul><h3 id=dpos-training-process>DPO&rsquo;s Training Process<a hidden class=anchor aria-hidden=true href=#dpos-training-process>#</a></h3><ol><li><strong>Data Collection</strong>: Gather preference pairs $(x, y_w, y_l)$ where $y_w$ is preferred over $y_l$ for prompt $x$</li><li><strong>Direct Optimization</strong>: Minimize the DPO loss $\mathcal{L}_{\text{DPO}}$</li><li><strong>Regularization</strong>: The KL divergence constraint from RLHF is implicitly maintained through the reference policy terms</li></ol><h3 id=advantages-of-dpo>Advantages of DPO<a hidden class=anchor aria-hidden=true href=#advantages-of-dpo>#</a></h3><ul><li><strong>Simplicity</strong><ul><li>Eliminates the need for reward model training</li><li>Reduces the training pipeline from 3 stages to 2 stages</li><li>Avoids the complexities of reinforcement learning</li></ul></li><li><strong>Stability</strong><ul><li>More stable training compared to PPO-based RLHF</li><li>No issues with reward model overoptimization</li><li>Direct gradient-based optimization</li></ul></li><li><strong>Efficiency</strong><ul><li>Requires less computational resources</li><li>Faster convergence</li><li>Easier hyperparameter tuning</li></ul></li></ul><h3 id=practical-implementation>Practical Implementation<a hidden class=anchor aria-hidden=true href=#practical-implementation>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Simplified DPO loss computation</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>dpo_loss</span><span class=p>(</span><span class=n>policy_chosen_logps</span><span class=p>,</span> <span class=n>policy_rejected_logps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>             <span class=n>reference_chosen_logps</span><span class=p>,</span> <span class=n>reference_rejected_logps</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_logratios</span> <span class=o>=</span> <span class=n>policy_chosen_logps</span> <span class=o>-</span> <span class=n>policy_rejected_logps</span>
</span></span><span class=line><span class=cl>    <span class=n>reference_logratios</span> <span class=o>=</span> <span class=n>reference_chosen_logps</span> <span class=o>-</span> <span class=n>reference_rejected_logps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>policy_logratios</span> <span class=o>-</span> <span class=n>reference_logratios</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>logsigmoid</span><span class=p>(</span><span class=n>logits</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span>
</span></span></code></pre></div><h3 id=limitations-and-considerations>Limitations and Considerations<a hidden class=anchor aria-hidden=true href=#limitations-and-considerations>#</a></h3><ol><li><strong>Data Quality</strong>: Heavily dependent on high-quality preference data</li><li><strong>Distribution Shift</strong>: May struggle with significant shifts from reference policy</li><li><strong>Preference Complexity</strong>: Works best with clear preference distinctions</li><li><strong>Hyperparameter Sensitivity</strong>: The β parameter requires careful tuning</li></ol><p>Comparison with RLHF</p><table><thead><tr><th>Aspect</th><th>RLHF</th><th>DPO</th></tr></thead><tbody><tr><td>Complexity</td><td>High (3 stages)</td><td>Medium (2 stages)</td></tr><tr><td>Stability</td><td>Can be unstable</td><td>More stable</td></tr><tr><td>Computational Cost</td><td>High</td><td>Lower</td></tr><tr><td>Flexibility</td><td>High</td><td>Moderate</td></tr></tbody></table><h2 id=online-dpo---direct-language-model-alignment-from-online-ai-feedback><strong>Online DPO - Direct Language Model Alignment from Online AI Feedback</strong><a hidden class=anchor aria-hidden=true href=#online-dpo---direct-language-model-alignment-from-online-ai-feedback>#</a></h2><p>Direct Alignment from Preferences (DAP) methods like DPO have emerged as efficient alternatives to RLHF, but they rely on pre-collected offline preference data. This leads to two key issues:</p><ol><li><strong>Offline Feedback</strong>: Preferences are static and not updated during training.</li><li><strong>Off-Policy Learning</strong>: Responses in the dataset are generated by a different model, causing distribution shift as the target model evolves.</li></ol><p>The proposed <cite>Online AI Feedback (OAIF) framework<sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup></cite> makes Online DAP methods online and on-policy by:</p><ol><li>Sampling two responses from the current model for each prompt.</li><li>Using an LLM annotator (e.g., PaLM 2) to provide real-time preference feedback by choosing the better response.</li><li>Updating the model using standard DAP losses (DPO, IPO, SLiC) with this online feedback.</li></ol><p>Key advantages:</p><ul><li>Avoids distribution shift by using on-policy generations.</li><li>Eliminates the need for a separate Reward Model (RM), unlike RLHF.</li><li>Enables controllable feedback via prompt engineering for the LLM annotator.</li></ul><p>Experiments and Results</p><ol><li><p><strong>Effectiveness vs. Offline DAP</strong>:</p><ul><li>Online DAP methods (DPO, IPO, SLiC) achieved an average 66% win rate over their offline counterparts in human evaluations.</li><li>Online DPO outperformed SFT baselines, RLHF, and RLAIF 58% of the time on the TL;DR task.</li></ul></li><li><p><strong>Generalization to Other DAP Methods</strong>:</p><ul><li>OAIF improved all three DAP methods, with online SLiC showing a 71.43% win rate over offline SLiC in TL;DR.</li></ul></li><li><p><strong>Comparison with RLHF/RLAIF</strong>:</p><ul><li>Online DPO was preferred 58% of the time in 4-way comparisons (vs. offline DPO, RLAIF, RLHF).</li><li>RLHF relies on static RMs, which struggle as the model evolves, while OAIF&rsquo;s LLM annotator adapts dynamically.</li></ul></li><li><p><strong>Controllability via Prompts</strong>:</p><ul><li>Instructing the LLM annotator to prefer shorter responses reduced average token length from ~120 to ~40, while maintaining quality above SFT baselines.</li></ul></li><li><p><strong>Impact of Annotator Size</strong>:</p><ul><li>Larger annotators (e.g., PaLM 2-L) improved performance, but even smaller annotators (PaLM 2-XS) outperformed RLHF in some cases.</li></ul></li></ol><p>OAIF addresses the offline and off-policy limitations of DAP methods, achieving better alignment with reduced human annotation. The approach paves the way for scalable LLM alignment using AI feedback, with potential for real-time user adaptation and qualitative objective control.</p><table><thead><tr><th>Method</th><th>No RM Needed</th><th>On-Policy</th><th>Online Feedback</th></tr></thead><tbody><tr><td>Offline DAP</td><td>✓</td><td>✗</td><td>✗</td></tr><tr><td>RLHF/RLAIF</td><td>✗</td><td>✓</td><td>✓</td></tr><tr><td>OAIF (Proposed)</td><td>✓</td><td>✓</td><td>✓</td></tr></tbody></table><h2 id=tdpo-token-level-direct-preference-optimization>TDPO: Token-level Direct Preference Optimization<a hidden class=anchor aria-hidden=true href=#tdpo-token-level-direct-preference-optimization>#</a></h2><p>DPO optimize models at the sentence level, evaluating full responses. However, LLMs generate text token-by-token in an auto-regressive manner, which creates a mismatch between evaluation and generation processes. DPO uses KL divergence to constrain models to a reference LLM but struggles with divergence efficiency: the KL divergence of dispreferred responses grows too quickly, limiting diversity. This motivates the need for a more granular, token-level optimization approach.</p><p><cite>TDPO<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup></cite> optimizes token-level preferences to improve sequence generation quality, addressing limitations of instance-level DPO in long-chain reasoning.</p><p><img loading=lazy src=/images/TDPO.png></p><p>TDPO models text generation as an MDP, where:</p><ul><li>State \( s_t = [x, y^{\lt t}] \) (prompt + partial response)</li><li>Action \( a_t = y^t \) (next token)</li><li>Reward \( R_t := R(s_t, a_t) = R\bigl([x, y^{\lt t}], y^t\bigr). \) (token-wise reward)</li></ul><p>The objective function combines the advantage function of a reference model with forward KL divergence:</p>\[
\max_{\pi_\theta} \mathbb{E}\left[ A_{\pi_{ref}}(s_t, z) - \beta D_{KL}(\pi_\theta(\cdot|s_t) \| \pi_{ref}(\cdot|s_t)) \right]
\]<p>where \( A_{\pi_{ref}} \) is the advantage function, and \( \beta \) weights the KL penalty.</p><p>TDPO transforms the sentence-level Bradley-Terry model into a token-level preference model, relating it to the Regret Preference Model. The key is expressing human preference probability as:</p>\[
P_{BT}(y_1 \succ y_2|x) = \sigma(u(x, y_1, y_2) - \delta(x, y_1, y_2))
\]<p>where \( u \) is the reward difference from DPO, represented as,</p>$$
u(x, y_1, y_2) = \beta \log \frac{\pi_\theta(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} - \beta \log \frac{\pi_\theta(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}
$$<p>and \( \delta \) is the weighted SeqKL difference between responses.</p>$$
\delta(x, y_1, y_2) = \beta D_{\text{SeqKL}}\left(x, y_2; \pi_{\text{ref}} \parallel \pi_\theta\right) - \beta D_{\text{SeqKL}}\left(x, y_1; \pi_{\text{ref}} \parallel \pi_\theta\right)
$$<p>Reformulate the Bradley-Terry model into a structure solely relevant to the policy. This formulate a likelihood maximization objective for a parametrized policy $\pi_\theta$, leading to the derivation of the loss function for the initial version of TDPO, $\text{TDPO}_1$:</p>$$
\mathcal{L}_{\text{TDPO}_1}\left( \pi_\theta; \pi_{\text{ref}} \right) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\left( u(x, y_w, y_l) - \delta(x, y_w, y_l) \right) \right]
$$<p>In $\text{TDPO}_1$, \( \delta \) depends on \( \pi_\theta \) (the current policy). During backpropagation, this causes <strong>gradient coupling</strong>: Updates to \( \pi_\theta \) affect both \( u \) and \( \delta \), leading to unstable training (e.g., gradient conflicts or explosions).</p><p>This issue is addressed by <strong>decoupling the gradient flow of \( \delta \)</strong> from \( \pi_\theta \) using a <strong>stop-gradient</strong> operation. Specifically:</p><ul><li>The SeqKL term in \( \delta \) is wrapped with a stop-gradient (e.g., <code>detach()</code> in PyTorch), so its gradient no longer propagates back to \( \pi_\theta \).</li><li>The updated loss function for ${\text{TDPO}_2}$ becomes:
$$
\mathcal{L}_{\text{TDPO}_2}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}} \left[ \log \sigma\left( u(x,y_w,y_l) - \alpha \cdot \text{stop\_grad}\left( \delta(x,y_w,y_l) \right) \right) \right]
$$</li><li>By isolating \( \delta \)’s gradient, TDPO₂ avoids feedback loops between \( u \) and \( \delta \), reducing training oscillations. The SeqKL term acts as a &ldquo;soft regularizer&rdquo; to control token-level divergence, while the main optimization focuses on aligning with human preferences (via \( u \)).</li></ul><p>Define the loss function for $\text{TDPO}_2$ as:</p>$$
\mathcal{L}_{\text{TDPO}_2}\left( \pi_\theta; \pi_{\text{ref}} \right) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\left( u(x, y_w, y_l) - \alpha \delta_2(x, y_w, y_l) \right) \right]
$$<p>where $\alpha$ is a parameter, and</p>$$
\delta_2(x, y_1, y_2) = \beta D_{\text{SeqKL}}\left( x, y_2; \pi_{\text{ref}} \parallel \pi_\theta \right) - sg\left( \beta D_{\text{SeqKL}}\left( x, y_1; \pi_{\text{ref}} \parallel \pi_\theta \right) \right)
$$<p>The $sg$ represents the stop-gradient operator, which blocks the propagation of gradients.</p><p>Implementation from:
<a href=https://github.com/Vance0124/Token-level-Direct-Preference-Optimization/blob/2a736ecb285394a8419b461816bce1ba3b093cc9/trainers.py#L46>https://github.com/Vance0124/Token-level-Direct-Preference-Optimization/blob/2a736ecb285394a8419b461816bce1ba3b093cc9/trainers.py#L46</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>BasicTrainer</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>tdpo_concatenated_forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>reference_model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                  <span class=n>batch</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Union</span><span class=p>[</span><span class=n>List</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]]):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Run the policy model and the reference model on the given batch of inputs, concatenating the chosen and rejected inputs together.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>           We do this to avoid doing two forward passes, because it&#39;s faster for FSDP.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>concatenated_batch</span> <span class=o>=</span> <span class=n>concatenated_inputs</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>all_logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>concatenated_batch</span><span class=p>[</span><span class=s1>&#39;concatenated_input_ids&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                           <span class=n>attention_mask</span><span class=o>=</span><span class=n>concatenated_batch</span><span class=p>[</span><span class=s1>&#39;concatenated_attention_mask&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>logits</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>reference_all_logits</span> <span class=o>=</span> <span class=n>reference_model</span><span class=p>(</span><span class=n>concatenated_batch</span><span class=p>[</span><span class=s1>&#39;concatenated_input_ids&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                                                   <span class=n>attention_mask</span><span class=o>=</span><span class=n>concatenated_batch</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                                                       <span class=s1>&#39;concatenated_attention_mask&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>logits</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>all_logps_margin</span><span class=p>,</span> <span class=n>all_position_kl</span><span class=p>,</span> <span class=n>all_logps</span> <span class=o>=</span> <span class=n>_tdpo_get_batch_logps</span><span class=p>(</span><span class=n>all_logits</span><span class=p>,</span> <span class=n>reference_all_logits</span><span class=p>,</span> <span class=n>concatenated_batch</span><span class=p>[</span><span class=s1>&#39;concatenated_labels&#39;</span><span class=p>],</span> <span class=n>average_log_prob</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>chosen_logps_margin</span> <span class=o>=</span> <span class=n>all_logps_margin</span><span class=p>[:</span><span class=n>batch</span><span class=p>[</span><span class=s1>&#39;chosen_input_ids&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_logps_margin</span> <span class=o>=</span> <span class=n>all_logps_margin</span><span class=p>[</span><span class=n>batch</span><span class=p>[</span><span class=s1>&#39;chosen_input_ids&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]:]</span>
</span></span><span class=line><span class=cl>        <span class=n>chosen_position_kl</span> <span class=o>=</span> <span class=n>all_position_kl</span><span class=p>[:</span><span class=n>batch</span><span class=p>[</span><span class=s1>&#39;chosen_input_ids&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_position_kl</span> <span class=o>=</span> <span class=n>all_position_kl</span><span class=p>[</span><span class=n>batch</span><span class=p>[</span><span class=s1>&#39;chosen_input_ids&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>chosen_logps</span> <span class=o>=</span> <span class=n>all_logps</span><span class=p>[:</span><span class=n>batch</span><span class=p>[</span><span class=s1>&#39;chosen_input_ids&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]]</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_logps</span> <span class=o>=</span> <span class=n>all_logps</span><span class=p>[</span><span class=n>batch</span><span class=p>[</span><span class=s1>&#39;chosen_input_ids&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]:]</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>chosen_logps_margin</span><span class=p>,</span> <span class=n>rejected_logps_margin</span><span class=p>,</span> <span class=n>chosen_position_kl</span><span class=p>,</span> <span class=n>rejected_position_kl</span><span class=p>,</span> \
</span></span><span class=line><span class=cl>            <span class=n>chosen_logps</span><span class=p>,</span> <span class=n>rejected_logps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>tdpo_loss</span><span class=p>(</span><span class=n>chosen_logps_margin</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>rejected_logps_margin</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>chosen_position_kl</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>rejected_position_kl</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>beta</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>alpha</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.5</span><span class=p>,</span> <span class=n>if_tdpo2</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Compute the TDPO loss for a batch of policy and reference model log probabilities.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        chosen_logps_margin: The difference of log probabilities between the policy model and the reference model for the chosen responses. Shape: (batch_size,)
</span></span></span><span class=line><span class=cl><span class=s2>        rejected_logps_margin: The difference of log probabilities between the policy model and the reference model for the rejected responses. Shape: (batch_size,)
</span></span></span><span class=line><span class=cl><span class=s2>        chosen_position_kl: The difference of sequential kl divergence between the policy model and the reference model for the chosen responses. Shape: (batch_size,)
</span></span></span><span class=line><span class=cl><span class=s2>        rejected_position_kl: The difference of sequential kl divergence between the policy model and the reference model for the rejected responses. Shape: (batch_size,)
</span></span></span><span class=line><span class=cl><span class=s2>        beta: Temperature parameter for the TDPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -&gt; 0.
</span></span></span><span class=line><span class=cl><span class=s2>        alpha: Temperature parameter for the TDPO loss, used to adjust the impact of sequential kl divergence.
</span></span></span><span class=line><span class=cl><span class=s2>        if_tdpo2: Determine whether to use method TDPO2, default is True; if False, then use method TDPO1.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        A tuple of two tensors: (losses, rewards).
</span></span></span><span class=line><span class=cl><span class=s2>        The losses tensor contains the TDPO loss for each example in the batch.
</span></span></span><span class=line><span class=cl><span class=s2>        The rewards tensors contain the rewards for response pair.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>chosen_values</span> <span class=o>=</span> <span class=n>chosen_logps_margin</span> <span class=o>+</span> <span class=n>chosen_position_kl</span>
</span></span><span class=line><span class=cl>    <span class=n>rejected_values</span> <span class=o>=</span> <span class=n>rejected_logps_margin</span> <span class=o>+</span> <span class=n>rejected_position_kl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>chosen_rejected_logps_margin</span> <span class=o>=</span> <span class=n>chosen_logps_margin</span> <span class=o>-</span> <span class=n>rejected_logps_margin</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>if_tdpo2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>chosen_rejected_logps_margin</span> <span class=o>-</span> <span class=p>(</span><span class=n>rejected_position_kl</span> <span class=o>-</span> <span class=n>chosen_position_kl</span><span class=p>)</span>  <span class=c1># tdpo1</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>chosen_rejected_logps_margin</span> <span class=o>-</span> <span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=n>rejected_position_kl</span> <span class=o>-</span> <span class=n>chosen_position_kl</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span>  <span class=c1># tdpo2</span>
</span></span><span class=line><span class=cl>    <span class=n>losses</span> <span class=o>=</span> <span class=o>-</span><span class=n>F</span><span class=o>.</span><span class=n>logsigmoid</span><span class=p>(</span><span class=n>beta</span> <span class=o>*</span> <span class=n>logits</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>chosen_rewards</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>chosen_values</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>rejected_rewards</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>rejected_values</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>losses</span><span class=p>,</span> <span class=n>chosen_rewards</span><span class=p>,</span> <span class=n>rejected_rewards</span>
</span></span></code></pre></div><h2 id=trust-region-dpo-tr-dpo>Trust Region DPO (TR-DPO)<a hidden class=anchor aria-hidden=true href=#trust-region-dpo-tr-dpo>#</a></h2><p>Offline alignment methods (e.g., DPO) fine-tune LLMs using pre-constructed datasets without needing explicit reward models. However, they suffer from <strong>overoptimization</strong>: as the trained policy (\(\pi_\theta\)) deviates too far from the fixed reference policy (\(\pi_{ref}\), typically a supervised fine-tuned model), the quality of generated outputs degrades. This is linked to increased probabilities of out-of-domain (OOD) data and a decline in in-domain (ID) data probabilities.</p><p>Overoptimization in offline methods arises due to vanishing curvature in the loss landscape (analyzed via Hessian dynamics), making it hard to reverse declining ID data probabilities. Updating \(\pi_{ref}\) &ldquo;resets&rdquo; the optimization process, restoring curvature and preventing OOD drift.</p><p><cite>TR-DPO<sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup></cite> address the overoptimization via dynamically updating the reference policy (\(\pi_{ref}\)) during training to mitigate overoptimization. This approach, inspired by trust region optimization, ensures the model stays within a &ldquo;trustworthy&rdquo; range of behavior while allowing beneficial divergence from the initial reference. Unlike game-theoretical approaches (which rely on online sampling), TR methods work entirely offline, using pre-existing datasets.</p><p>Two update strategies are introduced:</p><ol><li><p><strong>Soft Update</strong>: Gradually merges the current policy into the reference policy using a weighted average:<br></p>$$\pi_{ref} \leftarrow \alpha \pi_\theta + (1-\alpha) \pi_{ref_{prev}}$$<p><br>where \(\alpha\) controls the update rate, and stop-gradient (\(sg\)) prevents backpropagating through \(\pi_{ref}\)</p></li><li><p><strong>Hard Update</strong>: Periodically replaces the reference policy with the current policy after a fixed number of steps (\(\tau\)):<br></p>$$\pi_{ref} \leftarrow \pi_\theta$$</li></ol><p><img loading=lazy src=/images/TR-DPO-1.png></p><p>These strategies are applied to existing methods, creating TR-DPO, TR-IPO, and TR-KTO.</p><p>The authors evaluate TR methods on both task-specific and general benchmarks, using models like Pythia (2.8B–12B) and Llama3 (8B):</p><ol><li><p><strong>Task-Specific Tasks</strong>:</p><ul><li>On Anthropic-HH (helpful/harmless dialogue) and Reddit TL;DR (summarization), TR methods outperformed vanilla DPO/IPO/KTO. For example, TR-DPO with \(\alpha=0.6\) or \(\tau=512\) achieved 8.4–15% higher win rates.</li></ul></li><li><p><strong>General Benchmarks</strong>:</p><ul><li>On AlpacaEval 2 and Arena-Hard, TR methods showed significant gains. TR-IPO with hard updates improved win rates by 15.1 points on Arena-Hard, while TR-DPO improved by 9.5 points.</li></ul></li><li><p><strong>Overoptimization Mitigation</strong>:</p><ul><li>At equivalent KL divergence (distance from the initial reference), TR methods maintained higher human-centric (HC) metrics (coherence, helpfulness, etc.) compared to vanilla methods, indicating reduced overoptimization.</li></ul></li></ol><p><img loading=lazy src=/images/TR-DPO-2.png></p><p>TR alignment methods (TR-DPO, TR-IPO, TR-KTO) effectively reduce overoptimization by dynamically updating the reference policy. They outperform traditional offline methods across tasks and model sizes, enabling LLMs to diverge beneficially from initial references while maintaining high quality. Future work will explore broader applicability and adaptive update strategies.</p><h2 id=step-dpo-step-wise-preference-optimization-for-long-chain-reasoning>Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning<a hidden class=anchor aria-hidden=true href=#step-dpo-step-wise-preference-optimization-for-long-chain-reasoning>#</a></h2><p>LLMs struggle with long-chain mathematical reasoning due to the need for precise step-by-step correctness. Traditional DPO fails to improve such reasoning effectively because it evaluates entire answers rather than individual steps, making it hard to identify subtle errors in intermediate reasoning.</p><p>Step-DPO treats each reasoning step as the unit for preference optimization (instead of holistic answers in DPO). By focusing on the first erroneous step in a chain, Step-DPO provides fine-grained supervision, enabling LLMs to locate and correct mistakes more accurately.</p><p><strong>Paper</strong>: Lai, Xin, et al. Step-DPO: Step-Wise Preference Optimization for Long-Chain Reasoning of LLMs. arXiv:2406.18629, arXiv, 26 June 2024. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2406.18629>https://doi.org/10.48550/arXiv.2406.18629</a>.</p><p>Specifically, the answer \( y \) can be decomposed into a sequence of reasoning steps \( y = s_1, \ldots, s_n \), where \( s_i \) is the \( i \)-th reasoning step.</p><p>Given a prompt \( x \) and a series of initial correct reasoning steps \( s_{1\sim k-1} = s_1, \ldots, s_{k-1} \), Step-DPO aims to maximize the probability of the correct next reasoning step \( s_{\textit{win}} \) and minimize the probability of the incorrect one \( s_{\textit{lose}} \). This objective can be formulated as:</p>$$
\mathcal{L}(\theta) = -\mathbb{E}_{(x, s_{1\sim k-1}, s_{\textit{win}}, s_{\textit{lose}}) \sim D} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(s_{\textit{win}} \mid x; s_{1\sim k-1})}{\pi_{\textit{ref}}(s_{\textit{win}} \mid x; s_{1\sim k-1})} - \beta \log \frac{\pi_\theta(s_{\textit{lose}} \mid x; s_{1\sim k-1})}{\pi_{\textit{ref}}(s_{\textit{lose}} \mid x; s_{1\sim k-1})} \right) \right]
$$<p><img loading=lazy src=/images/Step-DPO.png></p><p>The authors propose a 3-step pipeline to build high-quality step-wise preference data (10K pairs):</p><ol><li>Error Collection: Use a reference model to generate incorrect answers for math problems, retaining cases where the final answer differs from the ground truth.</li><li>Step Localization: Identify the first erroneous step in each incorrect answer manually or with GPT-4.</li><li>Rectification: Generate correct next steps by sampling from the reference model given the initial correct steps, ensuring in-distribution data (self-generated) over out-of-distribution data (human/GPT-4-generated), which proves more effective.</li></ol><p>Performance:</p><ul><li>Step-DPO achieves up to 3% accuracy gain on MATH with as few as 10K data pairs and &lt;500 training steps for 70B+ parameter models.</li><li>Qwen2-72B-Instruct + Step-DPO reaches 70.8% on MATH and 94.0% on GSM8K, outperforming closed-source models like GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro.</li></ul><p>Ablation Studies:</p><ul><li>Step-DPO outperforms DPO by 1.8–2.6% on MATH.</li><li>In-distribution data yields 0.7% higher accuracy than out-of-distribution data.</li></ul><h2 id=identity-preference-optimization-ipo><strong>Identity Preference Optimization (IPO)</strong><a hidden class=anchor aria-hidden=true href=#identity-preference-optimization-ipo>#</a></h2><p>RLHF and DPO can overfit deterministic preferences because the logit transformation (Ψ) amplifies small preference differences near 1, weakening KL regularization. This leads to policies deviating from the reference policy, even with large regularization parameters.</p><p>ΨPO is defined as maximizing a non-linear function of preference probabilities (Ψ) balanced by KL regularization to a reference policy. RLHF and DPO are shown to be special cases when Ψ is the logit function, relying on the Bradley-Terry model.</p><p>Identity-PO (IPO) is an approach setting Ψ to the identity function, bypassing the Bradley-Terry model. IPO optimizes total preferences directly, maintaining effective regularization even with deterministic preferences. An empirical sampled loss is derived for practical implementation, avoiding reward modeling.</p><p><strong>Paper</strong>: <a href=https://arxiv.org/abs/2310.12036>A General Theoretical Paradigm to Understand Learning from Human Preferences</a>.</p><p>IPO Loss function:</p>$$
\mathbb{E}_{(y_w, y_l) \sim D} \left[ \left( h_\pi(y_w, y_l) - \frac{\tau^{-1}}{2} \right)^2 \right]
$$<p>IPO learns from preferences dataset simply by regressing the gap between log-likelihood ratios $\log(\pi(y_w)/\pi(y_l))$ and $\log(\pi_{\text{ref}}(y_w)/\pi_{\text{ref}}(y_l))$ to $\frac{\tau^{-1}}{2}$.</p><p>So the weaker the regularisation becomes, the higher would be the log-likelihood ratio of $y_w$ to $y_l$.</p><p>In other words IPO, unlike DPO, always regularizes its solution towards $\pi_{\text{ref}}$ by controlling the gap between the log-likelihood ratios $\log(\pi(y_w)/\pi(y_l))$ and $\log(\pi_{\text{ref}}(y_w)/\pi_{\text{ref}}(y_l))$, thus avoiding the over-fitting to the preference dataset.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ... calculate logits the same as DPO</span>
</span></span><span class=line><span class=cl><span class=c1># https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/dpo_trainer.py#L974</span>
</span></span><span class=line><span class=cl><span class=c1># eqn (17) of the paper where beta is the regularization parameter for the IPO loss, denoted by tau in the paper.</span>
</span></span><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=p>(</span><span class=n>logits</span> <span class=o>-</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span><span class=p>))</span> <span class=o>**</span> <span class=mi>2</span>
</span></span></code></pre></div><h2 id=sppo---self-play-preference-optimization-for-language-model-alignment>SPPO - Self-Play Preference Optimization for Language Model Alignment<a hidden class=anchor aria-hidden=true href=#sppo---self-play-preference-optimization-for-language-model-alignment>#</a></h2><p><strong>Paper</strong>: Wu, Yue, et al. Self-Play Preference Optimization for Language Model Alignment. arXiv:2405.00675, arXiv, 4 Oct. 2024. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2405.00675>https://doi.org/10.48550/arXiv.2405.00675</a>.</p><h2 id=dmpo-direct-multi-turn-preference-optimization><strong>DMPO: Direct Multi-Turn Preference Optimization</strong><a hidden class=anchor aria-hidden=true href=#dmpo-direct-multi-turn-preference-optimization>#</a></h2><p>Extends DPO to multi-turn dialogue by optimizing preferences across conversation history, improving coherence and relevance in multi-step interactions.</p><p><strong>Paper</strong>: Shi, Wentao, et al. Direct Multi-Turn Preference Optimization for Language Agents. arXiv:2406.14868, arXiv, 23 Feb. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2406.14868>https://doi.org/10.48550/arXiv.2406.14868</a>.</p><h2 id=wpo---enhancing-rlhf-with-weighted-preference-optimization>WPO - Enhancing RLHF with Weighted Preference Optimization<a hidden class=anchor aria-hidden=true href=#wpo---enhancing-rlhf-with-weighted-preference-optimization>#</a></h2><h1 id=iv-reference-free-optimization><strong>IV. Reference-Free Optimization</strong><a hidden class=anchor aria-hidden=true href=#iv-reference-free-optimization>#</a></h1><h2 id=simpo-simple-preference-optimization-with-a-reference-free-reward><strong>SimPO: Simple Preference Optimization with a Reference-Free Reward</strong><a hidden class=anchor aria-hidden=true href=#simpo-simple-preference-optimization-with-a-reference-free-reward>#</a></h2><p>SimPO aligns the reward function with the generation metric, eliminates the need for a reference model, and introduces a target reward margin to enhance performance.</p><p><img loading=lazy src=/images/SimPO.png></p><p>In DPO, for any triple $(x, y_w, y_l)$, satisfying the reward ranking $r(x, y_w) > r(x, y_l)$ does not necessarily gaurantee the likelihood ranking $p_\theta(y_w \mid x) > p_\theta(y_l \mid x)$. The paper found that only roughly 50% of the triples from a held-out set satisfy this condition when trained with DPO.</p><p><strong>Paper</strong>: <a href=https://arxiv.org/abs/2405.14734>SimPO: Simple Preference Optimization with a Reference-Free Reward</a>.</p><p><strong>Length-Normalized Reward formulation</strong>:
Replacing the reward formulation in DPO with $p_\theta$ in the average log likelihood</p>$$
p_\theta(y \mid x) = \frac{1}{|y|} \log \pi_\theta(y \mid x) = \frac{1}{|y|} \sum_{i=1}^{|y|} \log \pi_\theta(y_i \mid x, y_{\lt i}).
$$<p>so that it aligns with the likelihood metric that guides generation, results in a length-normalized reward:</p>$$
r_{\text{SimPO}}(x, y) = \frac{\beta}{|y|} \log \pi_\theta(y \mid x) = \frac{\beta}{|y|} \sum_{i=1}^{|y|} \log \pi_\theta(y_i \mid x, y_{\lt i})
$$<p>where $\beta$ is a constant that controls the scaling of the reward difference.</p><p>This reward eliminates the need for a reference model, enhancing memory and computational efficiency.</p><p>Removing the length normalization term from the reward formulation results in a bias toward generating longer but lower-quality sequences.</p><p><strong>Target Reward Margin</strong>:
SimPO incorporates a target margin $\gamma$ into the Bradley-Terry objective to ensure the reward difference between winning ($y_w$) and losing ($y_l$) responses exceeds $\gamma$:</p>$$
\mathcal{L}_{\text{SimPO}} = -\mathbb{E}\left[\log \sigma\left(\frac{\beta}{|y_w|}\log\pi_{\theta}(y_w|x) - \frac{\beta}{|y_l|}\log\pi_{\theta}(y_l|x) - \gamma\right)\right]
$$<p>This margin enhances the model’s ability to distinguish between high-quality and low-quality responses, improving generalization.</p><p><strong>Implementation</strong>:
<a href=https://docs.pytorch.org/torchtune/0.3/_modules/torchtune/rlhf/loss/dpo.html#SimPOLoss.forward>https://docs.pytorch.org/torchtune/0.3/_modules/torchtune/rlhf/loss/dpo.html#SimPOLoss.forward</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>simpo_loss</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_chosen_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_rejected_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>simpo_gamma</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>beta</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=p>(</span><span class=n>policy_chosen_logps</span> <span class=o>-</span> <span class=n>policy_rejected_logps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>gamma_logratios</span> <span class=o>=</span> <span class=n>simpo_gamma</span> <span class=o>/</span> <span class=n>beta</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span> <span class=o>-</span> <span class=n>gamma_logratios</span>
</span></span><span class=line><span class=cl>    <span class=n>losses</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=o>-</span><span class=n>F</span><span class=o>.</span><span class=n>logsigmoid</span><span class=p>(</span><span class=n>beta</span> <span class=o>*</span> <span class=n>logits</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>label_smoothing</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>-</span> <span class=n>F</span><span class=o>.</span><span class=n>logsigmoid</span><span class=p>(</span><span class=n>beta</span> <span class=o>*</span> <span class=n>logits</span><span class=p>)</span> <span class=o>*</span> <span class=n>label_smoothing</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>losses</span>
</span></span></code></pre></div><h2 id=odds-ratio-preference-optimization-orpo><strong>Odds Ratio Preference Optimization (ORPO)</strong><a hidden class=anchor aria-hidden=true href=#odds-ratio-preference-optimization-orpo>#</a></h2><p>ORPO (Odds Ratio Preference Optimization) combines SFT and preference optimization in a single monolithic training stage, eliminating reference model requirements. This approach streamlines training while maintaining competitive performance across multiple benchmarks.</p><p><strong>Paper</strong>: Hong, S., et al. (2024). <a href=https://arxiv.org/abs/2403.07691>ORPO: Monolithic Preference Optimization without Reference Model</a>.</p><p>Existing methods like RLHF and DPO often require:</p><ul><li>A supervised fine-tuning (SFT) warm-up stage.</li><li>A reference model for comparison.</li><li>Complex multi-stage processes, which are resource-intensive.</li></ul><p>The core insight: SFT can be enhanced to directly incorporate preference alignment by penalizing undesired generation styles, eliminating the need for extra stages or reference models.</p><h3 id=orpo-algorithm><strong>ORPO Algorithm</strong><a hidden class=anchor aria-hidden=true href=#orpo-algorithm>#</a></h3><p><img loading=lazy src=/images/ORPO.png></p><p>ORPO integrates preference alignment into SFT using an <strong>odds ratio</strong> to contrast favored (\(y_w\)) and disfavored (\(y_l\)) responses. The key components are:</p><ul><li><strong>Objective Function</strong>: \(\mathcal{L}_{ORPO} = \mathcal{L}_{SFT} + \lambda \cdot \mathcal{L}_{OR}\), where:<ul><li>\(\mathcal{L}_{SFT}\) is the standard negative log-likelihood (NLL) loss for SFT.</li><li>\(\mathcal{L}_{OR}\) is a new loss term that maximizes the odds ratio between \(y_w\) and \(y_l\), defined as:
\[
\mathcal{L}_{OR} = -\log \sigma\left(\log \frac{odds_\theta(y_w|x)}{odds_\theta(y_l|x)}\right)
\]
where \(odds_\theta(y|x) = \frac{P_\theta(y|x)}{1-P_\theta(y|x)}\) and \(\sigma\) is the sigmoid function.</li></ul></li><li><strong>Gradient Analysis</strong>: The gradient of \(\mathcal{L}_{OR}\) dynamically penalizes disfavored responses, accelerating adaptation to desired styles while preserving domain knowledge from SFT.</li></ul><p>Implementation from <a href=https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/orpo_trainer.py#L623>https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/orpo_trainer.py#L623</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>odds_ratio_loss</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_chosen_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_rejected_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Compute ORPO&#39;s odds ratio (OR) loss for a batch of policy and reference model log probabilities.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)
</span></span></span><span class=line><span class=cl><span class=s2>            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).
</span></span></span><span class=line><span class=cl><span class=s2>            The losses tensor contains the ORPO loss for each example in the batch.
</span></span></span><span class=line><span class=cl><span class=s2>            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.
</span></span></span><span class=line><span class=cl><span class=s2>            The log odds ratio of the chosen responses over the rejected responses ratio for logging purposes.
</span></span></span><span class=line><span class=cl><span class=s2>            The `log(sigmoid(log_odds_chosen))` for logging purposes.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Derived from Eqs. (4) and (7) from https://huggingface.co/papers/2403.07691 by using log identities and exp(log(P(y|x)) = P(y|x)</span>
</span></span><span class=line><span class=cl>        <span class=n>log_odds</span> <span class=o>=</span> <span class=p>(</span><span class=n>policy_chosen_logps</span> <span class=o>-</span> <span class=n>policy_rejected_logps</span><span class=p>)</span> <span class=o>-</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>log1p</span><span class=p>(</span><span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>policy_chosen_logps</span><span class=p>))</span> <span class=o>-</span> <span class=n>torch</span><span class=o>.</span><span class=n>log1p</span><span class=p>(</span><span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>policy_rejected_logps</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ratio</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>logsigmoid</span><span class=p>(</span><span class=n>log_odds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>losses</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>ratio</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>chosen_rewards</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>policy_chosen_logps</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>accelerator</span><span class=o>.</span><span class=n>device</span><span class=p>))</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_rewards</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>policy_rejected_logps</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>accelerator</span><span class=o>.</span><span class=n>device</span><span class=p>))</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>losses</span><span class=p>,</span> <span class=n>chosen_rewards</span><span class=p>,</span> <span class=n>rejected_rewards</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>ratio</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>log_odds</span><span class=p>)</span>
</span></span></code></pre></div><p>Advantages over Existing Methods**</p><ul><li>Monolithic Design: ORPO performs preference alignment in a single stage during SFT, unlike RLHF/DPO’s multi-stage workflows.</li><li>No Reference Model: Avoids the need for a frozen SFT model, reducing memory usage and computational cost (half the forward passes per batch).</li><li>Stability with Odds Ratio: Compared to probability ratios, the odds ratio provides a milder penalty, preventing over-suppression of disfavored responses (Figure 6).</li></ul><h1 id=v-non-preference-based-methods><strong>V. Non-preference-based methods</strong><a hidden class=anchor aria-hidden=true href=#v-non-preference-based-methods>#</a></h1><h2 id=kto---model-alignment-as-prospect-theoretic-optimization><strong>KTO - Model Alignment as Prospect Theoretic Optimization</strong><a hidden class=anchor aria-hidden=true href=#kto---model-alignment-as-prospect-theoretic-optimization>#</a></h2><p>The paper introduces <strong>Kahneman-Tversky Optimization (KTO)</strong>, a approach maximizes the utility of generations instead of maximizing the log-likelihood of preferences(as DPO does).</p><p>Theoretically, KTO align LLM with human preferences by framing alignment as a problem of prospect theoretic optimization. Drawing from Kahneman and Tversky’s prospect theory, which describes how humans make decisions under uncertainty with biases like loss aversion, the authors argue that existing alignment methods (e.g., DPO, PPO) implicitly incorporate these biases. They formalize these as <strong>Human-Aware Losses (HALOs)</strong> and propose KTO as a principled HALO that directly maximizes human utility rather than the log-likelihood of preferences.</p><p><strong>Paper</strong>: <a href=https://arxiv.org/abs/2402.01306>KTO: Model Alignment as Prospect Theoretic Optimization</a>.</p><p><strong>KTO Method</strong>:</p><ul><li>Derives a HALO using Kahneman-Tversky’s value function, replacing exponents with logistic functions for stability.</li><li>Requires only binary feedback (desirable/undesirable) instead of preference pairs, making data collection cheaper and more scalable.</li><li>Introduces hyperparameters β (risk aversion) and λ_D/λ_U (loss aversion for desirable/undesirable outputs) to model human decision biases.</li></ul><p><strong>Prospect Theory in Alignment</strong>:</p><ul><li><strong>Value Function</strong>: Models human utility relative to a reference point, with concavity in gains and steeper slopes in losses (loss aversion).</li><li><strong>Reference Point</strong>: In KTO, the reference is the KL divergence between the current policy and the reference model, estimated via microbatch mismatches for stability.</li></ul><p><strong>Loss Function</strong>:</p><p>$\lambda_y$ denotes $\lambda_D$ ($\lambda_U$) when $y$ is desirable(undesirable) respectively, the default KTO loss is:</p>$$
\mathcal{L}_{\text{KTO}}(\pi_\theta, \pi_{\text{ref}}) \triangleq \mathbb{E}_{x,y \sim D} \left[ \lambda_y - v(x,y) \right] \tag{8}
$$<p>where</p>$$
r_\theta(x,y) = \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)},
$$<p></p>$$
z_0 = \text{KL}\bigl(\pi_\theta(y'|x) \big\| \pi_{\text{ref}}(y'|x)\bigr),
$$<p></p>$$
v(x,y) =
\begin{cases}
\lambda_D \sigma\bigl(\beta(r_\theta(x,y) - z_0)\bigr) & \text{if } y \sim y_{\text{desirable}}|x, \\
\lambda_U \sigma\bigl(\beta(z_0 - r_\theta(x,y))\bigr) & \text{if } y \sim y_{\text{undesirable}}|x.
\end{cases}
$$<p>Implementation from <a href=https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/kto_trainer.py#L1090>https://github.com/huggingface/trl/blob/2c49300910e55fd7482ad80019feee4cdaaf272c/trl/trainer/kto_trainer.py#L1090</a>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>kto_loss</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_chosen_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_rejected_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_KL_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>reference_chosen_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>reference_rejected_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>reference_KL_logps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>kl</span> <span class=o>=</span> <span class=p>(</span><span class=n>policy_KL_logps</span> <span class=o>-</span> <span class=n>reference_KL_logps</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1># Chosen losses</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>policy_chosen_logps</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>!=</span> <span class=mi>0</span> <span class=ow>or</span> <span class=n>reference_chosen_logps</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>chosen_logratios</span> <span class=o>=</span> <span class=n>policy_chosen_logps</span> <span class=o>-</span> <span class=n>reference_chosen_logps</span>
</span></span><span class=line><span class=cl>        <span class=n>chosen_losses</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>F</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>chosen_logratios</span> <span class=o>-</span> <span class=n>kl</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>chosen_rewards</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>chosen_logratios</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>chosen_losses</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>        <span class=n>chosen_rewards</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Rejected losses</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>policy_rejected_logps</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>!=</span> <span class=mi>0</span> <span class=ow>or</span> <span class=n>reference_rejected_logps</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_logratios</span> <span class=o>=</span> <span class=n>policy_rejected_logps</span> <span class=o>-</span> <span class=n>reference_rejected_logps</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_losses</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>F</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>kl</span> <span class=o>-</span> <span class=n>rejected_logratios</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_rewards</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>rejected_logratios</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_losses</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_rewards</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>losses</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=n>chosen_losses</span><span class=p>,</span> <span class=n>rejected_losses</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>losses</span>
</span></span></code></pre></div><h1 id=vi-others><strong>VI. Others</strong><a hidden class=anchor aria-hidden=true href=#vi-others>#</a></h1><h2 id=spo---a-minimaximalist-approach-to-reinforcement-learning-from-human-feedback><strong>SPO - A Minimaximalist Approach to Reinforcement Learning from Human Feedback</strong><a hidden class=anchor aria-hidden=true href=#spo---a-minimaximalist-approach-to-reinforcement-learning-from-human-feedback>#</a></h2><p>Uses self-play to generate preference data, where models compete to maximize rewards. It employs a minimax approach to balance exploration and exploitation.
<strong>Paper</strong>: OpenAI. (2024). <a href=https://arxiv.org/abs/2401.04056>A Minimaximalist Approach to Reinforcement Learning from Human Feedback</a>.</p><h1 id=vii-key-trends-and-challenges><strong>VII. Key Trends and Challenges</strong><a hidden class=anchor aria-hidden=true href=#vii-key-trends-and-challenges>#</a></h1><p>[]: <a href=https://github.com/volcengine/verl>https://github.com/volcengine/verl</a></p><ol><li><strong>Scalability</strong>: Methods like RLHF and DPO face challenges with large models (e.g., 405B parameters) due to memory and computational costs. Frameworks like Verl address this via asynchronous distributed training .</li><li><strong>Data Efficiency</strong>: SimPO and ORPO reduce reliance on reference models and pairwise data, while Step-DPO and KTO focus on fine-grained optimization .</li><li><strong>Generalization</strong>: SPO and DMPO aim to improve generalization across tasks (e.g., dialogue, math) by leveraging self-play and multi-turn optimization .</li><li><strong>Theoretical Foundations</strong>: KTO and SimPO provide theoretical insights into preference learning, linking RL to prospect theory and information theory .</li></ol><h1 id=viii-future-directions><strong>VIII. Future Directions</strong><a hidden class=anchor aria-hidden=true href=#viii-future-directions>#</a></h1><ul><li><strong>Multi-Modality</strong>: IPO and DMPO highlight the need for alignment across text, video, and dialogue.</li><li><strong>Self-Supervised Learning</strong>: RLAIF and SPO explore AI-generated feedback to reduce human labeling.</li><li><strong>Efficiency</strong>:</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Schulman, John, et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, arXiv, 28 Aug. 2017. arXiv.org, <a href=http://arxiv.org/abs/1707.06347>http://arxiv.org/abs/1707.06347</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Ouyang, Long, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155, arXiv, 4 Mar. 2022. arXiv.org, <a href=http://arxiv.org/abs/2203.02155>http://arxiv.org/abs/2203.02155</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Bai, Yuntao, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, arXiv, 15 Dec. 2022. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2212.08073>https://doi.org/10.48550/arXiv.2212.08073</a>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Lee, Harrison, et al. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. arXiv:2309.00267, arXiv, 3 Sept. 2024. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2309.00267>https://doi.org/10.48550/arXiv.2309.00267</a>.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Shao, Zhihong, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, arXiv, 27 Apr. 2024. arXiv.org, <a href=http://arxiv.org/abs/2402.03300>http://arxiv.org/abs/2402.03300</a>.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>J. Schulman. Approximating kl divergence, 2020. URL <a href=http://joschu.net/blog/kl-approx.html>http://joschu.net/blog/kl-approx.html</a>.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2501.12948>https://doi.org/10.48550/arXiv.2501.12948</a>.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Ahmadian, Arash, et al. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs. arXiv:2402.14740, arXiv, 26 Feb. 2024. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2402.14740>https://doi.org/10.48550/arXiv.2402.14740</a>.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p><a href=https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo>https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Huang, Shengyi, et al. A2C Is a Special Case of PPO. arXiv:2205.09123, arXiv, 18 May 2022. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2205.09123>https://doi.org/10.48550/arXiv.2205.09123</a>.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Li, Ziniu, et al. ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. arXiv:2310.10505, arXiv, 16 May 2024. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2310.10505>https://doi.org/10.48550/arXiv.2310.10505</a>.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p><a href=https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights>https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights</a>&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Hu, Jian, et al. REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models. arXiv:2501.03262, arXiv, 6 Apr. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2501.03262>https://doi.org/10.48550/arXiv.2501.03262</a>.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Yu, Qiying, et al. DAPO: An Open-Source LLM Reinforcement Learning System at Scale. arXiv:2503.14476, arXiv, 20 May 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2503.14476>https://doi.org/10.48550/arXiv.2503.14476</a>.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Liu, Zichen, et al. Understanding R1-Zero-Like Training: A Critical Perspective. arXiv:2503.20783, arXiv, 26 Mar. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2503.20783>https://doi.org/10.48550/arXiv.2503.20783</a>.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Feng, Lang, et al. Group-in-Group Policy Optimization for LLM Agent Training. arXiv:2505.10978, arXiv, 16 May 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2505.10978>https://doi.org/10.48550/arXiv.2505.10978</a>.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p><a href=https://arxiv.org/abs/2305.18290>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Guo, Shangmin, et al. Direct Language Model Alignment from Online AI Feedback. arXiv:2402.04792, arXiv, 29 Feb. 2024. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2402.04792>https://doi.org/10.48550/arXiv.2402.04792</a>.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p><a href=https://arxiv.org/abs/2404.11999>Token-level Direct Preference Optimization</a>.&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Gorbatovski, Alexey, et al. Learn Your Reference Model for Real Good Alignment. arXiv:2404.09656, arXiv, 25 Feb. 2025. arXiv.org, <a href=https://doi.org/10.48550/arXiv.2404.09656>https://doi.org/10.48550/arXiv.2404.09656</a>.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://congchan.github.io/tags/2025/>2025</a></li><li><a href=https://congchan.github.io/tags/large-language-model/>Large Language Model</a></li><li><a href=https://congchan.github.io/tags/post-training/>Post-Training</a></li></ul><nav class=paginav><a class=next href=https://congchan.github.io/posts/multi-token-prediction/><span class=title>Next »</span><br><span>Multi-token Prediction</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Awesome Large Language Model (LLM) Post-training - [2025 Update] on x" href="https://x.com/intent/tweet/?text=Awesome%20Large%20Language%20Model%20%28LLM%29%20Post-training%20-%20%5b2025%20Update%5d&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fawesome-large-language-model-llm-post-training-2025-update%2f&amp;hashtags=2025%2cLargeLanguageModel%2cPost-training"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Awesome Large Language Model (LLM) Post-training - [2025 Update] on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fawesome-large-language-model-llm-post-training-2025-update%2f&amp;title=Awesome%20Large%20Language%20Model%20%28LLM%29%20Post-training%20-%20%5b2025%20Update%5d&amp;summary=Awesome%20Large%20Language%20Model%20%28LLM%29%20Post-training%20-%20%5b2025%20Update%5d&amp;source=https%3a%2f%2fcongchan.github.io%2fposts%2fawesome-large-language-model-llm-post-training-2025-update%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Awesome Large Language Model (LLM) Post-training - [2025 Update] on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcongchan.github.io%2fposts%2fawesome-large-language-model-llm-post-training-2025-update%2f&title=Awesome%20Large%20Language%20Model%20%28LLM%29%20Post-training%20-%20%5b2025%20Update%5d"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Awesome Large Language Model (LLM) Post-training - [2025 Update] on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcongchan.github.io%2fposts%2fawesome-large-language-model-llm-post-training-2025-update%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Awesome Large Language Model (LLM) Post-training - [2025 Update] on whatsapp" href="https://api.whatsapp.com/send?text=Awesome%20Large%20Language%20Model%20%28LLM%29%20Post-training%20-%20%5b2025%20Update%5d%20-%20https%3a%2f%2fcongchan.github.io%2fposts%2fawesome-large-language-model-llm-post-training-2025-update%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Awesome Large Language Model (LLM) Post-training - [2025 Update] on telegram" href="https://telegram.me/share/url?text=Awesome%20Large%20Language%20Model%20%28LLM%29%20Post-training%20-%20%5b2025%20Update%5d&amp;url=https%3a%2f%2fcongchan.github.io%2fposts%2fawesome-large-language-model-llm-post-training-2025-update%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Awesome Large Language Model (LLM) Post-training - [2025 Update] on ycombinator" href="https://news.ycombinator.com/submitlink?t=Awesome%20Large%20Language%20Model%20%28LLM%29%20Post-training%20-%20%5b2025%20Update%5d&u=https%3a%2f%2fcongchan.github.io%2fposts%2fawesome-large-language-model-llm-post-training-2025-update%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://congchan.github.io/>Cong's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>