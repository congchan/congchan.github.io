<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="googlee4f5b3d387f2fae7">
  <meta name="msvalidate.01" content="B49368B5E1218EA9380A07C97E0E97B4">
  <meta name="yandex-verification" content="0da69d506cf33dfe">
  <meta name="baidu-site-verification" content="Elnplp8Jq5">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"congchan.github.io","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="参考CS229: Machine Learning, Stanford 什么是机器学习？目前有两个定义。 亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。 汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义：E：经验，即历史的数据集。T：某类任务。P：任务的绩效衡量。若该计算机程序通过利用经验E在任务T上获">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Note - cs229 - Stanford">
<meta property="og:url" content="https://congchan.github.io/machine-learning/index.html">
<meta property="og:site_name" content="Fly Me to the Moon">
<meta property="og:description" content="参考CS229: Machine Learning, Stanford 什么是机器学习？目前有两个定义。 亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。 汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义：E：经验，即历史的数据集。T：某类任务。P：任务的绩效衡量。若该计算机程序通过利用经验E在任务T上获">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/linearR_cost.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/lr.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/sigmoid.png">
<meta property="og:image" content="https://congchan.github.io/image/cs229-notes1-pic01.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/probability_density.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/entropy.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/infogain.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/SplitEntropy.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/svm.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/kernel1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/kernel2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/knn.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/cluster_dist.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Dendrogram.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Estep.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Mstep1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Mstep2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_e.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_m1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_m2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/km&em.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/pca_norm.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/roc.png">
<meta property="article:published_time" content="2017-12-04T16:00:00.000Z">
<meta property="article:modified_time" content="2017-12-04T16:00:00.000Z">
<meta property="article:author" content="Cong">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/linearR_cost.png">


<link rel="canonical" href="https://congchan.github.io/machine-learning/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>Machine Learning Note - cs229 - Stanford | Fly Me to the Moon</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Fly Me to the Moon</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Supervised-Learning"><span class="nav-number">1.</span> <span class="nav-text">Supervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Regression"><span class="nav-number">1.1.</span> <span class="nav-text">Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LMS-least-mean-squares-algorithm"><span class="nav-number">1.1.1.</span> <span class="nav-text">LMS(least mean squares) algorithm:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-normal-equations"><span class="nav-number">1.1.2.</span> <span class="nav-text">The normal equations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Probabilistic-interpretation"><span class="nav-number">1.1.3.</span> <span class="nav-text">Probabilistic interpretation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Locally-weighted-linear-regression-LWR-algorithm"><span class="nav-number">1.1.4.</span> <span class="nav-text">Locally weighted linear regression (LWR) algorithm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification-and-logistic-regression"><span class="nav-number">1.2.</span> <span class="nav-text">Classification and logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic-regression"><span class="nav-number">1.2.1.</span> <span class="nav-text">Logistic regression</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Generalized-Linear-Models"><span class="nav-number">1.3.</span> <span class="nav-text">Generalized Linear Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-exponential-family"><span class="nav-number">1.3.1.</span> <span class="nav-text">The exponential family</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Constructing-GLMs"><span class="nav-number">1.3.2.</span> <span class="nav-text">Constructing GLMs</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Softmax-Regression"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">Softmax Regression</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Naive-Bayes-classification-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">1.4.</span> <span class="nav-text">Naive Bayes classification 朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Gaussian-Naive-Bayes"><span class="nav-number">1.4.1.</span> <span class="nav-text">Gaussian Naive Bayes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Missing-data-instances-in-NB"><span class="nav-number">1.4.2.</span> <span class="nav-text">Missing data instances in NB</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Generative-and-Discriminative-Algorithm"><span class="nav-number">1.5.</span> <span class="nav-text">Generative and Discriminative Algorithm:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decision-trees-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">1.6.</span> <span class="nav-text">Decision trees 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ID3-algorithm"><span class="nav-number">1.6.1.</span> <span class="nav-text">ID3 algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Entropy"><span class="nav-number">1.6.2.</span> <span class="nav-text">Entropy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting-in-Decision-Trees"><span class="nav-number">1.6.3.</span> <span class="nav-text">Overfitting in Decision Trees</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Decision-boundary"><span class="nav-number">1.6.4.</span> <span class="nav-text">Decision boundary</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Random-Decision-forest"><span class="nav-number">1.6.5.</span> <span class="nav-text">Random Decision forest</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM"><span class="nav-number">1.7.</span> <span class="nav-text">SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Kernels"><span class="nav-number">1.7.1.</span> <span class="nav-text">Kernels</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM-vs-Logistic-regression"><span class="nav-number">1.7.2.</span> <span class="nav-text">SVM vs. Logistic regression</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Nearest-Neighbour"><span class="nav-number">1.8.</span> <span class="nav-text">K Nearest Neighbour</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-D-Trees"><span class="nav-number">1.8.1.</span> <span class="nav-text">K-D Trees</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inverted-lists"><span class="nav-number">1.8.2.</span> <span class="nav-text">Inverted lists</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Locality-sensitive-hashing"><span class="nav-number">1.8.3.</span> <span class="nav-text">Locality-sensitive hashing</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsupervised-learning-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">Unsupervised learning 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Clustering"><span class="nav-number">2.1.</span> <span class="nav-text">Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means"><span class="nav-number">2.1.1.</span> <span class="nav-text">K-means</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hierarchical-K-means"><span class="nav-number">2.1.2.</span> <span class="nav-text">Hierarchical K-means</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Agglomerative-Clustering"><span class="nav-number">2.1.3.</span> <span class="nav-text">Agglomerative Clustering</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gaussian-Mixtures"><span class="nav-number">2.1.4.</span> <span class="nav-text">Gaussian Mixtures</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EM-algorithm"><span class="nav-number">2.1.5.</span> <span class="nav-text">EM algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means-vs-EM"><span class="nav-number">2.1.6.</span> <span class="nav-text">K-means vs. EM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dimensionality-Reduction"><span class="nav-number">2.2.</span> <span class="nav-text">Dimensionality Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Factor-analysis"><span class="nav-number">2.2.1.</span> <span class="nav-text">Factor analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Principal-Components-Analysis"><span class="nav-number">2.2.2.</span> <span class="nav-text">Principal Components Analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Discriminant-Analysis"><span class="nav-number">2.2.3.</span> <span class="nav-text">Linear Discriminant Analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Singular-Value-Decomposition"><span class="nav-number">2.2.4.</span> <span class="nav-text">Singular Value Decomposition</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generalization-and-evaluation"><span class="nav-number">3.</span> <span class="nav-text">Generalization and evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Receiver-Operating-Characteristic"><span class="nav-number">3.1.</span> <span class="nav-text">Receiver Operating Characteristic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Confidence-interval"><span class="nav-number">3.2.</span> <span class="nav-text">Confidence interval</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Cong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">106</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/congchan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;congchan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/congchan" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://congchan.github.io/machine-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Cong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Fly Me to the Moon">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning Note - cs229 - Stanford
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-12-05 00:00:00" itemprop="dateCreated datePublished" datetime="2017-12-05T00:00:00+08:00">2017-12-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/machine-learning/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="machine-learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>参考<br><a target="_blank" rel="noopener" href="http://cs229.stanford.edu/notes">CS229: Machine Learning, Stanford</a></p>
<p>什么是机器学习？目前有两个定义。</p>
<p>亚瑟·塞缪尔（Arthur Samuel）将其描述为：“不需要通过具体的编程，使计算机能够学习”。这是一个较老的，非正式的定义。</p>
<p>汤姆·米切尔（Tom Mitchell）提供了一个更现代的定义：<br>E：经验，即历史的数据集。<br>T：某类任务。<br>P：任务的绩效衡量。<br>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习<br>“如果计算机程序能够利用经验E，提升实现任务T的成绩P，则可以认为这个计算机程序能够从经验E中学习任务T”。<br>例如：玩跳棋。E =玩许多棋子游戏的经验，T = 玩跳棋的任务。P = 程序将赢得下一场比赛的概率。</p>
<a id="more"></a>

<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a><a target="_blank" rel="noopener" href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Supervised Learning</a></h2><h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><ul>
<li>Weights(parameters) θ: parameterizing the space of linear functions mapping from X to Y</li>
<li>Intercept term: to simplify notation, introduce the convention of letting x<sub>0</sub> = 1</li>
<li>Cost function J(θ): <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/linearR_cost.png">  a function that measures, for each value of the θ’s, how close the h(x<sup>(i)</sup>)’s are to the corresponding y<sup>(i)</sup>’s</li>
<li>Purpose: to choose θ so as to minimize J(θ).</li>
<li>Implementation: By using a search algorithm that starts with some “initial guess” for θ, and that repeatedly changes θ to make J(θ) smaller, until hopefully we converge to a value of θ that minimizes J(θ).</li>
</ul>
<h4 id="LMS-least-mean-squares-algorithm"><a href="#LMS-least-mean-squares-algorithm" class="headerlink" title="LMS(least mean squares) algorithm:"></a>LMS(least mean squares) algorithm:</h4><ul>
<li>gradient descent</li>
<li>learning rate</li>
<li>error term</li>
<li>batch gradient descent：looks at every example in the entire training set on every step</li>
<li>stochastic gradient descent(incremental gradient descent)：repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to<br>the gradient of the error with respect to that single training example only.</li>
<li>particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent.</li>
</ul>
<h4 id="The-normal-equations"><a href="#The-normal-equations" class="headerlink" title="The normal equations"></a>The normal equations</h4><p>performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize J by explicitly taking its derivatives with respect to the θ<sub>j</sub>’s, and setting them to zero.<br>To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, let’s introduce some notation for doing calculus with matrices</p>
<ul>
<li>Matrix derivatives: the gradient ∇<sub>A</sub>f(A) is itself an m-by-n matrix, whose (i, j)-element is ∂f/∂A<sub>ij</sub></li>
<li>Least squares revisited: Given a training set,<ul>
<li>define the design matrix X to be the m-by-n matrix (actually m-by-n + 1, if we include the intercept term) that contains the training examples’ input values in its rows,</li>
<li>let y be the m-dimensional vector containing all the target values from the training set,</li>
<li>used the fact that the trace of a real number is just the real number( trace operator, written “tr.” For an n-by-n matrix A, the trace of A is defined to be the sum of its diagonal entries: trA = ΣA<sub>ii</sub></li>
<li>To minimize J, find its derivatives with respect to θ: ∇<sub>θ</sub>J(θ) = X<sup>T</sup>Xθ − X<sup>T</sup>y</li>
<li>To minimize J, we set its derivatives to zero, and obtain the normal equations: X<sup>T</sup>Xθ = X<sup>T</sup>y</li>
<li>Thus the value of θ that minimizes J(θ) is given in closed form by the equation: θ = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y</li>
</ul>
</li>
</ul>
<h4 id="Probabilistic-interpretation"><a href="#Probabilistic-interpretation" class="headerlink" title="Probabilistic interpretation"></a>Probabilistic interpretation</h4><p>why the least-squares cost function J is a reasonable choice? With a set a probabilistic assumptions, under which least-squares regression is derived as a very natural algorithm.</p>
<h4 id="Locally-weighted-linear-regression-LWR-algorithm"><a href="#Locally-weighted-linear-regression-LWR-algorithm" class="headerlink" title="Locally weighted linear regression (LWR) algorithm"></a>Locally weighted linear regression (LWR) algorithm</h4><p>assuming there is sufficient training data, makes the choice of features less critical.</p>
<ul>
<li>In the original linear regression algorithm, to make a prediction at a query point x (i.e., to evaluate h(x)), we would:<ol>
<li>Fit θ to minimize Σ<sub>i</sub>(y<sup>(i)</sup> − θ<sup>T</sup>x<sup>(i)</sup>)<sup>2</sup>.</li>
<li>Output θ<sup>T</sup>x.</li>
</ol>
</li>
<li>The locally weighted linear regression algorithm does the following:<ol>
<li>Fit θ to minimize Σ<sub>i</sub>w<sup>(i)</sup>(y<sup>(i)</sup> − θ<sup>T</sup>x<sup>(i)</sup>)<sup>2</sup>.</li>
<li>Output θ<sup>T</sup>x.</li>
</ol>
</li>
<li>Here, the w<sup>(i)</sup>’s are non-negative valued <strong>weights</strong></li>
<li>Intuitively, if w<sup>(i)</sup> is large for a particular value of i, then in picking θ, we’ll try hard to make (y<sup>(i)</sup> − θ<sup>T</sup>x<sup>(i)</sup>)<sup>2</sup> small. If w<sup>(i)</sup> is small, then the error term will be pretty much ignored in the fit.</li>
<li>A fairly standard choice for the weights is w<sup>(i)</sup> = exp(-(x<sup>(i)</sup>-x)<sup>2</sup> / 2τ<sup>2</sup> )</li>
<li>if |x<sup>(i)</sup>-x| is small, then w<sup>(i)</sup> ≈ 1; if large, then w<sup>(i)</sup> is small. Hence, θ is chosen giving a much higher “weight” to the (errors on) training examples close to the query point x.</li>
<li>The parameter τ controls how quickly the weight of a training example falls off with distance of its x<sup>(i)</sup>, from the query point x; τ is called the <strong>bandwidth</strong> parameter</li>
</ul>
<h3 id="Classification-and-logistic-regression"><a href="#Classification-and-logistic-regression" class="headerlink" title="Classification and logistic regression"></a>Classification and logistic regression</h3><h4 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h4><ul>
<li>logistic function or the <strong>sigmoid function</strong>: g(z) = (1 + e<sup>−z</sup>)<sup>-1</sup>. <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/lr.png"><br>g(z) tends towards 1 as z → ∞, and g(z) tends towards 0 as z → −∞. <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/sigmoid.png"></li>
<li>derivative of the sigmoid function:  g(z)<sup>‘</sup> = g(z)(1 - g(z))</li>
<li>endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likelihood:<ul>
<li>Similar to our derivation in the case of linear regression, we can use gradient ascent to maximize the likelihood.</li>
<li>updates will therefore be given by θ := θ + α∇<sub>θ</sub>ℓ(θ). (Note the positive rather than negative sign in the update formula, since we’re maximizing,rather than minimizing, a function now.)</li>
<li>This therefore gives us the stochastic gradient ascent rule: θ<sub>j</sub> := θ<sub>j</sub> + α(y<sup>(i)</sup>− h<sub>θ</sub>(x<sup>(i)</sup>))x<sup>(i)</sup><sub>j</sub></li>
<li>If we compare this to the LMS update rule, we see that it looks identical; but this is not the same algorithm, because h<sub>θ</sub>(x<sup>(i)</sup>) is now defined as a non-linear function of θ<sup>T</sup>x<sup>(i)</sup>.<br>     * Nonetheless, it’s a little surprising that we end up with the same update rule for a rather different algorithm and learning problem. Is this coincidence, or is there a deeper reason behind this? Check <a href="#generalized-linear-models">GLM models</a>.  </li>
</ul>
</li>
</ul>
<h3 id="Generalized-Linear-Models"><a href="#Generalized-Linear-Models" class="headerlink" title="Generalized Linear Models"></a>Generalized Linear Models</h3><h4 id="The-exponential-family"><a href="#The-exponential-family" class="headerlink" title="The exponential family"></a>The exponential family</h4><ul>
<li>Bernoulli distributions</li>
<li>Gaussianexponential distributions</li>
<li>multinomial</li>
<li>Poisson (for modelling count-data)</li>
<li>beta and the Dirichlet (for distributions over probabilities)</li>
</ul>
<h4 id="Constructing-GLMs"><a href="#Constructing-GLMs" class="headerlink" title="Constructing GLMs"></a>Constructing GLMs</h4><ul>
<li>Ordinary Least Squares</li>
<li>Logistic Regression</li>
<li>Softmax Regression</li>
</ul>
<h5 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h5><p>Consider a classification problem in which the response variable y can take on any one of k values, so y ∈ {1, 2, . . . , k}. We will thus model it as distributed according to a multinomial distribution.</p>
<ul>
<li>parameterize the multinomial with only k − 1 parameters, φ<sub>1</sub>, . . . , φ<sub>k−1</sub>, where φ<sub>i</sub> = p(y = i; φ), and p(y = k; φ) = 1 − Σ<sup>k</sup><sub>i=1</sub>φ<sub>i</sub>.</li>
<li>To express the multinomial as an exponential family distribution, we will definee T(y) ∈ R<sup>k-1</sup>：<br><img src="/image/cs229-notes1-pic01.png" alt="T(y)"><br>  * η = [log(φ<sub>1</sub>/φ<sub>k</sub>),…,log(φ<sub>k-1</sub>/φ<sub>k</sub>)], the η<sub>i</sub>’s are linearly related to the x’s.<br>  * softmax function: a mapping from the η’s to the φ’s: φ<sub>i</sub> = e<sup>ηi</sup> / Σ<sup>k</sup><sub>j=1</sub>e<sup>ηi</sup></li>
<li>softmax regression:  the model, which applies to classification problems where y ∈ {1, . . . , k}: p(y = i|x; θ) = φ<sub>i</sub> = e<sup>θ<sup>T</sup><sub>i</sub> x</sup> / Σ<sup>k</sup><sub>j=1</sub>e<sup>θ<sup>T</sup><sub>i</sub> x</sup>  </li>
<li>This hypothesis will output the estimated probability that p(y = i|x; θ), for every value of i = 1, . . . , k.</li>
<li>parameter fitting: obtain the maximum likelihood estimate of the parameters by maximizing ℓ(θ) in terms of θ, using a method such as gradient ascent or Newton’s method.</li>
</ul>
<h3 id="Naive-Bayes-classification-朴素贝叶斯"><a href="#Naive-Bayes-classification-朴素贝叶斯" class="headerlink" title="Naive Bayes classification 朴素贝叶斯"></a>Naive Bayes classification 朴素贝叶斯</h3><p>以二元分类为例: 根据A和B各自的先验概率和条件概率, 算出针对某一特征事件的后验概率, 然后正则化(正则化后两个后验概率之和为1, 但不影响对事件的触发对象是A或B的判断)</p>
<ul>
<li>Why naïve: 忽略了事件发生的顺序, 故称之为”朴素”</li>
<li>Strength and Weakness: 高效, 快速, 但对于组合性的短语词组, 当这些短语与其组成成分的字的意思不同时, NB的效果就不好了</li>
<li>详见<a target="_blank" rel="noopener" href="https://github.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/blob/master/%E5%8A%A0%E9%80%9F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.md#naive-bayes-classifier">加速自然语言处理-朴素贝叶斯</a></li>
<li>Problem: how to deal with continuous values features? Use Gaussian Naive Bayes.</li>
</ul>
<h4 id="Gaussian-Naive-Bayes"><a href="#Gaussian-Naive-Bayes" class="headerlink" title="Gaussian Naive Bayes"></a>Gaussian Naive Bayes</h4><p>With real-valued inputs, we can calculate the mean and standard deviation of input values (x) for each class to summarize the distribution. This means that in addition to the probabilities for each class, we also store the mean μ and standard deviations σ of each feature for each class.</p>
<ul>
<li>The class conditional probability P(x|c) is estimated by probability density of the normal distribution <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/probability_density.png">:</li>
<li>Algorithm – continuous Xi (but still discrete Y)<ul>
<li>Train Naïve Bayes (examples)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for each class value yk:</span><br><span class="line">    estimate P(Yk)</span><br><span class="line">    for each attribute Xi:</span><br><span class="line">        estimate class conditional mean, variance</span><br></pre></td></tr></table></figure></li>
<li>Classify(xnew): <code> Ynew &lt;- argmax(k) ∏P(xi|Yk)P(Yk)</code></li>
</ul>
</li>
<li>Short: classes with the same distribution</li>
</ul>
<h4 id="Missing-data-instances-in-NB"><a href="#Missing-data-instances-in-NB" class="headerlink" title="Missing data instances in NB"></a>Missing data instances in NB</h4><ul>
<li>Ignore attribute in instance where its value is missing</li>
<li>compute likelihood based on observed attribtues</li>
<li>no need to “fill in” or explicitly model missing values</li>
<li>based on conditional independence between attributes</li>
</ul>
<h3 id="Generative-and-Discriminative-Algorithm"><a href="#Generative-and-Discriminative-Algorithm" class="headerlink" title="Generative and Discriminative Algorithm:"></a>Generative and Discriminative Algorithm:</h3><ul>
<li>Generative classifiers learn a model of the joint probability, p(x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(y|x), and then picking the most likely label y.</li>
<li>Discriminative classifiers model the posterior p(y|x) directly, or learn a direct map(hypothesis/functions) from inputs x to the class labels.</li>
<li>Generative models advantage:<ul>
<li>Can be good with missing data, naive Bayes handles missing data</li>
<li>good for detecting outliers</li>
<li>to generate likely input (x,y).</li>
</ul>
</li>
</ul>
<h3 id="Decision-trees-决策树"><a href="#Decision-trees-决策树" class="headerlink" title="Decision trees 决策树"></a>Decision trees 决策树</h3><ul>
<li>Algorithm: ID3 algorithm</li>
<li>Decision trees with continuous attributes: Create split based on threshold</li>
</ul>
<h4 id="ID3-algorithm"><a href="#ID3-algorithm" class="headerlink" title="ID3 algorithm"></a>ID3 algorithm</h4><p>Recursive Split( node, {examples} ):<br>    1. A &lt;- the best attribute for splitting the {examples}<br>    2. For each value of A, create new child node<br>    3. Split training {examples} to child nodes<br>    4. For each child node, subset:<br>     * If subset is pure - stop<br>     * Else: split(child_node, {subset} )<br>How to decide which attribute is the best to split on: Entropy</p>
<h4 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h4><ul>
<li><img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/entropy.png"></li>
<li>Use log2 here is to represent concepts of information - on average how many bits needed to tell X split purity<ul>
<li>To represent two classes, need one bit “0, 1”, to represent 4 classes, need 2 bits “00, 01, 10, 11”</li>
<li>If x is pure(one class only), entropy is 0.</li>
</ul>
</li>
<li>Information Gain: Expected drop in entropy after split, Gain( P, C) = Entropy(parent) - Σw*Entropy(children), w is weighted average matrix.<img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/infogain.png">, A is the split attribute<ul>
<li>Problems: tend to pick attributes with lots of values, could not generalize well on new data.</li>
<li>use GainRation: for attribute A with many different values V, the SplitEntropy will be large, <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/SplitEntropy.png"></li>
</ul>
</li>
</ul>
<h4 id="Overfitting-in-Decision-Trees"><a href="#Overfitting-in-Decision-Trees" class="headerlink" title="Overfitting in Decision Trees"></a>Overfitting in Decision Trees</h4><p>the tree split too deep to try to classify almost every single sample. As a result the model could not predict new data well.</p>
<ul>
<li>Sub-tree replacement pruning<ol>
<li>For each node:<ul>
<li>Pretend remove node + all children from the tree</li>
<li>Measure performance on validation set</li>
</ul>
</li>
<li>Remove node that results in greatest improvement</li>
<li>Repeat until further pruning is harmful</li>
</ol>
</li>
</ul>
<h4 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h4><p>Logistic Regression and trees differ in the way that they generate decision boundaries</p>
<ul>
<li>Decision Trees bisect the space into smaller and smaller regions,</li>
<li>Logistic Regression fits a single line/hyperplane to divide the space exactly into two.</li>
</ul>
<h4 id="Random-Decision-forest"><a href="#Random-Decision-forest" class="headerlink" title="Random Decision forest"></a>Random Decision forest</h4><ul>
<li>Grow K different decision trees:<ul>
<li>pick a random subset Sr of training examples</li>
<li>grow a full ID3 tree (no prunning):<ul>
<li>When splitting: pick from d&lt;&lt;D random attributes</li>
<li>Computing gain based on Sr instead of full set</li>
</ul>
</li>
<li>repeat for r =1…K</li>
</ul>
</li>
<li>Given a new data point X:<ul>
<li>classify X using each of the trees T1 …. Tk</li>
<li>use majority vote: class predicted most often</li>
</ul>
</li>
</ul>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><ul>
<li>Intuition: Suppose there is a good hyperplane to seperate data set, h(x)=g(w<sup>T</sup>x+b), (relation with fully connected layer and activation funciton in DNN).<ul>
<li>Want functional margin of hyperplane to be large: for dataset (xi,yi), functional margin γi = yi(w<sup>T</sup>xi+b), if yi=1, need w<sup>T</sup>xi+b&gt;&gt;0, if yi=-1, need w<sup>T</sup>xi+b&lt;&lt;0. Thus γi&gt;0 means the classification is correct.</li>
<li>Geometric margins: Define the hyperplane as w<sup>T</sup>x+b=0, the normal of the hyperplane is w/||w||, thus a point A(xi)’s, which represents the input x(i) of some training example with label y(i) = 1, projection on the hyperplane is point B = xi - γi·w/||w||, where γi is xi’s distance to the decision boundary. Thus w<sup>T</sup>(xi - γi·w/||w||) + b=0 =&gt; γi =  (w/||w||)<sup>T</sup>xi+ b/||w||. More generally, the geometric margin of (w, b) with respect to a training example (xi, yi) is γi = yi· (w/||w||)<sup>T</sup>xi+ b/||w||</li>
<li>If ||w|| = 1, then the functional margin equals the geometric margin</li>
</ul>
</li>
<li>The optimal margin classifier: Given a training set, a natural desideratum is to try to find a decision boundary that maximizes the minimum (geometric) margin, i.e want min(γi) as large as possible. Via some <a target="_blank" rel="noopener" href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">transformation</a>, the object turns to minimize ||w||<sup>2</sup>, subject to y(i)·(w<sup>T</sup>xi+b) ≥ 1,</li>
<li><a target="_blank" rel="noopener" href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">Lagrange duality</a>: solving constrained optimization problems. w = Σαiyixi, αi is Lagrange multipliers.</li>
<li>Support vector: The points with the smallest margins. The number of support vectors can be much smaller than the size the training set</li>
<li>Training: fit our model’s parameters to a training set, and now wish to make a prediction at a new point input x. We would then calculate w<sup>T</sup>x + b, and predict y = 1 if and only if this quantity is bigger than zero. <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/svm.png">.<ul>
<li>In order to make a prediction, we have to calculate it which depends only on the inner product between x and the points in the training set.</li>
<li>Moreover, αi’s will all be zero except for the support vectors. Thus, many of the terms in the sum above will be zero, and we need to find only the inner products between x and the support vectors (of which there is often only a small number) in order to make our prediction.</li>
<li>The inner product &lt;xi,x&gt; could be replaced by kernel k(xi,x)</li>
</ul>
</li>
</ul>
<h4 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h4><ul>
<li>Define the “original” input value x as the input attributes of a problem. When that is mapped to some new set of quantities that are then passed to the learning algorithm, we’ll call those new quantities the input features.</li>
<li>φ denote the feature mapping, which maps from the attributes to the features. E.g. φ(x) = [x, x^2, x^3]</li>
<li>given a feature mapping φ, we define the corresponding Kernel to be K(x, z) = φ(x)<sup>T</sup>φ(z)</li>
<li>Often, φ(x) itself may be very expensive to calculate (perhaps because it is an extremely high dimensional vector, require memory), K(x, z) may be very inexpensive to calculate.</li>
<li>We can get SVMs to learn in the high dimensional feature space given by φ, but without ever having to explicitly find or represent vectors φ(x). E.g.<img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/kernel1.png"> <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/kernel2.png"></li>
<li>Based on <a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf">Mercer’s Theorem</a>, you can either explicitly map the data with a φ and take the dot product, or you can take any kernel and use it right away, without knowing nor caring what φ looks like</li>
<li>Keep in mind however that the idea of kernels has significantly broader applicability than SVMs. Specifically, if you have any learning algorithm that you can write in terms of only inner products &lt;x, z&gt; between input attribute vectors, then by replacing this with K(x, z) where K is a kernel, you can allow your algorithm to work efficiently in the high dimensional feature space corresponding to K.</li>
</ul>
<h4 id="SVM-vs-Logistic-regression"><a href="#SVM-vs-Logistic-regression" class="headerlink" title="SVM vs. Logistic regression"></a>SVM vs. Logistic regression</h4><ul>
<li>Logistic regression focuses on maximizing the probability of the data. The further the data lies from the separating hyperplane (on the correct side), the happier LR is.</li>
<li>An SVM don’t care about getting the right probability, i.e the right P(y=1|x), but only care about P(y=1|x)/P(y=0|x)≥ c. It tries to find the separating hyperplane that maximizes the distance of the closest points to the margin (the support vectors). If a point is not a support vector, it doesn’t really matter.</li>
<li>P(y=1|x)/P(y=0|x) &gt; c, if c=1, that means P(y=1|x) &gt; P(y=0|x), thus y=1, take log of both side, and plug in P(y=1|x) = sigmoid(w<sup>T</sup>x + b), P(y=0|x)=1-P(y=1|x), recall the <a href="#logistic-regression">sigmoid</a>, we get w<sup>T</sup>x + b &gt; 0</li>
<li>Underlying basic idea of linear prediction is the same, but error functions differ, the r = P(y=1|x)/P(y=0|x) = exp(w<sup>T</sup>x + b), different classifiers assigns different cost to r<ul>
<li>If cost(r)=log(1 + 1/r), this is logistic regression</li>
<li>If cost(r)=max(0, 1-log(r))=max(0, 1-(w<sup>T</sup>x + b)), then SVM</li>
<li>Logistic regression (non-sparse) vs SVM (hinge loss, sparse solution)</li>
<li>Linear regression (squared error) vs SVM (ϵ insensitive error)</li>
</ul>
</li>
</ul>
<h3 id="K-Nearest-Neighbour"><a href="#K-Nearest-Neighbour" class="headerlink" title="K Nearest Neighbour"></a>K Nearest Neighbour</h3><p>Intuition: predict based on nearby/similar training data.</p>
<ul>
<li>Algorithm: for a test data<ol>
<li>compute its distance to every training example xi</li>
<li>select k closest training instances</li>
<li>prediction:<ul>
<li>For Classification: predict as the most frequent label among the k instances.</li>
<li>For regression: predict as the mean of label among the k instances.</li>
</ul>
</li>
</ol>
</li>
<li>Choose k<ul>
<li>large k: everything classified as the most probable class</li>
<li>small k: highly variable, unstable decision boundaries</li>
<li>affects “smoothness” of the boundary</li>
<li>Use train-validation to choose k</li>
</ul>
</li>
<li>Distance meansures:<ul>
<li>Euclidian: symmetric, spherical, treats all dimensions equally, but sensitive to extreme differences in single attribtue</li>
<li>Hamming: number of attribtues that differ</li>
</ul>
</li>
<li>Resolve ties:<ul>
<li>random</li>
<li>prior: pick class with greater prior</li>
<li>nearest: use 1-NN classifier to decide</li>
</ul>
</li>
<li>Missing values: have to fill in the missing values, otherwise cannot compute distance.</li>
<li>Pro and cons:<ul>
<li>Almost no assumptions about data</li>
<li>easy to update in online setting: just add new item to training set</li>
<li>Need to handle missing data: fill-in or create a special distance</li>
<li>Sensitive to outliers</li>
<li>Sensitve to lots of irrelevant aeributes (affect distance)</li>
<li>Computationally expensive: need to compute distance to all examples O(nd) - <a target="_blank" rel="noopener" href="http://cs229.stanford.edu/section/vec_demo/Vectorization_Section.pdf">Vectorization</a> <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/knn.png"></li>
</ul>
</li>
<li>Faster knn: K-D Trees, Inverted lists, Locality-sensitive hashing</li>
</ul>
<h4 id="K-D-Trees"><a href="#K-D-Trees" class="headerlink" title="K-D Trees"></a>K-D Trees</h4><p>low-dimensional, real-valued data</p>
<ul>
<li>A kd-tree is a binary tree data structure for storing a finite set of points from a k-dimensional space.</li>
<li>Build the tree: Pick random dimension, Find median, Split data</li>
<li>Nearest neighbor search: Traverse the whole tree, BUT make two modifications to prune to search space:<ul>
<li>Keep variable of closest point C found so far. Prune subtrees once their bounding boxes say that they can’t contain any point closer than C</li>
<li>Search the subtrees in order that maximizes the chance for pruning</li>
</ul>
</li>
</ul>
<h4 id="Inverted-lists"><a href="#Inverted-lists" class="headerlink" title="Inverted lists"></a>Inverted lists</h4><p>high-dimensional, discrete data, sparse</p>
<ul>
<li>Application: text classification, most attribute values are zero (sparseness),</li>
<li>training: list all training examples that contain particular attribute</li>
<li>Testing: merge inverted list for attribtues presented in the test set, and choose those instances in the new inverted list as the neighbours</li>
</ul>
<h4 id="Locality-sensitive-hashing"><a href="#Locality-sensitive-hashing" class="headerlink" title="Locality-sensitive hashing"></a>Locality-sensitive hashing</h4><p>high-d, discrete or real-valued</p>
<h2 id="Unsupervised-learning-无监督学习"><a href="#Unsupervised-learning-无监督学习" class="headerlink" title="Unsupervised learning 无监督学习"></a>Unsupervised learning 无监督学习</h2><h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><h4 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h4><p>split data into a specified number of populations</p>
<ul>
<li>Input: <ul>
<li>K (number of clusters in the data)</li>
<li>Training set {x1, x2, x3 …, xn) </li>
</ul>
</li>
<li>Algorithm:<ul>
<li>Randomly initialize K cluster centroids as {μ1, μ2, μ3 … μK}, now centroid could represent cluster.</li>
<li>Repeat until converge:<ul>
<li>Inner loop 1: repeatedly sets the c(i) variable to be the index of the closes variable of cluster centroid closes to xi, i.e. take ith example, measure squared distance to each cluster centroid, assign c(i)to the  closest cluster(centroid)</li>
<li>Inner loop 2: For each cluster j, new centroid c(j) = average mean of all the points assigned to the cluster j in previous step.</li>
</ul>
</li>
</ul>
</li>
<li>Target (Distortion) function: J(c,μ)=Σ|| xi-μi ||^2, coordinate ascent, decrease monotonically, thus guarantee to converge.</li>
<li>What if there’s a centroid with no data:<ul>
<li>Remove that centroid, so end up with K-1 classes,</li>
<li>Or, randomly reinitialize it, not sure when though…</li>
</ul>
</li>
<li>How to choose cluster numbers: scree plot to find the best k.</li>
</ul>
<h4 id="Hierarchical-K-means"><a href="#Hierarchical-K-means" class="headerlink" title="Hierarchical K-means"></a>Hierarchical K-means</h4><ul>
<li>A Top-down approach<ol>
<li>run k-means algorithm on the original dataset</li>
<li>for each of the resulting clusters, recursively run k-means</li>
</ol>
</li>
<li>Pro cons:<ul>
<li>Fast</li>
<li>nearby points may end up in different clusters</li>
</ul>
</li>
</ul>
<h4 id="Agglomerative-Clustering"><a href="#Agglomerative-Clustering" class="headerlink" title="Agglomerative Clustering"></a>Agglomerative Clustering</h4><p>A bottom up algorithm:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. starts with a collections of singleton clusters</span><br><span class="line">2. repeat until only one cluster is left:</span><br><span class="line">    1. Find a pair of clusters that is closest</span><br><span class="line">    2. Merge the pair of clusters into one new cluster</span><br><span class="line">    3. Remove the old pair of clusters</span><br></pre></td></tr></table></figure>
<ul>
<li>Need to define a distance metric over clusters<br><img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/cluster_dist.png"></li>
<li>Produce a dendrogram: Hierarchical tree of clusters<br><img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Dendrogram.png"></li>
<li>slow</li>
</ul>
<h4 id="Gaussian-Mixtures"><a href="#Gaussian-Mixtures" class="headerlink" title="Gaussian Mixtures"></a>Gaussian Mixtures</h4><p>For non-Gaussian distribution data, assume it is a mixture of several(k) Gaussians.</p>
<ul>
<li>Algorithm: EM</li>
</ul>
<h4 id="EM-algorithm"><a href="#EM-algorithm" class="headerlink" title="EM algorithm"></a>EM algorithm</h4><p>strategy will be to repeatedly construct a lower-bound on ℓ(E-step) based on <a target="_blank" rel="noopener" href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">Jensen’s inequality</a>, and then optimize that lower-bound(M-step).</p>
<ul>
<li>E step: For each i, let Qi be some distribution over the z’s (Σ<sub>z</sub>Qi(z) = 1, Qi(z) ≥ 0). z(i) indicating which of the k Gaussians each x(i) had come from, get P(Z)=φ, then compute the conditional probability wj as P(x|Z) via <a href="#gaussian-naive-bayes">Gaussian Naive Bayes</a>: <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Estep.png"></li>
<li>M step: maximize, with respect to our parameters φ, µ, Σ, the quantity<img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Mstep1.png">,<br>  by updating parameter(φ, µ, σ) <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/Mstep2.png"></li>
<li>举例：start with two randomly placed Gaussians (μa, σa), (μb, σb), assume a uniform prior (P(a)=P(b)=0.5), iterate until convergence:<ul>
<li>E-step: for each point: P(b|xi), P(a|xi)=1-P(b|xi) <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_e.png">, does it look like it came from b or a?</li>
<li>M-step: adjust (μa, σa) and (μb, σb) to fit points <strong>soft</strong> assigned to them, <img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_m1.png"><br><img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/EM_m2.png"></li>
</ul>
</li>
<li>The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the “hard” cluster assignments c, we instead have the “soft” assignments w. Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea.</li>
<li>How to pick k: cannot discover K, likelihood keeps growing with K</li>
</ul>
<h4 id="K-means-vs-EM"><a href="#K-means-vs-EM" class="headerlink" title="K-means vs. EM"></a>K-means vs. EM</h4><p><img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/km&em.jpg"></p>
<h3 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h3><ul>
<li>Pros:<ul>
<li>reflects human intuitions about the data</li>
<li>allows estimating probabilities in highadimensional data: no need to assume independence etc.</li>
<li>dramatic reduction in size of data: faster processing (as long as reduction is fast), smaller storage</li>
</ul>
</li>
<li>Cons<ul>
<li>too expensive for many applications (Twitter, web)</li>
<li>disastrous for tasks with fine-grained classes</li>
<li>understand assumptions behind the methods (linearity etc.): there may be better ways to deal with sparseness</li>
</ul>
</li>
</ul>
<h4 id="Factor-analysis"><a href="#Factor-analysis" class="headerlink" title="Factor analysis"></a>Factor analysis</h4><p>If the features n ≫ m, or n≈m, in such a problem, it might be difficult to model the data even with a single Gaussian, 更别提高斯混合了. Because the variance matrix Σ becomes singular - <a target="_blank" rel="noopener" href="http://cs229.stanford.edu/notes/cs229-notes9.pdf">non invertable</a>.</p>
<h4 id="Principal-Components-Analysis"><a href="#Principal-Components-Analysis" class="headerlink" title="Principal Components Analysis"></a>Principal Components Analysis</h4><p>PCA, automatically detect and reduce data to lower dimension k, k &lt;&lt; n, preserve dimenson that affects class separability most.</p>
<ul>
<li>Algorithm:<ul>
<li>Pre-process: data normalization to 0 mean and unit variance<img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/pca_norm.png">,<br>Steps (3-4) may be omitted if we had apriori knowledge that the different attributes are all on the same scale</li>
<li>to project data into a k-dimensional subspace (k &lt; n), we should choose e1,… ek to be the top k eigenvectors of Σ. The e’s now form a new, orthogonal basis for the data.</li>
<li>To represent a training data point x with d dimension into this basis (k dimension), e1<sup>T</sup>x,…ek<sup>T</sup>x</li>
</ul>
</li>
<li>The vectors u1,…, uk are called the first k principal components of the data.</li>
<li>Eigenvalue λi = variance along ei.<ul>
<li>Pick ei that explain the most variance by sorting eigenvectors s.t. λ1 ≥ λ2 ≥…≥ λn</li>
<li>pick first k eigenvectors which explain 90% or 95% of the total variance Σλ(i).</li>
</ul>
</li>
<li>Maximize the variance of projection of x onto a unit vector u,</li>
<li>Application: eigenfaces</li>
</ul>
<h4 id="Linear-Discriminant-Analysis"><a href="#Linear-Discriminant-Analysis" class="headerlink" title="Linear Discriminant Analysis"></a>Linear Discriminant Analysis</h4><p>LDA</p>
<ul>
<li>Idea: pick a new dimension that gives<ul>
<li>maximum separation between means of projected classes</li>
<li>minimum variance within each projected class</li>
</ul>
</li>
<li>How: eigenvectors based on between-class and within-class covariance matrices</li>
<li>LDA not guaranteed to be better for Classification<ul>
<li>assumes classes are unimodal Gaussians</li>
<li>fails when discriminatory information is not in the mean, but in the variance of the data</li>
</ul>
</li>
</ul>
<h4 id="Singular-Value-Decomposition"><a href="#Singular-Value-Decomposition" class="headerlink" title="Singular Value Decomposition"></a>Singular Value Decomposition</h4><h2 id="Generalization-and-evaluation"><a href="#Generalization-and-evaluation" class="headerlink" title="Generalization and evaluation"></a>Generalization and evaluation</h2><h3 id="Receiver-Operating-Characteristic"><a href="#Receiver-Operating-Characteristic" class="headerlink" title="Receiver Operating Characteristic"></a>Receiver Operating Characteristic</h3><p>ROC, plot TPR(Sensitivity) vs. FPR(Specificity) as t varies from ∞ to -∞, shows performance of system across all possible thresholds<br><img src="https://raw.githubusercontent.com/ShootingSpace/Computer-Science-and-Artificial-Intelligence/master/image/roc.png"></p>
<ul>
<li>A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner. Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test</li>
<li>AUC: area under ROC curve, popular alternative to Accuracy</li>
</ul>
<h3 id="Confidence-interval"><a href="#Confidence-interval" class="headerlink" title="Confidence interval"></a>Confidence interval</h3><p>tell us how closed our estimation</p>
<ul>
<li>E = probability that misclassify a random instance: Take a random set of n instances, how many misclassified? Equal to Binomial distribution with mean = nE, variance = nE(1-E)</li>
<li>Efuture: the next instance’s probability of misclassified = average #misclassifed = variance / n = mean E=  E(1-E)/n, small variance means big confidence interval, a Gaussian distribution with one variance distance extend from mean will cover 2/3 future test sets</li>
<li>p% Confidence interval for future error, 95% confidence interval needs about 2 variance extends from mean.</li>
</ul>
<p>.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/machine-learning-with-sklearn/" rel="prev" title="Machine Learning with Scikit-learn (Sklearn) 机器学习实践">
                  <i class="fa fa-chevron-left"></i> Machine Learning with Scikit-learn (Sklearn) 机器学习实践
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/NLP-topic-modeling/" rel="next" title="Topic Modelling - 主题建模以及隐变量模型">
                  Topic Modelling - 主题建模以及隐变量模型 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cong Chan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b35f789bd238372" async="async"></script>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="/js/local-search.js"></script>






  




  


<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://shootingspace.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://congchan.github.io/machine-learning/";
    this.page.identifier = "machine-learning/";
    this.page.title = "Machine Learning Note - cs229 - Stanford";
    };
  NexT.utils.loadComments('#disqus_thread', () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://shootingspace.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
